<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记41-多模态-Sora浅析 - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。 本篇基于OpenAI发布的技"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记41-多模态-Sora浅析"><meta property="og:description" content="Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。 本篇基于OpenAI发布的技"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/"><meta property="article:published_time" content="2024-02-26T10:00:59+08:00"><meta property="article:modified_time" content="2024-02-26T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记41-多模态-Sora浅析"><meta itemprop=description content="Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。 本篇基于OpenAI发布的技"><meta itemprop=datePublished content="2024-02-26T10:00:59+08:00"><meta itemprop=dateModified content="2024-02-26T10:00:59+08:00"><meta itemprop=wordCount content="2848"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记41-多模态-Sora浅析"><meta name=twitter:description content="Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。 本篇基于OpenAI发布的技"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记41-多模态-Sora浅析</h1><div class=post-meta><time datetime=2024-02-26 class=post-time>2024-02-26</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2848 字</span>
<span class=more-meta>预计阅读 6 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1sora是什么>1.Sora是什么？</a></li><li><a href=#2sora原理浅析和技术优势>2.Sora原理浅析和技术优势</a><ul><li><a href=#21sora是否采用了新的模型架构技术>2.1.Sora是否采用了新的模型架构&技术？</a></li><li><a href=#22sora对现有模型架构技术的创新>2.2.Sora对现有模型架构&技术的创新</a><ul><li><a href=#1用transformer架构学习视频特征>(1)用Transformer架构，学习视频特征</a></li><li><a href=#2用结合了transformer的diffusion模型学习还原视频的特征>(2)用结合了Transformer的Diffusion模型，学习还原视频的特征</a></li></ul></li><li><a href=#23sora的技术优势>2.3.Sora的技术优势</a><ul><li><a href=#1sora的video-compression-network依托于强大的算力>(1)Sora的Video compression network依托于强大的算力</a></li><li><a href=#2gpt4加速孵化多模态大模型多模态大模型反哺llm>(2)GPT4加速孵化多模态大模型，多模态大模型反哺LLM</a></li></ul></li></ul></li><li><a href=#3sora有哪些创意玩法>3.Sora有哪些创意玩法</a></li><li><a href=#4sora对视频相关领域的影响>4.Sora对视频相关领域的影响</a></li><li><a href=#5参考>5.参考</a></li></ul></nav></div></div><div class=post-content><p>Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。</p><p>本篇基于OpenAI发布的技术报告对Sora的技术特点和原理进行解读。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226144755598.png alt=image-20240226144755598></p><h1 id=1sora是什么>1.Sora是什么？</h1><p>Sora是一个文生视频的AI模型，可以根据文本信息生成真实且富有想象力的视频内容。主要特点：</p><ul><li><strong>以自然语言为输入(提示词)，生成符合提示词描述的视频</strong>。</li><li><strong>可生成1分钟内容连贯的视频，视频尺寸/分辨率可调整，目前只有Sora做到</strong>。其它模型只能生成4秒以内、256x256固定尺寸的视频。</li><li><strong>真实世界的模拟器</strong>，不仅理解物理实体(如人、猫、狗、&mldr;)，还懂得物理规律(如光照、碰撞、粒子、&mldr;)。</li></ul><p>Sora出道即颠峰，让一众顶级文生视频模型望尘莫及。我们来直观感受下Sora生成的视频的震撼效果：</p><p>提示词如下：</p><blockquote><p>Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field.</p></blockquote><p>视频如下：</p><iframe src="//player.bilibili.com/player.html?aid=1301071173&bvid=BV1zu4m1c7gg&cid=1452188766&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><h1 id=2sora原理浅析和技术优势>2.Sora原理浅析和技术优势</h1><p>OpenAI没有公开Sora的模型细节，本文后续分析依据OpenAI的"Technic Report"推测所得。</p><h2 id=21sora是否采用了新的模型架构技术>2.1.Sora是否采用了新的模型架构&技术？</h2><p>答案是没有。</p><p>文生视频可能涉及的模型架构如下：</p><ul><li>RRN (循环神经网络)</li><li>GAN (生成式对抗网络)</li><li>自回归Transformer</li><li>Diffusion (扩散模型)</li></ul><p>Sora采用的是：</p><ul><li>自回归Transformer</li><li>Diffusion (扩散模型)</li></ul><h2 id=22sora对现有模型架构技术的创新>2.2.Sora对现有模型架构&技术的创新</h2><p>Sora架构是结合了Diffusion扩散模型和Transformer架构的创新设计。</p><h3 id=1用transformer架构学习视频特征>(1)用Transformer架构，学习视频特征</h3><p>灵感来自于Tranformer架构在GPT中的成功应用，OpenAI把自然语言的特征表示方法引入到了视频处理中：</p><ul><li>通过编码器，把视频的每一帧转换为有时序的向量，若干帧形成了<strong>向量矩阵</strong>，Sora称之为<strong>Turning visual data into patches</strong>。(大语言模型是把文字转换成一个个token，Sora则是把视频转换成一个个patch。)</li><li>与大语言模型中token线性序列不同，由patch组成的高维向量矩阵包含了时序、分辨率、高宽比等丰富信息。</li><li>和大语言模型一样(文本信息压缩网络)，Sora模型本质是一个视频压缩网络<strong>Video compression network</strong>，使用该视频压缩网络把高维向量矩阵压缩成单维向量序列。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240223161152675.png alt=image-20240223161152675></p><h3 id=2用结合了transformer的diffusion模型学习还原视频的特征>(2)用结合了Transformer的Diffusion模型，学习还原视频的特征</h3><h4 id=--什么是扩散模型diffusion-model>- 什么是扩散模型Diffusion Model</h4><p>Diffusion Model的基本原理是将原始图片逐渐加入噪声(Noise)，让原本清晰的图片逐渐变成全是噪声的状态。</p><p>Diffusion Model主要有两个过程：前向处理和反向处理。图解如下：</p><ul><li><p><strong>前向处理</strong></p><ul><li><p>AI训练过程中的1次噪声扩散</p></li><li><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226131740185.png alt=image-20240226131740185></p></li><li><p>AI训练过程中，进行N次噪声扩散</p><ul><li>进行N次噪声扩散，将图片变成全是噪点的图像</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132019285.png alt=image-20240226132019285></li></ul></li></ul></li><li><p><strong>后向处理</strong></p><ul><li>AI训练过程中，进行N次噪声降噪，变成清晰图片</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132143014.png alt=image-20240226132143014></li></ul></li></ul><p>经过如上扩散处理，我们得到了<strong>噪声与图像的关系。</strong></p><p>利用transformer模型，既可以理解噪声的特性向量，也可以理解自然语言的特征向量，那么就可以建立<strong>噪声与提示词的关系</strong>。基于这种机制，模型就能够根据提示词来生成图片了。图片是视频的一帧，文生视频也可以用同样的原理来生成。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240227084715255.png alt=image-20240227084715255></p><h4 id=--sora的diffusion-transformer模型>- Sora的Diffusion Transformer模型</h4><p>在Diffusion Model的基础上，引入Transformer技术方法：</p><ul><li>通过编码器，将每一帧噪声转换为向量，若干帧形成了<strong>噪声向量矩阵</strong>，Sora称之为<strong>Noisy patches</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104456184.png alt=image-20240226104456184></p><ul><li>Sora的创新点：<strong>一次性生成</strong>Noise Vector Cude，用以保证帧与帧之间的逻辑关系。从而生成时间和空间更流畅、更连贯的视频。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104436506.png alt=image-20240226104436506></p><h2 id=23sora的技术优势>2.3.Sora的技术优势</h2><p>虽然OpenAI没有公布Sora模型的训练细节，但即使公布了，其他厂家可能也很难复制或追赶。</p><h3 id=1sora的video-compression-network依托于强大的算力>(1)Sora的Video compression network依托于强大的算力</h3><p>模型成功的一个重要因素是海量的训练数据，训练需要消耗大量算力。Sora采用高维向量矩阵模式的视频处理方式，意味着更大算力消耗。中信证券曾简单估算，一个6~8秒的视频（约60帧）需要约6万个Patches，如果去噪步数是20的话，相当于要生成120万个Tokens，这是相当大的计算量。那么Sora生成1分钟视频需要的算力可想而知。</p><p>同一个提示词，算力越高，生成视频效果越好。Sora给出基础算力、4倍算力、32倍算力下的效果展示：</p><iframe src="//player.bilibili.com/player.html?bvid=BV1uC411s7y9&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe>
<iframe src="//player.bilibili.com/player.html?bvid=BV1ey421q72G&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><h3 id=2gpt4加速孵化多模态大模型多模态大模型反哺llm>(2)GPT4加速孵化多模态大模型，多模态大模型反哺LLM</h3><ul><li>遥遥领先的自然语言理解能力：<ul><li>文生视频模型的训练离不开大量带有文字标注的视频，OpenAI基于GPT4专门训练一个高度描述性的字幕模型，使用它来为训练数据集中的所有视频生成文本字幕。这样进行训练极大的保证了文本保真度以及视频的整体质量。</li><li>同时，GPT优秀的文本扩展能力，可以丰富用户输入的提示词，让文本描述更丰富、更细致，从而生成丰富、细腻的视频。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226141517838.png alt=image-20240226141517838></p><h1 id=3sora有哪些创意玩法>3.Sora有哪些创意玩法</h1><ul><li><strong>Animating DALL·E images</strong>：图生视频<ul><li>左边是图片和提示词，右边是生成的视频</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV1pZ421y7RF&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Extending generated videos</strong>：视频续写<ul><li>基于原视频，向前或向后续写新的视频</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV1oK42187Kc&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Video-to-video editing</strong>：编辑视频<ul><li>左边是原视频，右边是根据提示词修改的视频</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV1Vu4m1w7QY&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Connecting videos</strong>：连接视频<ul><li>通过左侧视频和右侧视频，生成中间过渡视频</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV19w4m1f7dr&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Image generation capabilities</strong>：生成高质量图片<ul><li>生成各种尺寸的图片，分辨率最高可达 2048x2048</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226173623135.png alt=image-20240226173623135></p><ul><li><p><strong>卓越的仿真能力</strong>：生成现实世界仿真视频</p><ul><li><p><strong>3D consistency</strong>：空间一致性</p><ul><li>模拟摄像机镜头旋转和运行，画面中的人和场景元素在三维空间中也一致的移动。</li></ul></li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV11K421t75e&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Long-range coherence and object permanence</strong>：时间一致性<ul><li>Sora能够有效地为短期和长期依赖关系建模。例如，模型可以保存人物、动物和物体，即使它们被遮挡或离开了框架(下左视频)。同样，可以基于单个样本生成同一角色的多个镜头，并在整个视频中保持其外观(下右视频)。</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV1Av421k7vD&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li><strong>Interacting with the world</strong>：模拟真实世界的动作和结果<ul><li>随着画笔的移动，画面中增加了花瓣；人吃汉堡，汉堡留下咬痕。</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?bvid=BV1vF4m157Hm&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><h1 id=4sora对视频相关领域的影响>4.Sora对视频相关领域的影响</h1><ul><li><p><strong>内容创作与媒体行业</strong>：</p><p>在内容创作领域，Sora将缩短创作周期，降低制作成本。</p><p>导演、视频编辑等岗位将直面Sora的影响。</p><p>抖音类短视频App是否会面临挑战？</p></li><li><p><strong>教育和培训：</strong>
教育工作者可以利用Sora创建生动的教学材料，提高课程互动性和学习效果。
但Sora仍然面临垂域微调的难题——我们尝试生成一个排序算法的Demo，得到的视频不尽如人意。</p></li><li><p><strong>营销和广告</strong>
广告设计师和品牌经理可以通过Sora来快速生成视频广告内容。</p></li><li><p><strong>游戏开发与动画制作</strong>
游戏设计师和动画制作人员可能会使用Sora快速原型制作和动画创建。
这不仅能够加快项目开发速度，还能使得复杂场景的测试和迭代变得更为高效。
不过，这也可能引发对传统动画和建模技艺的重新评估。</p></li><li><p><strong>上下游基础设施：</strong>
上游厂商：AI服务器、AI芯片、通信行业、云厂商。
下游应用：大量的短/中/长视频应用和服务需求。</p></li></ul><h1 id=5参考>5.参考</h1><blockquote><p><a href=https://openai.com/research/video-generation-models-as-world-simulators>https://openai.com/research/video-generation-models-as-world-simulators</a></p></blockquote></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-02-26</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记42-LLM微调技术概览</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/><span class="next-text nav-default">【chatGPT】学习笔记40-LLM应用-如何构建RAG数据集</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>