<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中) - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。 但GPT3距离ChatGPT还有很远"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)"><meta property="og:description" content="根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。 但GPT3距离ChatGPT还有很远"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/"><meta property="article:published_time" content="2023-10-18T18:00:59+08:00"><meta property="article:modified_time" content="2023-10-18T18:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)"><meta itemprop=description content="根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。 但GPT3距离ChatGPT还有很远"><meta itemprop=datePublished content="2023-10-18T18:00:59+08:00"><meta itemprop=dateModified content="2023-10-18T18:00:59+08:00"><meta itemprop=wordCount content="3446"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)"><meta name=twitter:description content="根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。 但GPT3距离ChatGPT还有很远"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)</h1><div class=post-meta><time datetime=2023-10-18 class=post-time>2023-10-18</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 3446 字</span>
<span class=more-meta>预计阅读 7 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1chatgpt的历史版本>1.ChatGPT的历史版本</a></li><li><a href=#2chatgpt的整体训练流程>2.ChatGPT的整体训练流程</a></li><li><a href=#3chatgpt的三大训练方法>3.ChatGPT的三大训练方法</a><ul><li><a href=#1方法1预训练pre-traning>(1)方法1：预训练(Pre-Traning)</a></li><li><a href=#2方法2指令调优instruction-tuning>(2)方法2：指令调优(Instruction Tuning)</a></li><li><a href=#3方法3对齐alignment>(3)方法3：对齐(Alignment)</a></li></ul></li><li><a href=#4方法1预训练pre-training>4.方法1：预训练(Pre-Training)</a><ul><li><a href=#1与chatgpt整体训练流程图的对应关系>(1)与ChatGPT整体训练流程图的对应关系</a></li><li><a href=#2辩证看待预训练模型涌现能力神经网络>(2)辩证看待预训练模型、涌现能力、神经网络</a><ul><li><a href=#思考1神经网络的本质是什么>思考1：神经网络的本质是什么？</a></li><li><a href=#思考2假设思考1正确如何解释神经网络的5种现象>思考2：假设思考1正确，如何解释神经网络的5种现象？</a></li><li><a href=#思考3辩证地看待预训练模型涌现能力transformer>思考3：辩证地看待预训练模型、涌现能力、Transformer</a></li></ul></li></ul></li><li><a href=#5方法2指令调优instruction-tuning>5.方法2：指令调优(Instruction Tuning)</a><ul><li><a href=#1与chatgpt整体训练流程图的对应关系-1>(1)与ChatGPT整体训练流程图的对应关系</a></li><li><a href=#2训练sft模型的伪码>(2)训练SFT模型的伪码</a></li><li><a href=#3sft实例>(3)SFT实例</a></li></ul></li><li><a href=#6小结>6.小结</a></li></ul></nav></div></div><div class=post-content><p>根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。</p><p>但GPT3距离ChatGPT还有很远的距离，这一段距离涉及OpenAI未公开论文、源码的关键技术。</p><p>我们接下来从OpenAPI已公开的信息来看看<font color=red><strong>ChatGPT是如何炼成的</strong></font>。</p><h1 id=1chatgpt的历史版本>1.ChatGPT的历史版本</h1><p>下图摘自Yule Wang的技术专栏，阐述了基于Transformer的大语言模型不同版本的脉络：</p><ul><li><strong>BERT</strong>是编码器Only架构，<strong>BART</strong>是编码器-解码器架构，它们延伸出去大语言模型下载后<font color=red><strong>不能直接使用，需要垂域微调</strong></font>。</li><li><strong>T5</strong>是编码器-解码器架构，<strong>GPT-2</strong>是解码器Only架构，它们下载后<font color=red><strong>不做垂域微调，也能完成一些AI任务</strong></font>。</li><li><strong>GPT-3、GPT-4</strong>是解码器Only架构，它们下载后<font color=red><strong>不做垂域微调，能完成大部分AI任务</strong></font>。</li><li>针对<strong>GPT-3</strong>进行<font color=red><strong>SFT(有监督微调)+RLHF(基于人类反馈的强化学习)<strong></font>，最终得到了</strong>ChatGPT</strong>，<strong>GPT-3.5、InstructGPT</strong>算是过渡产品。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019083053465.png alt=image-20231019083053465></p><h1 id=2chatgpt的整体训练流程>2.ChatGPT的整体训练流程</h1><p>在《Introducing ChatGPT》(详见https://openai.com/blog/chatgpt)中，给出了从GPT3演进到ChatGPT的整体训练流程图：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/ChatGPT_Diagram.svg alt=ChatGPT_Diagram></p><p>以前看这张图很模糊，通过前面复现Transformer架构、简版GPT的代码，才逐渐变得清晰。</p><p>接下来，我们来逐一拆解。</p><h1 id=3chatgpt的三大训练方法>3.ChatGPT的三大训练方法</h1><p>下图比较形象地归纳了ChatGPT整体训练流程：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019094239673.png alt=image-20231019094239673></p><h2 id=1方法1预训练pre-traning>(1)方法1：预训练(Pre-Traning)</h2><ul><li>构建好大语言模型的神经网络架构后，<strong>通过大数据、大算力进行训练</strong>，得到预训练模型。如：GPT3就属于这类预训练模型。</li><li>大语言模型的本质是生成内容，构建训练数据基本都是自动化的，所以<strong>预训练的过程属于无监督学习</strong>。</li><li>预训练模型非常庞大，<strong>算是通才</strong>，具备<strong>基本的自然语言处理能力、世界知识</strong>，甚至还有了<strong>顿悟能力(涌现能力)</strong>。</li><li>就好像下图红框内庞大的AI大脑(<strong>画的有点恶心</strong>)。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150044511.png alt=image-20231019150044511></p><h2 id=2方法2指令调优instruction-tuning>(2)方法2：指令调优(Instruction Tuning)</h2><ul><li>虽然花了巨大的时间成本和空间成本获得了预训练模型，但是它的水平，依然无法像一个真人，与人类对话。</li><li>接下来还要进行微调——人工<strong>准备少量的数据</strong>，对<strong>预训练模型进行增量训练</strong>——因此也称为<strong>有监督微调(SFT，Supervised Fine-tunning)</strong>。</li><li><strong>指令微调</strong>本质是站在巨人的肩膀上<strong>对预训练模型进行增量训练</strong>。它可以针对世界知识进行增强，也可以针对某个垂直领域进行增强。</li><li>指令微调后得到的大语言训练模型，将会接近于一个真人。</li><li>就好像下图红框内的红色人脸(<strong>画的也挺恶心</strong>)。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150129023.png alt=image-20231019150129023></p><h2 id=3方法3对齐alignment>(3)方法3：对齐(Alignment)</h2><ul><li><p>在指令调优后，大语言模型虽然接近于真人，但生成的内容依然会很生硬。</p></li><li><p>为什么呢？这就是人类的主观感受——人类对某些问题的答案，会有情绪、语气、风格等主观的特征。</p></li><li><p>那么，又如何<strong>让AI学会人类的主观感受</strong>呢？这<strong>就是对齐Alignment</strong>。</p></li><li><p>RLHF(基于人类反馈的强化学习)就是对齐的具体实现之一，利用这种强化学习手段，让大语言模型学会人类的主观感受，GPT3就演进成为了ChatGPT。</p></li><li><p>就好像下图右侧红框内小黄球(<strong>画的依然挺恶心</strong>)。</p></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151032099.png alt=image-20231019151032099></p><p>接下来，我们逐一分析这三大方法。</p><h1 id=4方法1预训练pre-training>4.方法1：预训练(Pre-Training)</h1><h2 id=1与chatgpt整体训练流程图的对应关系>(1)与ChatGPT整体训练流程图的对应关系</h2><p>我们对比两张图：</p><ul><li><strong>预训练</strong>得到的大语言模型，就<strong>是ChatGPT整体训练流程STEP1的输入</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151849173.png alt=image-20231019151849173></p><h2 id=2辩证看待预训练模型涌现能力神经网络>(2)辩证看待预训练模型、涌现能力、神经网络</h2><p>前段时间，师父问了我3个终极问题，让我一时语塞：</p><ul><li><strong>为什么ChatGPT大力能出奇迹，小力就不行？</strong></li><li><strong>为什么这样的神经网络模型就可以大力出奇迹，换个神经网络模型行不行？</strong></li><li><strong>如果无论什么神经网络模型只要能做到大力就能出奇迹，那么这些神经网络的本质是什么？</strong></li></ul><p>前面半年一直在复现Transformer论文的细节中，的确缺少了对宏观本质的思考，结合预训练这个章节的写作，正好梳理一下我的宏观思考：</p><h3 id=思考1神经网络的本质是什么>思考1：神经网络的本质是什么？</h3><ul><li>观点1：客观世界中，<strong>一切问题都能用函数表达</strong>。<ul><li>只不过有的函数极其复杂，只有上帝才知道这个函数是什么。</li></ul></li><li>观点2：<strong>神经网络的本质进行函数近似(Function Approximation)的工具</strong>。<ul><li><strong>神经网络的输入</strong>：是大量的数据，数据中隐藏了某个问题背后的函数的数学规律。</li><li><strong>神经网络的输出</strong>：找到无限逼近于某个问题背后的函数的近似函数。</li><li>数学上，已经证明<strong>神经网络能够近似出任意一个问题背后的函数</strong>。</li><li><strong>神经网络的结构</strong>，<strong>决定了找到这个近似函数的成本</strong>(时间成本、空间成本……)。</li></ul></li></ul><h3 id=思考2假设思考1正确如何解释神经网络的5种现象>思考2：假设思考1正确，如何解释神经网络的5种现象？</h3><ul><li>现象1：<strong>过于简单的神经网络</strong>，需要很大力才能出奇迹。<ul><li>任务是"让AI对红点和蓝点进行分类&rdquo;，由于神经网络过于简单，所以迭代了2000多次才找出答案。</li></ul></li></ul><iframe src="//player.bilibili.com/player.html?aid=364782652&bvid=BV1F94y1b7MW&cid=1304339810&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li>现象2：<strong>过于简单的任务</strong>，不需要大力也能出奇迹。</li></ul><iframe src="//player.bilibili.com/player.html?aid=874844018&bvid=BV1EN4y1C7v2&cid=1304340008&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li>现象3：恒定难度的任务，<strong>加宽神经网络</strong>，大力能出奇迹。</li></ul><iframe src="//player.bilibili.com/player.html?aid=234774107&bvid=BV1N8411r78C&cid=1304339757&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li>现象4：恒定难度的任务，<strong>加深神经网络</strong>，大力能出奇迹。</li></ul><iframe src="//player.bilibili.com/player.html?aid=789763364&bvid=BV1Ry4y1N7CU&cid=1304340937&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><ul><li>现象5：恒定难度的任务，<strong>增强神经元</strong>，大力能出奇迹。</li></ul><iframe src="//player.bilibili.com/player.html?aid=277344904&bvid=BV14w411w7Nx&cid=1304343623&p=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen height=600px></iframe><p>从上述5种现象，应该可以得到2个结论：</p><ul><li>在神经网络结构恒定的前提下，<strong>待执行的任务难度</strong>，决定了<strong>能否大力出奇迹、是否需要大力</strong>。</li><li>在待执行的任务难度恒定的前提下，<strong>神经网络结构</strong>，决定了<strong>能否大力出奇迹、是否需要大力</strong>。</li></ul><h3 id=思考3辩证地看待预训练模型涌现能力transformer>思考3：辩证地看待预训练模型、涌现能力、Transformer</h3><p>当GPT3.5出现<strong>涌现能力(emergent capabilities)<strong>后，它似乎被神话成了</strong>人类尚无办法解释的神迹</strong>。</p><p>我们应该如此这般辩证地看待预训练模型、涌现、Transformer</p><ul><li>预训练模型不是ChatGPT首创，在深度学习时代就有了，有很多神经网络都是预训练好，节省后来人的训练成本。</li><li>Transformer只是GPT3这种预训练模型遵循的神经网络结构，<strong>Transformer这种神经网络结构本质是换了一种姿势寻找近似函数</strong>。</li><li>如果问题P1、问题P2、……问题Pn背后的函数都极为相近，当神经网络结构找到了问题P1的近似函数，那么这个近似函数也能用来解决问题P2、……问题Pn——这就是经过训练，神经网络能自我学习到一些额外的能力的原因，即涌现能力。<ul><li>进一步思考：深度学习时代有没有涌现呢？可能有，只是n这个值比较小，不太明显吧。</li></ul></li></ul><h1 id=5方法2指令调优instruction-tuning>5.方法2：指令调优(Instruction Tuning)</h1><h2 id=1与chatgpt整体训练流程图的对应关系-1>(1)与ChatGPT整体训练流程图的对应关系</h2><ul><li>指令调优对应于ChatGPT整体训练流程的STEP1。</li><li><strong>SFT</strong>：有监督微调，属于有监督学习。包括In-context Tuning(上下文调优)和Instruction Tuning(指令调优)。<ul><li><strong>In-context Tuning</strong>：上下文调优，这种调优方式的本质是将多轮对话的聊天记录一起发送给大语言模型，有了上下文，大语言模型就能更好地回答问题。</li><li><strong>Instruction Tuning</strong>：指令调优，这种调优方式的本质就是问题中带有明确的指令、明确的要求。Instruction Tuning是OpenAI在GPT-3.5-turbo模型中引入的一种新方法，是在传统的微调过程上的一种变体。</li><li>对于上述两种调优方式，在提示词工程中都体现了它们的思想——<strong>&ldquo;上下文&rdquo;、&ldquo;指令&rdquo;</strong>。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019170641604.png alt=image-20231019170641604></p><h2 id=2训练sft模型的伪码>(2)训练SFT模型的伪码</h2><p>前文讲了很多理论，还是需要撸一下代码比较便于理解。</p><blockquote><p>由于OpenAI对于SFT、RLHF并未公开源码，所以在这里只编写伪码，在后续实例中展示真实代码。</p></blockquote><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019181420212.png alt=image-20231019181420212></p><h2 id=3sft实例>(3)SFT实例</h2><p>针对简版GPT，只需要增加如下代码：</p><ul><li>首先，通过<code>torch.load方法</code>加载上一篇已经预训练好的简版GPT模型，这个模型的训练数据仅包含了维基百科的基本数据。</li><li>然后，将新增的医学知识数据集，对简版GPT模型进行增量训练。</li><li>最后，通过<code>torch.save方法</code>保存增量训练的简版GPT模型，此时，这个模型就包含了医学知识了。</li><li>具体代码详见下图：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019182641050.png alt=image-20231019182641050></p><h1 id=6小结>6.小结</h1><p>讲到这里，内容已经比较饱和了，我们将在最后一篇阐述<strong>方法3：对齐训练</strong>以及<strong>RLHF的实例</strong>。</p><p>我们接下来对本文进行一下小结：</p><ul><li>ChatGPT的历史版本：<strong>GPT3、InstructGPT、ChatGPT</strong>。</li><li>ChatGPT<strong>整体训练流程</strong>，支撑了<strong>GPT3到ChatGPT的演进</strong>。</li><li>预训练模型是什么？如何<strong>辩证地</strong>看待<strong>预训练模型、涌现能力、神经网络的学习能力</strong>？</li><li>SFT(有监督微调)的概念、原理，最后展示了<strong>如何针对简版GPT进行SFT</strong>。</li></ul><p>我们下一步继续针对简版ChatGPT开展RLHF，且听下回分解。</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-10-18</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记19-自己实现一个简版ChatGPT(下)</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/><span class="next-text nav-default">【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2023
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>