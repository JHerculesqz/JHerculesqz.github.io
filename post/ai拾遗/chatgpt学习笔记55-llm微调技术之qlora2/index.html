<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记55-LLM微调技术之QLoRA(2) - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Op"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)"><meta property="og:description" content="笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Op"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/"><meta property="article:published_time" content="2024-10-18T11:00:59+08:00"><meta property="article:modified_time" content="2024-10-18T11:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)"><meta itemprop=description content="笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Op"><meta itemprop=datePublished content="2024-10-18T11:00:59+08:00"><meta itemprop=dateModified content="2024-10-18T11:00:59+08:00"><meta itemprop=wordCount content="2775"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)"><meta name=twitter:description content="笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Op"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)</h1><div class=post-meta><time datetime=2024-10-18 class=post-time>2024-10-18</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2775 字</span>
<span class=more-meta>预计阅读 6 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1模型剪枝>1.模型剪枝</a><ul><li><a href=#1什么是模型剪枝>(1)什么是模型剪枝？</a></li><li><a href=#2模型剪枝的分类>(2)模型剪枝的分类</a></li><li><a href=#3模型剪枝的流程>(3)模型剪枝的流程</a></li><li><a href=#4模型剪枝的收益>(4)模型剪枝的收益</a></li><li><a href=#5模型剪枝的挑战>(5)模型剪枝的挑战</a></li></ul></li><li><a href=#2optimal-brain-damage>2.Optimal Brain Damage</a><ul><li><a href=#1obd的推导过程>(1)OBD的推导过程</a></li><li><a href=#2obd的执行流程>(2)OBD的执行流程</a></li></ul></li><li><a href=#3optimal-brain-surgeon>3.Optimal Brain Surgeon</a><ul><li><a href=#1obs的推导过程>(1)OBS的推导过程</a></li><li><a href=#2obs为什么优于obd>(2)OBS为什么优于OBD？</a></li></ul></li><li><a href=#4总结>4.总结</a></li></ul></nav></div></div><div class=post-content><p>笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Optimal Brain Damage》，以及1992年的《Second order derivatives for network pruning: Optimal Brain Surgeon》，来理解一下大神们在90年代如何解决模型剪枝问题的。</p><h1 id=1模型剪枝>1.模型剪枝</h1><h2 id=1什么是模型剪枝>(1)什么是模型剪枝？</h2><p>模型剪枝是一种模型优化技术，通过移除神经网络中的冗余连接或神经元，达到降低模型复杂度的目的。</p><p>降低模型复杂度，除了通过降低模型计算量提高模型的推理速度，还能在一定程度上提升模型的泛化能力。</p><h2 id=2模型剪枝的分类>(2)模型剪枝的分类</h2><p>模型剪枝主要分两大类：结构化剪枝和非结构化剪枝。</p><ul><li><p><strong>非结构化剪枝</strong>：这种剪枝方式涉及单个权重或神经元的移除，结果是权重矩阵中出现不规则的稀疏模式。</p><ul><li>由于这种剪枝方式不遵循特定的规则，因此压缩后的模型可能需要特殊的处理才能被有效地存储和计算。</li></ul></li><li><p><strong>结构化剪枝</strong>：这种剪枝方式会按照一定的规则移除权重，保持模型原有的结构。</p><ul><li>这种方法实现成本低，剪枝后的模型更容易被深度学习框架1支持。</li></ul></li></ul><h2 id=3模型剪枝的流程>(3)模型剪枝的流程</h2><ul><li><strong>STEP1.预训练</strong>：首先在大量数据集上预训练一个深度学习模型。</li><li><strong>STEP2.剪枝</strong>：根据一定的剪枝准则(如：权重的绝对值、激活值、梯度信息等)，选择并剪除那些对模型性能影响较小的连接或神经元。</li><li><strong>STEP3.微调</strong>：剪枝后，在剩余的数据集上对剪枝后模型进行微调，以恢复或提升模型的性能。</li><li><strong>STEP4.评估</strong>：最后对剪枝后的模型进行评估，以确保其性能满足特定任务的要求。</li></ul><h2 id=4模型剪枝的收益>(4)模型剪枝的收益</h2><ul><li>减少模型的计算复杂度和存储空间。</li><li>改善模型的泛化能力。</li><li>移除不必要的参数，使得模型的结构更加清晰，更易于理解和解释，提高模型的可解释性。</li><li>适应不同的任务和环境，特别是在资源受限的设备上部署模型时尤为重要</li></ul><h2 id=5模型剪枝的挑战>(5)模型剪枝的挑战</h2><ul><li>模型剪枝时，剪哪里、怎么剪，需要对模型的深度理解和数学能力，如：本文即将解读的OBD、OBS就是给出了两种剪枝方法及数学推导。</li><li>剪枝不当可能消耗大量算力后却损失了信息，反而导致模型性能下降。</li><li>剪枝后模型还需要进一步调整和优化，以适应不同更广泛的场景。</li></ul><h1 id=2optimal-brain-damage>2.Optimal Brain Damage</h1><h2 id=1obd的推导过程>(1)OBD的推导过程</h2><p><strong>首先，我们了解一下OBD解决什么问题：</strong></p><ul><li>这篇论文由Lecun发表，Optimal Brain Damage (OBD)是一种神经网络模型剪枝技术。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018144556744.png alt=image-20241018144556744></p><ul><li>在论文的开篇，Lecun就提出了问题：模型剪枝就是删除没用的权重，那么如何衡量"这个权重没用？"，如何"删除权重，清零吗？&rdquo;</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018154321755.png alt=image-20241018154321755></p><ul><li>因此，Lecun提出了Optimal Brain Damage (OBD)，通过选择性地删除网络中的权重来减少模型的大小。</li><li>“选择性”的核心思想是：利用目标函数的二阶导数信息来评估每个权重的重要性，即通过海森矩阵(Hessian matrix)来确定权重的“贡献度”或“saliency”。<ul><li>权重的贡献度：指删除该权重后对目标函数(通常是损失函数)的影响程度。</li></ul></li><li>OBD的目标是找到一个参数集合，使得删除这些参数后目标函数的增加最小，从而在减少网络复杂度的同时尽量保持模型性能。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018154349795.png alt=image-20241018154349795></p><p><strong>然后，我们再看看推导过程的关键细节：</strong></p><ul><li>目标函数E：比如目标函数E就是损失函数。</li><li>泰勒展开：我们对目标函数E进行泰勒展开，就得到"参数向量影响δU对目标函数E的影响公式&rdquo;。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018155414749.png alt=image-20241018155414749></p><ul><li>g<sub>i</sub>是U对E的梯度，h<sub>ij</sub>是U对E的海森矩阵的元素。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018155355488.png alt=image-20241018155355488></p><ul><li>但即使是泰勒展开了，公式(1)依然有很大的计算量，Lecun做了三个近似简化：<ul><li>数学意义上对角线近似简化上述公式的第3项，业务意义上就是证明了删除权重a导致的损失函数的Loss增大和删除权重b导致的Loss增大是独立的。</li><li>数学意义上通过极值点简化上述公式的第1项，业务意义上就是证明了如果神经网络已经训练收敛了，U对E就达到了局部最小值，即U对E的梯度等于0。</li><li>数学意义上假设E是二次函数简化上述公式的第4项。</li><li>综合上述3个简化，公式(1)简化为公式(3)</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018160514805.png alt=image-20241018160514805></p><h2 id=2obd的执行流程>(2)OBD的执行流程</h2><p>有了上述公式，OBD的执行流程如下：</p><ul><li>STEP1.选择1个神经网络结构</li><li>STEP2.训练这个神经网络</li><li>STEP3.计算每个权重的二阶导h<sub>kk</sub></li><li>STEP4.计算每个参数对目标函数的影响s<sub>k</sub></li><li>STEP5.删除最低影响的参数</li><li>STEP6.回到STEP2</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018160651088.png alt=image-20241018160651088></p><h1 id=3optimal-brain-surgeon>3.Optimal Brain Surgeon</h1><h2 id=1obs的推导过程>(1)OBS的推导过程</h2><p><strong>首先，我们也了解一下OBS解决什么问题：</strong></p><ul><li>这篇论文是对OBD的优化，Optimal Brain Surgeon(OBS)也是一种神经网络模型剪枝技术。</li><li>Optimal Brain Damage(OBD)衡量了某个权重对贡献度并删除贡献度最低的权重，但一旦删除了某个错误的权重，反而会导致在错误的剪枝方向上越走越远。</li><li>OBS的目标就是允许比其他方法剪枝更多的权重，在测试数据上产生更好的泛化。其核心思想就是从训练数据和网络的结构信息中计算逆Hessian矩阵H的逆。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018161259795.png alt=image-20241018161259795></p><ul><li>OBS本质是考虑的"局部最优&rdquo;，即训练得到局部最小误差。那么我们将相对权重扰动的Loss函数进行泰勒展开：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018161832293.png alt=image-20241018161832293></p><ul><li>其中，H是Loss对权重的海森矩阵，依然对公式(1)进行近似简化，忽略公式(1)的第1项和第3项。</li><li>那么，删除权重w<sub>q</sub>的操作可以表示为公式(2)，其中e<sub>q</sub>是权重空间w中相对于w<sub>q</sub>的单位向量。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162056961.png alt=image-20241018162056961></p><ul><li>那么，OBS的目标就是求解公式(3)的最优化问题：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162249685.png alt=image-20241018162249685></p><ul><li>转换为拉格朗日乘法：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162354240.png alt=image-20241018162354240></p><ul><li>求解出δw，即剪切w<sub>q</sub>的同时，调整了其它权重，并保证Loss变动最小。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162518230.png alt=image-20241018162518230></p><h2 id=2obs为什么优于obd>(2)OBS为什么优于OBD？</h2><p>论文基于上述推导，解释了为什么OBS优于OBD：</p><ul><li>基于OBD，<ul><li>从w<sup>*</sup>处的局部最小值开始，基于贡献度误删除了权重2(假设：上帝知道权重2不应该被删掉)。</li><li>通过再次训练，权重1将增加，权重1将在错误的方向上越走越远，直到达到平衡，但此时不是局部最优。</li></ul></li><li>基于OBS，<ul><li>从w<sup>*</sup>处的局部最小值开始，删除权重2的同时，权重1也会被修正。</li><li>通过再次训练，由于局部最优的约束，会尽快达到稳态。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018163243850.png alt=image-20241018163243850></p><h1 id=4总结>4.总结</h1><p>为了解读QLoRA的原理，我们扩展阅读了模型剪枝技术，并解读了经典的两篇论文OBD和OBS：</p><ul><li>模型剪枝：通过移除神经网络中的冗余连接或神经元，达到降低模型复杂度的目的。</li><li>关键问题：删什么权重，怎么删除？</li><li>OBD：Optimal Brain Damage (OBD)，利用目标函数的二阶导数信息来评估每个权重的重要性，即通过海森矩阵(Hessian matrix)来确定权重的“贡献度”或“saliency”。</li><li>OBS：Optimal Brain Surgeon(OBS)，利用相对权重扰动的Loss函数进行泰勒展开，剪切w<sub>q</sub>的同时，调整了其它权重，并保证Loss变动最小。</li></ul><p>论文链接1：https://arxiv.org/pdf/2303.10512</p><p>论文链接2：https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-10-18</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-agentscope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/><span class="next-text nav-default">【chatGPT】学习笔记56-AgentScope解读-模型</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>