<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上) - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="接下来，我们用三篇文章阐述**如何实现一个简版ChatGPT。** 1.回顾 想实现一个简版ChatGPT，依赖于如下前置知识： 机器学习基本原理"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)"><meta property="og:description" content="接下来，我们用三篇文章阐述**如何实现一个简版ChatGPT。** 1.回顾 想实现一个简版ChatGPT，依赖于如下前置知识： 机器学习基本原理"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/"><meta property="article:published_time" content="2023-10-16T18:00:59+08:00"><meta property="article:modified_time" content="2023-10-16T18:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)"><meta itemprop=description content="接下来，我们用三篇文章阐述**如何实现一个简版ChatGPT。** 1.回顾 想实现一个简版ChatGPT，依赖于如下前置知识： 机器学习基本原理"><meta itemprop=datePublished content="2023-10-16T18:00:59+08:00"><meta itemprop=dateModified content="2023-10-16T18:00:59+08:00"><meta itemprop=wordCount content="2176"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)"><meta name=twitter:description content="接下来，我们用三篇文章阐述**如何实现一个简版ChatGPT。** 1.回顾 想实现一个简版ChatGPT，依赖于如下前置知识： 机器学习基本原理"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</h1><div class=post-meta><time datetime=2023-10-16 class=post-time>2023-10-16</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2176 字</span>
<span class=more-meta>预计阅读 5 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1回顾>1.回顾</a></li><li><a href=#2实现简版gpt>2.实现简版GPT</a><ul><li><a href=#1整体>(1)整体</a></li><li><a href=#2局部1正弦位置编码表>(2)局部1：正弦位置编码表</a></li><li><a href=#3局部2解码器堆栈>(3)局部2：解码器堆栈</a></li><li><a href=#4局部3解码器>(4)局部3：解码器</a></li><li><a href=#5局部4多头注意力>(5)局部4：多头注意力</a></li><li><a href=#6局部5前向传播网络>(6)局部5：前向传播网络</a></li><li><a href=#7局部6填充位置掩码>(7)局部6：填充位置掩码</a></li><li><a href=#8局部7后续位置掩码>(8)局部7：后续位置掩码</a></li><li><a href=#9模型训练>(9)模型训练</a></li><li><a href=#10模型测试>(10)模型测试</a></li></ul></li><li><a href=#3小结>3.小结</a></li></ul></nav></div></div><div class=post-content><p>接下来，我们用三篇文章阐述<font color=red>**如何实现一个简版ChatGPT。**</font></p><h1 id=1回顾>1.回顾</h1><p>想实现一个简版ChatGPT，依赖于如下前置知识：</p><ul><li><strong>机器学习基本原理</strong>，可参考笔者这几篇文章：<ul><li>《【chatGPT】学习笔记3-机器学习基本原理(上)》</li><li>《【chatGPT】学习笔记4-机器学习基本原理(下)》</li></ul></li><li><strong>经典NLP相关技术</strong>，可参考笔者这几篇文章：<ul><li><strong>N-Gram</strong>：《【chatGPT】学习笔记6-手撸一个上古GPT》</li><li><strong>Embedding</strong>：《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》</li><li><strong>神经概率语言模型</strong>：《【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2》</li><li><strong>Seq2Seq</strong>：《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》</li></ul></li><li><strong>现代NLP相关技术</strong>，可参考笔者这几篇文章：<ul><li><strong>注意力机制</strong>：《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》</li><li><strong>Transformer</strong>：《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》</li></ul></li></ul><h1 id=2实现简版gpt>2.实现简版GPT</h1><p>这是参考Transformer架构绘制的简版ChatGPT整体架构，我们将对它进行拆解：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017113929233.png alt=image-20231017113929233></p><h2 id=1整体>(1)整体</h2><p>将上图抽象化，我们可以看到：</p><ul><li>简版GPT遵循Transfomer架构，但是没有实现编码器</li><li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017134713086.png alt=image-20231017134713086></p><p>代码实现如下：</p><ul><li><strong>1</strong>：对应上图将<strong>Outputs</strong>输入给<strong>Decoder</strong>。</li><li><strong>2</strong>：对应上图将<strong>Decoder的输出</strong>，传入给线性层。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017135734066.png alt=image-20231017135734066></p><h2 id=2局部1正弦位置编码表>(2)局部1：正弦位置编码表</h2><p>首先，我们来细化<strong>Outputs</strong>和<strong>Decoder</strong>之间的流程：</p><ul><li><strong>STEP1</strong>.对<strong>Outputs</strong>实施词嵌入，得到词向量。</li><li><strong>STEP2</strong>.在<strong>词向量</strong>和<strong>Decoder</strong>之间增加了<strong>位置编码表</strong>(也是一个向量)，这个位置编码表体现了<strong>词和词序的关系</strong>。<ul><li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立<strong>词和词序的关系</strong>。</li></ul></li><li><strong>STEP3</strong>.将STEP2的<strong>位置表码表</strong>向量和<strong>词向量</strong>相加，输入给<strong>Decoder</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140335543.png alt=image-20231017140335543></p><p>正弦位置编码表的计算原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&rdquo;(2)局部1：正弦位置编码表"章节，本文不再赘述。</p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140636308.png alt=image-20231017140636308></p><h2 id=3局部2解码器堆栈>(3)局部2：解码器堆栈</h2><p>我们再来细化<strong>解码器</strong>：</p><ul><li>编码器本质上由<strong>N个解码器</strong>串联而成的<strong>解码器堆栈</strong>。</li><li>我们的实现，也按照论文的设定层数，<strong>N=6</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150909585.png alt=image-20231017150909585></p><h2 id=4局部3解码器>(4)局部3：解码器</h2><p>我们再进一步细化<strong>解码器Decoder</strong>：</p><ul><li><strong>解码器Decoder</strong>由<strong>多头注意力</strong>和<strong>前向传播网络</strong>组成。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150925061.png alt=image-20231017150925061></p><p><strong>解码器堆栈</strong>的代码实现如下：</p><ul><li><strong>①</strong>：这就是创建的<strong>位置编码层</strong>，再将<strong>词向量和位置编码向量相加</strong>。</li><li><strong>②</strong>：这就是将多个<strong>解码器</strong>叠加成<strong>解码器堆栈</strong>，每个<strong>解码器的输入</strong>是<strong>上个解码器的输出</strong>和<strong>上个解码器输出的注意力权重</strong>。</li><li><strong>③</strong>：这就是表示<strong>解码器堆栈</strong>输出的<strong>解码器输出</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017142049236.png alt=image-20231017142049236></p><p><strong>解码器</strong>的代码实现如下：</p><ul><li><strong>①</strong>：就是<strong>多头注意力层</strong>，<strong>第一个解码器</strong>的<strong>多头注意力层</strong>的输入是<strong>词嵌入+位置编码向量之和</strong>以及<strong>自注意力掩码</strong>，<strong>后续解码器</strong>的<strong>多头注意力层</strong>的输入是<strong>上一个解码器的输出</strong>和<strong>自注意力掩码</strong>。</li><li><strong>②</strong>：就是<strong>前向传播网络</strong>，它的输入是<strong>多头注意力层的输出</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145220345.png alt=image-20231017145220345></p><p>我们接下来看<strong>多头注意力层、前向传播网络、自注意力位置掩码</strong>如何实现？</p><h2 id=5局部4多头注意力>(5)局部4：多头注意力</h2><p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&rdquo;(5)局部4：多头注意力"章节，本文不再赘述。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150943298.png alt=image-20231017150943298></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145622219.png alt=image-20231017145622219></p><h2 id=6局部5前向传播网络>(6)局部5：前向传播网络</h2><p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&rdquo;(6)局部5：前向传播网络"章节，本文不再赘述。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151000709.png alt=image-20231017151000709></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150114991.png alt=image-20231017150114991></p><h2 id=7局部6填充位置掩码>(7)局部6：填充位置掩码</h2><p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&rdquo;(7)局部6：填充位置掩码"章节，本文不再赘述。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151014381.png alt=image-20231017151014381></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150413819.png alt=image-20231017150413819></p><h2 id=8局部7后续位置掩码>(8)局部7：后续位置掩码</h2><p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&rdquo;(10)局部9：后续位置掩码"章节，本文不再赘述。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151026419.png alt=image-20231017151026419></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150515799.png alt=image-20231017150515799></p><h2 id=9模型训练>(9)模型训练</h2><p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：</p><ul><li>数据集如下：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151515137.png alt=image-20231017151515137></p><ul><li>模型训练：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151651529.png alt=image-20231017151651529></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151717714.png alt=image-20231017151717714></p><ul><li>训练好后，会生成简版GPT的pth模型文件：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151835127.png alt=image-20231017151835127></p><h2 id=10模型测试>(10)模型测试</h2><ul><li>测试用例采用贪婪编码和集束编码，比较简单，具体代码如下：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151956136.png alt=image-20231017151956136></p><h1 id=3小结>3.小结</h1><ul><li>本文基于Transformer架构，复现了简版ChatGPT，其关键在于只有解码器。</li><li>理论上，如果有足够算力、足够训练数据，可以将此简版ChatGPT训练到GPT3的水平。</li><li>那么，GPT3到ChatGPT还有一定的距离，我们知道ChatGPT公开的信息中，还对GPT3进行了<strong>监督学习微调SFT</strong>、<strong>基于人类反馈的强化学习RLHF</strong>等，得到了InstructGPT，进而得到了ChatGPT。</li></ul><p>我们下一步继续针对简版ChatGPT开展SFT和RLHF，且听下回分解。</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-10-16</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/><span class="next-text nav-default">【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>