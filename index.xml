<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 10 Nov 2023 12:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记25-提示词解读2-通用技巧</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/</link><pubDate>Fri, 10 Nov 2023 12:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/</guid><description>&lt;p>上一篇，我们了解了提示词基本概念，本篇我们继续解读吴恩达老师的《ChatGPT Prompt Engineering for Developers》课程，看一下提示词常用技巧。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231108115607959.png" alt="image-20231108115607959">&lt;/p>
&lt;h1 id="1技巧构建合理的提示词结构">1.技巧：构建合理的提示词结构&lt;/h1>
&lt;p>完整的提示词包含以下四个要素：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>要素&lt;/th>
&lt;th>说明&lt;/th>
&lt;th>举例&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>指令词&lt;/strong>&lt;/td>
&lt;td>想要模型执行的特定任务或指令&lt;/td>
&lt;td>如：翻译、总结、生成&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>背景(上下文)&lt;/strong>&lt;/td>
&lt;td>包含外部信息或额外的上下文信息，引导语言模型更好地响应&lt;/td>
&lt;td>如：“在人工智能领域”, “在医学领域”&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>输入&lt;/strong>&lt;/td>
&lt;td>用户输入的内容或问题&lt;/td>
&lt;td>如：需要总结的文章&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>输出要求&lt;/strong>&lt;/td>
&lt;td>指定输出的类型或格式&lt;/td>
&lt;td>如：以JSON格式输出&amp;hellip;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;font color=red>**注意：**提示词结构取决于您的任务类型，并非所有以上要素都是必须的。&lt;/font>&lt;/p>
&lt;h1 id="2技巧设定角色">2.技巧：设定角色&lt;/h1>
&lt;h2 id="1设定llm的角色">(1)设定LLM的角色&lt;/h2>
&lt;p>在提示词中设定LLM角色，&lt;strong>让模型进行角色扮演&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>直接提问&lt;/strong>，ChatGPT返回的答案较笼统，没有针对性。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110154844353.png" alt="image-20231110154844353">&lt;/p>
&lt;ul>
&lt;li>在提示词中&lt;strong>让LLM角色扮演&lt;/strong>，ChatGPT再次返回的答案就会出现医学领域的专有名词和专业指导。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110155206497.png" alt="image-20231110155206497">&lt;/p>
&lt;h2 id="2设定提问者的角色">(2)设定提问者的角色&lt;/h2>
&lt;p>除了让大语言模型进行角色扮演，还可以设定提问者的角色，为不同的提问对象生成定制化的答案。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>设定提问者的角色是一位百岁老人&lt;/strong>，ChatGPT的回答会考虑到老人的身体状况。(PS：百岁老人你都敢建议他去跑马拉松&amp;hellip;)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110155639302.png" alt="image-20231110155639302">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>设定提问者的角色是缺乏运动的程序员&lt;/strong>，ChatGPT的回答会提醒程序员循序渐进。(PS：这个缺乏运动的程序员就是我&amp;hellip;)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110155824839.png" alt="image-20231110155824839">&lt;/p>
&lt;h1 id="3技巧分隔符划分指令和内容">3.技巧：分隔符划分指令和内容&lt;/h1>
&lt;p>如果提示词包含2个要素：&lt;strong>指令词&lt;/strong>和&lt;strong>输入&lt;/strong>，那么我们如何让大语言模型知道哪些是&lt;strong>指令词&lt;/strong>，哪些是&lt;strong>输入&lt;/strong>？&lt;/p>
&lt;ul>
&lt;li>我们可以&lt;strong>使用分隔符&lt;/strong>(如```、&amp;quot;&amp;quot;&amp;quot;、&amp;lt;&amp;gt;等)区分&lt;strong>指令&lt;/strong>和待处理的&lt;strong>输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>看一个例子：&lt;/p>
&lt;ul>
&lt;li>我们不是想大语言模型给100岁老人参加马拉松的建议，而是希望将这段文字翻译为英文——我们可以通过&lt;code>&amp;quot;&amp;quot;&amp;quot;&lt;/code>，&lt;strong>区分出指令和输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110160402261.png" alt="">&lt;/p>
&lt;h1 id="4技巧指定文本判断条件">4.技巧：指定文本判断条件&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>在提示词中&lt;strong>指定文本判断条件&lt;/strong>，激发大语言模型对文字的分类能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>示例：在本示例中，激发了大语言模型对**&amp;ldquo;是否为指令&amp;rdquo;**的分类能力。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110161203289.png" alt="image-20231110161203289">&lt;/p>
&lt;h1 id="5技巧指定输出的格式">5.技巧：指定输出的格式&lt;/h1>
&lt;ul>
&lt;li>在提示词中&lt;strong>指定输出答案的格式&lt;/strong>，便于应用软件系统获得答案后的文本处理。&lt;/li>
&lt;li>示例：在本示例中，ChatGPT按照提示词中设定的JSON格式返回了答案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110163709627.png" alt="image-20231110163709627">&lt;/p>
&lt;h1 id="6-技巧few-shot">6. 技巧：Few-Shot&lt;/h1>
&lt;ul>
&lt;li>在提示词中，给出&lt;strong>一些示例的问答&lt;/strong>，可能&lt;strong>激发大语言模型的模仿能力&lt;/strong>。&lt;/li>
&lt;li>根据给出的示例问答的数量，可分为：
&lt;ul>
&lt;li>
&lt;p>&lt;strong>zero-shot&lt;/strong>：零样本提示。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>one-shot&lt;/strong>：单样本提示。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>few-shot&lt;/strong>：少样本提示。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们来看一个示例：&lt;/p>
&lt;ul>
&lt;li>直接问大语言模型问题，属于&lt;strong>zero-shot&lt;/strong>，大语言模型的答案风格比较自由。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110162842622.png" alt="image-20231110162842622">&lt;/p>
&lt;ul>
&lt;li>问大语言模型的同时，给出了老师、学生的一个问答对，属于&lt;strong>one-shot&lt;/strong>，大语言模型的答案风格就会&lt;strong>大致模仿&lt;/strong>一下示例问答对的风格。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110163038324.png" alt="image-20231110163038324">&lt;/p>
&lt;ul>
&lt;li>问大语言模型的同时，给出了老师、学生的多个问答对，属于&lt;strong>few-shot&lt;/strong>，大语言模型的答案风格就会&lt;strong>模仿&lt;/strong>示例问答对的风格及&lt;strong>老师的情绪&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110163115561.png" alt="image-20231110163115561">&lt;/p>
&lt;h1 id="7技巧cot">7.技巧：CoT&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>CoT&lt;/strong>：Chain of Thought，思维链。AI科学家在研究中发现，只需要在提示词最后增加一句话——&amp;ldquo;让我们一步一步思考&amp;rdquo;，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我们看一下示例：我们的提问是大语言模型目前的短板能力(数学问题)，在没有任何提示的情况下，答案是错的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110165008611.png" alt="image-20231110165008611">&lt;/p>
&lt;ul>
&lt;li>我们加上这句神奇的咒语——&lt;code>Let's think step by step.&lt;/code>，ChatGPT就可以回答正确了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110165440078.png" alt="image-20231110165440078">&lt;/p>
&lt;h1 id="8技巧自洽self-consistency">8.技巧：自洽(SELF-CONSISTENCY)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>Self-Consistency&lt;/strong>：自洽，即推理过程中，用多种推理路径得到结果，出现最大的答案大概率就是正确答案，从而体现了&lt;strong>自洽性&lt;/strong>。&lt;/li>
&lt;li>随着大语言模型能力日益增强，Self-Consistency已成为大语言模型的内部能力，需要多次实验才能观测到自洽的推理过程。&lt;/li>
&lt;li>我们来看一个例子：通过CoT，激发大语言模型推理思考，从它的回答中，可以看出大语言模型的推理过程产生了多种不同推理路径及答案，最终大语言模型自行选择了一个自洽的回答。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/image-20231110170209475.png" alt="image-20231110170209475">&lt;/p>
&lt;h1 id="9小结">9.小结&lt;/h1>
&lt;p>本文阐述了多种提示词常用技巧，实战中需要综合应用上述技巧，根据场景激发大语言模型的不同能力：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>技巧1：构建合理的提示词结构&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>技巧2：设定角色&lt;/strong>，设定LLM角色、设定提问者角色。&lt;/li>
&lt;li>&lt;strong>技巧3：分隔符划分指令和内容&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>技巧4：指定文本判断条件&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>技巧5：指定输出格式&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>技巧6：Few-Shot。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>技巧7：CoT&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>技巧8：Self-Consistency&lt;/strong>。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记24-提示词解读1-提示词基本概念</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link><pubDate>Thu, 09 Nov 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid><description>&lt;p>吴恩达老师的《ChatGPT Prompt Engineering for Developers》是一门学习提示词工程不错的课程，我们接下来用几篇文章来解读这门课程。&lt;/p>
&lt;h1 id="1什么是提示词工程">1.什么是提示词工程&lt;/h1>
&lt;p>首先看1个问题：&lt;/p>
&lt;ul>
&lt;li>向大语言模型输入&amp;quot;一二三四五&amp;rdquo;，它很可能回答&amp;quot;上山打老虎&amp;rdquo;。但，我们的意图是希望它把&amp;quot;一二三四五&amp;rdquo;&lt;strong>翻译成英文&lt;/strong>，怎么办？&lt;/li>
&lt;/ul>
&lt;p>我们的解决方法是：&lt;/p>
&lt;ul>
&lt;li>问大语言模型：&amp;ldquo;请将如下文字翻译为英文：一二三四五&amp;rdquo;，大语言模型就会回答&amp;quot;One Two Three Four Five&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>上述解决方法本质是什么呢？&lt;/p>
&lt;ul>
&lt;li>即在问题里明确表达&lt;strong>期望大语言模型如何处理&lt;/strong>一段文字。&lt;/li>
&lt;li>明确表达期望大语言模型做什么，就是一条指令，也就是&lt;strong>提示词&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>提示词理论的出现，源于两个假设：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设1&lt;/strong>：大语言模型已经掌握了很多世界知识，但由于知道的太多，一时想不起来。&lt;/li>
&lt;li>&lt;strong>假设2&lt;/strong>：自然语言存在二义性，需要更多的提示，才能准确地向大语言模型表达人类的真实意图。&lt;/li>
&lt;/ul>
&lt;p>因此，人类通过提示词，可以唤起大语言模型对已有知识的记忆，也可以让大语言模型更加准确地理解人类意图。&lt;/p>
&lt;p>开发提示词、优化提示词的过程，被称为&lt;strong>提示词工程&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>提示词工程包含了诸多工程方法，如：设计有效的提示策略、优化提示词表达等。&lt;/li>
&lt;/ul>
&lt;p>伴随着大语言模型的发展，提示词工程也形成了一套体系化的工程方法。它成为AI领域的热点技术之一，是AI工程师的必备技能。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231110144405400.png" alt="From《Pre-train, Prompt, and Predict: A Systematic Survey ofPrompting Methods in Natural Language Processing》">&lt;/p>
&lt;h1 id="2提示词基本原则">2.提示词基本原则&lt;/h1>
&lt;h2 id="1原则1简洁明确">(1)原则1：简洁明确&lt;/h2>
&lt;p>假想两个人类在说话，两人的表达能力有限，词不达意、含糊其辞、鸡同鸭讲&amp;hellip;，最终就导致两人之间的沟通极其困难。&lt;/p>
&lt;p>与大语言模型交互，和人类交流类似，也需要简洁明确的同问，向大语言模型清晰表达意图。&lt;/p>
&lt;p>我们来看一个例子：&lt;/p>
&lt;ul>
&lt;li>我们让ChatGPT写一首诗，它以立冬为题生成一首诗：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231110145755795.png" alt="image-20231110145755795">&lt;/p>
&lt;ul>
&lt;li>如果我们希望这首诗是五言绝句，则进一步这样提问，可以看到ChatGPT生成了一首以立冬为题的五言绝句。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231110145923990.png" alt="image-20231110145923990">&lt;/p>
&lt;p>上述示例，可以看出：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>写一首诗&lt;/strong>是一条&lt;strong>简洁的指令&lt;/strong>。&lt;/li>
&lt;li>相较于写一首诗，&lt;strong>写一首五言绝句&lt;/strong>是一条更加&lt;strong>简洁明确的指令&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="2原则2迭代式提示词开发">(2)原则2：迭代式提示词开发&lt;/h2>
&lt;p>我们要知道两个事实：&lt;/p>
&lt;ul>
&lt;li>没有可以适应所有场景的完美提示词，需要我们针对不同场景，开发不同的提示词。&lt;/li>
&lt;li>即使在一个很小的场景下存在完美提示词，我们也无法一次性就找到它。&lt;/li>
&lt;/ul>
&lt;p>针对某个场景，寻找提示词的过程，就是&lt;strong>迭代式提示词开发&lt;/strong>。&lt;/p>
&lt;p>迭代式提示词开发的过程如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Idea阶段&lt;/strong>：针对特定场景，设计初始提示词。&lt;/li>
&lt;li>&lt;strong>Implementation阶段&lt;/strong>：将初始提示词传递给大语言模型，获得返回结果(&lt;code>Experimental result&lt;/code>)。&lt;/li>
&lt;li>&lt;strong>Error Analysis阶段&lt;/strong>：分析返回结果，思考改进提示词的方法，重新进入&lt;strong>Idea阶段&lt;/strong>，直到找到特定场景下的完美提示词。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231108102255487.png" alt="From 吴恩达提示词课程">&lt;/p>
&lt;p>我们再来看一个示例：我们需要一段用于营销的产品介绍。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Idea阶段&lt;/strong>：针对产品介绍场景，设计初始提示词。&lt;/li>
&lt;li>&lt;strong>Implementation阶段&lt;/strong>：将初始提示词传递给大语言模型，获得返回结果。可以看到ChatGPT返回了一段不错的文案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231108110737288.png" alt="image-20231108110737288">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Error Analysis阶段&lt;/strong>：从上一步获得的ChatGPT文案内容太长，我们思考改进提示词的方法是进一步明确字数要求。&lt;/li>
&lt;li>&lt;strong>Idea阶段&lt;/strong>：根据上一阶段的改进思路，编写改进后的提示词。&lt;/li>
&lt;li>&lt;strong>Implementation阶段&lt;/strong>：将改进后的提示词传递给大语言模型，获得返回结果。可以看到ChatGPT返回了一段简短的文案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231108110819852.png" alt="image-20231108110819852">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Error Analysis阶段&lt;/strong>：从上一步获得的ChatGPT文案没有突出要点，我们思考改进提示词的方法是限定明确要突出的关键点。&lt;/li>
&lt;li>&lt;strong>Idea阶段&lt;/strong>：根据上一阶段的改进思路，编写改进后的提示词。&lt;/li>
&lt;li>&lt;strong>Implementation阶段&lt;/strong>：将改进后的提示词传递给大语言模型，获得返回结果。可以看到ChatGPT返回了一段简短、要点突出的文案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/image-20231108110848960.png" alt="image-20231108110848960">&lt;/p>
&lt;p>这就是&lt;strong>迭代式提示词开发&lt;/strong>的流程，核心要点是&lt;strong>多次迭代&lt;/strong>。&lt;/p>
&lt;h2 id="3原则3选择合适的提示词风格">(3)原则3：选择合适的提示词风格&lt;/h2>
&lt;p>提示词工程是一种&lt;code>Instruct-Tuning&lt;/code>技术，提示词的风格包含：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Instruction&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>初学者极易混淆这两种风格，我们来看一个例子：&lt;/p>
&lt;ul>
&lt;li>幼儿园老师希望引导小朋友唱歌，于是她说：在小小的花园里面挖呀挖呀挖&amp;hellip;，老师此时停顿下来并对小朋友投去了期待的目光，小朋友按耐不住激动的心情，接下句：种小小的种子开小小的花——这就是&lt;strong>Prompt(提示)&lt;/strong>。&lt;/li>
&lt;li>幼儿园老师希望小朋友背诵五言绝句，于是她说：请背诵《鹅鹅鹅》，小朋友在老师又一次期待的目光下，整齐划一地背诵出&amp;quot;鹅鹅鹅曲项向天歌&amp;hellip;&amp;quot;——这就是&lt;strong>Instruction(指令)&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>我们了解了提示词的两大风格后，就要针对不同场景、不同意图，选择不同的提示词风格。&lt;/p>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>提示词工程是一项热门技术，本文对提示词工程基本概念进行了阐述：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示词工程是开发和优化提示词的过程&lt;/strong>，包括选择合适的提示词、设计有效的提示策略，以及优化提示词的表达方式等。&lt;/li>
&lt;li>提示词三个基本原则：
&lt;ul>
&lt;li>&lt;strong>简洁、明确&lt;/strong>&lt;/li>
&lt;li>&lt;strong>迭代式提示词开发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>选择合适的提示词风格&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记23-让我们一起来翻译吴恩达LLM课程</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BEllm%E8%AF%BE%E7%A8%8B/</link><pubDate>Thu, 02 Nov 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BEllm%E8%AF%BE%E7%A8%8B/</guid><description>&lt;p>哈喽小伙伴，我是小牛，初次见面，请多关照。&lt;/p>
&lt;p>最近跟着猴哥的学习笔记各种手撸GPT，理解了LLM原理。&lt;/p>
&lt;p>但我跟GPT的交情好像只停留在say hello的程度，说好的能替代“表哥”、“码农”、“ppt经理”的能力呢？&lt;/p>
&lt;p>然后chatGPT跟我说，有个叫吴恩达的大咖，有好几门神课，理论+实践，易学易懂，学会了就可以在职场上大放异彩。那还犹豫什么，卷！&lt;/p>
&lt;p>这些课程都是英文视频，为了方便学习，我们准备把它们翻译成中文。&lt;/p>
&lt;p>欢迎感兴趣的小伙伴们一起来翻译。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BELLM%E8%AF%BE%E7%A8%8B/image-20231102181630937.png" alt="image-20231102181630937">&lt;/p>
&lt;h1 id="1已翻译的课程">1.已翻译的课程&lt;/h1>
&lt;ul>
&lt;li>《提示词工程 by 吴恩达》：https://jherculesqz.gitbook.io/chatgpt-prompt-engineering-for-developers-1&lt;/li>
&lt;li>《LangChain by 吴恩达》：https://jherculesqz.gitbook.io/langchain-by-wu-en-da&lt;/li>
&lt;/ul>
&lt;h2 id="11提示词工程-by-吴恩达简介">1.1.《提示词工程 by 吴恩达》简介&lt;/h2>
&lt;p>GPT神器虽好，但要会用。本课程将介绍：&lt;/p>
&lt;ul>
&lt;li>如何构建清晰、明确的提示词，有哪些技巧&lt;/li>
&lt;li>如何通过提示词发掘大语言模型总结、推理、转换以及生成文本的能力&lt;/li>
&lt;li>动手做一个自己的聊天机器人&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BELLM%E8%AF%BE%E7%A8%8B/image-20231102181807030.png" alt="image-20231102181807030">&lt;/p>
&lt;h2 id="12langchain-by-吴恩达简介">1.2.《LangChain by 吴恩达》简介&lt;/h2>
&lt;p>LangChain是开发LLM应用的开发框架，是当红炸子鸡。本课程将介绍：&lt;/p>
&lt;ul>
&lt;li>什么是LangChain&lt;/li>
&lt;li>LangChain有哪些Chain&lt;/li>
&lt;li>如何用LangChain做聊天模型、QA问答系统&lt;/li>
&lt;li>如何用LangChain的agent功能使能大模型的推理能力&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BELLM%E8%AF%BE%E7%A8%8B/image-20231102181929184.png" alt="image-20231102181929184">&lt;/p>
&lt;h1 id="2待翻译的课程">2.待翻译的课程&lt;/h1>
&lt;p>我们已经计划翻译的课程、论文有：&lt;/p>
&lt;ul>
&lt;li>《Finetuning Large Language Models》&lt;/li>
&lt;li>《Attention Is All You Need》&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>如果小伙伴们对其它LLM相关的文献、课程感兴趣，欢迎后台联系我们。&lt;/p>
&lt;blockquote>
&lt;p>免责声明：我们的翻译出于对技术的执着和个人兴趣，开源免费，仅供个人学习，不得商用。&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记22-LangChain之Agent，对LLM的抽象5</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-langchain%E4%B9%8Bagent%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A15/</link><pubDate>Tue, 31 Oct 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-langchain%E4%B9%8Bagent%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A15/</guid><description>&lt;p>业界目前的AI助手应用集中于两大功能：&lt;/p>
&lt;ul>
&lt;li>基于垂直领域知识的&lt;strong>智能问答&lt;/strong>&lt;/li>
&lt;li>基于垂直领域知识的&lt;strong>智能行动&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>在《【chatGPT】学习笔记21-LangChain之Retrieval，对LLM的抽象4》中，着重介绍了&lt;strong>智能问答&lt;/strong>依赖的技术点。&lt;/p>
&lt;p>本文重点介绍与&lt;strong>智能行动&lt;/strong>依赖的技术点。&lt;/p>
&lt;h1 id="1什么是react">1.什么是ReAct&lt;/h1>
&lt;h2 id="1基本概念">(1)基本概念&lt;/h2>
&lt;p>常见的LLM推理模式有两种：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Reason Only&lt;/strong>：比如CoT，利用提示词诱导LLM进行逻辑推理，进而回答偏数理逻辑类的问题。&lt;/li>
&lt;li>&lt;strong>Act Only&lt;/strong>：比如指令微调提到的，明确在问题中带有指令。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ReAct&lt;/strong>提出了一种新的推理模式——&lt;strong>Reason + Act&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>LLM推理出&amp;rdquo;&lt;strong>把大象装进冰箱需要几步&lt;/strong>&amp;quot;，&lt;/li>
&lt;li>再针对待执行的每一步&lt;strong>选择合适的工具&lt;/strong>，&lt;/li>
&lt;li>在执行当前步骤后，通过环境的反馈，&lt;strong>决策下一步是什么&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-LangChain%E4%B9%8BAgent%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A15/image-20231031152602671.png" alt="image-20231031152602671">&lt;/p>
&lt;h2 id="2react优势">(2)ReAct优势&lt;/h2>
&lt;p>在论文《ReAct: Synergizing Reasoning and Acting in Language Models》中，阐述了ReAct的训练效果比&lt;code>Reson Only&lt;/code>和&lt;code>Act Only&lt;/code>都好：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-LangChain%E4%B9%8BAgent%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A15/image-20231031153243968.png" alt="image-20231031153243968">&lt;/p>
&lt;h2 id="3agents内部结构">(3)Agents内部结构&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>数据结构&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>AgentAction&lt;/code>：表示Agent应采取的行为。包含1个&lt;code>tool&lt;/code>属性和一个&lt;code>tool_input&lt;/code>属性，表示这个行为可采用的工具及工具输入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>AgentFinish&lt;/code>：表示Agent完成后的返回结果。包含1个&lt;code>return_values&lt;/code>属性，字典类型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>intermediate_steps&lt;/code>：表示之前的Agent行为和返回结果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Agent&lt;/strong>：智能体，可以分析出&amp;rdquo;&lt;strong>将大象放进冰箱里需要几步&lt;/strong>&amp;quot;，内部实现&lt;strong>依赖LLM的逻辑推理能力&lt;/strong>。它的输入包括：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>工具列表&lt;/strong>：&lt;code>List of available tools&lt;/code>，Agent可以用的工具集合。&lt;/li>
&lt;li>&lt;strong>用户输入&lt;/strong>：User input。&lt;/li>
&lt;li>&lt;strong>之前的Agent行为和返回结果&lt;/strong>：Any previously executed steps，就是&lt;code>intermediate_steps&lt;/code>对象。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tools&lt;/strong>：它是Agent某个行为可以使用的工具。每个工具需要有两个明确信息：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>工具的可用性&lt;/strong>：如&amp;quot;使用Google搜索&amp;quot;是一个工具，这个工具本身要真的可用、可靠。&lt;/li>
&lt;li>&lt;strong>工具的准确描述&lt;/strong>：对工具的作用有准确的描述，这段描述最终会变成提示词，帮助Agent知道要执行什么行为的时候，应该选择哪个工具更合适。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>AgentExecutor&lt;/strong>：Agent执行器。能够调用&lt;code>Agent&lt;/code>，不断获得下一步&lt;code>action(行为)&lt;/code>并执行。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>除了&lt;code>AgentExecutor&lt;/code>，LangChain还提供了实验性质的其它Agent执行器：&lt;/p>
&lt;ul>
&lt;li>Plan-and-execute Agent&lt;/li>
&lt;li>Baby AGI&lt;/li>
&lt;li>Auto GPT&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>AgentExecutor&lt;/code>核心代码如下：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">next_action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">agent&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_action&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">next_action&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">AgentFinish&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">observation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_action&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">next_action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">agent&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_action&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_action&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">observation&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">next_action&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="4langchain已经支持的agent和tool列表">(4)LangChain已经支持的Agent和Tool列表&lt;/h2>
&lt;ul>
&lt;li>LangChain已经提供了很多&lt;code>Agent&lt;/code>和&lt;code>Tools&lt;/code>，具体如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-LangChain%E4%B9%8BAgent%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A15/image-20231031151549996.png" alt="image-20231031151549996">&lt;/p>
&lt;h1 id="2代码示例">2.代码示例&lt;/h1>
&lt;p>&lt;code>Agent&lt;/code>是当前LLM App的一个热点方向，展开阐述会有很多的内容。本文仅通过1个经典问题代码示例，为各位小伙伴建立&lt;code>Agent&lt;/code>的宏观认识。后续本专栏再详细阐述&lt;code>Agent&lt;/code>开发的内容。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：周杰伦的老婆是谁？她现在的年龄的0.43次方等于多少？&lt;/li>
&lt;li>&lt;strong>代码&lt;/strong>：
&lt;ul>
&lt;li>创建了&lt;code>tools&lt;/code>工具集对象&lt;/li>
&lt;li>初始化了&lt;code>agent&lt;/code>对象，&lt;code>agent&lt;/code>对象包含1个&lt;code>LLM&lt;/code>和1个&lt;code>tools&lt;/code>对象&lt;/li>
&lt;li>执行&lt;code>agent&lt;/code>对象的&lt;code>run&lt;/code>方法，由&lt;code>LLM&lt;/code>自行开展行动，直到获得最终答案。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>运行结果分析&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>&lt;code>LLM&lt;/code>首先明确任务目标&lt;/strong>：背后应该是由一个复杂的提示词驱动的。&lt;/li>
&lt;li>&lt;strong>&lt;code>LLM&lt;/code>再分解出三个步骤&lt;/strong>：每个步骤的输出，都会作为下一个步骤的输入。&lt;/li>
&lt;li>&lt;strong>&lt;code>LLM&lt;/code>最后汇总结果&lt;/strong>：&lt;code>LLM&lt;/code>判断如果达成目标，则给出最终答案，并终止执行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-LangChain%E4%B9%8BAgent%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A15/image-20231031154927730.png" alt="image-20231031154927730">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-LangChain%E4%B9%8BAgent%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A15/image-20231031160426352.png" alt="image-20231031160426352">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文关键要点如下：&lt;/p>
&lt;ul>
&lt;li>简述了&lt;code>Agent&lt;/code>理论基础&lt;code>ReAct&lt;/code>&lt;/li>
&lt;li>描述了论文&lt;code>ReAct: Synergizing Reasoning and Acting in Language Models&lt;/code>中的实验现象、&lt;code>ReAct&lt;/code>的优势。&lt;/li>
&lt;li>给出了一个有关&lt;strong>大语言模型不知道的有时效性的知识&lt;/strong>的问题，&lt;code>Agent&lt;/code>如何&lt;strong>利用大语言模型的推理能力&lt;/strong>解决此类问题的代码示例。&lt;/li>
&lt;/ul>
&lt;p>如果说上篇文章的&lt;code>Retrieval&lt;/code>帮助AI助手具备了&lt;strong>智能问答&lt;/strong>能力，这篇文章的&lt;code>Agent&lt;/code>则帮助AI助手实现了&lt;strong>智能行动&lt;/strong>。&lt;/p>
&lt;p>所以，&lt;code>Agent&lt;/code>这个方向是目前开发LLM应用的诸多厂商的关注热点，这里还有一些未及展开讨论的内容：&lt;/p>
&lt;ul>
&lt;li>如何自定义1个满足垂直领域的&lt;strong>自定义Agent&lt;/strong>？&lt;/li>
&lt;li>如何自定义1组满足垂直领域的&lt;strong>自定义Tool&lt;/strong>？&lt;/li>
&lt;/ul>
&lt;p>这些高级话题，留待本专栏后续分解，敬请期待。&lt;/p></description></item><item><title>【chatGPT】学习笔记21-LangChain之Retrieval，对LLM的抽象4</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-langchain%E4%B9%8Bretrieval%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A14/</link><pubDate>Mon, 30 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-langchain%E4%B9%8Bretrieval%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A14/</guid><description>&lt;p>本专栏通过解读了Transformer模型，实现简版GPT后，帮大家建立了对NLP原理、关键技术的理解。&lt;/p>
&lt;p>接下来，我们再关注一下应用层面的技术——如何开发LLM App。&lt;/p>
&lt;p>前面我们了解了LangChain的三大重要模块**&amp;ldquo;Model I/O、Memory、Chain&amp;rdquo;**，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LangChain的Model I/O&lt;/strong>：详见《【chatGPT】学习笔记10-LangChain之Model I/O，对LLM的抽象1》&lt;/li>
&lt;li>&lt;strong>LangChain的Memory&lt;/strong>：详见《【chatGPT】学习笔记14-LangChain之Memory，对LLM的抽象2》&lt;/li>
&lt;li>&lt;strong>LangChain的Chain&lt;/strong>：详见《【chatGPT】学习笔记15-LangChain之Chain，对LLM的抽象3》&lt;/li>
&lt;/ul>
&lt;p>本文继续展示LangChain的第四大模块&amp;rdquo;&lt;strong>Retrieval&lt;/strong>&amp;quot;。&lt;/p>
&lt;blockquote>
&lt;p>LangChain曾经把此模块称为Data Connection，其实更为贴切。&lt;/p>
&lt;/blockquote>
&lt;h1 id="1整体流程">1.整体流程&lt;/h1>
&lt;p>如果我们要开发一个基于LLM的AI问答系统，整体流程如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文档加载&lt;/strong>：加载各类文档。&lt;/li>
&lt;li>&lt;strong>文档转换&lt;/strong>：将文档分割为一段段的文本。&lt;/li>
&lt;li>&lt;strong>文档向量化&lt;/strong>：将文本进行词嵌入，并存储于向量数据库。&lt;/li>
&lt;li>&lt;strong>文档检索&lt;/strong>：针对用户的问题进行向量检索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231030140913593.png" alt="image-20231030140913593">&lt;/p>
&lt;p>我们接下来看看，LangChain如何支撑上述整体流程。&lt;/p>
&lt;h1 id="2文档加载器">2.文档加载器&lt;/h1>
&lt;h2 id="21基本概念">2.1.基本概念&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>LangChain提供的文档加载器(&lt;code>Document loaders&lt;/code>)，支持从不同数据源加载数据，最终输出为&lt;code>Document&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LangChain的厉害之处在于：它支持了100+的数据源，丰富程度基本覆盖了所有的数据源。具体支持的Loader如下：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231030143143187.png" alt="image-20231030143143187">&lt;/p>
&lt;p>接下来，我们通过阅读LangChain的源码，深入理解文档加载器。&lt;/p>
&lt;h2 id="22document">2.2.Document&lt;/h2>
&lt;h3 id="1源码解读">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Document&lt;/code>类是文档加载器的输出。&lt;/li>
&lt;li>&lt;code>Document&lt;/code>类的代码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/document.py&lt;/li>
&lt;li>&lt;code>Document&lt;/code>类的关键代码如下：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Document&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Serializable&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Class for storing a piece of text and associated metadata.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">page_content&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;String text.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">文档内容&lt;/span>
&lt;span class="n">metadata&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default_factory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">dict&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Arbitrary metadata about the page content (e.g., source, relationships to other
&lt;/span>&lt;span class="s2"> documents, etc.).
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">type&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Literal&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;Document&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Document&amp;#34;&lt;/span>
&lt;span class="nd">@classmethod&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">is_lc_serializable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">cls&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return whether this class is serializable.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">True&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="23baseloader">2.3.BaseLoader&lt;/h2>
&lt;h3 id="1源码解读-1">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;code>BaseLoader&lt;/code>类是各种不同&lt;code>Loader&lt;/code>的基类。&lt;/li>
&lt;li>&lt;code>BaseLoader&lt;/code>类的代码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/document_loaders/base.py&lt;/li>
&lt;li>&lt;code>BaseLoader&lt;/code>类的关键代码如下：关键方法是&lt;code>load&lt;/code>、&lt;code>load_and_split&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">BaseLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Interface for Document Loader.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Implementations should implement the lazy-loading method using generators
&lt;/span>&lt;span class="s2"> to avoid loading all Documents into memory at once.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The `load` method will remain as is for backwards compatibility, but its
&lt;/span>&lt;span class="s2"> implementation should be just `list(self.lazy_load())`.
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="c1"># Sub-classes should implement this method&lt;/span>
&lt;span class="c1"># as return list(self.lazy_load()).&lt;/span>
&lt;span class="c1"># This method returns a List which is materialized in memory.&lt;/span>
&lt;span class="nd">@abstractmethod&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load data into Document objects.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">加载数据并将其转换为文档对象&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_and_split&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text_splitter&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">TextSplitter&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load Documents and split into chunks. Chunks are returned as Documents.
&lt;/span>&lt;span class="s2"> 加载文档并分割成块
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Args:
&lt;/span>&lt;span class="s2"> text_splitter: TextSplitter instance to use for splitting documents.
&lt;/span>&lt;span class="s2"> Defaults to RecursiveCharacterTextSplitter.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Returns:
&lt;/span>&lt;span class="s2"> List of Documents.
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">text_splitter&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_text_splitter&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">TextSplitter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RecursiveCharacterTextSplitter&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_text_splitter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">text_splitter&lt;/span>
&lt;span class="n">docs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">_text_splitter&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split_documents&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">docs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Attention: This method will be upgraded into an abstractmethod once it&amp;#39;s&lt;/span>
&lt;span class="c1"># implemented in all the existing subclasses.&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">lazy_load&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Iterator&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;A lazy loader for Documents.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">NotImplementedError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;{self.__class__.__name__} does not implement lazy_load()&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="24textloader">2.4.TextLoader&lt;/h2>
&lt;h3 id="1源码解读-2">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;code>TextLoader&lt;/code>类是用于加载文本的Loader，继承于&lt;code>BaseLoader&lt;/code>。&lt;/li>
&lt;li>&lt;code>TextLoader&lt;/code>类的代码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/document_loaders/text.py&lt;/li>
&lt;li>&lt;code>TextLoader&lt;/code>类的关键代码如下：关键方法是实现了自己的&lt;code>load&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">TextLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseLoader&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load text file.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Args:
&lt;/span>&lt;span class="s2"> file_path: Path to the file to load.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> encoding: File encoding to use. If `None`, the file will be loaded
&lt;/span>&lt;span class="s2"> with the default system encoding.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> autodetect_encoding: Whether to try to autodetect the file encoding
&lt;/span>&lt;span class="s2"> if the specified encoding fails.
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">file_path&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">encoding&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">autodetect_encoding&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Initialize with file path.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">file_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">file_path&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encoding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">encoding&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">autodetect_encoding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">autodetect_encoding&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load from file path.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">file_path&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">encoding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encoding&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">UnicodeDecodeError&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">autodetect_encoding&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">detected_encodings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">detect_file_encodings&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">file_path&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">encoding&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">detected_encodings&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">logger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">debug&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Trying encoding: {encoding.encoding}&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">file_path&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">encoding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">encoding&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encoding&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">break&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">UnicodeDecodeError&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">continue&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">RuntimeError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Error loading {self.file_path}&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">e&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">Exception&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">RuntimeError&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Error loading {self.file_path}&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">e&lt;/span>
&lt;span class="n">metadata&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;source&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">file_path&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">page_content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">metadata&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">metadata&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2代码示例">(2)代码示例&lt;/h3>
&lt;ul>
&lt;li>构造TextLoader，读取txt文件，返回docs对象。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031071416742.png" alt="image-20231031071416742">&lt;/p>
&lt;ul>
&lt;li>观测docs对象&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031071531911.png" alt="image-20231031071531911">&lt;/p>
&lt;h2 id="25arxivloader">2.5.ArxivLoader&lt;/h2>
&lt;h3 id="1源码解读-3">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;code>ArxivLoader&lt;/code>类是用于加载Arxiv的Loader，继承于&lt;code>BaseLoader&lt;/code>。&lt;/li>
&lt;li>&lt;code>ArxivLoader&lt;/code>类的代码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/document_loaders/arxiv.py&lt;/li>
&lt;li>&lt;code>ArxivLoader&lt;/code>类的关键代码如下：关键方法是实现了自己的&lt;code>load&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ArxivLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseLoader&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load a query result from `Arxiv`.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The loader converts the original PDF format into the text.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Args:
&lt;/span>&lt;span class="s2"> Supports all arguments of `ArxivAPIWrapper`.
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">doc_content_chars_max&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span>
&lt;span class="p">):&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">query&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ArxivAPIWrapper&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">doc_content_chars_max&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">doc_content_chars_max&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2代码示例-1">(2)代码示例&lt;/h3>
&lt;ul>
&lt;li>读取指定版本号的论文，得到docs对象，docs对象包含了指定版本号的相关论文信息。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031071610543.png" alt="image-20231031071610543">&lt;/p>
&lt;h2 id="26unstructuredurlloader">2.6.UnstructuredURLLoader&lt;/h2>
&lt;h3 id="1源码解读-4">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;code>UnstructuredURLLoader&lt;/code>类是用于加载指定URL的Loader，继承于&lt;code>BaseLoader&lt;/code>。&lt;/li>
&lt;li>&lt;code>UnstructuredURLLoader&lt;/code>类的代码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/document_loaders/url.py&lt;/li>
&lt;li>&lt;code>UnstructuredURLLoader&lt;/code>类的关键代码如下：关键方法是实现了自己的&lt;code>load&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt"> 10
&lt;/span>&lt;span class="lnt"> 11
&lt;/span>&lt;span class="lnt"> 12
&lt;/span>&lt;span class="lnt"> 13
&lt;/span>&lt;span class="lnt"> 14
&lt;/span>&lt;span class="lnt"> 15
&lt;/span>&lt;span class="lnt"> 16
&lt;/span>&lt;span class="lnt"> 17
&lt;/span>&lt;span class="lnt"> 18
&lt;/span>&lt;span class="lnt"> 19
&lt;/span>&lt;span class="lnt"> 20
&lt;/span>&lt;span class="lnt"> 21
&lt;/span>&lt;span class="lnt"> 22
&lt;/span>&lt;span class="lnt"> 23
&lt;/span>&lt;span class="lnt"> 24
&lt;/span>&lt;span class="lnt"> 25
&lt;/span>&lt;span class="lnt"> 26
&lt;/span>&lt;span class="lnt"> 27
&lt;/span>&lt;span class="lnt"> 28
&lt;/span>&lt;span class="lnt"> 29
&lt;/span>&lt;span class="lnt"> 30
&lt;/span>&lt;span class="lnt"> 31
&lt;/span>&lt;span class="lnt"> 32
&lt;/span>&lt;span class="lnt"> 33
&lt;/span>&lt;span class="lnt"> 34
&lt;/span>&lt;span class="lnt"> 35
&lt;/span>&lt;span class="lnt"> 36
&lt;/span>&lt;span class="lnt"> 37
&lt;/span>&lt;span class="lnt"> 38
&lt;/span>&lt;span class="lnt"> 39
&lt;/span>&lt;span class="lnt"> 40
&lt;/span>&lt;span class="lnt"> 41
&lt;/span>&lt;span class="lnt"> 42
&lt;/span>&lt;span class="lnt"> 43
&lt;/span>&lt;span class="lnt"> 44
&lt;/span>&lt;span class="lnt"> 45
&lt;/span>&lt;span class="lnt"> 46
&lt;/span>&lt;span class="lnt"> 47
&lt;/span>&lt;span class="lnt"> 48
&lt;/span>&lt;span class="lnt"> 49
&lt;/span>&lt;span class="lnt"> 50
&lt;/span>&lt;span class="lnt"> 51
&lt;/span>&lt;span class="lnt"> 52
&lt;/span>&lt;span class="lnt"> 53
&lt;/span>&lt;span class="lnt"> 54
&lt;/span>&lt;span class="lnt"> 55
&lt;/span>&lt;span class="lnt"> 56
&lt;/span>&lt;span class="lnt"> 57
&lt;/span>&lt;span class="lnt"> 58
&lt;/span>&lt;span class="lnt"> 59
&lt;/span>&lt;span class="lnt"> 60
&lt;/span>&lt;span class="lnt"> 61
&lt;/span>&lt;span class="lnt"> 62
&lt;/span>&lt;span class="lnt"> 63
&lt;/span>&lt;span class="lnt"> 64
&lt;/span>&lt;span class="lnt"> 65
&lt;/span>&lt;span class="lnt"> 66
&lt;/span>&lt;span class="lnt"> 67
&lt;/span>&lt;span class="lnt"> 68
&lt;/span>&lt;span class="lnt"> 69
&lt;/span>&lt;span class="lnt"> 70
&lt;/span>&lt;span class="lnt"> 71
&lt;/span>&lt;span class="lnt"> 72
&lt;/span>&lt;span class="lnt"> 73
&lt;/span>&lt;span class="lnt"> 74
&lt;/span>&lt;span class="lnt"> 75
&lt;/span>&lt;span class="lnt"> 76
&lt;/span>&lt;span class="lnt"> 77
&lt;/span>&lt;span class="lnt"> 78
&lt;/span>&lt;span class="lnt"> 79
&lt;/span>&lt;span class="lnt"> 80
&lt;/span>&lt;span class="lnt"> 81
&lt;/span>&lt;span class="lnt"> 82
&lt;/span>&lt;span class="lnt"> 83
&lt;/span>&lt;span class="lnt"> 84
&lt;/span>&lt;span class="lnt"> 85
&lt;/span>&lt;span class="lnt"> 86
&lt;/span>&lt;span class="lnt"> 87
&lt;/span>&lt;span class="lnt"> 88
&lt;/span>&lt;span class="lnt"> 89
&lt;/span>&lt;span class="lnt"> 90
&lt;/span>&lt;span class="lnt"> 91
&lt;/span>&lt;span class="lnt"> 92
&lt;/span>&lt;span class="lnt"> 93
&lt;/span>&lt;span class="lnt"> 94
&lt;/span>&lt;span class="lnt"> 95
&lt;/span>&lt;span class="lnt"> 96
&lt;/span>&lt;span class="lnt"> 97
&lt;/span>&lt;span class="lnt"> 98
&lt;/span>&lt;span class="lnt"> 99
&lt;/span>&lt;span class="lnt">100
&lt;/span>&lt;span class="lnt">101
&lt;/span>&lt;span class="lnt">102
&lt;/span>&lt;span class="lnt">103
&lt;/span>&lt;span class="lnt">104
&lt;/span>&lt;span class="lnt">105
&lt;/span>&lt;span class="lnt">106
&lt;/span>&lt;span class="lnt">107
&lt;/span>&lt;span class="lnt">108
&lt;/span>&lt;span class="lnt">109
&lt;/span>&lt;span class="lnt">110
&lt;/span>&lt;span class="lnt">111
&lt;/span>&lt;span class="lnt">112
&lt;/span>&lt;span class="lnt">113
&lt;/span>&lt;span class="lnt">114
&lt;/span>&lt;span class="lnt">115
&lt;/span>&lt;span class="lnt">116
&lt;/span>&lt;span class="lnt">117
&lt;/span>&lt;span class="lnt">118
&lt;/span>&lt;span class="lnt">119
&lt;/span>&lt;span class="lnt">120
&lt;/span>&lt;span class="lnt">121
&lt;/span>&lt;span class="lnt">122
&lt;/span>&lt;span class="lnt">123
&lt;/span>&lt;span class="lnt">124
&lt;/span>&lt;span class="lnt">125
&lt;/span>&lt;span class="lnt">126
&lt;/span>&lt;span class="lnt">127
&lt;/span>&lt;span class="lnt">128
&lt;/span>&lt;span class="lnt">129
&lt;/span>&lt;span class="lnt">130
&lt;/span>&lt;span class="lnt">131
&lt;/span>&lt;span class="lnt">132
&lt;/span>&lt;span class="lnt">133
&lt;/span>&lt;span class="lnt">134
&lt;/span>&lt;span class="lnt">135
&lt;/span>&lt;span class="lnt">136
&lt;/span>&lt;span class="lnt">137
&lt;/span>&lt;span class="lnt">138
&lt;/span>&lt;span class="lnt">139
&lt;/span>&lt;span class="lnt">140
&lt;/span>&lt;span class="lnt">141
&lt;/span>&lt;span class="lnt">142
&lt;/span>&lt;span class="lnt">143
&lt;/span>&lt;span class="lnt">144
&lt;/span>&lt;span class="lnt">145
&lt;/span>&lt;span class="lnt">146
&lt;/span>&lt;span class="lnt">147
&lt;/span>&lt;span class="lnt">148
&lt;/span>&lt;span class="lnt">149
&lt;/span>&lt;span class="lnt">150
&lt;/span>&lt;span class="lnt">151
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">UnstructuredURLLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseLoader&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load files from remote URLs using `Unstructured`.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Use the unstructured partition function to detect the MIME type
&lt;/span>&lt;span class="s2"> and route the file to the appropriate partitioner.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> You can run the loader in one of two modes: &amp;#34;single&amp;#34; and &amp;#34;elements&amp;#34;.
&lt;/span>&lt;span class="s2"> If you use &amp;#34;single&amp;#34; mode, the document will be returned as a single
&lt;/span>&lt;span class="s2"> langchain Document object. If you use &amp;#34;elements&amp;#34; mode, the unstructured
&lt;/span>&lt;span class="s2"> library will split the document into elements such as Title and NarrativeText.
&lt;/span>&lt;span class="s2"> You can pass in additional unstructured kwargs after mode to apply
&lt;/span>&lt;span class="s2"> different unstructured settings.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Examples
&lt;/span>&lt;span class="s2"> --------
&lt;/span>&lt;span class="s2"> from langchain.document_loaders import UnstructuredURLLoader
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> loader = UnstructuredURLLoader(
&lt;/span>&lt;span class="s2"> ursl=[&amp;#34;&amp;lt;url-1&amp;gt;&amp;#34;, &amp;#34;&amp;lt;url-2&amp;gt;&amp;#34;], mode=&amp;#34;elements&amp;#34;, strategy=&amp;#34;fast&amp;#34;,
&lt;/span>&lt;span class="s2"> )
&lt;/span>&lt;span class="s2"> docs = loader.load()
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> References
&lt;/span>&lt;span class="s2"> ----------
&lt;/span>&lt;span class="s2"> https://unstructured-io.github.io/unstructured/bricks.html#partition
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">urls&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">continue_on_failure&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">mode&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;single&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">show_progress_bar&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="o">**&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Initialize with file path.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">unstructured&lt;/span> &lt;span class="c1"># noqa:F401&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">unstructured.__version__&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">__version__&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">__unstructured_version__&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">__unstructured_version__&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">ImportError&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ImportError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s2">&amp;#34;unstructured package not found, please install it with &amp;#34;&lt;/span>
&lt;span class="s2">&amp;#34;`pip install unstructured`&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_validate_mode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mode&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mode&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mode&lt;/span>
&lt;span class="n">headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">unstructured_kwargs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;headers&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{})&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">headers&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">keys&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">warn_about_headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_non_html_available&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="n">warn_about_headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_headers_available_for_non_html&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">warn_about_headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_headers_available_for_html&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">warn_about_headers&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">logger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">warning&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s2">&amp;#34;You are using an old version of unstructured. &amp;#34;&lt;/span>
&lt;span class="s2">&amp;#34;The headers parameter is ignored&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">urls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">urls&lt;/span>
&lt;span class="err">待加载网页&lt;/span> &lt;span class="n">URL&lt;/span> &lt;span class="err">列表&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">continue_on_failure&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">continue_on_failure&lt;/span>
&lt;span class="err">默认&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="err">，某个&lt;/span>&lt;span class="n">URL加载失败后&lt;/span>&lt;span class="err">，是否继续&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">headers&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">unstructured_kwargs&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show_progress_bar&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">show_progress_bar&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_validate_mode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_valid_modes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;single&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;elements&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">mode&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">_valid_modes&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Got {mode} for `mode`, but should be one of `{_valid_modes}`&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">__is_headers_available_for_html&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;-&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">_unstructured_version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;.&amp;#34;&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">__is_headers_available_for_non_html&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;-&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">_unstructured_version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;.&amp;#34;&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">13&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">__is_non_html_available&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;-&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">_unstructured_version&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;.&amp;#34;&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">unstructured_version&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">12&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Load file.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">unstructured.partition.auto&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">partition&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">unstructured.partition.html&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">partition_html&lt;/span>
&lt;span class="n">docs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show_progress_bar&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">tqdm&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">tqdm&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">ImportError&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ImportError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s2">&amp;#34;Package tqdm must be installed if show_progress_bar=True. &amp;#34;&lt;/span>
&lt;span class="s2">&amp;#34;Please install with &amp;#39;pip install tqdm&amp;#39; or set &amp;#34;&lt;/span>
&lt;span class="s2">&amp;#34;show_progress_bar=False.&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">e&lt;/span>
&lt;span class="n">urls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tqdm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">urls&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">urls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">urls&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">url&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">urls&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">try&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_non_html_available&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_headers_available_for_non_html&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="n">elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">partition&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">headers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">headers&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">partition&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">__is_headers_available_for_html&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="n">elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">partition_html&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">headers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">headers&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">partition_html&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unstructured_kwargs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">except&lt;/span> &lt;span class="ne">Exception&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">continue_on_failure&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">logger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">error&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Error fetching or processing {url}, exception: {e}&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">continue&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="n">e&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mode&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;single&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">el&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">el&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">elements&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">metadata&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;source&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">url&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="n">docs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">page_content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">metadata&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">metadata&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mode&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;elements&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">element&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">elements&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">metadata&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">metadata&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to_dict&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">metadata&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;category&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">category&lt;/span>
&lt;span class="n">docs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Document&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">page_content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">metadata&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">metadata&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">docs&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2代码示例-2">(2)代码示例&lt;/h3>
&lt;ul>
&lt;li>读取指定URL网页，返回data对象&lt;/li>
&lt;li>data对象包含了网页中各类元素信息&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031071807666.png" alt="image-20231031071807666">&lt;/p>
&lt;h1 id="3文档转换器">3.文档转换器&lt;/h1>
&lt;h2 id="31基本概念">3.1.基本概念&lt;/h2>
&lt;ul>
&lt;li>LangChain提供的文档转器(&lt;code>Document transformers&lt;/code>)，支持用不同的分割方法，将&lt;code>Loader&lt;/code>获取的文档进行分割。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031072618565.png" alt="image-20231031072618565">&lt;/p>
&lt;p>这些文档转换器并不复杂，需要实操熟练掌握，本文不赘述，重点演示一下最常用的&lt;code>RecursiveCharacterTextSplitter&lt;/code>。&lt;/p>
&lt;h2 id="32recursivecharactertextsplitter">3.2.RecursiveCharacterTextSplitter&lt;/h2>
&lt;h3 id="1源码解读-5">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>作用：可输入字符列表作为切块分隔符，并根据第一个字符进行切块。如果切块太大，则使用下一个字符切块，以此类推。&lt;/li>
&lt;li>源码路径：https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/text_splitter.py&lt;/li>
&lt;li>核心源码如下：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt"> 10
&lt;/span>&lt;span class="lnt"> 11
&lt;/span>&lt;span class="lnt"> 12
&lt;/span>&lt;span class="lnt"> 13
&lt;/span>&lt;span class="lnt"> 14
&lt;/span>&lt;span class="lnt"> 15
&lt;/span>&lt;span class="lnt"> 16
&lt;/span>&lt;span class="lnt"> 17
&lt;/span>&lt;span class="lnt"> 18
&lt;/span>&lt;span class="lnt"> 19
&lt;/span>&lt;span class="lnt"> 20
&lt;/span>&lt;span class="lnt"> 21
&lt;/span>&lt;span class="lnt"> 22
&lt;/span>&lt;span class="lnt"> 23
&lt;/span>&lt;span class="lnt"> 24
&lt;/span>&lt;span class="lnt"> 25
&lt;/span>&lt;span class="lnt"> 26
&lt;/span>&lt;span class="lnt"> 27
&lt;/span>&lt;span class="lnt"> 28
&lt;/span>&lt;span class="lnt"> 29
&lt;/span>&lt;span class="lnt"> 30
&lt;/span>&lt;span class="lnt"> 31
&lt;/span>&lt;span class="lnt"> 32
&lt;/span>&lt;span class="lnt"> 33
&lt;/span>&lt;span class="lnt"> 34
&lt;/span>&lt;span class="lnt"> 35
&lt;/span>&lt;span class="lnt"> 36
&lt;/span>&lt;span class="lnt"> 37
&lt;/span>&lt;span class="lnt"> 38
&lt;/span>&lt;span class="lnt"> 39
&lt;/span>&lt;span class="lnt"> 40
&lt;/span>&lt;span class="lnt"> 41
&lt;/span>&lt;span class="lnt"> 42
&lt;/span>&lt;span class="lnt"> 43
&lt;/span>&lt;span class="lnt"> 44
&lt;/span>&lt;span class="lnt"> 45
&lt;/span>&lt;span class="lnt"> 46
&lt;/span>&lt;span class="lnt"> 47
&lt;/span>&lt;span class="lnt"> 48
&lt;/span>&lt;span class="lnt"> 49
&lt;/span>&lt;span class="lnt"> 50
&lt;/span>&lt;span class="lnt"> 51
&lt;/span>&lt;span class="lnt"> 52
&lt;/span>&lt;span class="lnt"> 53
&lt;/span>&lt;span class="lnt"> 54
&lt;/span>&lt;span class="lnt"> 55
&lt;/span>&lt;span class="lnt"> 56
&lt;/span>&lt;span class="lnt"> 57
&lt;/span>&lt;span class="lnt"> 58
&lt;/span>&lt;span class="lnt"> 59
&lt;/span>&lt;span class="lnt"> 60
&lt;/span>&lt;span class="lnt"> 61
&lt;/span>&lt;span class="lnt"> 62
&lt;/span>&lt;span class="lnt"> 63
&lt;/span>&lt;span class="lnt"> 64
&lt;/span>&lt;span class="lnt"> 65
&lt;/span>&lt;span class="lnt"> 66
&lt;/span>&lt;span class="lnt"> 67
&lt;/span>&lt;span class="lnt"> 68
&lt;/span>&lt;span class="lnt"> 69
&lt;/span>&lt;span class="lnt"> 70
&lt;/span>&lt;span class="lnt"> 71
&lt;/span>&lt;span class="lnt"> 72
&lt;/span>&lt;span class="lnt"> 73
&lt;/span>&lt;span class="lnt"> 74
&lt;/span>&lt;span class="lnt"> 75
&lt;/span>&lt;span class="lnt"> 76
&lt;/span>&lt;span class="lnt"> 77
&lt;/span>&lt;span class="lnt"> 78
&lt;/span>&lt;span class="lnt"> 79
&lt;/span>&lt;span class="lnt"> 80
&lt;/span>&lt;span class="lnt"> 81
&lt;/span>&lt;span class="lnt"> 82
&lt;/span>&lt;span class="lnt"> 83
&lt;/span>&lt;span class="lnt"> 84
&lt;/span>&lt;span class="lnt"> 85
&lt;/span>&lt;span class="lnt"> 86
&lt;/span>&lt;span class="lnt"> 87
&lt;/span>&lt;span class="lnt"> 88
&lt;/span>&lt;span class="lnt"> 89
&lt;/span>&lt;span class="lnt"> 90
&lt;/span>&lt;span class="lnt"> 91
&lt;/span>&lt;span class="lnt"> 92
&lt;/span>&lt;span class="lnt"> 93
&lt;/span>&lt;span class="lnt"> 94
&lt;/span>&lt;span class="lnt"> 95
&lt;/span>&lt;span class="lnt"> 96
&lt;/span>&lt;span class="lnt"> 97
&lt;/span>&lt;span class="lnt"> 98
&lt;/span>&lt;span class="lnt"> 99
&lt;/span>&lt;span class="lnt">100
&lt;/span>&lt;span class="lnt">101
&lt;/span>&lt;span class="lnt">102
&lt;/span>&lt;span class="lnt">103
&lt;/span>&lt;span class="lnt">104
&lt;/span>&lt;span class="lnt">105
&lt;/span>&lt;span class="lnt">106
&lt;/span>&lt;span class="lnt">107
&lt;/span>&lt;span class="lnt">108
&lt;/span>&lt;span class="lnt">109
&lt;/span>&lt;span class="lnt">110
&lt;/span>&lt;span class="lnt">111
&lt;/span>&lt;span class="lnt">112
&lt;/span>&lt;span class="lnt">113
&lt;/span>&lt;span class="lnt">114
&lt;/span>&lt;span class="lnt">115
&lt;/span>&lt;span class="lnt">116
&lt;/span>&lt;span class="lnt">117
&lt;/span>&lt;span class="lnt">118
&lt;/span>&lt;span class="lnt">119
&lt;/span>&lt;span class="lnt">120
&lt;/span>&lt;span class="lnt">121
&lt;/span>&lt;span class="lnt">122
&lt;/span>&lt;span class="lnt">123
&lt;/span>&lt;span class="lnt">124
&lt;/span>&lt;span class="lnt">125
&lt;/span>&lt;span class="lnt">126
&lt;/span>&lt;span class="lnt">127
&lt;/span>&lt;span class="lnt">128
&lt;/span>&lt;span class="lnt">129
&lt;/span>&lt;span class="lnt">130
&lt;/span>&lt;span class="lnt">131
&lt;/span>&lt;span class="lnt">132
&lt;/span>&lt;span class="lnt">133
&lt;/span>&lt;span class="lnt">134
&lt;/span>&lt;span class="lnt">135
&lt;/span>&lt;span class="lnt">136
&lt;/span>&lt;span class="lnt">137
&lt;/span>&lt;span class="lnt">138
&lt;/span>&lt;span class="lnt">139
&lt;/span>&lt;span class="lnt">140
&lt;/span>&lt;span class="lnt">141
&lt;/span>&lt;span class="lnt">142
&lt;/span>&lt;span class="lnt">143
&lt;/span>&lt;span class="lnt">144
&lt;/span>&lt;span class="lnt">145
&lt;/span>&lt;span class="lnt">146
&lt;/span>&lt;span class="lnt">147
&lt;/span>&lt;span class="lnt">148
&lt;/span>&lt;span class="lnt">149
&lt;/span>&lt;span class="lnt">150
&lt;/span>&lt;span class="lnt">151
&lt;/span>&lt;span class="lnt">152
&lt;/span>&lt;span class="lnt">153
&lt;/span>&lt;span class="lnt">154
&lt;/span>&lt;span class="lnt">155
&lt;/span>&lt;span class="lnt">156
&lt;/span>&lt;span class="lnt">157
&lt;/span>&lt;span class="lnt">158
&lt;/span>&lt;span class="lnt">159
&lt;/span>&lt;span class="lnt">160
&lt;/span>&lt;span class="lnt">161
&lt;/span>&lt;span class="lnt">162
&lt;/span>&lt;span class="lnt">163
&lt;/span>&lt;span class="lnt">164
&lt;/span>&lt;span class="lnt">165
&lt;/span>&lt;span class="lnt">166
&lt;/span>&lt;span class="lnt">167
&lt;/span>&lt;span class="lnt">168
&lt;/span>&lt;span class="lnt">169
&lt;/span>&lt;span class="lnt">170
&lt;/span>&lt;span class="lnt">171
&lt;/span>&lt;span class="lnt">172
&lt;/span>&lt;span class="lnt">173
&lt;/span>&lt;span class="lnt">174
&lt;/span>&lt;span class="lnt">175
&lt;/span>&lt;span class="lnt">176
&lt;/span>&lt;span class="lnt">177
&lt;/span>&lt;span class="lnt">178
&lt;/span>&lt;span class="lnt">179
&lt;/span>&lt;span class="lnt">180
&lt;/span>&lt;span class="lnt">181
&lt;/span>&lt;span class="lnt">182
&lt;/span>&lt;span class="lnt">183
&lt;/span>&lt;span class="lnt">184
&lt;/span>&lt;span class="lnt">185
&lt;/span>&lt;span class="lnt">186
&lt;/span>&lt;span class="lnt">187
&lt;/span>&lt;span class="lnt">188
&lt;/span>&lt;span class="lnt">189
&lt;/span>&lt;span class="lnt">190
&lt;/span>&lt;span class="lnt">191
&lt;/span>&lt;span class="lnt">192
&lt;/span>&lt;span class="lnt">193
&lt;/span>&lt;span class="lnt">194
&lt;/span>&lt;span class="lnt">195
&lt;/span>&lt;span class="lnt">196
&lt;/span>&lt;span class="lnt">197
&lt;/span>&lt;span class="lnt">198
&lt;/span>&lt;span class="lnt">199
&lt;/span>&lt;span class="lnt">200
&lt;/span>&lt;span class="lnt">201
&lt;/span>&lt;span class="lnt">202
&lt;/span>&lt;span class="lnt">203
&lt;/span>&lt;span class="lnt">204
&lt;/span>&lt;span class="lnt">205
&lt;/span>&lt;span class="lnt">206
&lt;/span>&lt;span class="lnt">207
&lt;/span>&lt;span class="lnt">208
&lt;/span>&lt;span class="lnt">209
&lt;/span>&lt;span class="lnt">210
&lt;/span>&lt;span class="lnt">211
&lt;/span>&lt;span class="lnt">212
&lt;/span>&lt;span class="lnt">213
&lt;/span>&lt;span class="lnt">214
&lt;/span>&lt;span class="lnt">215
&lt;/span>&lt;span class="lnt">216
&lt;/span>&lt;span class="lnt">217
&lt;/span>&lt;span class="lnt">218
&lt;/span>&lt;span class="lnt">219
&lt;/span>&lt;span class="lnt">220
&lt;/span>&lt;span class="lnt">221
&lt;/span>&lt;span class="lnt">222
&lt;/span>&lt;span class="lnt">223
&lt;/span>&lt;span class="lnt">224
&lt;/span>&lt;span class="lnt">225
&lt;/span>&lt;span class="lnt">226
&lt;/span>&lt;span class="lnt">227
&lt;/span>&lt;span class="lnt">228
&lt;/span>&lt;span class="lnt">229
&lt;/span>&lt;span class="lnt">230
&lt;/span>&lt;span class="lnt">231
&lt;/span>&lt;span class="lnt">232
&lt;/span>&lt;span class="lnt">233
&lt;/span>&lt;span class="lnt">234
&lt;/span>&lt;span class="lnt">235
&lt;/span>&lt;span class="lnt">236
&lt;/span>&lt;span class="lnt">237
&lt;/span>&lt;span class="lnt">238
&lt;/span>&lt;span class="lnt">239
&lt;/span>&lt;span class="lnt">240
&lt;/span>&lt;span class="lnt">241
&lt;/span>&lt;span class="lnt">242
&lt;/span>&lt;span class="lnt">243
&lt;/span>&lt;span class="lnt">244
&lt;/span>&lt;span class="lnt">245
&lt;/span>&lt;span class="lnt">246
&lt;/span>&lt;span class="lnt">247
&lt;/span>&lt;span class="lnt">248
&lt;/span>&lt;span class="lnt">249
&lt;/span>&lt;span class="lnt">250
&lt;/span>&lt;span class="lnt">251
&lt;/span>&lt;span class="lnt">252
&lt;/span>&lt;span class="lnt">253
&lt;/span>&lt;span class="lnt">254
&lt;/span>&lt;span class="lnt">255
&lt;/span>&lt;span class="lnt">256
&lt;/span>&lt;span class="lnt">257
&lt;/span>&lt;span class="lnt">258
&lt;/span>&lt;span class="lnt">259
&lt;/span>&lt;span class="lnt">260
&lt;/span>&lt;span class="lnt">261
&lt;/span>&lt;span class="lnt">262
&lt;/span>&lt;span class="lnt">263
&lt;/span>&lt;span class="lnt">264
&lt;/span>&lt;span class="lnt">265
&lt;/span>&lt;span class="lnt">266
&lt;/span>&lt;span class="lnt">267
&lt;/span>&lt;span class="lnt">268
&lt;/span>&lt;span class="lnt">269
&lt;/span>&lt;span class="lnt">270
&lt;/span>&lt;span class="lnt">271
&lt;/span>&lt;span class="lnt">272
&lt;/span>&lt;span class="lnt">273
&lt;/span>&lt;span class="lnt">274
&lt;/span>&lt;span class="lnt">275
&lt;/span>&lt;span class="lnt">276
&lt;/span>&lt;span class="lnt">277
&lt;/span>&lt;span class="lnt">278
&lt;/span>&lt;span class="lnt">279
&lt;/span>&lt;span class="lnt">280
&lt;/span>&lt;span class="lnt">281
&lt;/span>&lt;span class="lnt">282
&lt;/span>&lt;span class="lnt">283
&lt;/span>&lt;span class="lnt">284
&lt;/span>&lt;span class="lnt">285
&lt;/span>&lt;span class="lnt">286
&lt;/span>&lt;span class="lnt">287
&lt;/span>&lt;span class="lnt">288
&lt;/span>&lt;span class="lnt">289
&lt;/span>&lt;span class="lnt">290
&lt;/span>&lt;span class="lnt">291
&lt;/span>&lt;span class="lnt">292
&lt;/span>&lt;span class="lnt">293
&lt;/span>&lt;span class="lnt">294
&lt;/span>&lt;span class="lnt">295
&lt;/span>&lt;span class="lnt">296
&lt;/span>&lt;span class="lnt">297
&lt;/span>&lt;span class="lnt">298
&lt;/span>&lt;span class="lnt">299
&lt;/span>&lt;span class="lnt">300
&lt;/span>&lt;span class="lnt">301
&lt;/span>&lt;span class="lnt">302
&lt;/span>&lt;span class="lnt">303
&lt;/span>&lt;span class="lnt">304
&lt;/span>&lt;span class="lnt">305
&lt;/span>&lt;span class="lnt">306
&lt;/span>&lt;span class="lnt">307
&lt;/span>&lt;span class="lnt">308
&lt;/span>&lt;span class="lnt">309
&lt;/span>&lt;span class="lnt">310
&lt;/span>&lt;span class="lnt">311
&lt;/span>&lt;span class="lnt">312
&lt;/span>&lt;span class="lnt">313
&lt;/span>&lt;span class="lnt">314
&lt;/span>&lt;span class="lnt">315
&lt;/span>&lt;span class="lnt">316
&lt;/span>&lt;span class="lnt">317
&lt;/span>&lt;span class="lnt">318
&lt;/span>&lt;span class="lnt">319
&lt;/span>&lt;span class="lnt">320
&lt;/span>&lt;span class="lnt">321
&lt;/span>&lt;span class="lnt">322
&lt;/span>&lt;span class="lnt">323
&lt;/span>&lt;span class="lnt">324
&lt;/span>&lt;span class="lnt">325
&lt;/span>&lt;span class="lnt">326
&lt;/span>&lt;span class="lnt">327
&lt;/span>&lt;span class="lnt">328
&lt;/span>&lt;span class="lnt">329
&lt;/span>&lt;span class="lnt">330
&lt;/span>&lt;span class="lnt">331
&lt;/span>&lt;span class="lnt">332
&lt;/span>&lt;span class="lnt">333
&lt;/span>&lt;span class="lnt">334
&lt;/span>&lt;span class="lnt">335
&lt;/span>&lt;span class="lnt">336
&lt;/span>&lt;span class="lnt">337
&lt;/span>&lt;span class="lnt">338
&lt;/span>&lt;span class="lnt">339
&lt;/span>&lt;span class="lnt">340
&lt;/span>&lt;span class="lnt">341
&lt;/span>&lt;span class="lnt">342
&lt;/span>&lt;span class="lnt">343
&lt;/span>&lt;span class="lnt">344
&lt;/span>&lt;span class="lnt">345
&lt;/span>&lt;span class="lnt">346
&lt;/span>&lt;span class="lnt">347
&lt;/span>&lt;span class="lnt">348
&lt;/span>&lt;span class="lnt">349
&lt;/span>&lt;span class="lnt">350
&lt;/span>&lt;span class="lnt">351
&lt;/span>&lt;span class="lnt">352
&lt;/span>&lt;span class="lnt">353
&lt;/span>&lt;span class="lnt">354
&lt;/span>&lt;span class="lnt">355
&lt;/span>&lt;span class="lnt">356
&lt;/span>&lt;span class="lnt">357
&lt;/span>&lt;span class="lnt">358
&lt;/span>&lt;span class="lnt">359
&lt;/span>&lt;span class="lnt">360
&lt;/span>&lt;span class="lnt">361
&lt;/span>&lt;span class="lnt">362
&lt;/span>&lt;span class="lnt">363
&lt;/span>&lt;span class="lnt">364
&lt;/span>&lt;span class="lnt">365
&lt;/span>&lt;span class="lnt">366
&lt;/span>&lt;span class="lnt">367
&lt;/span>&lt;span class="lnt">368
&lt;/span>&lt;span class="lnt">369
&lt;/span>&lt;span class="lnt">370
&lt;/span>&lt;span class="lnt">371
&lt;/span>&lt;span class="lnt">372
&lt;/span>&lt;span class="lnt">373
&lt;/span>&lt;span class="lnt">374
&lt;/span>&lt;span class="lnt">375
&lt;/span>&lt;span class="lnt">376
&lt;/span>&lt;span class="lnt">377
&lt;/span>&lt;span class="lnt">378
&lt;/span>&lt;span class="lnt">379
&lt;/span>&lt;span class="lnt">380
&lt;/span>&lt;span class="lnt">381
&lt;/span>&lt;span class="lnt">382
&lt;/span>&lt;span class="lnt">383
&lt;/span>&lt;span class="lnt">384
&lt;/span>&lt;span class="lnt">385
&lt;/span>&lt;span class="lnt">386
&lt;/span>&lt;span class="lnt">387
&lt;/span>&lt;span class="lnt">388
&lt;/span>&lt;span class="lnt">389
&lt;/span>&lt;span class="lnt">390
&lt;/span>&lt;span class="lnt">391
&lt;/span>&lt;span class="lnt">392
&lt;/span>&lt;span class="lnt">393
&lt;/span>&lt;span class="lnt">394
&lt;/span>&lt;span class="lnt">395
&lt;/span>&lt;span class="lnt">396
&lt;/span>&lt;span class="lnt">397
&lt;/span>&lt;span class="lnt">398
&lt;/span>&lt;span class="lnt">399
&lt;/span>&lt;span class="lnt">400
&lt;/span>&lt;span class="lnt">401
&lt;/span>&lt;span class="lnt">402
&lt;/span>&lt;span class="lnt">403
&lt;/span>&lt;span class="lnt">404
&lt;/span>&lt;span class="lnt">405
&lt;/span>&lt;span class="lnt">406
&lt;/span>&lt;span class="lnt">407
&lt;/span>&lt;span class="lnt">408
&lt;/span>&lt;span class="lnt">409
&lt;/span>&lt;span class="lnt">410
&lt;/span>&lt;span class="lnt">411
&lt;/span>&lt;span class="lnt">412
&lt;/span>&lt;span class="lnt">413
&lt;/span>&lt;span class="lnt">414
&lt;/span>&lt;span class="lnt">415
&lt;/span>&lt;span class="lnt">416
&lt;/span>&lt;span class="lnt">417
&lt;/span>&lt;span class="lnt">418
&lt;/span>&lt;span class="lnt">419
&lt;/span>&lt;span class="lnt">420
&lt;/span>&lt;span class="lnt">421
&lt;/span>&lt;span class="lnt">422
&lt;/span>&lt;span class="lnt">423
&lt;/span>&lt;span class="lnt">424
&lt;/span>&lt;span class="lnt">425
&lt;/span>&lt;span class="lnt">426
&lt;/span>&lt;span class="lnt">427
&lt;/span>&lt;span class="lnt">428
&lt;/span>&lt;span class="lnt">429
&lt;/span>&lt;span class="lnt">430
&lt;/span>&lt;span class="lnt">431
&lt;/span>&lt;span class="lnt">432
&lt;/span>&lt;span class="lnt">433
&lt;/span>&lt;span class="lnt">434
&lt;/span>&lt;span class="lnt">435
&lt;/span>&lt;span class="lnt">436
&lt;/span>&lt;span class="lnt">437
&lt;/span>&lt;span class="lnt">438
&lt;/span>&lt;span class="lnt">439
&lt;/span>&lt;span class="lnt">440
&lt;/span>&lt;span class="lnt">441
&lt;/span>&lt;span class="lnt">442
&lt;/span>&lt;span class="lnt">443
&lt;/span>&lt;span class="lnt">444
&lt;/span>&lt;span class="lnt">445
&lt;/span>&lt;span class="lnt">446
&lt;/span>&lt;span class="lnt">447
&lt;/span>&lt;span class="lnt">448
&lt;/span>&lt;span class="lnt">449
&lt;/span>&lt;span class="lnt">450
&lt;/span>&lt;span class="lnt">451
&lt;/span>&lt;span class="lnt">452
&lt;/span>&lt;span class="lnt">453
&lt;/span>&lt;span class="lnt">454
&lt;/span>&lt;span class="lnt">455
&lt;/span>&lt;span class="lnt">456
&lt;/span>&lt;span class="lnt">457
&lt;/span>&lt;span class="lnt">458
&lt;/span>&lt;span class="lnt">459
&lt;/span>&lt;span class="lnt">460
&lt;/span>&lt;span class="lnt">461
&lt;/span>&lt;span class="lnt">462
&lt;/span>&lt;span class="lnt">463
&lt;/span>&lt;span class="lnt">464
&lt;/span>&lt;span class="lnt">465
&lt;/span>&lt;span class="lnt">466
&lt;/span>&lt;span class="lnt">467
&lt;/span>&lt;span class="lnt">468
&lt;/span>&lt;span class="lnt">469
&lt;/span>&lt;span class="lnt">470
&lt;/span>&lt;span class="lnt">471
&lt;/span>&lt;span class="lnt">472
&lt;/span>&lt;span class="lnt">473
&lt;/span>&lt;span class="lnt">474
&lt;/span>&lt;span class="lnt">475
&lt;/span>&lt;span class="lnt">476
&lt;/span>&lt;span class="lnt">477
&lt;/span>&lt;span class="lnt">478
&lt;/span>&lt;span class="lnt">479
&lt;/span>&lt;span class="lnt">480
&lt;/span>&lt;span class="lnt">481
&lt;/span>&lt;span class="lnt">482
&lt;/span>&lt;span class="lnt">483
&lt;/span>&lt;span class="lnt">484
&lt;/span>&lt;span class="lnt">485
&lt;/span>&lt;span class="lnt">486
&lt;/span>&lt;span class="lnt">487
&lt;/span>&lt;span class="lnt">488
&lt;/span>&lt;span class="lnt">489
&lt;/span>&lt;span class="lnt">490
&lt;/span>&lt;span class="lnt">491
&lt;/span>&lt;span class="lnt">492
&lt;/span>&lt;span class="lnt">493
&lt;/span>&lt;span class="lnt">494
&lt;/span>&lt;span class="lnt">495
&lt;/span>&lt;span class="lnt">496
&lt;/span>&lt;span class="lnt">497
&lt;/span>&lt;span class="lnt">498
&lt;/span>&lt;span class="lnt">499
&lt;/span>&lt;span class="lnt">500
&lt;/span>&lt;span class="lnt">501
&lt;/span>&lt;span class="lnt">502
&lt;/span>&lt;span class="lnt">503
&lt;/span>&lt;span class="lnt">504
&lt;/span>&lt;span class="lnt">505
&lt;/span>&lt;span class="lnt">506
&lt;/span>&lt;span class="lnt">507
&lt;/span>&lt;span class="lnt">508
&lt;/span>&lt;span class="lnt">509
&lt;/span>&lt;span class="lnt">510
&lt;/span>&lt;span class="lnt">511
&lt;/span>&lt;span class="lnt">512
&lt;/span>&lt;span class="lnt">513
&lt;/span>&lt;span class="lnt">514
&lt;/span>&lt;span class="lnt">515
&lt;/span>&lt;span class="lnt">516
&lt;/span>&lt;span class="lnt">517
&lt;/span>&lt;span class="lnt">518
&lt;/span>&lt;span class="lnt">519
&lt;/span>&lt;span class="lnt">520
&lt;/span>&lt;span class="lnt">521
&lt;/span>&lt;span class="lnt">522
&lt;/span>&lt;span class="lnt">523
&lt;/span>&lt;span class="lnt">524
&lt;/span>&lt;span class="lnt">525
&lt;/span>&lt;span class="lnt">526
&lt;/span>&lt;span class="lnt">527
&lt;/span>&lt;span class="lnt">528
&lt;/span>&lt;span class="lnt">529
&lt;/span>&lt;span class="lnt">530
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">RecursiveCharacterTextSplitter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TextSplitter&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Splitting text by recursively look at characters.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Recursively tries to split by different characters to find one
&lt;/span>&lt;span class="s2"> that works.
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">separators&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">keep_separator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">is_separator_regex&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Create a new TextSplitter.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">keep_separator&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">keep_separator&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_separators&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">separators&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_is_separator_regex&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">is_separator_regex&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_split_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">separators&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Split incoming text and return chunks.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">final_chunks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="c1"># Get appropriate separator to use&lt;/span>
&lt;span class="n">separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">separators&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">new_separators&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_s&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">separators&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">_separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_s&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_is_separator_regex&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">escape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">_s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">_s&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_s&lt;/span>
&lt;span class="k">break&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">search&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">_separator&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_s&lt;/span>
&lt;span class="n">new_separators&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">separators&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;span class="k">break&lt;/span>
&lt;span class="n">_separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">separator&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_is_separator_regex&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">escape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">separator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">splits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">_split_text_with_regex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_separator&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_keep_separator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Now go merging things, recursively splitting longer texts.&lt;/span>
&lt;span class="n">_good_splits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="n">_separator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_keep_separator&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">separator&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">splits&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length_function&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_chunk_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">_good_splits&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">_good_splits&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">merged_text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_merge_splits&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">_good_splits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_separator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">final_chunks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">merged_text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">_good_splits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">new_separators&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">final_chunks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">other_info&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_split_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_separators&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">final_chunks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">other_info&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">_good_splits&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">merged_text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_merge_splits&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">_good_splits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_separator&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">final_chunks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">merged_text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">final_chunks&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">split_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_split_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_separators&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="nd">@classmethod&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">from_language&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">cls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">language&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">RecursiveCharacterTextSplitter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">separators&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">cls&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_separators_for_language&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">language&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">cls&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">separators&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">separators&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">is_separator_regex&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="nd">@staticmethod&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_separators_for_language&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">language&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CPP&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">void &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">int &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">float &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">double &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GO&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">func &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">var &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">const &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">type &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">JAVA&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">public &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">protected &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">private &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">static &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">KOTLIN&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">public &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">protected &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">private &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">internal &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">companion &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">fun &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">val &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">var &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">when &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">else &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">JS&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">function &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">const &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">let &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">var &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">default &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TS&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">enum &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">interface &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">namespace &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">type &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">function &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">const &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">let &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">var &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">default &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PHP&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">function &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">foreach &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">do &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PROTO&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along message definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">message &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along service definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">service &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along enum definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">enum &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along option definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">option &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along import statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">import &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along syntax declarations&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">syntax &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">PYTHON&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># First, try to split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">def &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\t&lt;/span>&lt;span class="s2">def &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Now split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RST&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along section titles&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">=+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">-+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\&lt;/span>&lt;span class="s2">*+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along directive markers&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">.. *&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RUBY&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">def &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">unless &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">do &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">begin &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">rescue &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RUST&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">fn &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">const &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">let &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">loop &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">match &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">const &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SCALA&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">object &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">def &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">val &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">var &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">match &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SWIFT&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along function definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">func &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">struct &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">enum &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">do &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MARKDOWN&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># First, try to split along Markdown headings (starting with level 2)&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">#{1,6} &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Note the alternative syntax for headings (below) is not handled here&lt;/span>
&lt;span class="c1"># Heading level 2&lt;/span>
&lt;span class="c1"># ---------------&lt;/span>
&lt;span class="c1"># End of code block&lt;/span>
&lt;span class="s2">&amp;#34;```&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Horizontal lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\&lt;/span>&lt;span class="s2">*&lt;/span>&lt;span class="se">\\&lt;/span>&lt;span class="s2">*&lt;/span>&lt;span class="se">\\&lt;/span>&lt;span class="s2">*+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">---+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">___+&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Note that this splitter doesn&amp;#39;t handle horizontal lines defined&lt;/span>
&lt;span class="c1"># by *three or more* of ***, ---, or ___, but this is not handled&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LATEX&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># First, try to split along Latex sections&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">chapter{&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">section{&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">subsection{&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">subsubsection{&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Now split by environments&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{enumerate}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{itemize}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{description}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{list}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{quote}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{quotation}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{verse}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\\&lt;/span>&lt;span class="s2">begin{verbatim}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Now split by math environments&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\\\b&lt;/span>&lt;span class="s2">egin{align}&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;$$&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;$&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Now split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">HTML&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># First, try to split along HTML tags&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;body&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;div&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;p&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;br&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;li&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h1&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h2&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h3&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h5&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;h6&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;span&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;table&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;tr&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;td&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;th&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;ul&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;ol&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;header&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;footer&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;nav&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Head&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;head&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;style&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;script&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;meta&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;lt;title&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CSHARP&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">interface &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">enum &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">implements &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">delegate &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">event &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along class definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">class &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">abstract &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">public &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">protected &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">private &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">static &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">return &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">continue &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">foreach &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">switch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">break &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">case &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">else &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by exceptions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">try &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">throw &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">finally &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">catch &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SOL&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along compiler information definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">pragma &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">using &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along contract definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">contract &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">interface &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">library &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along method definitions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">constructor &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">type &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">function &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">event &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">modifier &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">error &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">struct &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">enum &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along control flow statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">if &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">for &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">do while &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">assembly &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">language&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">Language&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">COBOL&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="c1"># Split along divisions&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">IDENTIFICATION DIVISION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">ENVIRONMENT DIVISION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">DATA DIVISION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">PROCEDURE DIVISION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along sections within DATA DIVISION&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">WORKING-STORAGE SECTION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">LINKAGE SECTION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">FILE SECTION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along sections within PROCEDURE DIVISION&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">INPUT-OUTPUT SECTION.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split along paragraphs and common statements&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">OPEN &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">CLOSE &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">READ &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">WRITE &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">IF &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">ELSE &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">MOVE &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">PERFORM &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">UNTIL &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">VARYING &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">ACCEPT &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">DISPLAY &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">STOP RUN.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># Split by the normal type of lines&lt;/span>
&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Language {language} is not supported! &amp;#34;&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;Please choose from {list(Language)}&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2代码示例-3">(2)代码示例&lt;/h3>
&lt;ul>
&lt;li>使用&lt;code>RecursiveCharacterTextSplitter&lt;/code>，定义好块的大小、切割重合度等参数，得到切分后的texts对象。&lt;/li>
&lt;li>texts对象就是将文档切分后的文本块集合。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231023063036651.png" alt="image-20231023063036651">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231023063047761.png" alt="image-20231023063047761">&lt;/p>
&lt;h1 id="4文档向量化">4.文档向量化&lt;/h1>
&lt;h2 id="41基本概念">4.1.基本概念&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>当我们获得了文本块以后，就需要将其向量化并持久化下来。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LangChain提供的词嵌入模型(&lt;code>Text Embedding Models&lt;/code>)，支持丰富的词嵌入模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LangChain提供的向量存储器(&lt;code>Vector Stores&lt;/code>)，支持丰富的向量数据库。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这部分也是LangChain很强大的特性之一，几乎覆盖了世界上所有&lt;strong>主流的词嵌入模型&lt;/strong>和&lt;strong>主流的向量数据库&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>词嵌入模型列表：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031082141950.png" alt="image-20231031082141950">&lt;/p>
&lt;ul>
&lt;li>向量数据库列表：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031073927426.png" alt="image-20231031073927426">&lt;/p>
&lt;h2 id="42代码示例">4.2.代码示例&lt;/h2>
&lt;ul>
&lt;li>以OpenAI的词嵌入模型为例，展示了如何使用LangChain的词嵌入模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231023065210069.png" alt="image-20231023065210069">&lt;/p>
&lt;h1 id="5检索器">5.检索器&lt;/h1>
&lt;h2 id="1基本概念">(1)基本概念&lt;/h2>
&lt;ul>
&lt;li>LangChain提供的检索器(&lt;code>Retrieves&lt;/code>)，支持丰富的检索能力。&lt;/li>
&lt;li>基础的，基于向量数据库的检索器如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031082838616.png" alt="image-20231031082838616">&lt;/p>
&lt;ul>
&lt;li>高级的(如：云原生)，基于云原生的检索器如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231031083352096.png" alt="image-20231031083352096">&lt;/p>
&lt;h2 id="2代码示例-4">(2)代码示例&lt;/h2>
&lt;ul>
&lt;li>如下代码演示了&lt;code>similarity_search&lt;/code>方法的使用：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-LangChain%E4%B9%8BRetrieval%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A14/image-20231023065351312.png" alt="image-20231023065351312">&lt;/p>
&lt;h1 id="6小结">6.小结&lt;/h1>
&lt;ul>
&lt;li>LangChain的&lt;code>Retrieval&lt;/code>特性的整体流程是：文档加载-&amp;gt;文档转换-&amp;gt;文档向量化-&amp;gt;数据检索&lt;/li>
&lt;li>文档加载器支持各类数据格式，各类数据源，实战时需熟悉各类文档加载器。&lt;/li>
&lt;li>文档转换器支持各类文本分割策略，实战时需熟悉各类文档分割策略。&lt;/li>
&lt;li>文档向量化时，有多种词嵌入模型，有多种向量数据库，实战时也需要熟练掌握。&lt;/li>
&lt;li>文档检索时，支持多种文档检索策略，实战时需熟练掌握。&lt;/li>
&lt;/ul>
&lt;p>下一篇，我们对LangChain最火的模块Agents进行探索，下篇见！&lt;/p></description></item><item><title>【chatGPT】学习笔记20-如何搭建ChatGLM3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</link><pubDate>Sun, 29 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</guid><description>&lt;h1 id="1chatglm3更新了什么">1.ChatGLM3更新了什么&lt;/h1>
&lt;h2 id="1模型列表">(1)模型列表&lt;/h2>
&lt;p>智谱AI刚刚发布了ChatGLM3，其中&lt;strong>ChatGLM3-6B&lt;/strong>的能力提升如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>更强大的基础模型&lt;/strong>： 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上表现更好。&lt;/li>
&lt;li>&lt;strong>更完整的功能支持&lt;/strong>：重新设计了Prompt模版格式，支持&lt;code>Function Call&lt;/code>、&lt;code>Code Interpreter&lt;/code>、&lt;code>Agent&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>除了ChatGLM3-6B，还发布了：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ChatGLM3-6B-Base&lt;/strong>：它是ChatGLM3-6B的预训练模型，在10B以下的表现同比更好。&lt;/li>
&lt;li>&lt;strong>ChatGLM3-6B-32K&lt;/strong>：适用于长文本对话场景。&lt;/li>
&lt;/ul>
&lt;p>ChatGLM3发布的&lt;strong>模型列表&lt;/strong>如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seq Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM3-6B&lt;/td>
&lt;td>8k&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-Base&lt;/td>
&lt;td>8k&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-32K&lt;/td>
&lt;td>32k&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="2测评结果">(2)测评结果&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>典型数据集测试&lt;/strong>：在8个中英文典型数据集上，ChatGLM3-6B-Base的性能表现如下：
&lt;ul>
&lt;li>&lt;strong>测试方法&lt;/strong>：BBH 采用3-shot测试，GSM8K(需要推理)采用0-shot CoT测试、MATH(需要推理)采用0-shot CoT测试，MBPP 采用0-shot生成后运行测例计算 Pass@1 ，其它选择题类型数据集均采用0-shot测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>MATH&lt;/th>
&lt;th>BBH&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>C-Eval&lt;/th>
&lt;th>CMMLU&lt;/th>
&lt;th>MBPP&lt;/th>
&lt;th>AGIEval&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM2-6B-Base&lt;/td>
&lt;td>32.4&lt;/td>
&lt;td>6.5&lt;/td>
&lt;td>33.7&lt;/td>
&lt;td>47.9&lt;/td>
&lt;td>51.7&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Best Baseline&lt;/td>
&lt;td>52.1&lt;/td>
&lt;td>13.1&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>60.1&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>62.2&lt;/td>
&lt;td>47.5&lt;/td>
&lt;td>45.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-Base&lt;/td>
&lt;td>72.3&lt;/td>
&lt;td>25.7&lt;/td>
&lt;td>66.1&lt;/td>
&lt;td>61.4&lt;/td>
&lt;td>69.0&lt;/td>
&lt;td>67.5&lt;/td>
&lt;td>52.4&lt;/td>
&lt;td>53.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>长文本测试&lt;/strong>：进行人工评估测试，ChatGLM3-6B-32K的性能表现如下。
&lt;ul>
&lt;li>&lt;strong>测试结论&lt;/strong>：与ChatGLM2相比，效果提升超50%(对论文阅读、文档摘要和财报分析等提升显著)。&lt;/li>
&lt;li>&lt;strong>测试方法&lt;/strong>：在LongBench评测集上进行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>平均&lt;/th>
&lt;th>Summary&lt;/th>
&lt;th>Single-Doc QA&lt;/th>
&lt;th>Multi-Doc QA&lt;/th>
&lt;th>Code&lt;/th>
&lt;th>Few-shot&lt;/th>
&lt;th>Synthetic&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM2-6B-32K&lt;/td>
&lt;td>41.5&lt;/td>
&lt;td>24.8&lt;/td>
&lt;td>37.6&lt;/td>
&lt;td>34.7&lt;/td>
&lt;td>52.8&lt;/td>
&lt;td>51.3&lt;/td>
&lt;td>47.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-32K&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>26.6&lt;/td>
&lt;td>45.8&lt;/td>
&lt;td>46.1&lt;/td>
&lt;td>56.2&lt;/td>
&lt;td>61.2&lt;/td>
&lt;td>65&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="2准备硬件资源及基础软件">2.准备硬件资源及基础软件&lt;/h1>
&lt;p>笔者准备的硬件资源及基础软件如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>：V100，32G显存，避免OOM问题。&lt;/li>
&lt;li>&lt;strong>CUDA&lt;/strong>：Cuda11.6&lt;/li>
&lt;li>&lt;strong>OS&lt;/strong>：Ubuntu22.04&lt;/li>
&lt;li>&lt;strong>Conda&lt;/strong>：Miniconda3&lt;/li>
&lt;li>&lt;strong>Python&lt;/strong>：Python3.10&lt;/li>
&lt;li>&lt;strong>Pytorch&lt;/strong>：Pytorch3.8&lt;/li>
&lt;/ul>
&lt;h1 id="3创建虚拟环境">3.创建虚拟环境&lt;/h1>
&lt;ul>
&lt;li>创建虚拟环境：&lt;code>conda create -p ./envs/HCZ_ChatGLM2 python=3.10&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030003623799.png" alt="image-20231030003623799">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>激活虚拟环境&lt;/strong>：&lt;code>conda activate ./envs/HCZ_ChatGLM3&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030003931661.png" alt="image-20231030003931661">&lt;/p>
&lt;h1 id="4上传模型及模型容器">4.上传模型及模型容器&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>chatglm3-6b下载地址&lt;/strong>：https://huggingface.co/THUDM/chatglm3-6b&lt;/li>
&lt;li>&lt;strong>chatglm3-6b容器下载地址&lt;/strong>：https://github.com/THUDM/ChatGLM3/archive/refs/heads/main.zip&lt;/li>
&lt;li>下载完成后，&lt;strong>上传到服务器&lt;/strong>，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030071806371.png" alt="image-20231030071806371">&lt;/p>
&lt;h1 id="5安装依赖包">5.安装依赖包&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>进入容器目录&lt;/strong>：&lt;code>cd /opt/model/THUDM_chatglm3-6b-container&lt;/code>&lt;/li>
&lt;li>&lt;strong>安装依赖包&lt;/strong>：&lt;code>pip install -r requirements.txt&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030071933817.png" alt="image-20231030071933817">&lt;/p>
&lt;h1 id="6构建restful接口">6.构建Restful接口&lt;/h1>
&lt;ul>
&lt;li>借鉴ChatGLM2的&lt;code>api.py&lt;/code>，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030074839307.png" alt="image-20231030074839307">&lt;/p>
&lt;h1 id="7运行chatglm3服务">7.运行ChatGLM3服务&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>进入容器目录&lt;/strong>：&lt;code>cd /opt/model/THUDM_chatglm3-6b-container&lt;/code>&lt;/li>
&lt;li>&lt;strong>运行服务&lt;/strong>：&lt;code>python api.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030075223648.png" alt="image-20231030075223648">&lt;/p>
&lt;h1 id="8测试">8.测试&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>向ChatGLM3提问&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030083402685.png" alt="image-20231030083402685">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>服务器端运行日志如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030083320605.png" alt="image-20231030083320605">&lt;/p>
&lt;h1 id="9小结">9.小结&lt;/h1>
&lt;ul>
&lt;li>本文阐述了ChatGLM3的官宣能力，并演示了如何搭建自己的ChatGLM3。&lt;/li>
&lt;li>ChatGLM3的新能力有待进一步集成到产品中进行验证。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记19-自己实现一个简版ChatGPT(下)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</link><pubDate>Fri, 20 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</guid><description>&lt;p>前两篇实现了简版GPT，并对其进行了SFT，我们接下来看ChatGPT整体训练流程的最后一个环节——&lt;strong>对齐训练(Alignment Training)&lt;/strong>。&lt;/p>
&lt;h1 id="1方法3对齐训练alignment-training">1.方法3：对齐训练(Alignment Training)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;ul>
&lt;li>方法3对应于&lt;strong>ChatGPT整体训练流程的STEP2、STEP3&lt;/strong>。&lt;/li>
&lt;li>方法3的核心思想是利用了强化学习，最终将GPT3演进为了&lt;strong>更通人性的ChatGPT&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020062050959.png" alt="image-20231020062050959">&lt;/p>
&lt;ul>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP2、STEP3&lt;/strong>，就是大名鼎鼎的&lt;strong>RLHF&lt;/strong>——&lt;strong>基于人类反馈的强化学习&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>RL：Reinforcement Learning&lt;/strong>&lt;/li>
&lt;li>&lt;strong>HF：Human Feedback&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP2&lt;/strong>，&lt;strong>对应于&lt;/strong>强化学习模型的&lt;strong>Interpreter模型&lt;/strong>。&lt;/li>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP3&lt;/strong>，&lt;strong>对应于&lt;/strong>强化学习模型的&lt;strong>Action模型&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020085551189.png" alt="image-20231020085551189">&lt;/p>
&lt;h2 id="2什么是对齐训练">(2)什么是对齐训练&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>对齐训练&lt;/strong>：Alignment Training，它就是一种机器学习的模型训练方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>核心思想&lt;/strong>：训练出人类主观感受的模型，这个模型具备预测人类的决策的能力。&lt;/p>
&lt;ul>
&lt;li>这样，训练好的模型，就可以在未见过的场景下，按照类似人的行为模式做出选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>对齐训练与强化学习的关系&lt;/strong>：OpenAI在对齐训练中，结合了强化学习。&lt;/p>
&lt;ul>
&lt;li>ChatGPT整体训练流程的STEP2就是对齐训练，学习出预测人类回答问题的偏好模型。&lt;/li>
&lt;li>ChatGPT整体训练流程的STEP3就是强化学习，STEP2输出的这个模型，作为强化学习的Interpreter模型。STEP3不断迭代，最终学习到Action模型。
&lt;ul>
&lt;li>通过SFT训练之后GPT3，本质就是一个能机械式地回答问题的机器人。&lt;/li>
&lt;li>通过RLHF学习的Action模型，才是帮助SFT之后的GPT3，类似人类回答问题的关键机关。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>细节&lt;/strong>：ChatGPT整体训练流程图中，出现了PPO算法，PPO算法是近端策略梯度优化，增加一个限制Action模型在训练过程中梯度上升速度，本质就是避免Action模型产生一个离谱的Action。&lt;/p>
&lt;ul>
&lt;li>PPO算法展开说内容太多，本文不赘述，详见论文：https://arxiv.org/abs/1707.06347&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3step2的reward-model模型训练伪码">(3)STEP2的Reward Model模型训练伪码&lt;/h2>
&lt;ul>
&lt;li>我们再来看看STEP2的伪码，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020093407332.png" alt="image-20231020093407332">&lt;/p>
&lt;h2 id="4step3的rlhf训练伪码">(4)STEP3的RLHF训练伪码&lt;/h2>
&lt;ul>
&lt;li>我们再来看看STEP3的伪码，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020095154134.png" alt="image-20231020095154134">&lt;/p>
&lt;h1 id="2deepspeed">2.DeepSpeed&lt;/h1>
&lt;p>RLHF，是ChatGPT最核心的技术机密，除了在《Introducing ChatGPT》(&lt;a href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt&lt;/a>)中提到了，并未公开过源码。&lt;/p>
&lt;p>在前文的伪码实现部分，虽然通过伪码描述了RLHF的核心逻辑，但距离商用还欠缺很多东西(如：分布式训练等)。&lt;/p>
&lt;p>幸好微软开源了类似的框架，DeepSpeed，我们可以通过阅读它的源码、使用它，开展RLHF。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020100752488.png" alt="image-20231020100752488">&lt;/p>
&lt;h1 id="3实例-开展rlhf训练">3.实例-开展RLHF训练&lt;/h1>
&lt;h2 id="step0前置准备">STEP0.前置准备&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>硬件&lt;/strong>：V100一块，32G显存&lt;/li>
&lt;li>&lt;strong>基础软件&lt;/strong>：Ubuntun20.04，Minicoda3，Pytorch3.8，CUDA11.6，Python3.10&lt;/li>
&lt;li>&lt;strong>预训练模型&lt;/strong>：选择Facebook的opt1.3B，即&lt;strong>13亿参数&lt;/strong>的预训练模型。&lt;/li>
&lt;li>&lt;strong>环境初始配置&lt;/strong>：创建虚拟环境，&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020102932305.png" alt="image-20231020102932305">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>安装依赖&lt;/strong>：进入DeepSpeed-Chat目录，安装相关依赖&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020103116451.png" alt="image-20231020103116451">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>环境测试&lt;/strong>：确认相关基础软件版本号。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020103548543.png" alt="image-20231020103548543">&lt;/p>
&lt;h2 id="step1sft">STEP1.SFT&lt;/h2>
&lt;ul>
&lt;li>开展SFT训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020104103388.png" alt="image-20231020104103388">&lt;/p>
&lt;ul>
&lt;li>设置待微调的预训练模型，以及输出路径。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training/step1_supervised_finetuning/evaluation_scripts/run_prompt.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020110616405.png" alt="image-20231020110616405">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_1.3b.sh，触发DeepSpeed开始SFT训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=959965022&amp;bvid=BV1Hp4y1M7Ly&amp;cid=1305868791&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step2rm">STEP2.RM&lt;/h2>
&lt;ul>
&lt;li>开展RM训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training_scripts/opt/single_gpu/run_350m.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020145149415.png" alt="image-20231020145149415">&lt;/p>
&lt;ul>
&lt;li>指定Reward Model的输出路径。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：evaluation_scripts/run_eval.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020145843742.png" alt="image-20231020145843742">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_350m.sh，触发DeepSpeed开始RW训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=534943541&amp;bvid=BV1gM411R7Z5&amp;cid=1305868897&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step3rlhf">STEP3.RLHF&lt;/h2>
&lt;ul>
&lt;li>开展RLHF训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>training_scripts/opt/single_gpu/run_1.3b.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020155704428.png" alt="image-20231020155704428">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_1.3b.sh，触发DeepSpeed开始RLHF训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=277474259&amp;bvid=BV1Ww411F7xw&amp;cid=1305868938&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step4模型测试">STEP4.模型测试&lt;/h2>
&lt;ul>
&lt;li>执行测试脚本&lt;code>python chat.py --path training/step3_rlhf_finetuning/output/actor&lt;/code>，不赘述。&lt;/li>
&lt;/ul>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文是实现简版GPT的三篇中的最后一篇，也是最难理解的一部分内容：&lt;/p>
&lt;ul>
&lt;li>对齐训练是什么？&lt;/li>
&lt;li>对齐训练和强化学习的关系是什么？&lt;/li>
&lt;li>ChatGPT整体训练流程的STEP2、STEP3与强化学习的Interpreter和Action模型如何对应？&lt;/li>
&lt;li>DeepSpeed的实际操作？&lt;/li>
&lt;/ul>
&lt;p>本文也有没有展开探讨的内容，待本专栏后续继续展开：&lt;/p>
&lt;ul>
&lt;li>RLHF的策略梯度优化算法&lt;/li>
&lt;li>PPO算法&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>编写本专栏受益匪浅，也非常感恩因为编写本专栏认识的大神们，期待与各位小伙伴持续的讨论和思辨！&lt;/p></description></item><item><title>【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</link><pubDate>Wed, 18 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</guid><description>&lt;p>根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。&lt;/p>
&lt;p>但GPT3距离ChatGPT还有很远的距离，这一段距离涉及OpenAI未公开论文、源码的关键技术。&lt;/p>
&lt;p>我们接下来从OpenAPI已公开的信息来看看&lt;font color=red>&lt;strong>ChatGPT是如何炼成的&lt;/strong>&lt;/font>。&lt;/p>
&lt;h1 id="1chatgpt的历史版本">1.ChatGPT的历史版本&lt;/h1>
&lt;p>下图摘自Yule Wang的技术专栏，阐述了基于Transformer的大语言模型不同版本的脉络：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BERT&lt;/strong>是编码器Only架构，&lt;strong>BART&lt;/strong>是编码器-解码器架构，它们延伸出去大语言模型下载后&lt;font color=red>&lt;strong>不能直接使用，需要垂域微调&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>T5&lt;/strong>是编码器-解码器架构，&lt;strong>GPT-2&lt;/strong>是解码器Only架构，它们下载后&lt;font color=red>&lt;strong>不做垂域微调，也能完成一些AI任务&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>GPT-3、GPT-4&lt;/strong>是解码器Only架构，它们下载后&lt;font color=red>&lt;strong>不做垂域微调，能完成大部分AI任务&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>针对&lt;strong>GPT-3&lt;/strong>进行&lt;font color=red>&lt;strong>SFT(有监督微调)+RLHF(基于人类反馈的强化学习)&lt;strong>&lt;/font>，最终得到了&lt;/strong>ChatGPT&lt;/strong>，&lt;strong>GPT-3.5、InstructGPT&lt;/strong>算是过渡产品。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019083053465.png" alt="image-20231019083053465">&lt;/p>
&lt;h1 id="2chatgpt的整体训练流程">2.ChatGPT的整体训练流程&lt;/h1>
&lt;p>在《Introducing ChatGPT》(详见https://openai.com/blog/chatgpt)中，给出了从GPT3演进到ChatGPT的整体训练流程图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/ChatGPT_Diagram.svg" alt="ChatGPT_Diagram">&lt;/p>
&lt;p>以前看这张图很模糊，通过前面复现Transformer架构、简版GPT的代码，才逐渐变得清晰。&lt;/p>
&lt;p>接下来，我们来逐一拆解。&lt;/p>
&lt;h1 id="3chatgpt的三大训练方法">3.ChatGPT的三大训练方法&lt;/h1>
&lt;p>下图比较形象地归纳了ChatGPT整体训练流程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019094239673.png" alt="image-20231019094239673">&lt;/p>
&lt;h2 id="1方法1预训练pre-traning">(1)方法1：预训练(Pre-Traning)&lt;/h2>
&lt;ul>
&lt;li>构建好大语言模型的神经网络架构后，&lt;strong>通过大数据、大算力进行训练&lt;/strong>，得到预训练模型。如：GPT3就属于这类预训练模型。&lt;/li>
&lt;li>大语言模型的本质是生成内容，构建训练数据基本都是自动化的，所以&lt;strong>预训练的过程属于无监督学习&lt;/strong>。&lt;/li>
&lt;li>预训练模型非常庞大，&lt;strong>算是通才&lt;/strong>，具备&lt;strong>基本的自然语言处理能力、世界知识&lt;/strong>，甚至还有了&lt;strong>顿悟能力(涌现能力)&lt;/strong>。&lt;/li>
&lt;li>就好像下图红框内庞大的AI大脑(&lt;strong>画的有点恶心&lt;/strong>)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150044511.png" alt="image-20231019150044511">&lt;/p>
&lt;h2 id="2方法2指令调优instruction-tuning">(2)方法2：指令调优(Instruction Tuning)&lt;/h2>
&lt;ul>
&lt;li>虽然花了巨大的时间成本和空间成本获得了预训练模型，但是它的水平，依然无法像一个真人，与人类对话。&lt;/li>
&lt;li>接下来还要进行微调——人工&lt;strong>准备少量的数据&lt;/strong>，对&lt;strong>预训练模型进行增量训练&lt;/strong>——因此也称为&lt;strong>有监督微调(SFT，Supervised Fine-tunning)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>指令微调&lt;/strong>本质是站在巨人的肩膀上&lt;strong>对预训练模型进行增量训练&lt;/strong>。它可以针对世界知识进行增强，也可以针对某个垂直领域进行增强。&lt;/li>
&lt;li>指令微调后得到的大语言训练模型，将会接近于一个真人。&lt;/li>
&lt;li>就好像下图红框内的红色人脸(&lt;strong>画的也挺恶心&lt;/strong>)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150129023.png" alt="image-20231019150129023">&lt;/p>
&lt;h2 id="3方法3对齐alignment">(3)方法3：对齐(Alignment)&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>在指令调优后，大语言模型虽然接近于真人，但生成的内容依然会很生硬。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为什么呢？这就是人类的主观感受——人类对某些问题的答案，会有情绪、语气、风格等主观的特征。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>那么，又如何&lt;strong>让AI学会人类的主观感受&lt;/strong>呢？这&lt;strong>就是对齐Alignment&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RLHF(基于人类反馈的强化学习)就是对齐的具体实现之一，利用这种强化学习手段，让大语言模型学会人类的主观感受，GPT3就演进成为了ChatGPT。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>就好像下图右侧红框内小黄球(&lt;strong>画的依然挺恶心&lt;/strong>)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151032099.png" alt="image-20231019151032099">&lt;/p>
&lt;p>接下来，我们逐一分析这三大方法。&lt;/p>
&lt;h1 id="4方法1预训练pre-training">4.方法1：预训练(Pre-Training)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;p>我们对比两张图：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>预训练&lt;/strong>得到的大语言模型，就&lt;strong>是ChatGPT整体训练流程STEP1的输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151849173.png" alt="image-20231019151849173">&lt;/p>
&lt;h2 id="2辩证看待预训练模型涌现能力神经网络">(2)辩证看待预训练模型、涌现能力、神经网络&lt;/h2>
&lt;p>前段时间，师父问了我3个终极问题，让我一时语塞：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>为什么ChatGPT大力能出奇迹，小力就不行？&lt;/strong>&lt;/li>
&lt;li>&lt;strong>为什么这样的神经网络模型就可以大力出奇迹，换个神经网络模型行不行？&lt;/strong>&lt;/li>
&lt;li>&lt;strong>如果无论什么神经网络模型只要能做到大力就能出奇迹，那么这些神经网络的本质是什么？&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>前面半年一直在复现Transformer论文的细节中，的确缺少了对宏观本质的思考，结合预训练这个章节的写作，正好梳理一下我的宏观思考：&lt;/p>
&lt;h3 id="思考1神经网络的本质是什么">思考1：神经网络的本质是什么？&lt;/h3>
&lt;ul>
&lt;li>观点1：客观世界中，&lt;strong>一切问题都能用函数表达&lt;/strong>。
&lt;ul>
&lt;li>只不过有的函数极其复杂，只有上帝才知道这个函数是什么。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点2：&lt;strong>神经网络的本质进行函数近似(Function Approximation)的工具&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>神经网络的输入&lt;/strong>：是大量的数据，数据中隐藏了某个问题背后的函数的数学规律。&lt;/li>
&lt;li>&lt;strong>神经网络的输出&lt;/strong>：找到无限逼近于某个问题背后的函数的近似函数。&lt;/li>
&lt;li>数学上，已经证明&lt;strong>神经网络能够近似出任意一个问题背后的函数&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>神经网络的结构&lt;/strong>，&lt;strong>决定了找到这个近似函数的成本&lt;/strong>(时间成本、空间成本……)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="思考2假设思考1正确如何解释神经网络的5种现象">思考2：假设思考1正确，如何解释神经网络的5种现象？&lt;/h3>
&lt;ul>
&lt;li>现象1：&lt;strong>过于简单的神经网络&lt;/strong>，需要很大力才能出奇迹。
&lt;ul>
&lt;li>任务是&amp;quot;让AI对红点和蓝点进行分类&amp;rdquo;，由于神经网络过于简单，所以迭代了2000多次才找出答案。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=364782652&amp;bvid=BV1F94y1b7MW&amp;cid=1304339810&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象2：&lt;strong>过于简单的任务&lt;/strong>，不需要大力也能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=874844018&amp;bvid=BV1EN4y1C7v2&amp;cid=1304340008&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象3：恒定难度的任务，&lt;strong>加宽神经网络&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=234774107&amp;bvid=BV1N8411r78C&amp;cid=1304339757&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象4：恒定难度的任务，&lt;strong>加深神经网络&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=789763364&amp;bvid=BV1Ry4y1N7CU&amp;cid=1304340937&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象5：恒定难度的任务，&lt;strong>增强神经元&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=277344904&amp;bvid=BV14w411w7Nx&amp;cid=1304343623&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;p>从上述5种现象，应该可以得到2个结论：&lt;/p>
&lt;ul>
&lt;li>在神经网络结构恒定的前提下，&lt;strong>待执行的任务难度&lt;/strong>，决定了&lt;strong>能否大力出奇迹、是否需要大力&lt;/strong>。&lt;/li>
&lt;li>在待执行的任务难度恒定的前提下，&lt;strong>神经网络结构&lt;/strong>，决定了&lt;strong>能否大力出奇迹、是否需要大力&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h3 id="思考3辩证地看待预训练模型涌现能力transformer">思考3：辩证地看待预训练模型、涌现能力、Transformer&lt;/h3>
&lt;p>当GPT3.5出现&lt;strong>涌现能力(emergent capabilities)&lt;strong>后，它似乎被神话成了&lt;/strong>人类尚无办法解释的神迹&lt;/strong>。&lt;/p>
&lt;p>我们应该如此这般辩证地看待预训练模型、涌现、Transformer&lt;/p>
&lt;ul>
&lt;li>预训练模型不是ChatGPT首创，在深度学习时代就有了，有很多神经网络都是预训练好，节省后来人的训练成本。&lt;/li>
&lt;li>Transformer只是GPT3这种预训练模型遵循的神经网络结构，&lt;strong>Transformer这种神经网络结构本质是换了一种姿势寻找近似函数&lt;/strong>。&lt;/li>
&lt;li>如果问题P1、问题P2、……问题Pn背后的函数都极为相近，当神经网络结构找到了问题P1的近似函数，那么这个近似函数也能用来解决问题P2、……问题Pn——这就是经过训练，神经网络能自我学习到一些额外的能力的原因，即涌现能力。
&lt;ul>
&lt;li>进一步思考：深度学习时代有没有涌现呢？可能有，只是n这个值比较小，不太明显吧。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="5方法2指令调优instruction-tuning">5.方法2：指令调优(Instruction Tuning)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系-1">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;ul>
&lt;li>指令调优对应于ChatGPT整体训练流程的STEP1。&lt;/li>
&lt;li>&lt;strong>SFT&lt;/strong>：有监督微调，属于有监督学习。包括In-context Tuning(上下文调优)和Instruction Tuning(指令调优)。
&lt;ul>
&lt;li>&lt;strong>In-context Tuning&lt;/strong>：上下文调优，这种调优方式的本质是将多轮对话的聊天记录一起发送给大语言模型，有了上下文，大语言模型就能更好地回答问题。&lt;/li>
&lt;li>&lt;strong>Instruction Tuning&lt;/strong>：指令调优，这种调优方式的本质就是问题中带有明确的指令、明确的要求。Instruction Tuning是OpenAI在GPT-3.5-turbo模型中引入的一种新方法，是在传统的微调过程上的一种变体。&lt;/li>
&lt;li>对于上述两种调优方式，在提示词工程中都体现了它们的思想——&lt;strong>&amp;ldquo;上下文&amp;rdquo;、&amp;ldquo;指令&amp;rdquo;&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019170641604.png" alt="image-20231019170641604">&lt;/p>
&lt;h2 id="2训练sft模型的伪码">(2)训练SFT模型的伪码&lt;/h2>
&lt;p>前文讲了很多理论，还是需要撸一下代码比较便于理解。&lt;/p>
&lt;blockquote>
&lt;p>由于OpenAI对于SFT、RLHF并未公开源码，所以在这里只编写伪码，在后续实例中展示真实代码。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019181420212.png" alt="image-20231019181420212">&lt;/p>
&lt;h2 id="3sft实例">(3)SFT实例&lt;/h2>
&lt;p>针对简版GPT，只需要增加如下代码：&lt;/p>
&lt;ul>
&lt;li>首先，通过&lt;code>torch.load方法&lt;/code>加载上一篇已经预训练好的简版GPT模型，这个模型的训练数据仅包含了维基百科的基本数据。&lt;/li>
&lt;li>然后，将新增的医学知识数据集，对简版GPT模型进行增量训练。&lt;/li>
&lt;li>最后，通过&lt;code>torch.save方法&lt;/code>保存增量训练的简版GPT模型，此时，这个模型就包含了医学知识了。&lt;/li>
&lt;li>具体代码详见下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019182641050.png" alt="image-20231019182641050">&lt;/p>
&lt;h1 id="6小结">6.小结&lt;/h1>
&lt;p>讲到这里，内容已经比较饱和了，我们将在最后一篇阐述&lt;strong>方法3：对齐训练&lt;/strong>以及&lt;strong>RLHF的实例&lt;/strong>。&lt;/p>
&lt;p>我们接下来对本文进行一下小结：&lt;/p>
&lt;ul>
&lt;li>ChatGPT的历史版本：&lt;strong>GPT3、InstructGPT、ChatGPT&lt;/strong>。&lt;/li>
&lt;li>ChatGPT&lt;strong>整体训练流程&lt;/strong>，支撑了&lt;strong>GPT3到ChatGPT的演进&lt;/strong>。&lt;/li>
&lt;li>预训练模型是什么？如何&lt;strong>辩证地&lt;/strong>看待&lt;strong>预训练模型、涌现能力、神经网络的学习能力&lt;/strong>？&lt;/li>
&lt;li>SFT(有监督微调)的概念、原理，最后展示了&lt;strong>如何针对简版GPT进行SFT&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>我们下一步继续针对简版ChatGPT开展RLHF，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</link><pubDate>Mon, 16 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</guid><description>&lt;p>接下来，我们用三篇文章阐述&lt;font color=red>**如何实现一个简版ChatGPT。**&lt;/font>&lt;/p>
&lt;h1 id="1回顾">1.回顾&lt;/h1>
&lt;p>想实现一个简版ChatGPT，依赖于如下前置知识：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>机器学习基本原理&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>《【chatGPT】学习笔记3-机器学习基本原理(上)》&lt;/li>
&lt;li>《【chatGPT】学习笔记4-机器学习基本原理(下)》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>经典NLP相关技术&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>&lt;strong>N-Gram&lt;/strong>：《【chatGPT】学习笔记6-手撸一个上古GPT》&lt;/li>
&lt;li>&lt;strong>Embedding&lt;/strong>：《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》&lt;/li>
&lt;li>&lt;strong>神经概率语言模型&lt;/strong>：《【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2》&lt;/li>
&lt;li>&lt;strong>Seq2Seq&lt;/strong>：《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>现代NLP相关技术&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>&lt;strong>注意力机制&lt;/strong>：《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》&lt;/li>
&lt;li>&lt;strong>Transformer&lt;/strong>：《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="2实现简版gpt">2.实现简版GPT&lt;/h1>
&lt;p>这是参考Transformer架构绘制的简版ChatGPT整体架构，我们将对它进行拆解：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017113929233.png" alt="image-20231017113929233">&lt;/p>
&lt;h2 id="1整体">(1)整体&lt;/h2>
&lt;p>将上图抽象化，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>简版GPT遵循Transfomer架构，但是没有实现编码器&lt;/li>
&lt;li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017134713086.png" alt="image-20231017134713086">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1&lt;/strong>：对应上图将&lt;strong>Outputs&lt;/strong>输入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>2&lt;/strong>：对应上图将&lt;strong>Decoder的输出&lt;/strong>，传入给线性层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017135734066.png" alt="image-20231017135734066">&lt;/p>
&lt;h2 id="2局部1正弦位置编码表">(2)局部1：正弦位置编码表&lt;/h2>
&lt;p>首先，我们来细化&lt;strong>Outputs&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间的流程：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.对&lt;strong>Outputs&lt;/strong>实施词嵌入，得到词向量。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.在&lt;strong>词向量&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间增加了&lt;strong>位置编码表&lt;/strong>(也是一个向量)，这个位置编码表体现了&lt;strong>词和词序的关系&lt;/strong>。
&lt;ul>
&lt;li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立&lt;strong>词和词序的关系&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.将STEP2的&lt;strong>位置表码表&lt;/strong>向量和&lt;strong>词向量&lt;/strong>相加，输入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140335543.png" alt="image-20231017140335543">&lt;/p>
&lt;p>正弦位置编码表的计算原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(2)局部1：正弦位置编码表&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140636308.png" alt="image-20231017140636308">&lt;/p>
&lt;h2 id="3局部2解码器堆栈">(3)局部2：解码器堆栈&lt;/h2>
&lt;p>我们再来细化&lt;strong>解码器&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>编码器本质上由&lt;strong>N个解码器&lt;/strong>串联而成的&lt;strong>解码器堆栈&lt;/strong>。&lt;/li>
&lt;li>我们的实现，也按照论文的设定层数，&lt;strong>N=6&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150909585.png" alt="image-20231017150909585">&lt;/p>
&lt;h2 id="4局部3解码器">(4)局部3：解码器&lt;/h2>
&lt;p>我们再进一步细化&lt;strong>解码器Decoder&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>解码器Decoder&lt;/strong>由&lt;strong>多头注意力&lt;/strong>和&lt;strong>前向传播网络&lt;/strong>组成。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150925061.png" alt="image-20231017150925061">&lt;/p>
&lt;p>&lt;strong>解码器堆栈&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：这就是创建的&lt;strong>位置编码层&lt;/strong>，再将&lt;strong>词向量和位置编码向量相加&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：这就是将多个&lt;strong>解码器&lt;/strong>叠加成&lt;strong>解码器堆栈&lt;/strong>，每个&lt;strong>解码器的输入&lt;/strong>是&lt;strong>上个解码器的输出&lt;/strong>和&lt;strong>上个解码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>③&lt;/strong>：这就是表示&lt;strong>解码器堆栈&lt;/strong>输出的&lt;strong>解码器输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017142049236.png" alt="image-20231017142049236">&lt;/p>
&lt;p>&lt;strong>解码器&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：就是&lt;strong>多头注意力层&lt;/strong>，&lt;strong>第一个解码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>词嵌入+位置编码向量之和&lt;/strong>以及&lt;strong>自注意力掩码&lt;/strong>，&lt;strong>后续解码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>上一个解码器的输出&lt;/strong>和&lt;strong>自注意力掩码&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：就是&lt;strong>前向传播网络&lt;/strong>，它的输入是&lt;strong>多头注意力层的输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145220345.png" alt="image-20231017145220345">&lt;/p>
&lt;p>我们接下来看&lt;strong>多头注意力层、前向传播网络、自注意力位置掩码&lt;/strong>如何实现？&lt;/p>
&lt;h2 id="5局部4多头注意力">(5)局部4：多头注意力&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(5)局部4：多头注意力&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150943298.png" alt="image-20231017150943298">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145622219.png" alt="image-20231017145622219">&lt;/p>
&lt;h2 id="6局部5前向传播网络">(6)局部5：前向传播网络&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(6)局部5：前向传播网络&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151000709.png" alt="image-20231017151000709">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150114991.png" alt="image-20231017150114991">&lt;/p>
&lt;h2 id="7局部6填充位置掩码">(7)局部6：填充位置掩码&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(7)局部6：填充位置掩码&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151014381.png" alt="image-20231017151014381">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150413819.png" alt="image-20231017150413819">&lt;/p>
&lt;h2 id="8局部7后续位置掩码">(8)局部7：后续位置掩码&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(10)局部9：后续位置掩码&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151026419.png" alt="image-20231017151026419">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150515799.png" alt="image-20231017150515799">&lt;/p>
&lt;h2 id="9模型训练">(9)模型训练&lt;/h2>
&lt;p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：&lt;/p>
&lt;ul>
&lt;li>数据集如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151515137.png" alt="image-20231017151515137">&lt;/p>
&lt;ul>
&lt;li>模型训练：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151651529.png" alt="image-20231017151651529">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151717714.png" alt="image-20231017151717714">&lt;/p>
&lt;ul>
&lt;li>训练好后，会生成简版GPT的pth模型文件：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151835127.png" alt="image-20231017151835127">&lt;/p>
&lt;h2 id="10模型测试">(10)模型测试&lt;/h2>
&lt;ul>
&lt;li>测试用例采用贪婪编码和集束编码，比较简单，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151956136.png" alt="image-20231017151956136">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;ul>
&lt;li>本文基于Transformer架构，复现了简版ChatGPT，其关键在于只有解码器。&lt;/li>
&lt;li>理论上，如果有足够算力、足够训练数据，可以将此简版ChatGPT训练到GPT3的水平。&lt;/li>
&lt;li>那么，GPT3到ChatGPT还有一定的距离，我们知道ChatGPT公开的信息中，还对GPT3进行了&lt;strong>监督学习微调SFT&lt;/strong>、&lt;strong>基于人类反馈的强化学习RLHF&lt;/strong>等，得到了InstructGPT，进而得到了ChatGPT。&lt;/li>
&lt;/ul>
&lt;p>我们下一步继续针对简版ChatGPT开展SFT和RLHF，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</link><pubDate>Tue, 26 Sep 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</guid><description>&lt;p>在《AI拾遗》这个专栏中，我们建立了从&lt;strong>N-Gram&lt;/strong>到&lt;strong>词嵌入&lt;/strong>再到&lt;strong>神经概率语言模型&lt;/strong>，从&lt;strong>Seq2Seq&lt;/strong>到&lt;strong>注意力机制&lt;/strong>的知识脉络。&lt;/p>
&lt;p>这条脉络本质就是NLP发展的路线图，有了这些知识储备，我们终于可以来理解论文**《Attention Is All You Need》**中大名鼎鼎的**Transformer架构**了！&lt;/p>
&lt;h1 id="1问题">1.问题&lt;/h1>
&lt;p>在《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》中，笔者为&lt;strong>编码器-解码器架构&lt;/strong>增加了&lt;strong>注意力机制&lt;/strong>，进而实现了&lt;strong>增强版的Seq2Seq模型&lt;/strong>，模型能力的确有所增强，但并不是彻底解决了&lt;strong>长距离依赖&lt;/strong>问题和&lt;strong>信息压缩&lt;/strong>问题。&lt;/p>
&lt;p>在《Attention Is All You Need》的第一章阐述了这个观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>howerver, remains&lt;/strong>(第二段最后一句)：历史上很多论文和技术都在增强&lt;strong>编码器-解码器架构&lt;/strong>，注意力机制也成为序列建模必备的技术，但&lt;strong>长距离依赖&lt;/strong>问题依然存在。&lt;/li>
&lt;li>&lt;strong>parallelization&lt;/strong>(第三段)：这里提到了并行化，这是前面没有提到的问题——RNN网络决定了Seq2Seq只能一个词一个词的处理。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926072417665.png" alt="image-20230926072417665">&lt;/p>
&lt;h1 id="2transformer逐步拆解">2.Transformer逐步拆解&lt;/h1>
&lt;p>这是论文中第三段绘制的Transformer整体架构，我们将对它进行拆解：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926074022967.png" alt="image-20230926074022967">&lt;/p>
&lt;h2 id="1整体transformer">(1)整体：Transformer&lt;/h2>
&lt;p>将上图抽象化，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>Transformer依然可以遵循Encoder-Decoder架构&lt;/li>
&lt;li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926082223230.png" alt="image-20230926082223230">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1&lt;/strong>：对应上图将&lt;strong>Inputs&lt;/strong>输入给&lt;strong>Encoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>2&lt;/strong>：对应上图将&lt;strong>Outputs+Encoder的输出&lt;/strong>，传入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>3&lt;/strong>：对应上图将&lt;strong>Decoder的输出&lt;/strong>，传入给线性层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083838772.png" alt="image-20230926083838772">&lt;/p>
&lt;p>PS：这里的&lt;code>corpus&lt;/code>是一个封装了&lt;code>Inputs&lt;/code>和&lt;code>Outputs&lt;/code>的工具类，代码如下(比较简单，不赘述)：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083336984.png" alt="image-20230926083336984">&lt;/p>
&lt;h2 id="2局部1正弦位置编码表">(2)局部1：正弦位置编码表&lt;/h2>
&lt;p>首先，我们来细化&lt;strong>Inputs&lt;/strong>和&lt;strong>Encoder&lt;/strong>之间的流程：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.对&lt;strong>Inputs&lt;/strong>实施词嵌入，得到词向量。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.在&lt;strong>词向量&lt;/strong>和&lt;strong>Encoder&lt;/strong>之间增加了&lt;strong>位置编码表&lt;/strong>(也是一个向量)，这个位置编码表体现了&lt;strong>词和词序的关系&lt;/strong>。
&lt;ul>
&lt;li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立&lt;strong>词和词序的关系&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.将STEP2的&lt;strong>位置表码表&lt;/strong>向量和&lt;strong>词向量&lt;/strong>相加，输入给&lt;strong>Encoder&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Outputs&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间的流程和上述流程一样。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926092023205.png" alt="image-20230926092023205">&lt;/p>
&lt;p>那么位置编码表如何计算呢？论文3.5章节详细阐述如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>d&lt;/strong>：词向量的维度。&lt;/li>
&lt;li>&lt;strong>pos&lt;/strong>：单词在句子中的位置。&lt;/li>
&lt;li>&lt;strong>i&lt;/strong>：词向量的维度的奇数维。&lt;/li>
&lt;li>&lt;strong>PE&lt;/strong>：指定位置的单词，在词向量的某一个维度上的数值。&lt;/li>
&lt;li>通俗地理解，&lt;strong>d个PE值构成了指定单词在整个句子中的位置向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926093855305.png" alt="image-20230926093855305">&lt;/p>
&lt;p>我们不必纠结于论文中这两个公式的证明，笔者绘制一个例子，可视化地理解正弦位置编码表的作用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：输入序列为&amp;rdquo;&lt;strong>想去新疆&lt;/strong>&amp;ldquo;四个字，词向量的维度为4维，即&lt;strong>d=4&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：通过正弦位置编码公式，&lt;strong>想&lt;/strong>字的&lt;strong>位置0&lt;/strong>，求得位置0的&lt;strong>位置向量[0, 1, 0, 1]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：通过正弦位置编码公式，&lt;strong>去&lt;/strong>字的&lt;strong>位置1&lt;/strong>，求得位置1的&lt;strong>位置向量[0.84, 0.54, 0.01, 1.0]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：通过正弦位置编码公式，&lt;strong>新&lt;/strong>字的&lt;strong>位置2&lt;/strong>，求得位置2的&lt;strong>位置向量[0.91, -0.42, 0.02, 1.0]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP4&lt;/strong>：通过正弦位置编码公式，&lt;strong>疆&lt;/strong>字的&lt;strong>位置3&lt;/strong>，求得位置3的&lt;strong>位置向量[0.14, -0.99, 0.03, 1.0]&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926111712233.png" alt="image-20230926111712233">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926110113612.png" alt="image-20230926110113612">&lt;/p>
&lt;h2 id="3局部2编码器堆栈">(3)局部2：编码器堆栈&lt;/h2>
&lt;p>我们再来细化&lt;strong>编码器&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>编码器本质上由&lt;strong>N个编码器&lt;/strong>串联而成的&lt;strong>编码器堆栈&lt;/strong>。&lt;/li>
&lt;li>论文中，&lt;strong>N=6&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926112341735.png" alt="image-20230926112341735">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>论文原文&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926113124754.png" alt="image-20230926113124754">&lt;/p>
&lt;h2 id="4局部3编码器">(4)局部3：编码器&lt;/h2>
&lt;p>我们再进一步细化&lt;strong>编码器Encoder&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编码器Encoder&lt;/strong>由&lt;strong>多头注意力&lt;/strong>和&lt;strong>前向传播网络&lt;/strong>组成。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926114259743.png" alt="image-20230926114259743">&lt;/p>
&lt;p>&lt;strong>编码器堆栈&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：这就是调用&lt;strong>正弦位置编码表&lt;/strong>，创建的&lt;strong>位置编码层&lt;/strong>，再将&lt;strong>词向量和位置编码向量相加&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：这就是将多个&lt;strong>编码器&lt;/strong>叠加成&lt;strong>编码器堆栈&lt;/strong>，每个&lt;strong>编码器的输入&lt;/strong>是&lt;strong>上个编码器的输出&lt;/strong>和&lt;strong>上个编码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>③&lt;/strong>：这就是表示&lt;strong>编码器堆栈&lt;/strong>输出的&lt;strong>编码器输出&lt;/strong>、&lt;strong>编码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926115605435.png" alt="image-20230926115605435">&lt;/p>
&lt;p>&lt;strong>编码器&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：就是&lt;strong>多头注意力层&lt;/strong>，&lt;strong>第一个编码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>词嵌入+位置编码向量之和&lt;/strong>以及&lt;strong>自注意力掩码&lt;/strong>，&lt;strong>后续编码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>上一个编码器的输出&lt;/strong>和&lt;strong>自注意力掩码&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：就是&lt;strong>前向传播网络&lt;/strong>，它的输入是&lt;strong>多头注意力层的输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926122832935.png" alt="image-20230926122832935">&lt;/p>
&lt;p>这里又埋下了几个问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多头注意力层&lt;/strong>如何实现？&lt;/li>
&lt;li>&lt;strong>前向传播网络&lt;/strong>如何实现？&lt;/li>
&lt;li>&lt;strong>自注意力位置掩码&lt;/strong>如何实现？&lt;/li>
&lt;/ul>
&lt;h2 id="5局部4多头注意力">(5)局部4：多头注意力&lt;/h2>
&lt;p>我们再来细化多头注意力：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>不赘述&lt;/strong>：关于&lt;strong>点积注意力&lt;/strong>、&lt;strong>缩放点积注意力&lt;/strong>、&lt;strong>编码器-解码器注意力&lt;/strong>、&lt;strong>QKV&lt;/strong>、&lt;strong>自注意力&lt;/strong>、&lt;strong>多头注意力&lt;/strong>，本文就不再赘述了。如果理解不太清晰，可以回看《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》。&lt;/li>
&lt;li>&lt;strong>多头注意力&lt;/strong>的结构：&lt;strong>多头注意力的输入&lt;/strong>是&lt;strong>词向量与位置编码向量之和&lt;/strong>，每一个注意力头都是对&lt;strong>多头注意力的输入&lt;/strong>进行矩阵乘法得到&lt;strong>QKV&lt;/strong>，再输入给&lt;strong>缩放点积注意力组件&lt;/strong>，这个组件输出的是&lt;strong>注意力权重&lt;/strong>。最后，将每个注意力头输出的注意力权重求和，输入给一个线性层。&lt;/li>
&lt;li>&lt;strong>缩放点积注意力&lt;/strong>的结构：就是典型的缩放点积注意力的计算公式，即：Q、K求点积=&amp;gt;缩放=&amp;gt;注意力掩码=&amp;gt;Softmax=&amp;gt;和V点积。&lt;/li>
&lt;li>&lt;strong>细节&lt;/strong>：这里增加了Add &amp;amp; Norm，就是深度学习里面的残差连接、层归一化，为了解决梯度爆炸问题，这不是Transformer特有的新知识。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926162228532.png" alt="image-20230926162228532">&lt;/p>
&lt;p>论文原文：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>缩放点积注意力&lt;/strong>计算公式：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161049010.png" alt="image-20230926161049010">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多头注意力&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161136259.png" alt="image-20230926161136259">&lt;/p>
&lt;p>&lt;strong>缩放点积注意力&lt;/strong>的代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161300468.png" alt="image-20230926161300468">&lt;/p>
&lt;p>&lt;strong>多头注意力&lt;/strong>的代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161409926.png" alt="image-20230926161409926">&lt;/p>
&lt;h2 id="6局部5前向传播网络">(6)局部5：前向传播网络&lt;/h2>
&lt;p>我们再来细化前向神经网络：&lt;/p>
&lt;ul>
&lt;li>前向神经网络的全称是&lt;strong>Position-wise Feed-Forward Network&lt;/strong>，即&lt;strong>基于位置的前馈神经网络&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>two linear transformations with a ReLU activation&lt;/strong>：首先使用第一个线性层做升维，接着使用ReLU激活函数，再使用第二个线性层做降维。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163125569.png" alt="image-20230926163125569">&lt;/p>
&lt;p>这个基于位置的前馈神经网络到底有啥用呢？&lt;/p>
&lt;ul>
&lt;li>就是论文中，多头注意力结构中的最后一步&lt;strong>Linear&lt;/strong>！&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163933555.png" alt="image-20230926163933555">&lt;/p>
&lt;p>论文原文：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163424383.png" alt="image-20230926163424383">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926164434302.png" alt="image-20230926164434302">&lt;/p>
&lt;h2 id="7局部6填充位置掩码">(7)局部6：填充位置掩码&lt;/h2>
&lt;p>我们再来细化填充位置掩码：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>填充位置掩码&lt;/strong>用在&lt;strong>词嵌入&lt;/strong>之后，&lt;strong>编码器&lt;/strong>输入之前。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png" alt="image-20230926170827083">&lt;/p>
&lt;ul>
&lt;li>填充位置掩码有什么作用呢？
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：给编码器输入了一句话——&amp;ldquo;我想去新疆滑雪&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：给编码器输入第二句话——&amp;ldquo;想去就去啊&amp;rdquo;。为了和上一句话保持长度统一，我们就会在在第二句话末尾增加两个占位符。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：生成填充位置掩码[1, 1, 1, 1, 1, 0, 0]。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：编码器会将第二句话和填充位置掩码求与，这样编码器实施多头注意力的时候，就不会注意毫无意义的两个占位符。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171245346.png" alt="image-20230926171245346">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171345069.png" alt="image-20230926171345069">&lt;/p>
&lt;p>至此，我们就把Transformer架构中编码器部分细化完成了，我们继续细化解码器部分：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png" alt="image-20230926170827083">&lt;/p>
&lt;h2 id="8局部7解码器堆栈">(8)局部7：解码器堆栈&lt;/h2>
&lt;p>解码器堆栈的思想到实现，和编码器堆栈完全一样，这里不再赘述，直接上图和代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174658980.png" alt="image-20230926174658980">&lt;/p>
&lt;h2 id="9局部8解码器">(9)局部8：解码器&lt;/h2>
&lt;p>解码器的思想到实现，和编码器堆栈大致一样，这里不再赘述，直接上图和代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174249742.png" alt="image-20230926174249742">&lt;/p>
&lt;p>&lt;strong>解码器堆栈&lt;/strong>代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174857116.png" alt="image-20230926174857116">&lt;/p>
&lt;p>&lt;strong>解码器&lt;/strong>代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174916494.png" alt="image-20230926174916494">&lt;/p>
&lt;h2 id="10局部9后续位置掩码">(10)局部9：后续位置掩码&lt;/h2>
&lt;p>在解码器中，还有最后一个遗留问题——后续位置掩码。&lt;/p>
&lt;p>后续位置掩码只是因为解码器实施多头注意力的时候，是不能注意到&lt;strong>未来&lt;/strong>的，也就是它还没有预测的后续词，所以要屏蔽掉。&lt;/p>
&lt;p>后续位置掩码和填充位置掩码的思想是一致的，不赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175344740.png" alt="image-20230926175344740">&lt;/p>
&lt;p>&lt;strong>后续位置掩码&lt;/strong>的代码实现：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175131684.png" alt="image-20230926175131684">&lt;/p>
&lt;h2 id="11模型训练">(11)模型训练&lt;/h2>
&lt;p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：&lt;/p>
&lt;ul>
&lt;li>数据集如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180145967.png" alt="image-20230926180145967">&lt;/p>
&lt;ul>
&lt;li>模型训练：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180246638.png" alt="image-20230926180246638">&lt;/p>
&lt;h2 id="12模型测试">(12)模型测试&lt;/h2>
&lt;ul>
&lt;li>测试用例采用贪婪编码，比较简单，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180356687.png" alt="image-20230926180356687">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>笔者在今年2月份第一次阅读论文《Attention Is All You Need》，读了好几遍，不得要领，只觉得非常抽象。&lt;/p>
&lt;p>随着在网上阅读各类资料、逐步摸索复现论文中Transformer架构的源码，逐渐理解这篇论文中所说的——&lt;strong>Attention Is All You Need&lt;/strong>的含义。&lt;/p>
&lt;p>本以为理解了论文含义，提笔准备写出这篇文章时，又卡了壳——因为理解了，又很难表达出来Transformer的精妙原理！&lt;/p>
&lt;p>此时，笔者才真正领悟之前听过一位大神所说的：&lt;font color=red>”LLM涉及的每一篇经典论文，不仅值得&lt;strong>反复阅读&lt;/strong>，甚至应该&lt;strong>背诵下来&lt;/strong>。“&lt;/font>的含义。&lt;/p>
&lt;p>这篇文章写完，我们下一步就可以实现并训练我们平民版的ChatGPT了，且听下回分解了。&lt;/p></description></item><item><title>【chatGPT】学习笔记15-LangChain之Chain，对LLM的抽象3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</link><pubDate>Wed, 20 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</guid><description>&lt;p>我们继续写点儿偏工程实践的内容——LangChain的核心模块3——Chain。&lt;/p>
&lt;h1 id="1核心模块3chain">1.核心模块3：Chain&lt;/h1>
&lt;p>在《【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)》中，我们知道LangChain-ChatChat有如下工作流程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921092837843.png" alt="image-20230921092837843">&lt;/p>
&lt;p>如何实现呢？其实，LangChain抽象了&lt;strong>Chain&lt;/strong>的概念。&lt;/p>
&lt;h2 id="11chain-class">1.1.Chain Class&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：Chain &amp;ndash;&amp;gt; &lt;name>Chain # Examples: LLMChain, MapReduceChain, RouterChain&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/base.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Chain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Serializable&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Runnable&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for creating structured sequences of calls to components.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Chains should be used to encode a sequence of calls to components like
&lt;/span>&lt;span class="s2"> models, document retrievers, other chains, etc., and provide a simple interface
&lt;/span>&lt;span class="s2"> to this sequence.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The Chain interface makes it easy to create apps that are:
&lt;/span>&lt;span class="s2"> - Stateful: add Memory to any Chain to give it state,
&lt;/span>&lt;span class="s2"> 有状态的：给Chain添加Memory，使其具有状态
&lt;/span>&lt;span class="s2"> - Observable: pass Callbacks to a Chain to execute additional functionality,
&lt;/span>&lt;span class="s2"> like logging, outside the main sequence of component calls,
&lt;/span>&lt;span class="s2"> 可观察的：向Chain传递Callback来执行额外的功能。
&lt;/span>&lt;span class="s2"> - Composable: the Chain API is flexible enough that it is easy to combine
&lt;/span>&lt;span class="s2"> Chains with other components, including other Chains.
&lt;/span>&lt;span class="s2"> 可组合的：Chain API足够灵活，可以轻松地将Chains与其他组件组合在一起，包括组合其他的Chain。
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The main methods exposed by chains are:
&lt;/span>&lt;span class="s2"> - `__call__`: Chains are callable. The `__call__` method is the primary way to
&lt;/span>&lt;span class="s2"> execute a Chain. This takes inputs as a dictionary and returns a
&lt;/span>&lt;span class="s2"> dictionary output.
&lt;/span>&lt;span class="s2"> 执行Chain的主要方式，输入是一个字典，输出也是一个字典。
&lt;/span>&lt;span class="s2"> - `run`: A convenience method that takes inputs as args/kwargs and returns the
&lt;/span>&lt;span class="s2"> output as a string or object. This method can only be used for a subset of
&lt;/span>&lt;span class="s2"> chains and cannot return as rich of an output as `__call__`.
&lt;/span>&lt;span class="s2"> 输入是args/kwargs，输出是字符串or对象。仅用于部分链，没有__call__方法通用。
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="o">............&lt;/span>
&lt;span class="n">memory&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMemory&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Optional memory object. Defaults to None.
&lt;/span>&lt;span class="s2"> Memory is a class that gets called at the start
&lt;/span>&lt;span class="s2"> and at the end of every chain. At the start, memory loads variables and passes
&lt;/span>&lt;span class="s2"> them along in the chain. At the end, it saves any returned variables.
&lt;/span>&lt;span class="s2"> There are many different types of memory - please see memory docs
&lt;/span>&lt;span class="s2"> for the full catalog.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">每个链开始和结束时调用。存储&lt;/span>&lt;span class="n">Memory&lt;/span>&lt;span class="err">。&lt;/span>
&lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exclude&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Optional list of callback handlers (or callback manager). Defaults to None.
&lt;/span>&lt;span class="s2"> Callback handlers are called throughout the lifecycle of a call to a chain,
&lt;/span>&lt;span class="s2"> starting with on_chain_start, ending with on_chain_end or on_chain_error.
&lt;/span>&lt;span class="s2"> Each custom chain can optionally call additional callback methods, see Callback docs
&lt;/span>&lt;span class="s2"> for full details.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">callback_manager&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseCallbackManager&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exclude&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Deprecated, use `callbacks` instead.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default_factory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">_get_verbosity&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Whether or not run in verbose mode. In verbose mode, some intermediate logs
&lt;/span>&lt;span class="s2"> will be printed to the console. Defaults to `langchain.verbose` value.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">开启后，输出更纤细的日志。&lt;/span>
&lt;span class="o">............&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LLMChain&lt;/strong>：创建了一个&lt;strong>步骤&lt;/strong>，该步骤可执行&lt;code>prompt&lt;/code>提示词。&lt;/li>
&lt;li>&lt;strong>run方法&lt;/strong>：调用LLMChain的&lt;code>run&lt;/code>方法，可以和LLM进行问答。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921094132633.png" alt="image-20230921094132633">&lt;/p>
&lt;h2 id="12顺序链">1.2.顺序链&lt;/h2>
&lt;h3 id="1源码解读">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>SequentialChain&lt;/strong>：继承于Chain，顺序链，允许将多个Chain链接起来，形成&lt;strong>Pipeline(流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/sequential.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">SequentialChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Chain where the outputs of one chain feed directly into next.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chains&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">input_variables&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">output_variables&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">return_all&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="err">……………………&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>SimpleSequentialChain&lt;/strong>：继承于SequentialChain，一种简化版的顺序链，允许将多个Chain链接起来，形成&lt;strong>Pipeline(流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/sequential.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">SimpleSequentialChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Simple chain where the outputs of one step feed directly into next.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chains&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">strip_outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="n">input_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;input&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">output_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;output&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2simplesequentialchain的代码示例">(2)SimpleSequentialChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了LLM模仿医生对病情进行介绍，并给出治疗方案。&lt;/p>
&lt;ul>
&lt;li>通过&lt;strong>SimpleSequentialChain&lt;/strong>，连接了病情摘要Chain和病情评论Chain。&lt;/li>
&lt;li>病情摘要Chain的&lt;strong>输出&lt;/strong>，作为了病情评论Chain的&lt;strong>输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921095733390.png" alt="image-20230921095733390">&lt;/p>
&lt;h3 id="3sequentialchain的代码示例">(3)SequentialChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了LLM模仿医生对病情进行介绍，并给出治疗方案。&lt;/p>
&lt;p>不同的是，要支持多输入、多输出。&lt;/p>
&lt;ul>
&lt;li>通过&lt;strong>SequentialChain&lt;/strong>，连接了病情摘要Chain和病情评论Chain。&lt;/li>
&lt;li>病情摘要Chain的&lt;strong>输入有多个&lt;/strong>，病情摘要Chain的&lt;strong>输出&lt;/strong>作为了病情评论Chain的&lt;strong>输入&lt;/strong>，病情评论Chain的&lt;strong>输出有多个&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921100917222.png" alt="image-20230921100917222">&lt;/p>
&lt;h2 id="13决策链">1.3.决策链&lt;/h2>
&lt;h3 id="1源码解读-1">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>RouterChain&lt;/strong>：继承于Chain，决策链，允许将多个Chain链接起来，可以实现条件判断的&lt;strong>分支Pipeline(分支流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/router/base.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">RouterChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Chain that outputs the name of a destination chain and the inputs to it.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">output_keys&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2"> Route inputs to a destination chain.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Args:
&lt;/span>&lt;span class="s2"> inputs: inputs to the chain
&lt;/span>&lt;span class="s2"> callbacks: callbacks to use for the chain
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Returns:
&lt;/span>&lt;span class="s2"> a Route object
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">async&lt;/span> &lt;span class="k">def&lt;/span> &lt;span class="nf">aroute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">await&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">acall&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2routerchain的代码示例">(2)RouterChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了根据问题内容，选择程序员角色的AI或测试工程师AI出来回答问题。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>创建两个决策分支&lt;/strong>：能回答软件开发问题的程序员、能回答软件测试问题的程序员。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921102913545.png" alt="image-20230921102913545">&lt;/p>
&lt;ul>
&lt;li>创建根据问题内容进行决策的&lt;strong>RouterChain&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921102959963.png" alt="image-20230921102959963">&lt;/p>
&lt;p>这个决策链是如何实现的决策功能呢？进一步看一下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>destinations_str&lt;/strong>：描述了根据问题内容，期望选择哪个AI来回答问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103109653.png" alt="image-20230921103109653">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>MULTI_PROMPT_ROUTER_TEMPLATE&lt;/strong>：LangChain提供了决策链的提示词模板。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103154608.png" alt="image-20230921103154608">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>router_template&lt;/strong>：最终的决策链提示词为：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103225927.png" alt="image-20230921103225927">&lt;/p>
&lt;ul>
&lt;li>将决策链和两个回答问题的链进行连接。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103257969.png" alt="image-20230921103257969">&lt;/p>
&lt;ul>
&lt;li>测试一下关于软件开发的问题：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103327682.png" alt="image-20230921103327682">&lt;/p>
&lt;ul>
&lt;li>测试一下关于软件测试的问题：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103359806.png" alt="image-20230921103359806">&lt;/p>
&lt;h1 id="2小结">2.小结&lt;/h1>
&lt;p>本文阐述了Chain模块的内部实现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Chain类&lt;/strong>：抽象了&lt;strong>PipeLine流水线&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>顺序链&lt;/strong>：SequentialChain，实现了多输入多输出的串行执行的工作流。&lt;/li>
&lt;li>&lt;strong>决策链&lt;/strong>：RouterChain，实现了根据问题内容，选择工作流走向的能力。&lt;/li>
&lt;li>Chain类拥有很多子类，实现不同的业务流程，可以根据实战需要继续阅读源码和实践。&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记14-LangChain之Memory，对LLM的抽象2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</link><pubDate>Tue, 19 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</guid><description>&lt;p>我们继续写点儿偏工程实践的内容——LangChain的核心模块2——Chain。&lt;/p>
&lt;h1 id="1核心模块2memory">1.核心模块2：Memory&lt;/h1>
&lt;p>实现一个问答系统，通常需要将历史上的问题和答案，作为本次问题的上下文。&lt;/p>
&lt;p>因此，LangChain提供了Memory模块，这个模块对记忆进行了抽象：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.当用户提出问题时，LangChain会去读&lt;strong>Memory&lt;/strong>，获得过去的消息&lt;strong>past_messages&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.LangChain构造提示词，格式为&amp;rdquo;&lt;strong>{past_messages}{question}&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.LLM进行回答后，得到答案**{answer:&amp;hellip;}**。&lt;/li>
&lt;li>STEP4.LangChain将本次的答案**{answer:&amp;hellip;}**，**写入Memory**。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920062809577.png" alt="image-20230920062809577">&lt;/p>
&lt;p>LangChain提供了多种ChatMessageHistory、ChatMemory，我们接下来详细解读。&lt;/p>
&lt;h2 id="11chatmessagehistory">1.1.ChatMessageHistory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：BaseChatMessageHistory &amp;ndash;&amp;gt; &lt;name>ChatMessageHistory # Example: ZepChatMessageHistory&lt;/li>
&lt;li>&lt;strong>BaseChatMessageHistory&lt;/strong>：聊天消息历史记录的基类，定义了一系列方法，由子类实现&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/schema/chat_history.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920064148308.png" alt="image-20230920064148308">&lt;/p>
&lt;h2 id="12basememory">1.2.BaseMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：BaseMemory &amp;ndash;&amp;gt; BaseChatMemory &amp;ndash;&amp;gt; &lt;name>Memory # Examples: ZepMemory, MotorheadMemory&lt;/li>
&lt;li>&lt;strong>BaseMemory&lt;/strong>：Memory基类，定义了一系列方法，由子类实现&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/schema/memory.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">BaseMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Serializable&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for memory in Chains.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Memory refers to state in Chains. Memory can be used to store information about
&lt;/span>&lt;span class="s2"> past executions of a Chain and inject that information into the inputs of
&lt;/span>&lt;span class="s2"> future executions of the Chain. For example, for conversational Chains Memory
&lt;/span>&lt;span class="s2"> can be used to store conversations and automatically add them to future model
&lt;/span>&lt;span class="s2"> prompts so that the model has the necessary context to respond coherently to
&lt;/span>&lt;span class="s2"> the latest input.
&lt;/span>&lt;span class="s2">这里的内存指的是Chains中的状态。内存可以用来存储Chain过去执行的信息，并将信息注入到Chain的未来执行的输入中。
&lt;/span>&lt;span class="s2">例如：对于会话型Chains，内存可以用来存储会话，并自动将它们添加到未来的模型提示词中，以便模型具有必要的上下文来连贯地响应最新的输入。
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> class Config:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Configuration&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">pydantic&lt;/span> &lt;span class="nb">object&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2"> 使用pydantic库，并在本类中定义抽象方法，待子类实现
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> arbitrary_types_allowed = True
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @property
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def memory_variables(self) -&amp;gt; List[str]:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">The&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">keys&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">memory&lt;/span> &lt;span class="k">class&lt;/span> &lt;span class="nc">will&lt;/span> &lt;span class="n">add&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">chain&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def load_memory_variables(self, inputs: Dict[str, Any]) -&amp;gt; Dict[str, Any]:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Return&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="n">pairs&lt;/span> &lt;span class="n">given&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">text&lt;/span> &lt;span class="nb">input&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">chain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -&amp;gt; None:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Save&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">context&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">chain&lt;/span> &lt;span class="n">run&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def clear(self) -&amp;gt; None:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Clear&lt;/span> &lt;span class="n">memory&lt;/span> &lt;span class="n">contents&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="13basechatmemory">1.3.BaseChatMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>BaseChatMemory&lt;/strong>：BaseMemoryd的子类，实现了一部分通用方法，剩余由子类扩展&lt;/li>
&lt;li>BaseChatMemory维护了ChatMessageHistory&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/chat_memory.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">BaseChatMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseMemory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for chat memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chat_memory&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">BaseChatMessageHistory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default_factory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">ChatMessageHistory&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">output_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="n">input_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="n">return_messages&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_get_input_output&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prompt_input_key&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">output_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Save context from this conversation to buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">input_str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_get_input_output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_user_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_str&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_ai_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output_str&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Clear memory contents.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来就可以看一下常用的几种Memory了。&lt;/p>
&lt;h2 id="14conversationbuffermemory">1.4.ConversationBufferMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationBufferMemory&lt;/strong>：一种Memory的具体实现。提供了记录历史聊天记录的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/buffer.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationBufferMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer for storing conversation memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Human&amp;#34;&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;AI&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;String buffer of memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_messages&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_str&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a string in case return_messages is True.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a list of messages in case return_messages is False.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们再来看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920072953322.png" alt="image-20230920072953322">&lt;/p>
&lt;ul>
&lt;li>针对第一个问题，LangChain发送给LLM真实的问题如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920073630423.png" alt="image-20230920073630423">&lt;/p>
&lt;ul>
&lt;li>针对第二个问题，LangChain会把&lt;strong>第一次发给LLM的问题和答案&lt;/strong>+&lt;strong>第二次的问题&lt;/strong>发送给LLM：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920073804453.png" alt="image-20230920073804453">&lt;/p>
&lt;h2 id="15conversationbufferwindowmemory">1.5.ConversationBufferWindowMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationBufferWindowMemory&lt;/strong>：一种Memory的具体实现。提供了带有滑动窗口的记录历史聊天记录的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/buffer_window.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationBufferWindowMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer for storing conversation memory inside a limited size window.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Human&amp;#34;&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;AI&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">k&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Number of messages to store in buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Union&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;String buffer of memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_messages&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_str&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a string in case return_messages is True.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">messages&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">messages&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a list of messages in case return_messages is False.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074018406.png" alt="image-20230920074018406">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第一次问答&lt;/strong>：LangChain真实发送的问题只有&amp;quot;你好&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074115794.png" alt="image-20230920074115794">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第二次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第一次问题答案&lt;/strong>+&lt;strong>第二次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074225759.png" alt="image-20230920074225759">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第三次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第一次问题答案&lt;/strong>+&lt;strong>第二次问题答案&lt;/strong>+&lt;strong>第三次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074328411.png" alt="image-20230920074328411">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第四次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第二次问题答案&lt;/strong>+&lt;strong>第三次问题答案&lt;/strong>+&lt;strong>第四次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074412212.png" alt="image-20230920074412212">&lt;/p>
&lt;p>为什么会有近3次问答内容的限制呢？因为初始化ConversationBufferWindowMemory时，设置了&lt;code>k=2&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074524136.png" alt="image-20230920074524136">&lt;/p>
&lt;h2 id="16conversationsummarybuffermemory">1.6.ConversationSummaryBufferMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationSummaryBufferMemory&lt;/strong>：一种Memory的具体实现。提供了记录历史聊天记录，并对历史聊天记录进行归纳总结的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/summary_buffer.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationSummaryBufferMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">SummarizerMixin&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer with summarizer for storing conversation memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2000&lt;/span>
&lt;span class="n">moving_summary_buffer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">first_messages&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">summary_message_cls&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">first_messages&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nb">buffer&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">final_buffer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">buffer&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">final_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="nb">buffer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">final_buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="nd">@root_validator&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">validate_prompt_input_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">cls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Validate that prompt input variables are consistent.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">prompt_variables&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;prompt&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_variables&lt;/span>
&lt;span class="n">expected_keys&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;summary&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;new_lines&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">expected_keys&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prompt_variables&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s2">&amp;#34;Got unexpected prompt input variables. The prompt expects &amp;#34;&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;{prompt_variables}, but it should have {expected_keys}.&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">values&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Save context from this conversation to buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prune&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">prune&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Prune buffer if it exceeds max token limit&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">llm&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_num_tokens_from_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">pruned_memory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">pruned_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">llm&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_num_tokens_from_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict_new_summary&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">pruned_memory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Clear memory contents.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074831714.png" alt="image-20230920074831714">&lt;/p>
&lt;ul>
&lt;li>从这段代码的输出，可以看到LangChain对历史问题和答案进行了概括总结：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>&lt;span class="err">&amp;#39;history&amp;#39;:&lt;/span> &lt;span class="err">[&lt;/span>
&lt;span class="err">SystemMessage(content=&amp;#39;\nThe&lt;/span> &lt;span class="err">human&lt;/span> &lt;span class="err">asks&lt;/span> &lt;span class="err">what&lt;/span> &lt;span class="err">the&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">thinks&lt;/span> &lt;span class="err">of&lt;/span> &lt;span class="err">artificial&lt;/span> &lt;span class="err">intelligence.&lt;/span> &lt;span class="err">The&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">thinks&lt;/span> &lt;span class="err">artificial&lt;/span> &lt;span class="err">intelligence&lt;/span> &lt;span class="err">is&lt;/span> &lt;span class="err">a&lt;/span> &lt;span class="err">force&lt;/span> &lt;span class="err">for&lt;/span> &lt;span class="err">good&lt;/span> &lt;span class="err">because&lt;/span> &lt;span class="err">it&lt;/span> &lt;span class="err">will&lt;/span> &lt;span class="err">help&lt;/span> &lt;span class="err">humans&lt;/span> &lt;span class="err">reach&lt;/span> &lt;span class="err">their&lt;/span> &lt;span class="err">full&lt;/span> &lt;span class="err">potential.&lt;/span> &lt;span class="err">The&lt;/span> &lt;span class="err">human&lt;/span> &lt;span class="err">then&lt;/span> &lt;span class="err">asks&lt;/span> &lt;span class="err">what&lt;/span> &lt;span class="err">LLM&lt;/span> &lt;span class="err">is,&lt;/span> &lt;span class="err">to&lt;/span> &lt;span class="err">which&lt;/span> &lt;span class="err">the&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">responds&lt;/span> &lt;span class="err">that&lt;/span> &lt;span class="err">it&lt;/span> &lt;span class="err">stands&lt;/span> &lt;span class="err">for&lt;/span> &lt;span class="err">Large&lt;/span> &lt;span class="err">Language&lt;/span> &lt;span class="err">Model,&lt;/span> &lt;span class="err">and&lt;/span> &lt;span class="err">provides&lt;/span> &lt;span class="err">a&lt;/span> &lt;span class="err">list&lt;/span> &lt;span class="err">of&lt;/span> &lt;span class="err">LLM&lt;/span> &lt;span class="err">models,&lt;/span> &lt;span class="err">including&lt;/span> &lt;span class="err">GPT-3,&lt;/span> &lt;span class="err">GPT-J-6B,&lt;/span> &lt;span class="err">CLIP,&lt;/span> &lt;span class="err">BERT,&lt;/span> &lt;span class="err">and&lt;/span> &lt;span class="err">T5.&amp;#39;,&lt;/span>
&lt;span class="err">additional_kwargs={&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="err">)&lt;/span>
&lt;span class="err">]}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="2小结">2.小结&lt;/h1>
&lt;p>本文阐述了Memory模块的内部实现：&lt;/p>
&lt;ul>
&lt;li>ChatMessageHistory：提供记录历史聊天记录的对象&lt;/li>
&lt;li>BaseChatMemory：维护1个ChatMessageHistory对象，并对外提供CRUD历史聊天记录的接口&lt;/li>
&lt;li>ConversationBufferMemory：BaseChatMemory的1种子类，对外提供最终的CRUD历史聊天记录的接口&lt;/li>
&lt;li>ConversationBufferWindowMemory：在ConversationBufferMemory的基础上，提供了滑动窗口能力&lt;/li>
&lt;li>ConversationSummaryBufferMemory：在ConversationBufferMemory的基础上，提供了历史聊天记录的摘要能力&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</link><pubDate>Mon, 18 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</guid><description>&lt;h1 id="1问题">1.问题&lt;/h1>
&lt;p>在《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》中，我们实现了Seq2Seq，看到了编码器-解码器架构的诸多优势。&lt;/p>
&lt;p>但，Seq2Seq也有不完美的地方：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>长距离依赖问题&lt;/strong>：读了后面，忘了前面。&lt;/li>
&lt;li>&lt;strong>信息压缩问题&lt;/strong>：Seq2Seq的上下文向量是固定长度的，很难将无限的信息压缩到有限长度的向量中。&lt;/li>
&lt;/ul>
&lt;p>这个视频，体现了长距离依赖问题和信息压缩问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>长距离依赖问题&lt;/strong>：沈腾的几个问题，对于老大爷，都是听了后面忘了前面。&lt;/li>
&lt;li>&lt;strong>信息压缩问题&lt;/strong>：沈腾的每个问题，对于老大爷，字数都太多。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=961160816&amp;bvid=BV1tH4y1S7Uu&amp;cid=1271780225&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px autoplay=0> &lt;/iframe>
&lt;p>&lt;font color=red>&lt;strong>长距离问题、信息压缩问题的本质，都是信息损失的问题&lt;/strong>。&lt;/font>&lt;/p>
&lt;h1 id="2注意力机制的核心思想">2.注意力机制的核心思想&lt;/h1>
&lt;p>理解了Seq2Seq的问题，我们会很自然地产生一种思路：如果将原始信息化繁为简，喂给LLM的是有效信息，而不是全量信息，是否可以解决长距离问题、信息压缩问题？&lt;/p>
&lt;p>我们再来看一段视频：&lt;/p>
&lt;ul>
&lt;li>从全量信息看，&amp;ldquo;关键问题&amp;quot;四个字高频出现。&lt;/li>
&lt;li>从有效信息看，这么长一段话也就是一句有效信息——&amp;ldquo;关键问题很重要&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>在人类世界，这些没用的废话叫艺术。在AI的世界，这些没用的废话叫干扰。&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919082648294.png" alt="image-20230919082648294">&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=661221372&amp;bvid=BV1Vh4y1a7uD&amp;cid=1271806425&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px autoplay=0> &lt;/iframe>
&lt;p>从直觉上理解，人类接收信息时，会做两件事：&lt;/p>
&lt;ul>
&lt;li>关注&lt;strong>关键信息&lt;/strong>。&lt;/li>
&lt;li>关注&lt;strong>不同维度的关键信息&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>比如：《八佰》这张海报，你看到了什么？&lt;/p>
&lt;ul>
&lt;li>&lt;strong>四行仓库&lt;/strong>：它是海报的背景，为什么会吸引观众的注意？&lt;/li>
&lt;li>&lt;strong>残垣断壁&lt;/strong>：正常的第一眼感觉是这里应该经历过多轮惨烈战斗，为什么我们不会关注断壁、废楼、电线杆的破坏程度？为什么我们不会关注零落的士兵？&lt;/li>
&lt;li>&lt;strong>对比鲜明&lt;/strong>：四行仓库和残垣断壁对比鲜明，我们应该可以感受到导演想表达的一种情绪。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919083211415.png" alt="image-20230919083211415">&lt;/p>
&lt;p>上述这些，就是&lt;font color=red>&lt;strong>注意力&lt;/strong>&lt;/font>，而且是&lt;font color=red>&lt;strong>不同维度的注意力&lt;/strong>&lt;/font>：&lt;/p>
&lt;ul>
&lt;li>当有人问&amp;rdquo;《八佰》这部电影发生的地点在哪里？&amp;quot;，我们的注意力在&amp;rdquo;&lt;strong>四行仓库&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>当游人问&amp;rdquo;《八佰》这部电影想表达什么主题？&amp;quot;，我们的注意力在&amp;quot;四行仓库与残垣断壁&amp;quot;的&lt;strong>对比鲜明&lt;/strong>——表现&amp;quot;八佰&amp;quot;勇士保卫上海最后一寸土地的英勇决绝。&lt;/li>
&lt;/ul>
&lt;p>论文《Attention Mechanisms in Computer Vision: A Survey》，更加形象化地展示了注意力在计算机视觉领域的实验效果：&lt;/p>
&lt;ul>
&lt;li>我们可以发现，加入了注意力机制的AI，会更加关注原始图像中的关键要素。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919084959023.png" alt="image-20230919084959023">&lt;/p>
&lt;h1 id="3注意力原理解读">3.注意力原理解读&lt;/h1>
&lt;p>接下来，我们尝试一下用代码实现一下直觉上理解的注意力机制。&lt;/p>
&lt;h2 id="31点积注意力">3.1.点积注意力&lt;/h2>
&lt;p>首先，回顾一下点积的定义，特别关注一下点积的代数表达：&lt;/p>
&lt;blockquote>
&lt;p>在数学中，&lt;strong>点积&lt;/strong>又称&lt;strong>数量积&lt;/strong>或&lt;strong>标量积&lt;/strong>，是一种接受两串等长的数字序列（通常是坐标向量）、返回单一数字的代数运算)。在欧几里得几何，两条笛卡尔坐标向量的点积常称为&lt;strong>内积&lt;/strong>。&lt;/p>
&lt;p>从代数角度看，先求两数字序列中每组对应元素的积，再求所有积之和，结果即为点积。&lt;/p>
&lt;p>从几何角度看，点积则是两向量的长度与它们夹角余弦的积。这两种定义在笛卡尔坐标系中等价。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919085940309.png" alt="image-20230919085940309">&lt;/p>
&lt;p>然后，我们再来看看&lt;strong>点积注意力&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：向量X1表示帅哥，假设向量X2表示美女。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：计算X1和X2的点积&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919090930858.png" alt="image-20230919090930858">，这个叫&lt;strong>原始权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对原始权重进行softmax&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919091338872.png" alt="image-20230919091338872">，得到&lt;strong>归一化注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：计算归一化注意力与X2的加权和&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092013095.png" alt="image-20230919092013095">，得到X1对X2的&lt;strong>最终注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>结果&lt;/strong>：经过上述代数计算，最终注意力的信息主体肯定是X1，最终注意力还包含了X2的信息。因此可以这么理解：
&lt;ul>
&lt;li>向量X1表示：没有坠入爱河的帅哥。&lt;/li>
&lt;li>最终向量表示：心中有美女的，坠入爱河的帅哥。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092053498.png" alt="image-20230919092053498">&lt;/p>
&lt;p>运行结果：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092401876.png" alt="image-20230919092401876">&lt;/p>
&lt;h2 id="32缩放点积注意力">3.2.缩放点积注意力&lt;/h2>
&lt;p>在&amp;quot;3.1.点积注意力&amp;quot;的STEP2中，对原始权重进行softmax，可能由于原始权重过大导致梯度过小甚至梯度消失。&lt;/p>
&lt;p>因此，缩放点积注意力的本质是对原始权重除以一个系数后，再进行softmax，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：向量X1表示帅哥，假设向量X2表示美女。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：计算X1和X2的点积&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919090930858.png" alt="image-20230919090930858">，这个叫&lt;strong>原始权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对原始权重缩放&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105751443.png" alt="image-20230919105751443">，得到&lt;strong>缩放权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对&lt;strong>缩放权重&lt;/strong>进行softmax&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105527069.png" alt="image-20230919105527069">，得到&lt;strong>归一化注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：计算归一化注意力与X2的加权和&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105715585.png" alt="image-20230919105715585">，得到X1对X2的&lt;strong>最终注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>结果&lt;/strong>：经过上述代数计算，最终注意力的信息主体肯定是X1，最终注意力还包含了X2的信息。因此可以这么理解：
&lt;ul>
&lt;li>向量X1表示：没有坠入爱河的帅哥。&lt;/li>
&lt;li>最终向量表示：心中有美女的，坠入爱河的帅哥。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105832012.png" alt="image-20230919105832012">&lt;/p>
&lt;p>运行结果：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105847714.png" alt="image-20230919105847714">&lt;/p>
&lt;h2 id="33解码器-编码器注意力">3.3.解码器-编码器注意力&lt;/h2>
&lt;p>有了&lt;strong>缩放点积注意力&lt;/strong>，我们就可以尝试一下：&lt;strong>对编码器-解码器架构&lt;/strong>增加&lt;strong>注意力机制&lt;/strong>了。&lt;/p>
&lt;p>回顾一下《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》，我们详细分析了各个时间点上编码器、解码器的处理流程。截取t5和t6时刻：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t5时刻&lt;/strong>：第一个解码器的输入是&lt;strong>编码器输出的上下文向量&lt;/strong>+&lt;strong>TeachForcing的第一个单词&lt;/strong>Who，第一个解码器的输出是最终答案的首词Who+&lt;strong>隐藏状态&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：第二个解码器的输入是&lt;strong>第一个解码器的隐藏状态&lt;/strong>+&lt;strong>TeachForcing的第二个单词&lt;/strong>are+第一个解码器预测的最终答案的首词Who。&lt;/li>
&lt;/ul>
&lt;p>我们可以看出，每一个解码器的输入本质包含了三要素：&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>&lt;strong>问题是什么&lt;/strong>&lt;/font>：每个编码器输出的隐藏状态，但隐藏状态无法记忆太长的问题序列，所以&lt;strong>每个解码器只知道问题的只言片语&lt;/strong>。&lt;/li>
&lt;li>&lt;font color=red>&lt;strong>参考答案是什么&lt;/strong>&lt;/font>：TeachForcing会告诉每个解码器，参考答案是什么。&lt;/li>
&lt;li>&lt;font color=red>&lt;strong>上一个解码器的答案是什么&lt;/strong>&lt;/font>：每个解码器都能知道上一个解码器预测的答案的一部分是什么。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919110424183.png" alt="image-20230919110424183">&lt;/p>
&lt;p>思考一下：注意力机制，可以&lt;strong>如何优化编码器-解码器架构&lt;/strong>？&lt;/p>
&lt;p>显然，只有优化&amp;quot;问题是什么&amp;quot;这个要素——如果我们将第一个解码器增加注意力层，就可以让输出的隐藏状态注意编码器输出的上下文向量——这样每个解码器就&lt;font color=red>&lt;strong>能用有限长度的向量&lt;/strong>&lt;/font>了解&lt;font color=red>&lt;strong>原始问题中最关键的部分&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161621858.png" alt="image-20230919161621858">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161638084.png" alt="image-20230919161638084">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161652216.png" alt="image-20230919161652216">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161724555.png" alt="image-20230919161724555">&lt;/p>
&lt;p>运行结果如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161901056.png" alt="image-20230919161901056">&lt;/p>
&lt;p>写到这里，我们要稍微小结一下(避免陷入术语、概念、数学公式的细节中)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>神经网络架构&lt;/strong>：实现Seq2Seq的&lt;strong>编码器-解码器&lt;/strong>架构，是神经网络架构中的一种。&lt;/li>
&lt;li>&lt;strong>注意力机制&lt;/strong>：注意力机制是一种专项技术，也有很多种实现，无论是哪一种，都是在&lt;strong>解决神经网络架构的长距离问题、信息压缩问题&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>如果您对&lt;strong>编码器-解码器&lt;/strong>这种&lt;strong>神经网络架构&lt;/strong>的细节记不太清，可以回看这篇《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》。&lt;/p>
&lt;p>如果您对&lt;strong>注意力机制&lt;/strong>这种专项技术的细节记不太清，可以回看本文的&lt;strong>点积注意力、缩放点积注意力、编码器-解码器注意力&lt;/strong>章节。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174642861.png" alt="image-20230919174642861">&lt;/p>
&lt;p>当我们对上述概念有了基本认识后，我们也就可以理解，自1990年注意力机制提出后，有很多科学家在思考：&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>有没有一种&lt;strong>更好的神经网络架构&lt;/strong>，充分发挥这种&lt;strong>仿人类的注意力机制&lt;/strong>的独特优势&lt;/font>？&lt;/li>
&lt;/ul>
&lt;p>于是如雷贯耳的Transformer架构诞生了，于是如雷贯耳的论文《Attention Is All Your Need》诞生了。&lt;/p>
&lt;p>笔者不在本文展开描述Transformer的实现原理，但我们可以在本文接下来的部分解读一下QKV、自注意力、多头注意力等Transformer架构中与注意力机制强相关的技术点，为后续解读Transformer做知识铺垫。&lt;/p>
&lt;h2 id="34qkv">3.4.QKV&lt;/h2>
&lt;p>QKV注意力(query-key-value attention)是注意力机制中的一种变体。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919165526267.png" alt="image-20230919165526267">&lt;/p>
&lt;p>上图来自于维基百科，但维基百科对于QKV的介绍非常详细，但笔者试图给出这些过于理论化的QKV理论的通俗解释：&lt;/p>
&lt;ul>
&lt;li>注意力机制的本质是&lt;strong>让向量X1关注向量X2&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>Q&lt;/strong>uery，就是&lt;strong>向量X1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>K&lt;/strong>ey、&lt;strong>V&lt;/strong>alue，就是&lt;strong>向量X2&lt;/strong>。Key和Value的区别，仅仅是数学公式的差异(注意力权重、注意力权重加权和)。&lt;/li>
&lt;li>&lt;strong>向量X1&lt;/strong>，也就是&lt;strong>Q&lt;/strong>uery，在编码器-解码器架构中，就是&lt;strong>解码器输出的隐藏状态&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>向量X2&lt;/strong>，也就是&lt;strong>K&lt;/strong>ey或者&lt;strong>V&lt;/strong>alue，在编码器-解码器架构中，就是&lt;strong>编码器输出的上下文向量&lt;/strong>。&lt;/li>
&lt;li>说白了，就是让&lt;strong>每个解码器的输出&lt;/strong>都关注&lt;strong>编码器输出的上下文向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>我们来看一下代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919171621721.png" alt="image-20230919171621721">&lt;/p>
&lt;h2 id="35自注意力">3.5.自注意力&lt;/h2>
&lt;p>自注意力的理论也非常抽象，笔者试图给出这些理论的通俗解释：&lt;/p>
&lt;ul>
&lt;li>注意力机制的本质是&lt;strong>让向量X1关注向量X2&lt;/strong>。自注意力的本质是&lt;strong>自己对自己&lt;/strong>的注意。&lt;/li>
&lt;li>&lt;strong>Q&lt;/strong>uery，就是&lt;strong>向量X1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>K&lt;/strong>ey、&lt;strong>V&lt;/strong>alue，也是&lt;strong>向量X2&lt;/strong>。而向量X2的信息本质还是向量X1，只是线性变换的公式不同而已。&lt;/li>
&lt;/ul>
&lt;p>我们来看看实现自注意力的代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919172737253.png" alt="image-20230919172737253">&lt;/p>
&lt;h2 id="36多头注意力">3.6.多头注意力&lt;/h2>
&lt;ul>
&lt;li>理解了QKV注意力，那么多头注意力仅仅是从更多维度对信息进行点积、缩放、Softmax、加权和而已。&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919173225726.png" alt="image-20230919173225726">&lt;/p>
&lt;h1 id="4总结">4.总结&lt;/h1>
&lt;ul>
&lt;li>本文通过介绍点积注意力、缩放点积注意力，进而解读了如何在编码器-解码器神经网络架构中，增加注意力机制。
&lt;ul>
&lt;li>本文的依据源于论文《An Attentive Survey of Attention Models》(&lt;a href="https://arxiv.org/abs/1904.02874">https://arxiv.org/abs/1904.02874&lt;/a>)&lt;/li>
&lt;li>本文试图通过通俗的比喻、实际的代码，解读论文中复杂、抽象的算法流程。如下图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174833717.png" alt="image-20230919174833717">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>本文进一步解读了QKV注意力、自注意力、多头注意力，为后续解读Transformer神经网络架构(如下图)奠定基础知识。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174725878.png" alt="image-20230919174725878">&lt;/p>
&lt;p>解读GPT背后的论文及实现技术实属不易，&lt;/p>
&lt;p>从N-Gram到词嵌入，再到神经概率语言模型，再到Seq2Seq，直到本文的注意力机制，&lt;/p>
&lt;p>我们距离自行实现一个简化版大语言模型越来越近了，同时笔者也受益匪浅。&lt;/p>
&lt;p>欢迎各位小伙伴探讨交流，我们继续探索这个有趣的技术领域！&lt;/p></description></item><item><title>【chatGPT】学习笔记12-昇腾计算产业发展白皮书解读</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</link><pubDate>Tue, 12 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</guid><description>&lt;p>本文来解读华为的《昇腾计算产业发展白皮书》，跟踪一下国内AI行业的宏观动态。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912224057905.png" alt="image-20230912224057905">&lt;/p>
&lt;h2 id="1ai发展趋势和挑战">1.AI发展趋势和挑战&lt;/h2>
&lt;h3 id="11ai发展趋势">1.1.AI发展趋势&lt;/h3>
&lt;p>白皮书首先阐述了AI发展趋势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI已成为推动社会发展的关键引擎&lt;/strong>：
&lt;ul>
&lt;li>AI在诸多特定领域超过人类能力。如：计算机视觉、语音识别、自然语言处理领域。&lt;/li>
&lt;li>AI将助力各产业实现智能化转型升级。根据弗若斯特沙利文数据，2019年中国AI市场规模为598.6亿元，2020~2024年复合增长率达34.8%。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>AI处于爆发式创新的前夜&lt;/strong>：
&lt;ul>
&lt;li>联接、AI、云、计算、行业应用等多种先进技术和机会将会互相催化、有机融合。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="12ai产业挑战">1.2.AI产业挑战&lt;/h3>
&lt;p>白皮书再阐述了AI产业的挑战：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>计算系统&lt;/strong>要满足AI场景的&lt;strong>复杂巨大、多样性的计算需求&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>算力增速快&lt;/strong>：2012~2018年，算力需求增加30万倍，远超摩尔定律。大语言模型时代，算力需求从TFLOPS级别，增至PFLOPS级别，甚至EFLOPS级别。&lt;/li>
&lt;li>&lt;strong>计算架构设计面临挑战&lt;/strong>：大规模算力需求，对计算系统的&lt;strong>计算性能、通信性能、可扩展性&lt;/strong>是巨大的挑战。&lt;/li>
&lt;li>&lt;strong>全方位面临挑战&lt;/strong>：基础软件、编程模型、编程语言、编译器、工具链、大规模运行时、调度系统、平台软件、通信组件、加速组件、加速引擎、AI框架、行业软件，都需要适配大规模算力需求。&lt;/li>
&lt;li>参考：https://openai.com/research/ai-and-compute&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/ai-and-compute-all.png" alt="ai-and-compute-all">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从算法到产品化落地&lt;/strong>面临8大鸿沟：
&lt;ul>
&lt;li>&lt;strong>模型获取&lt;/strong>鸿沟：针对行业数据，选择、测试合适地模型，需要巨大的时间成本和算力成本。&lt;/li>
&lt;li>&lt;strong>数据准备&lt;/strong>鸿沟：构建能够真实反映实际业务数据分布的数据集，面临较大挑战。&lt;/li>
&lt;li>&lt;strong>模型训练&lt;/strong>鸿沟：超参数调优等模型训练环节，对大量传统行业开发者挡在AI门外。&lt;/li>
&lt;li>&lt;strong>准确度验证&lt;/strong>鸿沟：模型泛化能力的验证，是阻碍算法快速落地的因素之一。&lt;/li>
&lt;li>&lt;strong>应用开发&lt;/strong>鸿沟：AI需要接受包含行业知识的各种输入数据，对AI应用开发系统产生整合要求，开发效率是重要影响因素之一。&lt;/li>
&lt;li>&lt;strong>NPU性能&lt;/strong>鸿沟：AI算力真正转化为行业生产率，实际的运行性能将决定系统最终的性价比和业务执行能力。&lt;/li>
&lt;li>&lt;strong>业务流程监控&lt;/strong>鸿沟：确保AI系统在业务环境的持续准确运行，是行业应用的重中之重。&lt;strong>AI算法需要具备持续更新、增量学习的能力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>适配开发&lt;/strong>鸿沟：面对不同业务场景，需要以服务化和API形式封装AI算法，封装AI服务、部署AI服务于复杂的计算系统上，都将面临挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912232100695.png" alt="image-20230912232100695">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>可信AI&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI具有自我演化潜力&lt;/strong>：人类对于AI核心运行机制、AI隐式编程、学习能力，依然没有彻底研究清楚。&lt;/li>
&lt;li>&lt;strong>可信AI是大规模商用的基础&lt;/strong>：保护隐私、避免偏见、防止滥用、可靠边界、可解释、鲁棒性、防攻击等方面，都是AI能够真正大规模应用的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>人才需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI人才缺口巨大&lt;/strong>：中国AI人才缺口500万，供求比1:10。&lt;/li>
&lt;li>&lt;strong>与美国差距巨大&lt;/strong>：中国的AI人才数量仅为美国四分之一。&lt;/li>
&lt;li>&lt;strong>新一代AI发展规划&lt;/strong>：国务院《新一代AI发展规划》发布，明确指出建立AI高端人才队伍是AI发展重中之重。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2昇腾计算产业">2.昇腾计算产业&lt;/h2>
&lt;h3 id="21昇腾计算产业体系和定位">2.1.昇腾计算产业体系和定位&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>昇腾计算产业覆盖全产业链，全生态链&lt;/strong>，包括：昇腾处理器、硬件、CANN(Compute Architecture for Neural Networks，异构计算架构)、AI计算框架、应用使能、开发工具链、运维工具、行业应用及服务。&lt;/li>
&lt;li>&lt;strong>昇腾计算硬件体系&lt;/strong>包括：
&lt;ul>
&lt;li>基于达芬奇内核的昇腾系列芯片，提供多样化AI算力。&lt;/li>
&lt;li>基于昇腾系列芯片的硬件产品，如嵌入式模组、板卡、小站、服务器、集群等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>昇腾计算软件体系&lt;/strong>包括：
&lt;ul>
&lt;li>异构计算架构CANN，对标CUDA。&lt;/li>
&lt;li>AI计算框架MindSpore，对标Pytorch。&lt;/li>
&lt;li>工具链：运行时、加速库、编译器、调试调优工具、开发工具链MindStudio及运维工具。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>昇腾应用使能&lt;/strong>：基于MindX支撑的ModelArts、HiAI等。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912234104109.png" alt="image-20230912234104109">&lt;/p>
&lt;h3 id="22昇腾计算产业价值">2.2.昇腾计算产业价值&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>硬件开放、软件开元、使能合作伙伴&lt;/strong>是华为构建昇腾生态的方式。&lt;/li>
&lt;li>华为&lt;strong>聚焦AI芯片、基础软件&lt;/strong>的创新与研发。&lt;/li>
&lt;li>&lt;strong>自有硬件+伙伴硬件&lt;/strong>，为客户提供多样化算力选择。&lt;/li>
&lt;li>昇腾通过模组、板卡、小站、服务器、集群等产品形态，&lt;strong>打造&amp;quot;端、边、云&amp;quot;的全场景AI基础设施解决方案&lt;/strong>。&lt;/li>
&lt;li>昇腾计算产业的三个愿景：
&lt;ul>
&lt;li>&lt;strong>用得起&lt;/strong>&lt;/li>
&lt;li>&lt;strong>用得好&lt;/strong>&lt;/li>
&lt;li>&lt;strong>用得放心&lt;/strong>：特指某国的芯片封锁吧。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3昇腾计算技术体系">3.昇腾计算技术体系&lt;/h2>
&lt;h3 id="31昇腾计算架构">3.1.昇腾计算架构&lt;/h3>
&lt;p>下图展开了昇腾计算架构细节，从架构中可以看出：&lt;/p>
&lt;ul>
&lt;li>昇腾计算架构支持端边云全场景。&lt;/li>
&lt;li>超强算力：Atlas训练卡提供320 TFLOPS FP16高算力，Atlas集群提供1024P FLOPS算力。&lt;/li>
&lt;li>全站开放，模块间具备相互协同能力、各层之间支持独立演进。&lt;/li>
&lt;li>MindX将设备资源、算力资源抽象并管理，应用软件无需了解底层硬件的复杂配置和调度。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912235737889.png" alt="image-20230912235737889">&lt;/p>
&lt;h3 id="32硬件体系">3.2.硬件体系&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>昇腾系列处理器&lt;/strong>：NPU针对矩阵运算进行专门优化设计，华为达芬奇架构面向AI计算设计的架构，独创16&lt;em>16&lt;/em>16的3D Cube设计。&lt;/li>
&lt;li>&lt;strong>模组和板卡&lt;/strong>：Atlas 200加速模块、Atlas 300推理卡、Atlas 900。&lt;/li>
&lt;li>&lt;strong>小站&lt;/strong>：基于昇腾系列处理器的边缘计算盒子。&lt;/li>
&lt;li>&lt;strong>服务器&lt;/strong>：Atlas 800、Atlas500。&lt;/li>
&lt;li>&lt;strong>集群&lt;/strong>：Atlas 900。&lt;/li>
&lt;/ul>
&lt;h3 id="33基础软件">3.3.基础软件&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>异构计算架构&lt;/strong>：
&lt;ul>
&lt;li>CANN支持10种设备形态、EMUI、Android、openEuler、UOS、Ubuntu、Debian、Suse等14种操作系统和AI计算框架。&lt;/li>
&lt;li>CANN是一个开发体系，包含了编程语言、编译器等编程模型。&lt;/li>
&lt;li>CANN包含4层：
&lt;ul>
&lt;li>Driver实现硬件和操作系统的适配。&lt;/li>
&lt;li>Runtime、DVPP、HCCL提供内存管理、算力分配、资源调度。其中，HCCL，Huawei Collective Communication Library，华为集合通信库，提供板间和框间通信能力。&lt;/li>
&lt;li>图引擎实现了大计算图拆分、图融合，最大化芯片算力利用率。&lt;/li>
&lt;li>AscendCL提供插件适配、开放图融合接口、支持自定义算子融合、提供Ascend IR中间表达接口、支持自定义模型、开放预置算子库。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CANN提供两种算子开发方式，
&lt;ul>
&lt;li>一种是TBE-DSL(Tensor Boost Engine-Domain Specific Language)，实现数据切分和调度。&lt;/li>
&lt;li>一种是TBE-TIK(Tensor Iterator Kernel)，通过指令级编程，实现数据编排、计算表达。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913000935774.png" alt="image-20230913000935774">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>开发工具链MindStudio，提供工程管理、编译、调试、运行、性能分析等全流程开发。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AI计算框架MindSpore，提供动静态图转换、自动并行、端边云协同能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>应用使能的核心是MindX DL和MindX Edge，封装了底层硬件、算子的协同调度能力。&lt;/p>
&lt;ul>
&lt;li>MindX DL架构图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913001119293.png" alt="image-20230913001119293">&lt;/li>
&lt;li>MindX Edge架构图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913001132605.png" alt="image-20230913001132605">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="4行业实践">4.行业实践&lt;/h2>
&lt;p>白皮书在本章节阐述了基于昇腾计算，构建的各行各业的解决方案：&lt;/p>
&lt;ul>
&lt;li>AI计算中心&lt;/li>
&lt;li>互联网&lt;/li>
&lt;li>制造&lt;/li>
&lt;li>机器人&lt;/li>
&lt;li>能源&lt;/li>
&lt;li>金融&lt;/li>
&lt;li>平安城市&lt;/li>
&lt;li>电信&lt;/li>
&lt;li>交通&lt;/li>
&lt;li>医疗&lt;/li>
&lt;/ul>
&lt;p>这里展开3个行业解决方案：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>计算中心&lt;/strong>解决方案：从解决方案看，昇腾计算覆盖了AI芯片、AI驱动，是AI最核心的基础设施，完全国产化，很牛。
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004001231.png" alt="image-20230913004001231">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>工业&lt;/strong>解决方案：针对半导体晶圆质检，基于昇腾强大算力，实现工业领域的目标定位、测量、质检等能力。这一点真正突破了AI在工业领域落地的，很牛。
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004208756.png" alt="image-20230913004208756">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>交通&lt;/strong>解决方案：
&lt;ul>
&lt;li>高速自由流收费稽核&lt;/li>
&lt;li>交通视频云&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="5产业生态">5.产业生态&lt;/h2>
&lt;p>在产业政策方面：&lt;/p>
&lt;ul>
&lt;li>昇腾与各地政府共建昇腾生态创新中心&lt;/li>
&lt;li>通过行业联盟聚拢厂家、ISV、用户，建立各行业标杆。&lt;/li>
&lt;/ul>
&lt;p>在开发者方面：&lt;/p>
&lt;ul>
&lt;li>90%的开发者聚焦于基于AI算法的应用软件开发。&lt;/li>
&lt;li>10%的开发者聚焦AI算法、AI驱动的开发。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004825461.png" alt="image-20230913004825461">&lt;/p>
&lt;p>在高校培养方面：&lt;/p>
&lt;ul>
&lt;li>昇腾与高校共建AI人才培养基地，与各高校推出昇腾计算体系课程与教材。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913005027655.png" alt="image-20230913005027655">&lt;/p>
&lt;p>在合作伙伴方面：&lt;/p>
&lt;ul>
&lt;li>昇腾推出覆盖软件开发商、硬件开发商、云服务供应商、设备制造商等各类合作伙伴计划，华为为合作伙伴提供了资金+市场。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913005123533.png" alt="image-20230913005123533">&lt;/p>
&lt;h2 id="6未来展望昇腾推动ai成为通用目的技术">6.未来展望：昇腾推动AI成为通用目的技术&lt;/h2>
&lt;p>引用白皮书中的一段话：&lt;strong>把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界&lt;/strong>。&lt;/p>
&lt;p>昇腾计算产业的愿景是作为中国的AI基石，AI的基础设施，成为通用目的技术。&lt;/p>
&lt;h2 id="7总结">7.总结&lt;/h2>
&lt;p>当我第一次看到昇腾的AI芯片层，CANN对标CUDA，无比惊讶、激动、自豪，这可是对标英伟达的大事，技术难度、商业难度都不可想象。&lt;/p>
&lt;p>昇腾计算已雏形初见，这可不是短期突击出来的。不得不说，华为在AI领域的持续投资令人钦佩！&lt;/p></description></item><item><title>【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</link><pubDate>Tue, 05 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</guid><description>&lt;p>今天我们再来写一篇关于大语言模型的实战应用——如何开发一个垂直领域的知识问答系统？&lt;/p>
&lt;h1 id="1原理">1.原理&lt;/h1>
&lt;h2 id="1基于传统技术的实现方案">(1)基于传统技术的实现方案&lt;/h2>
&lt;p>在没有大语言模型之前，有如下传统技术实现知识问答系统：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>关键词匹配&lt;/strong>：在系统内预设一些关键词，系统根据用户提出的问题进行关键词匹配，从中提取出匹配的答案。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：仅适用于简单、明确的问题，但复杂问题、多义词等，效果就不好。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>规则匹配&lt;/strong>：在系统内预设一些规则模板，系统根据用户提出的问题结构进行匹配，这些规则模板包括：语法规则、语义规则、业务领域规则。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家编写规则模板，对领域知识的抽象和表达能力有一定要求。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>统计方法&lt;/strong>：系统基于统计学模型构建，如：条件随机场(CRF，Conditional Random Field)，进行问题分类、命名实体识别等。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：强依赖数据，数据采集、数据清洗、数据标注，都要消耗人类巨大的工作量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>知识图谱&lt;/strong>：系统基于领域相关的知识图谱或实体构建，可以通过图谱中的实体、关系和属性实现问题解析和答案生成。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家对业务领域的知识进行建模、抽象，好不容易构建好，知识刷新了。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2基于大语言模型的实现方案">(2)基于大语言模型的实现方案&lt;/h2>
&lt;p>和老李师傅聊大语言模型，他说：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>大语言模型，是从&lt;strong>word&lt;/strong>到&lt;strong>world&lt;/strong>的过程——AI学习一堆word的关系，然后就具备了概括、抽象、推理的能力去描述世界。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>人类程序员，是从&lt;strong>world&lt;/strong>到&lt;strong>word&lt;/strong>的过程——人类学习这个世界，然后用代码描述世界(代码就是一堆word的组合)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>最后，他老人家感叹道：&amp;ldquo;N年后，别人看我们，就像我们看伏尔加河上的纤夫。纤夫光着腚，我们光着头。&amp;rdquo;&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230905163751495.png" alt="image-20230905163751495">&lt;/p>
&lt;p>的确如此，相较于前述传统技术，基于大语言模型实现知识问答系统，有很多天然优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学习效率高&lt;/strong>：大语言模型学习速度极快。后文实践章节中，有这么一个例子：我找公路军团的同学要了1000多份交通专业的文档和书籍，大语言模型&lt;strong>1分钟学习10个文档，2小时学完1000份&lt;/strong>，而&lt;strong>每小时只消耗了1.39元&lt;/strong>。这位交通领域的专家无比惊讶地告诉我——这1000多个文档，他也只详细看完了其中200个。&lt;/li>
&lt;li>&lt;strong>无需人工干预&lt;/strong>：传统技术需要大量的人工干预，特别强依赖人类专家那些&lt;strong>只可意会、不可言传&lt;/strong>的经验。而大语言模型可以自动学习和更新知识，无需人工干预。这意味着问答应用可以及时获取最新的知识，随着时间的推移变得更加智能和准确。&lt;/li>
&lt;li>&lt;strong>多轮对话&lt;/strong>：大语言模型还可以处理复杂的问题和多轮对话。它能够理解问题的语义和上下文，并根据用户的追问进行适当的回答。这使得问答应用更加交互式和人性化，提供更好的用户体验。&lt;/li>
&lt;li>……好处太多，省略千言万语……&lt;/li>
&lt;/ul>
&lt;h2 id="3大语言模型选型">(3)大语言模型选型&lt;/h2>
&lt;p>大语言模型的选型需要根据LLM App的应用场景，并且也不是只选1个，而是选择N个，形成大语言模型矩阵。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906090930061.png" alt="image-20230906090930061">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>世界知识&lt;/strong>：从世界知识看，目前主流的LLM如下：
&lt;ul>
&lt;li>&lt;strong>GPT系&lt;/strong>：当红炸子鸡的它，提供的接口包括&lt;code>Model&lt;/code>、&lt;code>Completions&lt;/code>、&lt;code>Chat&lt;/code>、&lt;code>Edits&lt;/code>、&lt;code>Images&lt;/code>、&lt;code>Embeddings&lt;/code>、&lt;code>Audio&lt;/code>、&lt;code>Files&lt;/code>、&lt;code>Fine-tunes&lt;/code>、&lt;code>Others&lt;/code>等10大类，完整地覆盖了&lt;strong>训练&lt;/strong>、&lt;strong>推理&lt;/strong>场景。针对&lt;strong>微调场景&lt;/strong>，覆盖了词嵌入、微调，在&lt;strong>推理&lt;/strong>场景，覆盖了预测、聊天、修正、多模态。&lt;strong>知识文档不涉密且不差钱&lt;/strong>的公司，可直接使用。笔者为了测试GPT4的API，&lt;font color=green>&lt;strong>半小时就花光了40大洋&lt;/strong>&lt;/font>，求赞助！&lt;/li>
&lt;li>&lt;strong>LLama系&lt;/strong>：卷王Meta的开源大戏，LLama一出，瞬间抢走了chatGPT的焦点。随后各顶流大学就开始了基于LLama的训练微调，推出了以Vicuna、Alpaca为代表的一系列模型。LLama2发布后更是好评无数，没几天又推出了最会写代码的CodeLLama。。。大语言模型界的卷王实至名归！&lt;/li>
&lt;li>&lt;strong>Claude系&lt;/strong>：谷歌前员工的大神们拉旗单干的产品，也是GPT系的强劲对手。&lt;/li>
&lt;li>&lt;strong>GLM系&lt;/strong>：清华大学出品，公司化商业运作。chatGLM的中文能力非常不错，GLM-130B成为2022年亚洲唯一入选全球30个主流大模型全方位测评报告的候选对象。国货之光，本文的知识问答系统就是演示的它。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPT系&lt;/td>
&lt;td>GPT3.5Turbo、GPT4&lt;/td>
&lt;td>OpenAI&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>LLama、LLama2、CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Claude系&lt;/td>
&lt;td>Claude-instant、Claude-2-100k&lt;/td>
&lt;td>Anthropic&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>chatGLM2、GLM-130B、VisualGLM-6B&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>领域知识&lt;/strong>：以编程辅助领域为例，目前表现不错的LLM如下：&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BigCode系&lt;/td>
&lt;td>StarCoder、OctoPack、SantaCoder&lt;/td>
&lt;td>Hugging Face&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>CodeGeeX2&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>综上分析：&lt;/p>
&lt;ul>
&lt;li>对世界知识的表现体现的是大语言模型的&lt;strong>通才&lt;/strong>，对领域知识的表现体现的是大语言模型的&lt;strong>专才&lt;/strong>。&lt;/li>
&lt;li>在没有一个大语言模型即是通才又是专才的限制下，垂直领域知识问答系统需要的是&lt;strong>大语言模型矩阵&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="4大语言模型之外的关键技术">(4)大语言模型之外的关键技术&lt;/h2>
&lt;p>是不是选择好了大语言模型，LLM APP就信手拈来呢？很不幸，不是！&lt;/p>
&lt;p>在最近的一次行业交流中，有两个议题值得思考：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量变引起质变&lt;/strong>：百度智能云/技术委员会主席王耀，针对大语言模型的训练推理场景，阐述了通用云计算、分布式云、智能计算的计算范式的变化。其中，最心酸的一个故事是：以前硬件上某个万分之一的故障是可以接受的，现在不可以了，因为大语言模型训练的计算量级无比巨大！&lt;/li>
&lt;li>&lt;strong>淘金时代的卖铲人&lt;/strong>：LangChain CEO Harrision Chase分享的议题是《Why LangChain，What I Saw When Building AI Application with LLM》，在各大厂商逐鹿中原的时候，LangChain忽然火了，按照这个趋势，它应该会成为LLM APP的开发框架了。&lt;/li>
&lt;/ul>
&lt;p>因为，&lt;strong>训练&lt;/strong>大语言模型的计算量剧增，对原有的云计算有新的诉求。&lt;/p>
&lt;p>因为，&lt;strong>粘合&lt;/strong>大语言模型需要有很多工作，需要有LLM APP的开发框架。&lt;/p>
&lt;p>通过LangChain的特性介绍和架构，我们可以知道大语言模型之外，我们还需要做如下事情：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>具体可以看笔者这篇文章：《【chatGPT】学习笔记10-LangChain之ModelIO，对LLM的抽象1》&lt;/p>
&lt;h2 id="5架构示例langchain-chatchat">(5)架构示例：LangChain-ChatChat&lt;/h2>
&lt;p>垂直领域问答系统的开源项目已经有很多了，以LangChain-ChatChat为例，可以看到这类LLM APP的软件架构：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095324183.png" alt="image-20230906095324183">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>训练环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unstructured Loader&lt;/strong>：负责将知识文档加载并解析。&lt;/li>
&lt;li>&lt;strong>Text Splitter&lt;/strong>：将文档按照标点符号，拆分断句。&lt;/li>
&lt;li>&lt;strong>Text Chunks&lt;/strong>：将断句分词分组，分词分组间存在一定的上下文关联。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095608282.png" alt="image-20230906095608282">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Embedding&lt;/strong>：词嵌入，并将词向量存储于向量数据库中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>应用环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Query Embedding&lt;/strong>：将用户提问转换了词向量。&lt;/li>
&lt;li>&lt;strong>Vector Similarity&lt;/strong>：在向量数据库中，用问题词向量，去搜索相关的知识文本向量。&lt;/li>
&lt;li>&lt;strong>Related Text Chunks&lt;/strong>：根据相关的知识文本向量，反查出对应的知识文本分词分组。这里有个细节，还会将文本分词分组的前后分词分组一并返回(因为这些文字可能有上下文关联关系)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100110319.png" alt="image-20230906100110319">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Template&lt;/strong>：提示词模板，这个模块是知识问答系统的关键！最简单的提示词模板就是：
&lt;ul>
&lt;li>&lt;strong>已知&lt;/strong>：[Related Text Chunks获得的知识文本分词分组]，&lt;strong>请问&lt;/strong>[用户的问题]如何答复？&lt;/li>
&lt;li>这，不就是开卷考试吗？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100510639.png" alt="image-20230906100510639">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>从上述架构可以看出来，曾经很复杂的知识问答系统，大部分核心难点，都被大语言模型搞定了。&lt;/p>
&lt;p>接下来，我们来动手实践一下，构建一个我们自己的知识问答系统。&lt;/p>
&lt;h1 id="2实践">2.实践&lt;/h1>
&lt;h2 id="21推理环境搭建">2.1.推理环境搭建&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.基于MiniCoda，创建虚拟环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101650577.png" alt="image-20230906101650577">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.激活虚拟环境，安装运行时环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101852342.png" alt="image-20230906101852342">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP3.下载依赖。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102057137.png" alt="image-20230906102057137">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP5.初始化向量数据库。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102553686.png" alt="image-20230906102553686">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP6.配置model_config.py、server_config.py、llm_api.py。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102733935.png" alt="image-20230906102733935">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102716203.png" alt="image-20230906102716203">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102703074.png" alt="image-20230906102703074">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102652268.png" alt="image-20230906102652268">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP7.启动应用&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103336869.png" alt="image-20230906103336869">&lt;/p>
&lt;h2 id="22模型部署">2.2.模型部署&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.下载大语言模型和词嵌入模型&lt;/strong>——chatGLM2-6B、m3e-base&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102213647.png" alt="image-20230906102213647">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102246753.png" alt="image-20230906102246753">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.部署大语言模型和词嵌入模型。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103747404.png" alt="image-20230906103747404">&lt;/p>
&lt;h2 id="23模型微调">2.3.模型微调&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.通过WebUI，上传知识文档。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103254463.png" alt="image-20230906103254463">&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>&lt;strong>STEP2.编写提示词模板&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104642146.png" alt="image-20230906104642146">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104709680.png" alt="image-20230906104709680">&lt;/p>
&lt;ul>
&lt;li>经过2小时的学习，LLM修炼完了1000多份智慧交通的资料，这学习效率杠杠滴！
&lt;ul>
&lt;li>PS：感谢公路军团的专家，提供珍藏多年的资料，改天请您吃饭！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906105032126.png" alt="image-20230906105032126">&lt;/p>
&lt;h2 id="24模型测试">2.4.模型测试&lt;/h2>
&lt;p>直接上视频，咨询了一下交通行业的这位专家，回答挺靠谱：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=830744059&amp;bvid=BV1C34y1N7QG&amp;cid=1258811184&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文阐述了基于大语言模型的知识问答系统的实现原理及原型实践过程，原型距离可商用还有很多细节需打磨，但我们可以得到如下观点：&lt;/p>
&lt;ul>
&lt;li>从实现原理看：
&lt;ul>
&lt;li>&lt;strong>技术关键点1&lt;/strong>：要&lt;font color=red>&lt;strong>选择合适的多种大语言模型，形成大语言模型的矩阵&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>技术关键点2&lt;/strong>：除大语言模型选型，还需&lt;strong>借助LangChain处理好诸多环节&lt;/strong>(文档结构化、分段分词、词嵌入、上下文、向量搜索、提示词等)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>从实践看：
&lt;ul>
&lt;li>&lt;strong>数据&lt;/strong>：数据收集、数据清洗、词嵌入等，极为重要。&lt;/li>
&lt;li>&lt;strong>大语言模型&lt;/strong>：需跟踪各大厂商大语言模型的最新能力，快速替换到应用软件中，&lt;font color=red>&lt;strong>保持大语言模型的先进性&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>提示词&lt;/strong>：提示词模板的设计，决定了能否&lt;font color=red>&lt;strong>充分发挥大语言模型的潜力&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>工具链&lt;/strong>：熟练运用LangChain、向量数据库等工具，有助于设计并实现适应业务场景的问答业务流。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>今天就写到这里，后续专栏会继续展示基于大语言模型的LLM应用，欢迎共同探索(上述环境已做成镜像，需要的同学可后台联系作者)。&lt;/p></description></item><item><title>【chatGPT】学习笔记10-LangChain之Model IO，对LLM的抽象1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</link><pubDate>Mon, 28 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</guid><description>&lt;p>最近的专栏都是在拆解大语言模型的内部实现及论文，我们再来写点儿偏工程实践的内容——LangChain。&lt;/p>
&lt;h1 id="1langchain简介">1.LangChain简介&lt;/h1>
&lt;h2 id="1如何实现基于ai的app">(1)如何实现基于AI的App？&lt;/h2>
&lt;p>如果我们想开发一个基于大语言模型的AI知识库，怎么做呢？这个App的架构如下图：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>准备环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP1.数据管理&lt;/strong>：需要将垂直领域的知识进行词嵌入，放到向量数据库中。&lt;/li>
&lt;li>&lt;strong>STEP2.提示词管理&lt;/strong>：需要构造好提示词模板。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>微调&amp;amp;应用环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP3.知识查询&lt;/strong>：将用户输入的自然语言问题向量化，寻找与输入问题相关的知识向量。&lt;/li>
&lt;li>&lt;strong>STEP4.提示词查询&lt;/strong>：找到和输入问题有关的提示词模板。&lt;/li>
&lt;li>&lt;strong>STEP5.调用大语言模型&lt;/strong>：将&lt;code>知识向量&lt;/code>+&lt;code>提示词模板&lt;/code>传递给大语言模型(大语言模型可能在本地，也可能在云端，大语言模型可能是1个也可能是多个)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901152726410.png" alt="image-20230901152726410">&lt;/p>
&lt;p>从这个架构可以看出，存在以下问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文档解析多样&lt;/strong>：垂直领域的知识文档，存在不同的文件格式，如何解析？&lt;/li>
&lt;li>&lt;strong>提示词需要复用&lt;/strong>：无论面向多少种不同厂商的大语言模型，提示词如何最大化复用？&lt;/li>
&lt;li>&lt;strong>模型多样&lt;/strong>：需要面向不同厂商大语言模型的不同调用接口，如何适配？&lt;/li>
&lt;/ul>
&lt;h2 id="2rag">(2)RAG&lt;/h2>
&lt;p>LangChain，为私有知识库App这类应用，定义了一个新的领域——&lt;strong>RAG&lt;/strong>(Retrieval Augmented Generation，生成式检索增强)。&lt;/p>
&lt;p>在RAG领域，通常有5类需要考虑的因素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154308580.png" alt="image-20230901154308580">&lt;/p>
&lt;h2 id="3langchain的体系结构">(3)LangChain的体系结构&lt;/h2>
&lt;p>LangChain为了解决RAG领域的五类问题，提供了6大模块，如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154845846.png" alt="image-20230901154845846">&lt;/p>
&lt;p>我们在接下来的内容中，逐一解读各个模块的特性以及关键源码。&lt;/p>
&lt;h1 id="2核心模块1model-io">2.核心模块1：Model I/O&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>LangChain的第一个核心模块，就是&lt;code>Model I/O&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O最重要的能力就是封装了各大厂商大语言模型的不同接口。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O包括三个子特性：&lt;/p>
&lt;ul>
&lt;li>Prompts&lt;/li>
&lt;li>Models&lt;/li>
&lt;li>Output Parsers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/model_io-1f23a36233d7731e93576d6885da2750.jpg" alt="model_io_diagram">&lt;/p>
&lt;h2 id="21models模型">2.1.Models(模型)&lt;/h2>
&lt;ul>
&lt;li>在Models特性中，LangChain抽象了两类模型：
&lt;ul>
&lt;li>语言模型：LLMs，LangChain在这里封装了各厂商大语言模型的接口。&lt;/li>
&lt;li>聊天模型：ChatModels，对语言模型的高层封装，提供输入一组聊天消息对象、输出聊天结果对象的模式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="211llm语言模型">2.1.1.LLM(语言模型)&lt;/h3>
&lt;h4 id="1langchain源码解读">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过3个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLM&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>与LLM相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>LLMReusult&lt;/code>，&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManagerForLLMRun&lt;/code>，&lt;code>AsyncCallbackManagerForLLMRun&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManager&lt;/code>, &lt;code>AsyncCallbackManager&lt;/code>&lt;/li>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901160851034.png" alt="image-20230901160851034">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>class &lt;strong>BaseLanguageModel&lt;/strong>：语言模型的基类，所有语言模型封装子类，都继承自本类。&lt;/p>
&lt;ul>
&lt;li>def &lt;strong>generate_prompt&lt;/strong>：这是一个抽象函数，该函数由各个语言模型封装子类自行实现，输入提示词序列，输出是语言模型返回的结果。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901161907636.png" alt="image-20230901161907636">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>BaseLLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>，实现了各语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162431013.png" alt="image-20230901162431013">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLLM&lt;/strong>，进一步封装个语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162650458.png" alt="image-20230901162650458">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>LangChain实现的各类语言模型封装的子类：&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163511284.png" alt="image-20230901163511284">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="2例子">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT的达芬奇003模型&lt;/strong>。其中，class &lt;strong>OpenAI&lt;/strong>就是对GPT的模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：给我一个Java的ArrayList的代码示例。&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：是一段ArrayList代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163837808.png" alt="image-20230901163837808">&lt;/p>
&lt;h3 id="212chatmodel聊天模型">2.1.2.ChatModel(聊天模型)&lt;/h3>
&lt;h4 id="1langchain源码解读-1">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过2个关键类实现聊天模型：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseChatModel&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>, &lt;code>HumanMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901164637931.png" alt="image-20230901164637931">&lt;/p>
&lt;h4 id="2例子-1">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT3.5 Turbo模型&lt;/strong>。其中，class &lt;strong>ChatOpenAI&lt;/strong>就是对GPT的聊天模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：一组聊天消息对象&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：聊天消息的回答对象。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165247536.png" alt="image-20230901165247536">&lt;/p>
&lt;h2 id="22prompts提示词">2.2.Prompts(提示词)&lt;/h2>
&lt;h3 id="1langchain源码解读-2">(1)LangChain源码解读&lt;/h3>
&lt;ul>
&lt;li>LangChain通过2个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BasePromptTemplate&lt;/strong>：基础提示词的基类。&lt;/li>
&lt;li>class &lt;strong>BaseMessagePromptTemplate&lt;/strong>：聊天模型提示词的基类。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165527920.png" alt="image-20230901165527920">&lt;/p>
&lt;h3 id="2例子-基础提示词">(2)例子-基础提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_template&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>lang&lt;/code>变量，通过1个提示词模板，实现了生成&lt;code>java&lt;/code>、&lt;code>python&lt;/code>、&lt;code>C++&lt;/code>的冒泡排序代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170042600.png" alt="image-20230901170042600">&lt;/p>
&lt;h3 id="2例子-chatmodel提示词">(2)例子-ChatModel提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_messages&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>user_input&lt;/code>变量，通过1个提示词模板，实现了生成对Java的ArrayList的知识点摘要。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了聊天模型的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170414376.png" alt="image-20230901170414376">&lt;/p>
&lt;h3 id="3例子-fewshot提示词">(3)例子-FewShot提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>FewShotPromptTemplate&lt;/code>构造了FewShot提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>input&lt;/code>变量，通过1个提示词模板，实现了一系列FewShot语料。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了FewShot的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170805004.png" alt="image-20230901170805004">&lt;/p>
&lt;h2 id="23output-parsers输出解析器">2.3.Output Parsers(输出解析器)&lt;/h2>
&lt;ul>
&lt;li>输出解析器是一个很有趣的特性，大语言模型返回的答案是千奇百怪的，如何解析呢？&lt;/li>
&lt;li>这里以&lt;code>List Parser&lt;/code>为例：
&lt;ul>
&lt;li>LangChain提供了class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>，这个类会在提示词中限定大语言模型的返回。&lt;/li>
&lt;li>比如：输入的问题是&lt;code>请问《圣斗士星矢》中，有哪几个主角？&lt;/code>。&lt;/li>
&lt;li>class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>会构造提示词：Your response should be a list of comma separated values, eg: &lt;code>foo, bar, baz&lt;/code>。&lt;/li>
&lt;li>大语言模型回答问题的文本就有了固定格式：&lt;code>星矢、紫龙、冰河、瞬、一辉&lt;/code>，这样就可以解析成list了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901171701655.png" alt="image-20230901171701655">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文介绍了：&lt;/p>
&lt;ul>
&lt;li>LangChain的功能，为大家构建了对LangChain的宏观认识。&lt;/li>
&lt;li>LangChain的Model I/O核心模块：
&lt;ul>
&lt;li>该模块通过Models，抽象了各厂商的大语言模型，我们可以使用统一接口去调用不同的大语言模型。&lt;/li>
&lt;li>该模块通过Prompts，提供了多种提示词模板的构建。&lt;/li>
&lt;li>该模块通过Output Parsers，提供了对大语言模型输出结果的结构化解析。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</link><pubDate>Mon, 21 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</guid><description>&lt;h1 id="1seq2seqtransformer的雏形">1.Seq2Seq，Transformer的雏形&lt;/h1>
&lt;h2 id="11为什么会出现seq2seq">1.1.为什么会出现Seq2Seq？&lt;/h2>
&lt;p>在神经概率语言模型NPLM出现后的很长一段时间，都是在这种网络架构下进行优化。但，依然面临很多难题(主要是循环神经网络RNNs的局限)：&lt;/p>
&lt;ul>
&lt;li>如：输入序列长度增加时性能下降&lt;/li>
&lt;li>如：顺讯处理导致计算效率低&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>在2014年，Seq2Seq的提出，给人类一个不错的启发。&lt;/p>
&lt;p>随后在自注意力机制的加持下，Transformer就诞生了。&lt;/p>
&lt;h2 id="12seq2seq架构概览">1.2.Seq2Seq架构概览&lt;/h2>
&lt;p>大神&lt;code>Ilya Sutskever&lt;/code>在2014年，以第一作者的身份，发表了论文《Sequence to Sequence Learning with Neural Networks》。&lt;/p>
&lt;blockquote>
&lt;p>论文地址：https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162206474.png" alt="image-20230831162206474">&lt;/p>
&lt;p>顺便说一嘴，大神&lt;code>Ilya Sutskever&lt;/code>就是这位大哥，OpenAI联合创始人和首席科学家，各大自媒体都在播放他老人家的演讲视频：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162529973.png" alt="image-20230831162529973">&lt;/p>
&lt;p>这篇论文的核心就是这张图，阐述了Seq2Seq的&lt;strong>编码器-解码器架构&lt;/strong>：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/p>
&lt;p>初次理解这张图，&lt;strong>需要费点儿脑细胞&lt;/strong>，我们接下来详细拆解。&lt;/p>
&lt;h2 id="13seq2seq架构详解">1.3.Seq2Seq架构详解&lt;/h2>
&lt;h3 id="1整体模型架构">(1)&amp;ldquo;整体模型&amp;quot;架构&lt;/h3>
&lt;p>Seq2Seq会将变长的&lt;strong>输入序列&lt;/strong>，转换为变长的&lt;strong>输出序列&lt;/strong>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170025140.png" alt="image-20230831170025140">&lt;/p>
&lt;p>这里举一个例子——将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;rdquo;：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：Seq2Seq输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：Seq2Seq输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：Seq2Seq输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170404160.png" alt="image-20230831170404160">&lt;/p>
&lt;h3 id="2编码器-解码器架构">(2)&amp;ldquo;编码器-解码器&amp;quot;架构&lt;/h3>
&lt;p>进一步拆解整体模型架构，Seq2Seq由编码器+解码器组成：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171003578.png" alt="image-20230831171003578">&lt;/p>
&lt;p>这里还是以将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;quot;例子来解析：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：编码器将学习到上下文向量，传递给解码器&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：解码器输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：解码器输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t8时刻&lt;/strong>：解码器输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171625206.png" alt="image-20230831171625206">&lt;/p>
&lt;h3 id="3编码器解码器微观逻辑">(3)&amp;ldquo;编码器、解码器&amp;quot;微观逻辑&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>编码器、解码器的具体实现&lt;/strong>：在论文中，它们都是采用RNN实现。如下图所示：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的输出&lt;/strong>：1号隐藏层状态向量 + 1号词输出向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输入&lt;/strong>：1号隐藏层状态向量 + 2号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输出&lt;/strong>：2号隐藏层状态向量 + 2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831222457684.png" alt="image-20230831222457684">&lt;/li>
&lt;li>&lt;strong>解码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的输出&lt;/strong>：1号词输出向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输入&lt;/strong>：0号隐藏层状态向量 + 1号词输出向量 + 2号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输出&lt;/strong>：2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225007834.png" alt="image-20230831225007834">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>这还是不太直观，我们再把编码器、解码器&lt;strong>按时序进一步展开&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器，编码器输出&lt;code>1号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字 + &lt;code>1号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>2号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字 + &lt;code>2号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>3号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;Who&amp;quot;字，输入给解码器，解码器输出&lt;code>Who&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;are&amp;quot;字 + 解码器输出&lt;code>who&lt;/code>，输入给解码器，解码器输出&lt;code>are&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;u&amp;quot;字 + 解码器输出&lt;code>are&lt;/code>，输入给解码器，解码器输出&lt;code>u&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225749199.png" alt="image-20230831225749199">&lt;/p>
&lt;ul>
&lt;li>这里有一个细节，什么是&lt;strong>Teach Forcing&lt;/strong>？
&lt;ul>
&lt;li>我们可以想象，如果1号解码器的预测错了，那么2号、3号解码器都会错，进而导致学习效率非常低。&lt;/li>
&lt;li>Teach Forcing好像&lt;strong>开卷考试&lt;/strong>，如果1号解码器预测错了，Teach Forcing会纠正预测结果，进而加速学习效率。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3优点">(3)优点&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>变长序列&lt;/strong>：由于编码器、解码器采用RNN实现，所以输入序列可以是变长、输出序列也可以是变长。而CNN、DNN都不支持变长序列。&lt;/li>
&lt;li>&lt;strong>信息压缩&lt;/strong>：隐藏层状态，或者叫上下文向量，本质上将输入序列进行了信息压缩，转变为含有上下文语义的向量。&lt;/li>
&lt;/ul>
&lt;h3 id="4劣势">(4)劣势&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>长序列信息损失&lt;/strong>：由于上下文向量为定长，所以当输入序列过长时编码器会出现信息损失。&lt;font color=red>&lt;strong>这就是注意力机制的发力点，先埋个伏笔&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>效率&lt;/strong>：采用RNN(如：LSTM、GRU)实现编码器、解码器，RNN会面临梯度消失、梯度爆炸等问题。&lt;/li>
&lt;/ul>
&lt;h1 id="2代码">2.代码&lt;/h1>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231557775.png" alt="image-20230831231557775">&lt;/p>
&lt;h2 id="step12结构化训练数据">STEP1.2.结构化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231753952.png" alt="image-20230831231753952">&lt;/p>
&lt;h2 id="step21定义编码器和解码器类">STEP2.1.定义编码器和解码器类&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231855888.png" alt="image-20230831231855888">&lt;/p>
&lt;h2 id="step22定义seq2seq模型">STEP2.2.定义Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231925444.png" alt="image-20230831231925444">&lt;/p>
&lt;h2 id="step3训练seq2seq模型">STEP3.训练Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232035429.png" alt="image-20230831232035429">&lt;/p>
&lt;h2 id="step4测试seq2seq模型">STEP4.测试Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232116723.png" alt="image-20230831232116723">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;ul>
&lt;li>本文解析了Seq2Seq的内部实现，理解下图的关键是——&lt;strong>按照时序&lt;/strong>，分析清楚每个编码器or解码器&lt;strong>有几个输入、几个输出&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/li>
&lt;li>理解了Seq2Seq，我们接下来就可以逐步实现完整的Transformer了，且听下回分解。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</link><pubDate>Sat, 19 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</guid><description>&lt;h1 id="1问题词嵌入的局限性">1.问题：词嵌入的局限性&lt;/h1>
&lt;p>在《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》中，我们了解了词嵌入。&lt;/p>
&lt;p>获取词向量后，在向量空间中可以获得每个词的向量表示，也可以通过向量了解词和词之间的语义关联性。&lt;/p>
&lt;p>似乎，仅通过词嵌入就能解决很多NLP问题了，但词嵌入存在如下局限性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>：Word2Vec，以及后续的GloVe，在训练完成后，学习到的词向量是不会再被更新的。&lt;/li>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>决定了：
&lt;ul>
&lt;li>&lt;strong>词嵌入无法应对多义词&lt;/strong>：1个词只有1个向量，无法表示多义词。&lt;/li>
&lt;li>&lt;strong>词嵌入无法应对未知词&lt;/strong>：没见过的词，显然Word2Vec不可能在向量空间中无中生有它对应的向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>于是，神经概率语言模型NPLM横空出世。&lt;/p>
&lt;h1 id="2对策神经概率语言模型nplm">2.对策：神经概率语言模型NPLM&lt;/h1>
&lt;h2 id="1什么是神经概率语言模型">(1)什么是神经概率语言模型？&lt;/h2>
&lt;p>科学家和大神们，很早就有了引入具有强大表示力和学习力的神经网络的想法。大致思想是：&lt;/p>
&lt;ul>
&lt;li>输入海量语料，神经网络学习词语在不同上下文中的概率分布。&lt;/li>
&lt;li>词最终还是会被向量化，只是这些词向量的学习过程成为神经网络的一部分，词向量表达的人类语言规律(词语、语义等)最终被记录到神经网络的参数中。&lt;/li>
&lt;/ul>
&lt;p>接下来，深度学习三巨头之一Bengio，在2003年推出了神作《A Neural Probabilistic Language Model》：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NPLM&lt;/strong>：Neural Probabilistic Language Model，神经概率语言模型。NPLM包含输入层、隐藏层、输出层。&lt;/li>
&lt;li>&lt;strong>输入层&lt;/strong>：将单词映射到连续的词向量空间。&lt;/li>
&lt;li>&lt;strong>隐藏层&lt;/strong>：通过非线性激活函数学习单词间的复杂关系。&lt;/li>
&lt;li>&lt;strong>输出层&lt;/strong>：通过Softmax层产生下一个单词的概率分布。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>论文链接：https://dl.acm.org/doi/pdf/10.5555/944919.944966&lt;/p>
&lt;/blockquote>
&lt;p>下图是Bengio论文中的阐述的神经概率语言模型的架构，后续NLP方向上的各种技术都是围绕这个架构进行各层的优化！&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230818065825957.png" alt="image-20230818065825957">&lt;/p>
&lt;p>对上图进行细化，笔者标出了输入层、隐藏层、输出层：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825094252544.png" alt="image-20230825094252544">&lt;/p>
&lt;h2 id="2输入层">(2)输入层&lt;/h2>
&lt;ul>
&lt;li>输入层的&lt;strong>职责&lt;/strong>：将输入的词，开展词嵌入，学习到词向量后存储在输入层。因此，输入层也可以叫嵌入层。&lt;/li>
&lt;li>输入层的&lt;strong>输入&lt;/strong>：词本身&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095132476.png" alt="image-20230825095132476">&lt;/li>
&lt;li>输入层的&lt;strong>输出&lt;/strong>：词向量&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095205472.png" alt="image-20230825095205472">&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095238666.png" alt="image-20230825095238666">&lt;/p>
&lt;h2 id="3隐藏层">(3)隐藏层&lt;/h2>
&lt;ul>
&lt;li>隐藏层的&lt;strong>职责&lt;/strong>：学习词与词之间的关系&lt;/li>
&lt;li>隐藏层采用了&lt;strong>非线性激活函数&lt;/strong>：采用非线性激活函数的本质是让神经网络的**&amp;ldquo;脑回路&amp;rdquo;**复杂起来(如：使用激活函数可以将线性层提升为非线性层)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095635192.png" alt="image-20230825095635192">&lt;/p>
&lt;h2 id="4输出层">(4)输出层&lt;/h2>
&lt;ul>
&lt;li>输出层的&lt;strong>职责&lt;/strong>：通过Softmax层，输出下个单词的概率分布&lt;/li>
&lt;li>&lt;strong>Softmax&lt;/strong>：归一化，Softmax层的输入是各个词得到的分数，输出是将这些分数归一化到0~1的值域内。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825102215771.png" alt="image-20230825102215771">&lt;/p>
&lt;h2 id="4隐藏层的优化">(4)隐藏层的优化&lt;/h2>
&lt;ul>
&lt;li>神经网络似乎天生不擅长长序列问题，所以后续很多NLP发展的技术，都是在&lt;strong>优化NPLM在长序列上的表现&lt;/strong>。
&lt;ul>
&lt;li>浅层网络无法捕捉文本中复杂的信息规律。&lt;/li>
&lt;li>普通的深层神经网络也不能很好处理长距离的依赖关系。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>NPLM的巧妙之处在于：&lt;strong>隐藏层可以使用任意的神经网络去替换&lt;/strong>。&lt;/li>
&lt;li>RNN、LSTM横空出世，在NLPM中长期霸榜：
&lt;ul>
&lt;li>RNN，循环神经网络，这种特殊的神经网络结构，可以将网络的输出作为网络的输入，使得神经网络能够处理数据的同时保留了&lt;strong>前世的记忆&lt;/strong>。&lt;/li>
&lt;li>LSTM，是RNN的经典代表作，很长一段时间作为NLP问题的SOTA(state of the art)模型
&lt;ul>
&lt;li>注意：被评为SOTA，而不是benchmark，或者baseline，是一种极高的荣誉。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3代码nplm">3.代码：NPLM&lt;/h1>
&lt;p>接下来，进入代码环节。&lt;/p>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络">STEP2.构建NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093351356.png" alt="image-20230826093351356">&lt;/p>
&lt;h2 id="step3训练nplm网络">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093407105.png" alt="image-20230826093407105">&lt;/p>
&lt;h2 id="step4测试nplm">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093420906.png" alt="image-20230826093420906">&lt;/p>
&lt;h1 id="4代码nplm优化">4.代码：NPLM优化&lt;/h1>
&lt;p>再使用RNN、LSTM优化NPLM架构。&lt;/p>
&lt;h2 id="step11构建数据集-1">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据-1">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络-1">STEP2.构建NPLM网络&lt;/h2>
&lt;ul>
&lt;li>核心就是在此替换隐藏层和输出层&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093621577.png" alt="image-20230826093621577">&lt;/p>
&lt;h2 id="step3训练nplm网络-1">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093643273.png" alt="image-20230826093643273">&lt;/p>
&lt;h2 id="step4测试nplm-1">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093701798.png" alt="image-20230826093701798">&lt;/p>
&lt;h1 id="5小结">5.小结&lt;/h1>
&lt;p>最后，我们来做一些对比和小结：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从目标看&lt;/strong>：NPLM是解决词汇出现概率的问题，Word2Vec是解决如何将词转换为向量的问题。&lt;/li>
&lt;li>&lt;strong>从实现看&lt;/strong>：NPLM和Word2Vec都是基于神经网络的模型，但Word2Vec没有激活函数，属于利用了浅层神经网络。&lt;/li>
&lt;li>&lt;strong>从词向量看&lt;/strong>：NPLM中的词向量是神经网络的一部分，基于NPLM的目标，人类对它的训练是不会停止的，训练一次，词向量就会变化一次。而基于Word2Vec的目标，人类只会对它训练一次，训练好，词向量就固化不变了。&lt;/li>
&lt;/ul>
&lt;p>NPLM可以算作如今大语言模型的祖师爷了，在学界存在极高的地位和价值，后续很多年的RNN、LSTM都只能算作对NPLM的架构优化。&lt;/p>
&lt;p>但，NPLM也存在历史局限性，于是才有了后来的大模型的关键部件Transformer，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</link><pubDate>Thu, 17 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</guid><description>&lt;p>人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。&lt;/p>
&lt;p>目前炙手可热的Transformer既是如此，LSTM、Word2Vec等是它的基石，共同构筑了现在的大语言模型的关键部件和理论基础。&lt;/p>
&lt;p>之前发了一个朋友圈，将对大语言模型影响深远的论文，梳理了三条脉络：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LSTM&lt;/strong>：在1997年那会儿解决了AI的记忆问题。&lt;/li>
&lt;li>&lt;strong>Word2Vec&lt;/strong>：在2013~2014年解决了将词转换为向量，将&amp;quot;文字游戏&amp;quot;转换为了高维向量空间中的&amp;quot;数学游戏&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>Transformer&lt;/strong>：2014年出现了注意力机制雏形，2017年那篇著名的《Attention is All you Need》引出的Transformer，随后是OpenAI2018年的GPT-1、Google2019的BERT。&lt;/li>
&lt;/ul>
&lt;p>本文重点阐述第2条技术线：&lt;strong>Word2Vec&lt;/strong>涉及的技术——词的向量化，这也是大语言模型的关键部件之一。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230817105511.jpg" alt="微信图片_20230817105511">&lt;/p>
&lt;h1 id="1什么是表示学习">1.什么是表示学习？&lt;/h1>
&lt;p>我们先看一下表示学习的定义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>表示学习&lt;/strong>：Representation Learning，通过学习算法&lt;strong>自动&lt;/strong>地从原始数据中&lt;strong>学习到一种数据表达形式&lt;/strong>。
&lt;ul>
&lt;li>表示学习的目标是将输入的数据转换到具有良好表现能力的特征空间中。&lt;/li>
&lt;li>特征空间中的数据有&lt;strong>可分性&lt;/strong>、&lt;strong>可解释性&lt;/strong>、&lt;strong>可推理性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们来解读一下上述这段话：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>：
&lt;ul>
&lt;li>我们知道，人类彼此的沟通介质是：文字、图像、视频、语音等。这些介质的本质是数据。&lt;/li>
&lt;li>我们还知道，从数据中寻找规律，是连接主义的哲学基础。&lt;/li>
&lt;li>我们还知道，可以被人类大脑理解的文字、图像、视频等数据，人工智能能理解吗？不能。&lt;/li>
&lt;li>人工智能无法理解这些数据，人工智能就无法从数据中寻找规律——因此，&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>向量化是数据可表示的一种实现&lt;/strong>：
&lt;ul>
&lt;li>向量化，可以让人工智能理解人类才能理解的数据。&lt;/li>
&lt;li>向量化，只是数据可表示的&lt;strong>一种&lt;/strong>实现，当然可以有其它的实现方式。
&lt;ul>
&lt;li>如：笔者前一篇《【chatGPT】学习笔记6-手撸一个上古GPT》中实现的N-Gram算法，并未对人类语言向量化，也让人工智能具备理解人类语言以及语言概率的能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>向量化，依然是目前数据可表示的若干种实现中最好的一种。
&lt;ul>
&lt;li>基于N-Gram的人工智能，能力很弱。它的智能停留在人类语言的&lt;strong>表面&lt;/strong>，它并不理解&lt;strong>语义&lt;/strong>。&lt;/li>
&lt;li>比如：&amp;ldquo;我去！&amp;ldquo;这个句子，基于N-Gram的人工智能无法识别，这句话到底想表达&amp;quot;我艹&amp;rdquo;，还是想表达&amp;quot;我要去某个地方&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>上述对表示学习的解读还是挺抽象。没关系，后文有形象的例子，各位小伙伴可以先看完后文，再回头看这一段，加深对表示学习的理论理解。&lt;/p>
&lt;h1 id="2什么是向量什么是嵌入">2.什么是向量？什么是嵌入？&lt;/h1>
&lt;h2 id="1向量">(1)向量&lt;/h2>
&lt;p>在NLP领域，无论语言模型的大小，都必须将词先表达为向量，词向量就是语言模型的输入。&lt;/p>
&lt;p>在CV领域，无论视觉模型的大小，也必须将图像先表达为向量。&lt;/p>
&lt;p>将文字、图像、视频、音频等数据向量化，本质是&lt;font color=red>**将&amp;quot;人类可理解的数据问题&amp;quot;转换为&amp;quot;机器可理解的机器学习问题&amp;rdquo;**&lt;/font>。&lt;/p>
&lt;p>我们来看一个向量化的例子：&lt;/p>
&lt;ul>
&lt;li>人类小孩儿第一次看到苹果，人类小孩儿是怎么记住这种东西就是苹果呢？&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143249956.png" alt="image-20230817143249956">&lt;/p>
&lt;ul>
&lt;li>人类小孩儿会从不同维度描述苹果的特征(如：纹理、颜色、形状、大小等)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143358437.png" alt="image-20230817143358437">&lt;/p>
&lt;ul>
&lt;li>假设人类小孩大脑的工作机制，是将不同维度的特征用数字表达并存储，这些维度的特征值就是&lt;strong>向量&lt;/strong>了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143456600.png" alt="image-20230817143456600">&lt;/p>
&lt;ul>
&lt;li>当人类小孩看到新的苹果，大脑也会将新苹果的特征提取为向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143619055.png" alt="image-20230817143619055">&lt;/p>
&lt;ul>
&lt;li>大苹果、小苹果，从颜色、大小、形状、纹理等等维度是很相似的，显然，把这些苹果抽象成多个向量，这些向量在向量空间中的距离肯定是很近的。&lt;/li>
&lt;li>这就是信息向量化，词的向量化就是信息向量化的一种。&lt;/li>
&lt;/ul>
&lt;p>上述例子，来自于一段腾讯云介绍向量数据库的视频，各位小伙伴可以通过视频，看到更多形象化理解向量的例子：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=489914023&amp;bvid=BV13N41167Q9&amp;cid=1237875667&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="2嵌入">(2)嵌入&lt;/h2>
&lt;p>先看一下嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>嵌入：Embedding，表示学习的一种形式，用于&lt;strong>将高维数据映射到低维空间&lt;/strong>。嵌入包括：
&lt;ul>
&lt;li>词嵌入&lt;/li>
&lt;li>图像嵌入&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这里以图像嵌入的一种经典算法t-SNE(t-Distributed Stochastic Neighbor Embedding)为例：&lt;/p>
&lt;ul>
&lt;li>图像嵌入的第一阶段是将图像转换为高维向量。&lt;/li>
&lt;li>图像嵌入的第二阶段是将高维向量映射到低维向量，但是要保证高维向量中邻近的点，在低维空间中也有相同的距离关系。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/t-sne_optimise.gif" alt="t-sne_optimise">&lt;/p>
&lt;p>&lt;strong>嵌入&lt;/strong>本身是一个很抽象的术语，但我们可以简单地认为&lt;strong>嵌入就是向量化&lt;/strong>的过程。&lt;/p>
&lt;p>对于嵌入，最关键的信息只有一个：&lt;strong>高维&lt;/strong>映射到&lt;strong>低维&lt;/strong>。&lt;/p>
&lt;p>什么是高维向量？什么是低维向量？为什么要从高维映射到低维？&lt;/p>
&lt;p>这些问题的有趣之处，和人类的学习过程非常相似：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>有一日未死之身，则有一日未闻之道&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类开始学习一门新知识，越学越觉得这个知识领域博大精深。这是人类学习的第一阶段——&amp;quot;&lt;strong>把书读厚&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>在嵌入过程中，也就是把信息向量化过程中，显然是把一个信息用更高维度的向量去描述，信息越准确。&lt;/li>
&lt;li>比如：小时候练习看图说话，老师会鼓励小朋友把图上看到的东西从更多角度去描绘出来，这些角度就是向量的维度。&lt;/li>
&lt;li>再比如：一个哲学思想，不同流派的哲学家会从不同角度、不同立场去阐述、论证和思辨，这些角度、立场也等效于向量的维度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>读书之道，愈进愈简，百卷如一页&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类学习新知识到了一个阶段，会出现&lt;strong>顿悟&lt;/strong>——发现、归纳各个知识点的内在联系。这是人类学习的第二阶段——&amp;quot;&lt;strong>把书读薄&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>比如：杨过的重剑无锋，大巧不工。比如：老子的大道至简，有言无言。&lt;/li>
&lt;li>在嵌入过程中，也有类似的过程，虽然单个高维向量有丰富的表达，但是多个高维向量存在某些内在联系，把它们降低为低维向量，不仅抓住了信息的本质，而且更加精炼、大道至简。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这就是&lt;strong>嵌入&lt;/strong>的&lt;strong>核心思想&lt;/strong>——将&lt;strong>高维向量降低成低维向量&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817221709020.png" alt="image-20230817221709020">&lt;/p>
&lt;h1 id="3什么是词向量什么是词嵌入">3.什么是词向量？什么是词嵌入？&lt;/h1>
&lt;p>先来看词向量、词嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的定义&lt;/strong>：将词语转换成对应数值向量的表达形式，便于计算机读取和运算。&lt;/li>
&lt;li>&lt;strong>词向量的数学表达&lt;/strong>：将字典D中的任意词w，设定固定长度的实值向量V(w)。其中：V(w)就是词向量，m表示词向量长度。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817142112513.png" alt="image-20230817142112513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词嵌入&lt;/strong>：Word Embedding，将词映射到向量空间，形成词向量的过程和方法，就是词嵌入。&lt;/li>
&lt;/ul>
&lt;p>再来看一个经典的例子：&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>男人、女人、国王、皇后&lt;/strong>开展词嵌入，它们变成了四个向量，存在与三维的向量空间中。&lt;/li>
&lt;li>假设词嵌入的具体代码实现没有问题，那么男人和国王的向量距离应该很近，女人和皇后的向量距离应该很近。&lt;/li>
&lt;li>这样，对人类的文字开展词嵌入后，有两个关键的输出：
&lt;ul>
&lt;li>&lt;strong>词以向量的形式存在&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>有关联的词对应的向量的距离也会很小&lt;/strong>(如：对两个词向量求余弦相似度)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817222631960.png" alt="image-20230817222631960">&lt;/p>
&lt;h1 id="4如何实现词嵌入">4.如何实现词嵌入？&lt;/h1>
&lt;p>词嵌入，或者说词的向量化过程分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>文本变为One-Hot编码。&lt;/li>
&lt;li>One-Hot编码变为低维词向量。&lt;/li>
&lt;/ul>
&lt;h2 id="1one-hot编码">(1)One-Hot编码&lt;/h2>
&lt;p>&lt;strong>One-Hot编码&lt;/strong>：One-Hot Representation，也叫独热编码。&lt;/p>
&lt;p>比如：对于男、女两个词，One-Hot编码可以表示为：&lt;/p>
&lt;ul>
&lt;li>男：[1, 0]，女：[0, 1]&lt;/li>
&lt;/ul>
&lt;p>再比如：对于初一、初二、初三三个词，可以表示为：&lt;/p>
&lt;ul>
&lt;li>初一：[1, 0, 0]，初二：[0, 1, 0]，初三：[0, 0, 1]&lt;/li>
&lt;/ul>
&lt;p>如果说One-Hot编码算得上广义的向量，那么它明显的缺点是：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>过于稀疏&lt;/strong>：在各个维度上，只有1个维度是1，其它维度都是0。&lt;/li>
&lt;/ul>
&lt;h2 id="2distributed表示">(2)Distributed表示&lt;/h2>
&lt;p>&lt;strong>Distributed表示&lt;/strong>：Distributed Representation，分布式表示，它是表示学习中的一种。&lt;/p>
&lt;ul>
&lt;li>Word2Vec、GloVe、fastText等都是其中一种具体实现。&lt;/li>
&lt;/ul>
&lt;p>分布式表示的核心思想是：&lt;/p>
&lt;ul>
&lt;li>通过训练，将词典里每个单词转换为固定长度的低维向量。&lt;/li>
&lt;li>这些向量之间可以通过余弦相似度之类的数学工具，表示向量之间的距离。语义越接近的向量之间，距离越近。&lt;/li>
&lt;/ul>
&lt;p>Distributed表示才算得上真正的向量，它明显的优点是：&lt;/p>
&lt;ul>
&lt;li>从&lt;strong>过于稀疏&lt;/strong>的One-Hot编码，变为了&lt;strong>密集向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="3word2vecdistributed表示的一种实现">(3)Word2Vec，Distributed表示的一种实现&lt;/h2>
&lt;p>Word2Vec是Distributed表示的一种具体实现：&lt;/p>
&lt;ul>
&lt;li>Word2Vec将词汇表中的每个词表示成固定长度的向量。&lt;/li>
&lt;li>Word2Vec的核心思想本质是——&lt;strong>近朱者赤，近墨者黑&lt;/strong>：
&lt;ul>
&lt;li>一个词可以根据它的的周边词，推测出自己的语义。&lt;/li>
&lt;li>一个词也可以通过自己的语义，推测出它的周边词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Word2Vec的实现算法有两种：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CBOW&lt;/strong>：Continuous Bag of Words，连续词袋，输入是周围词，输出是中心词。&lt;/li>
&lt;li>&lt;strong>Skip-Gram&lt;/strong>：跳字模型，输入是中心词，输出是周围词。&lt;/li>
&lt;/ul>
&lt;p>Word2Vec基于神经网络实现：&lt;/p>
&lt;ul>
&lt;li>但，无论是CBOW，还是Skip-Gram，都属于浅层神经网络。&lt;/li>
&lt;li>一说浅，显然就不那么高级。浅层神经网络的效果可以参考Word2Vec的原始论文：https://arxiv.org/pdf/1301.3781.pdf&lt;/li>
&lt;/ul>
&lt;h1 id="5代码实例手撸word2vec">5.代码实例：手撸Word2Vec&lt;/h1>
&lt;h2 id="step1构建语料库">STEP1.构建语料库&lt;/h2>
&lt;ul>
&lt;li>首先，以四个句子作为训练语料&lt;/li>
&lt;li>然后，形成&lt;strong>词+索引的Map&lt;/strong>和&lt;strong>索引+词的Map&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233124145.png" alt="image-20230817233124145">&lt;/p>
&lt;h2 id="step2生成skip-gram数据">STEP2.生成Skip-Gram数据&lt;/h2>
&lt;ul>
&lt;li>根据句子中的各个词，将相邻的词形成词对。&lt;/li>
&lt;li>如：(小美，是)、(是，小美)，(美女，小美)，(小美，美女)等&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233354716.png" alt="image-20230817233354716">&lt;/p>
&lt;h2 id="step3对skip-gram数据进行one-hot编码">STEP3.对Skip-Gram数据进行One-Hot编码&lt;/h2>
&lt;ul>
&lt;li>将STEP2的各个词对进行One-Hot编码&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233557328.png" alt="image-20230817233557328">&lt;/p>
&lt;h2 id="step4定义skip-gram模型">STEP4.定义Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>我们复现出论文描述的神经网络结构
&lt;ul>
&lt;li>一个线性层学习出词向量&lt;/li>
&lt;li>一个输出层验证学习到词向量作为中心词，是否能够预测出周围词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233757848.png" alt="image-20230817233757848">&lt;/p>
&lt;p>看一下具体的代码实现：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234116518.png" alt="image-20230817234116518">&lt;/p>
&lt;h2 id="step5训练skip-gram模型">STEP5.训练Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>训练3000次，每一次都会从输出层触发反向传播，更新线性层的参数，最终确定本次学习到的词向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234302231.png" alt="image-20230817234302231">&lt;/p>
&lt;ul>
&lt;li>这是损失函数曲线，经过3000次的训练，损失逐渐降低。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234532709.png" alt="image-20230817234532709">&lt;/p>
&lt;h2 id="step6显示词向量">STEP6.显示词向量&lt;/h2>
&lt;ul>
&lt;li>将词向量打印出来，并绘制在向量空间中&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234606438.png" alt="image-20230817234606438">&lt;/p>
&lt;ul>
&lt;li>绘制出来的向量空间，结果如下：
&lt;ul>
&lt;li>小美和大漂亮的词向量分别为： [-0.45549557 0.4471412 ]，[-0.47936678 0.40784693]&lt;/li>
&lt;li>小帅和大壮的词向量分别为：[0.0154954 1.6643344]，[0.01839364 1.6814741 ]&lt;/li>
&lt;li>小美和大漂亮两个词关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>小帅和大壮两个词也关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>但，美女、帅哥两个词向量学习的不好。小美、大漂亮更接近于帅哥，出现了&lt;strong>不一定斩男但是一定斩女&lt;/strong>的现象。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234631498.png" alt="image-20230817234631498">&lt;/p>
&lt;h1 id="6再看表示学习嵌入">6.再看表示学习、嵌入&lt;/h1>
&lt;p>笔者在第5章以Word2Vec的Skip-Gram的代码实现，给大家阐述了词嵌入的实现过程和训练过程。&lt;/p>
&lt;p>回头再来看1~4章出现的各种概念和理论，可以归纳词嵌入的几个关键知识点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自然语言问题转换为数学问题&lt;/strong>：将文字向量化的本质是将信息转换为向量，所以文字的语义关系巧妙地转换为了向量距离的数学问题。&lt;/li>
&lt;li>&lt;strong>自动化的学习过程&lt;/strong>：获得文字的向量是一个自动化的过程，通过神经网络这种工具自动学习得到词向量。&lt;/li>
&lt;li>&lt;strong>词向量的降维技术&lt;/strong>：对于词嵌入，学界有很多种降维技术，Word2Vec的CBOW和Skip-Gram都是实现了降维的具体算法。&lt;/li>
&lt;/ul>
&lt;p>再看第1章的表示学习，我们可以重新理解一下表示学习的三个特性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>可分性&lt;/strong>：不相关的向量距离会很远，不相关的向量之间应该会有明显的分界线。这就体现了可分性——不同类别之间的向量样本应该有明显的边界或区别。&lt;/li>
&lt;li>&lt;strong>可解释性&lt;/strong>：为什么提到唐伯虎，人类会立刻联想到秋香？向量空间中相关的向量距离就会很近，这就体现了可解释性。&lt;/li>
&lt;li>&lt;strong>可推理性&lt;/strong>：给出小美，我们可以联想到大漂亮、帅哥，进而可能就会联想到小帅、大壮，说不定小美和小帅之间就一段故事，这就体现了可推理性。&lt;/li>
&lt;/ul>
&lt;p>写到这里，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>以Word2Vec为代表的词向量化技术极大地推进了语言模型的进步&lt;/li>
&lt;li>词嵌入也成为了如今大语言模型GPT、LLama、Bert的核心组件之一。&lt;/li>
&lt;/ul>
&lt;p>在了解了词嵌入相关技术之后，我们下一步就来看大语言模型的另一个核心组件的起源：神经概率语言模型。&lt;/p></description></item></channel></rss>