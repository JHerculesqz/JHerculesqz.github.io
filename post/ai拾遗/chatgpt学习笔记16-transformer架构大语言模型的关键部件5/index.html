<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5 - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="在《AI拾遗》这个专栏中，我们建立了从N-Gram到词嵌入再到神经概率语言模型，从Seq2Seq到注意力机制的知识脉络。 这条脉络本质就是NL"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5"><meta property="og:description" content="在《AI拾遗》这个专栏中，我们建立了从N-Gram到词嵌入再到神经概率语言模型，从Seq2Seq到注意力机制的知识脉络。 这条脉络本质就是NL"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/"><meta property="article:published_time" content="2023-09-26T18:00:59+08:00"><meta property="article:modified_time" content="2023-09-26T18:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5"><meta itemprop=description content="在《AI拾遗》这个专栏中，我们建立了从N-Gram到词嵌入再到神经概率语言模型，从Seq2Seq到注意力机制的知识脉络。 这条脉络本质就是NL"><meta itemprop=datePublished content="2023-09-26T18:00:59+08:00"><meta itemprop=dateModified content="2023-09-26T18:00:59+08:00"><meta itemprop=wordCount content="3397"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5"><meta name=twitter:description content="在《AI拾遗》这个专栏中，我们建立了从N-Gram到词嵌入再到神经概率语言模型，从Seq2Seq到注意力机制的知识脉络。 这条脉络本质就是NL"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5</h1><div class=post-meta><time datetime=2023-09-26 class=post-time>2023-09-26</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 3397 字</span>
<span class=more-meta>预计阅读 7 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1问题>1.问题</a></li><li><a href=#2transformer逐步拆解>2.Transformer逐步拆解</a><ul><li><a href=#1整体transformer>(1)整体：Transformer</a></li><li><a href=#2局部1正弦位置编码表>(2)局部1：正弦位置编码表</a></li><li><a href=#3局部2编码器堆栈>(3)局部2：编码器堆栈</a></li><li><a href=#4局部3编码器>(4)局部3：编码器</a></li><li><a href=#5局部4多头注意力>(5)局部4：多头注意力</a></li><li><a href=#6局部5前向传播网络>(6)局部5：前向传播网络</a></li><li><a href=#7局部6填充位置掩码>(7)局部6：填充位置掩码</a></li><li><a href=#8局部7解码器堆栈>(8)局部7：解码器堆栈</a></li><li><a href=#9局部8解码器>(9)局部8：解码器</a></li><li><a href=#10局部9后续位置掩码>(10)局部9：后续位置掩码</a></li><li><a href=#11模型训练>(11)模型训练</a></li><li><a href=#12模型测试>(12)模型测试</a></li></ul></li><li><a href=#3小结>3.小结</a></li></ul></nav></div></div><div class=post-content><p>在《AI拾遗》这个专栏中，我们建立了从<strong>N-Gram</strong>到<strong>词嵌入</strong>再到<strong>神经概率语言模型</strong>，从<strong>Seq2Seq</strong>到<strong>注意力机制</strong>的知识脉络。</p><p>这条脉络本质就是NLP发展的路线图，有了这些知识储备，我们终于可以来理解论文**《Attention Is All You Need》**中大名鼎鼎的**Transformer架构**了！</p><h1 id=1问题>1.问题</h1><p>在《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》中，笔者为<strong>编码器-解码器架构</strong>增加了<strong>注意力机制</strong>，进而实现了<strong>增强版的Seq2Seq模型</strong>，模型能力的确有所增强，但并不是彻底解决了<strong>长距离依赖</strong>问题和<strong>信息压缩</strong>问题。</p><p>在《Attention Is All You Need》的第一章阐述了这个观点：</p><ul><li><strong>howerver, remains</strong>(第二段最后一句)：历史上很多论文和技术都在增强<strong>编码器-解码器架构</strong>，注意力机制也成为序列建模必备的技术，但<strong>长距离依赖</strong>问题依然存在。</li><li><strong>parallelization</strong>(第三段)：这里提到了并行化，这是前面没有提到的问题——RNN网络决定了Seq2Seq只能一个词一个词的处理。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926072417665.png alt=image-20230926072417665></p><h1 id=2transformer逐步拆解>2.Transformer逐步拆解</h1><p>这是论文中第三段绘制的Transformer整体架构，我们将对它进行拆解：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926074022967.png alt=image-20230926074022967></p><h2 id=1整体transformer>(1)整体：Transformer</h2><p>将上图抽象化，我们可以看到：</p><ul><li>Transformer依然可以遵循Encoder-Decoder架构</li><li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926082223230.png alt=image-20230926082223230></p><p>代码实现如下：</p><ul><li><strong>1</strong>：对应上图将<strong>Inputs</strong>输入给<strong>Encoder</strong>。</li><li><strong>2</strong>：对应上图将<strong>Outputs+Encoder的输出</strong>，传入给<strong>Decoder</strong>。</li><li><strong>3</strong>：对应上图将<strong>Decoder的输出</strong>，传入给线性层。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083838772.png alt=image-20230926083838772></p><p>PS：这里的<code>corpus</code>是一个封装了<code>Inputs</code>和<code>Outputs</code>的工具类，代码如下(比较简单，不赘述)：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083336984.png alt=image-20230926083336984></p><h2 id=2局部1正弦位置编码表>(2)局部1：正弦位置编码表</h2><p>首先，我们来细化<strong>Inputs</strong>和<strong>Encoder</strong>之间的流程：</p><ul><li><strong>STEP1</strong>.对<strong>Inputs</strong>实施词嵌入，得到词向量。</li><li><strong>STEP2</strong>.在<strong>词向量</strong>和<strong>Encoder</strong>之间增加了<strong>位置编码表</strong>(也是一个向量)，这个位置编码表体现了<strong>词和词序的关系</strong>。<ul><li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立<strong>词和词序的关系</strong>。</li></ul></li><li><strong>STEP3</strong>.将STEP2的<strong>位置表码表</strong>向量和<strong>词向量</strong>相加，输入给<strong>Encoder</strong>。</li></ul><p><strong>Outputs</strong>和<strong>Decoder</strong>之间的流程和上述流程一样。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926092023205.png alt=image-20230926092023205></p><p>那么位置编码表如何计算呢？论文3.5章节详细阐述如下：</p><ul><li><strong>d</strong>：词向量的维度。</li><li><strong>pos</strong>：单词在句子中的位置。</li><li><strong>i</strong>：词向量的维度的奇数维。</li><li><strong>PE</strong>：指定位置的单词，在词向量的某一个维度上的数值。</li><li>通俗地理解，<strong>d个PE值构成了指定单词在整个句子中的位置向量</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926093855305.png alt=image-20230926093855305></p><p>我们不必纠结于论文中这两个公式的证明，笔者绘制一个例子，可视化地理解正弦位置编码表的作用：</p><ul><li><strong>假设</strong>：输入序列为&rdquo;<strong>想去新疆</strong>&ldquo;四个字，词向量的维度为4维，即<strong>d=4</strong>。</li><li><strong>STEP1</strong>：通过正弦位置编码公式，<strong>想</strong>字的<strong>位置0</strong>，求得位置0的<strong>位置向量[0, 1, 0, 1]</strong>。</li><li><strong>STEP2</strong>：通过正弦位置编码公式，<strong>去</strong>字的<strong>位置1</strong>，求得位置1的<strong>位置向量[0.84, 0.54, 0.01, 1.0]</strong>。</li><li><strong>STEP3</strong>：通过正弦位置编码公式，<strong>新</strong>字的<strong>位置2</strong>，求得位置2的<strong>位置向量[0.91, -0.42, 0.02, 1.0]</strong>。</li><li><strong>STEP4</strong>：通过正弦位置编码公式，<strong>疆</strong>字的<strong>位置3</strong>，求得位置3的<strong>位置向量[0.14, -0.99, 0.03, 1.0]</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926111712233.png alt=image-20230926111712233></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926110113612.png alt=image-20230926110113612></p><h2 id=3局部2编码器堆栈>(3)局部2：编码器堆栈</h2><p>我们再来细化<strong>编码器</strong>：</p><ul><li>编码器本质上由<strong>N个编码器</strong>串联而成的<strong>编码器堆栈</strong>。</li><li>论文中，<strong>N=6</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926112341735.png alt=image-20230926112341735></p><ul><li><strong>论文原文</strong>：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926113124754.png alt=image-20230926113124754></p><h2 id=4局部3编码器>(4)局部3：编码器</h2><p>我们再进一步细化<strong>编码器Encoder</strong>：</p><ul><li><strong>编码器Encoder</strong>由<strong>多头注意力</strong>和<strong>前向传播网络</strong>组成。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926114259743.png alt=image-20230926114259743></p><p><strong>编码器堆栈</strong>的代码实现如下：</p><ul><li><strong>①</strong>：这就是调用<strong>正弦位置编码表</strong>，创建的<strong>位置编码层</strong>，再将<strong>词向量和位置编码向量相加</strong>。</li><li><strong>②</strong>：这就是将多个<strong>编码器</strong>叠加成<strong>编码器堆栈</strong>，每个<strong>编码器的输入</strong>是<strong>上个编码器的输出</strong>和<strong>上个编码器输出的注意力权重</strong>。</li><li><strong>③</strong>：这就是表示<strong>编码器堆栈</strong>输出的<strong>编码器输出</strong>、<strong>编码器输出的注意力权重</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926115605435.png alt=image-20230926115605435></p><p><strong>编码器</strong>的代码实现如下：</p><ul><li><strong>①</strong>：就是<strong>多头注意力层</strong>，<strong>第一个编码器</strong>的<strong>多头注意力层</strong>的输入是<strong>词嵌入+位置编码向量之和</strong>以及<strong>自注意力掩码</strong>，<strong>后续编码器</strong>的<strong>多头注意力层</strong>的输入是<strong>上一个编码器的输出</strong>和<strong>自注意力掩码</strong>。</li><li><strong>②</strong>：就是<strong>前向传播网络</strong>，它的输入是<strong>多头注意力层的输出</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926122832935.png alt=image-20230926122832935></p><p>这里又埋下了几个问题：</p><ul><li><strong>多头注意力层</strong>如何实现？</li><li><strong>前向传播网络</strong>如何实现？</li><li><strong>自注意力位置掩码</strong>如何实现？</li></ul><h2 id=5局部4多头注意力>(5)局部4：多头注意力</h2><p>我们再来细化多头注意力：</p><ul><li><strong>不赘述</strong>：关于<strong>点积注意力</strong>、<strong>缩放点积注意力</strong>、<strong>编码器-解码器注意力</strong>、<strong>QKV</strong>、<strong>自注意力</strong>、<strong>多头注意力</strong>，本文就不再赘述了。如果理解不太清晰，可以回看《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》。</li><li><strong>多头注意力</strong>的结构：<strong>多头注意力的输入</strong>是<strong>词向量与位置编码向量之和</strong>，每一个注意力头都是对<strong>多头注意力的输入</strong>进行矩阵乘法得到<strong>QKV</strong>，再输入给<strong>缩放点积注意力组件</strong>，这个组件输出的是<strong>注意力权重</strong>。最后，将每个注意力头输出的注意力权重求和，输入给一个线性层。</li><li><strong>缩放点积注意力</strong>的结构：就是典型的缩放点积注意力的计算公式，即：Q、K求点积=>缩放=>注意力掩码=>Softmax=>和V点积。</li><li><strong>细节</strong>：这里增加了Add & Norm，就是深度学习里面的残差连接、层归一化，为了解决梯度爆炸问题，这不是Transformer特有的新知识。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926162228532.png alt=image-20230926162228532></p><p>论文原文：</p><ul><li><strong>缩放点积注意力</strong>计算公式：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161049010.png alt=image-20230926161049010></p><ul><li><strong>多头注意力</strong>：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161136259.png alt=image-20230926161136259></p><p><strong>缩放点积注意力</strong>的代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161300468.png alt=image-20230926161300468></p><p><strong>多头注意力</strong>的代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161409926.png alt=image-20230926161409926></p><h2 id=6局部5前向传播网络>(6)局部5：前向传播网络</h2><p>我们再来细化前向神经网络：</p><ul><li>前向神经网络的全称是<strong>Position-wise Feed-Forward Network</strong>，即<strong>基于位置的前馈神经网络</strong>。</li><li><strong>two linear transformations with a ReLU activation</strong>：首先使用第一个线性层做升维，接着使用ReLU激活函数，再使用第二个线性层做降维。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163125569.png alt=image-20230926163125569></p><p>这个基于位置的前馈神经网络到底有啥用呢？</p><ul><li>就是论文中，多头注意力结构中的最后一步<strong>Linear</strong>！</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163933555.png alt=image-20230926163933555></p><p>论文原文：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163424383.png alt=image-20230926163424383></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926164434302.png alt=image-20230926164434302></p><h2 id=7局部6填充位置掩码>(7)局部6：填充位置掩码</h2><p>我们再来细化填充位置掩码：</p><ul><li><strong>填充位置掩码</strong>用在<strong>词嵌入</strong>之后，<strong>编码器</strong>输入之前。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png alt=image-20230926170827083></p><ul><li>填充位置掩码有什么作用呢？<ul><li><strong>t1时刻</strong>：给编码器输入了一句话——&ldquo;我想去新疆滑雪&rdquo;。</li><li><strong>t2时刻</strong>：给编码器输入第二句话——&ldquo;想去就去啊&rdquo;。为了和上一句话保持长度统一，我们就会在在第二句话末尾增加两个占位符。</li><li><strong>t3时刻</strong>：生成填充位置掩码[1, 1, 1, 1, 1, 0, 0]。</li><li><strong>t4时刻</strong>：编码器会将第二句话和填充位置掩码求与，这样编码器实施多头注意力的时候，就不会注意毫无意义的两个占位符。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171245346.png alt=image-20230926171245346></p><p>代码实现如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171345069.png alt=image-20230926171345069></p><p>至此，我们就把Transformer架构中编码器部分细化完成了，我们继续细化解码器部分：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png alt=image-20230926170827083></p><h2 id=8局部7解码器堆栈>(8)局部7：解码器堆栈</h2><p>解码器堆栈的思想到实现，和编码器堆栈完全一样，这里不再赘述，直接上图和代码：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174658980.png alt=image-20230926174658980></p><h2 id=9局部8解码器>(9)局部8：解码器</h2><p>解码器的思想到实现，和编码器堆栈大致一样，这里不再赘述，直接上图和代码：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174249742.png alt=image-20230926174249742></p><p><strong>解码器堆栈</strong>代码如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174857116.png alt=image-20230926174857116></p><p><strong>解码器</strong>代码如下：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174916494.png alt=image-20230926174916494></p><h2 id=10局部9后续位置掩码>(10)局部9：后续位置掩码</h2><p>在解码器中，还有最后一个遗留问题——后续位置掩码。</p><p>后续位置掩码只是因为解码器实施多头注意力的时候，是不能注意到<strong>未来</strong>的，也就是它还没有预测的后续词，所以要屏蔽掉。</p><p>后续位置掩码和填充位置掩码的思想是一致的，不赘述。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175344740.png alt=image-20230926175344740></p><p><strong>后续位置掩码</strong>的代码实现：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175131684.png alt=image-20230926175131684></p><h2 id=11模型训练>(11)模型训练</h2><p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：</p><ul><li>数据集如下：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180145967.png alt=image-20230926180145967></p><ul><li>模型训练：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180246638.png alt=image-20230926180246638></p><h2 id=12模型测试>(12)模型测试</h2><ul><li>测试用例采用贪婪编码，比较简单，具体代码如下：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180356687.png alt=image-20230926180356687></p><h1 id=3小结>3.小结</h1><p>笔者在今年2月份第一次阅读论文《Attention Is All You Need》，读了好几遍，不得要领，只觉得非常抽象。</p><p>随着在网上阅读各类资料、逐步摸索复现论文中Transformer架构的源码，逐渐理解这篇论文中所说的——<strong>Attention Is All You Need</strong>的含义。</p><p>本以为理解了论文含义，提笔准备写出这篇文章时，又卡了壳——因为理解了，又很难表达出来Transformer的精妙原理！</p><p>此时，笔者才真正领悟之前听过一位大神所说的：<font color=red>”LLM涉及的每一篇经典论文，不仅值得<strong>反复阅读</strong>，甚至应该<strong>背诵下来</strong>。“</font>的含义。</p><p>这篇文章写完，我们下一步就可以实现并训练我们平民版的ChatGPT了，且听下回分解了。</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-09-26</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/><span class="next-text nav-default">【chatGPT】学习笔记15-LangChain之Chain，对LLM的抽象3</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>