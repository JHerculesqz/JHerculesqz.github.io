<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Wed, 05 Jun 2024 10:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记49-AiDD 2024_AI人才培养分论坛参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Wed, 05 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>&lt;p>在AIGC时代，AI对高等教育及人才培养有怎样的变革？如何培养适应时代发展需求的计算机人才？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240606075433249.png" alt="image-20240606075433249">&lt;/p>
&lt;h1 id="1aigc时代下的人才需求趋势">1.AIGC时代下的人才需求趋势&lt;/h1>
&lt;h2 id="1时代背景">(1)时代背景&lt;/h2>
&lt;p>生成式人工智能(AIGC)的快速发展已悄然推动&amp;quot;第四次工业革命&amp;rdquo;，AI能力从感知理解世界跃迁到生成创造世界，工业应用的焦点将快速从自动化全面转变为智能化。Gartner预测，到2026年将有超过80%的企业将使用生成式人工智能。&lt;/p>
&lt;p>与历次工业革命一样，智能化不仅仅是技术和经济层面的变革，更是一场认知和思维的革命。作为塑造未来、培养创新人才的教育行业，必然要拥抱AIGC引发教育和教学变革。我国教育部也一直在大力推动“工业化教育模式”向“智能化教育模式”转变。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240603090307442-17176300245711.png" alt="image-20240603090307442">&lt;/p>
&lt;h2 id="2社会对人才需求及人力结构的转变">(2)社会对人才需求及人力结构的转变&lt;/h2>
&lt;p>在AIGC时代，社会对人才的需求和人力结构会发展巨大转变。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>人力结构转变&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>初级专业技术岗位将大幅减少，高级技术和创新型岗位将激增。&lt;/strong>
&lt;ul>
&lt;li>麦肯锡《生成式人工智能和美国工作的未来》报告提出2030年美国&lt;strong>工资最低的岗位将减少110万个&lt;/strong>，但&lt;strong>工资最高的岗位可能增加 380万个&lt;/strong>。&lt;/li>
&lt;li>世界经济论坛发布《未来就业报告2023》显示，未来5年内，&lt;strong>人工智能、商业智能分析师、数据科学&lt;/strong>等大数据相关职位的需求增长最快。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>人力需求转变&lt;/strong>：
&lt;ul>
&lt;li>**培养创新性人才：**创造性、分析性思维，技术素养、好奇心与学习能力、韧性、灵活性与敏捷性等通识素养是未来最需要培养的技能。&lt;/li>
&lt;li>**培养跨专业人才：**学科壁垒不断消融，数字技术、人工智能等科创技术能力与人文素养、通识教育并重，跨专业人才需求紧俏。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这意味着，AIGC时代人力需求不减反增，但需要的是具有创造力和深度思考能力，并且具备人工智能技术的人。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240606081142441.png" alt="image-20240606081142441">&lt;/p>
&lt;h2 id="3社会对计算机人才需求及人力结构的转变">(3)社会对计算机人才需求及人力结构的转变&lt;/h2>
&lt;p>计算机学科的人才们创造了AIGC，那AIGC对未来计算机专业人才的需求有哪些转变呢？&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>人力结构转变：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>那些会消失的岗位：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础业务代码编写与维护&lt;/strong>：会逐步消失，简单的编码任务、有标准代码模板的任务、修复常见的编程错误、基础业务逻辑代码，都可能被AI取代。&lt;/li>
&lt;li>&lt;strong>初级测试&lt;/strong>：AI会自动生成常规测试用例，保证基本测试质量。&lt;/li>
&lt;li>&lt;strong>简单的数据分析&lt;/strong>：也会逐步消失，AI已经可以对于数据集进行初步分析了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>那些会新增的岗位&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型训练&amp;amp;AI应用开发&lt;/strong>：设计、训练、微调、部署AI模型，在垂直领域开发AI应用，将成为新兴的岗位。&lt;/li>
&lt;li>&lt;strong>人机交互设计师&lt;/strong>：为了提高AI应用的可用性和用户体验，专注于人机交互的角色会非常重要。&lt;/li>
&lt;li>&lt;strong>AI解决方案工程师&lt;/strong>：对特定行业、垂直领域设计定制化解决方案。&lt;/li>
&lt;li>&lt;strong>AI伦理和审计&lt;/strong>：处理伦理、合规、隐私、AI决策透明度，开发和监督AI应用的使用准则。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>人力需求转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>与AIGC相关的工作岗位将占到甚至超过总体人力需求的50%&lt;/li>
&lt;li>AIGC将促成大量非计算机专业交叉学科的计算机人才，也使&amp;quot;超级个体&amp;quot;的产生变得更容易。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180823443.png" alt="image-20240605180823443">&lt;/p>
&lt;h1 id="2计算机人才培养目标变革">2.计算机人才培养目标变革&lt;/h1>
&lt;h2 id="1教育理念变革">(1)教育理念变革&lt;/h2>
&lt;p>以&lt;strong>布鲁姆学习目标分类体系、斯金纳程序教学模式&lt;/strong>为代表的传统教育体系，在第一次工业革命建立起来后，其教育理念表现为：&lt;/p>
&lt;ul>
&lt;li>强调&lt;strong>结构化和标准化教学&lt;/strong>&lt;/li>
&lt;li>一定程度地&lt;strong>限制了创新能力和批判性思维&lt;/strong>的培养。&lt;/li>
&lt;/ul>
&lt;p>而在AIGC时代背景下，亟需在新的教育理念指引下，尽快转变为&amp;quot;融合AI的超级个体&amp;quot;教学模式。教育理念有如下变革：&lt;/p>
&lt;ul>
&lt;li>**知识观：**智能时代知识更新速度快、知识获取难度低，&lt;strong>知识不是教学的核心&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>教学观：&lt;/strong>
&lt;ul>
&lt;li>教学目标改变：&lt;strong>培养综合能力和高阶思维&lt;/strong>。&lt;/li>
&lt;li>教学内容改变：&lt;strong>构建软知识的思路和框架&lt;/strong>。&lt;/li>
&lt;li>教学方法改变：&lt;strong>发现式教学和分享式教学&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>学习观：&lt;/strong>
&lt;ul>
&lt;li>学习目标改变：&lt;strong>综合能力提升和批判性思维&lt;/strong>。&lt;/li>
&lt;li>学习内容改变：&lt;strong>以问题求解为中心&lt;/strong>。&lt;/li>
&lt;li>学习方式改变：&lt;strong>&amp;ldquo;人-机&amp;quot;合作式学习&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>人才观：&lt;strong>智能时代人类主要承担创造性工作，要&lt;/strong>培养创新型人才和跨领域人才，培养善于与AI共同工作的人才&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605193942764.png" alt="image-20240605193942764">&lt;/p>
&lt;h2 id="2培养目标变革">(2)培养目标变革&lt;/h2>
&lt;p>AIGC时代需要具备AI技能且善于思考创新的人才，结合学科知识体系，可分解为如下六项培养目标：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>四项能力：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础和全流程能力&lt;/strong>：除了编程能力，还强调基础算法和数据结构、机器学习、数据科学的理解与应用。&lt;/li>
&lt;li>&lt;strong>终身学习和适应能力&lt;/strong>：学习新知识、掌握新技术、适应新环境的能力。&lt;/li>
&lt;li>&lt;strong>解决跨学科问题的能力&lt;/strong>：探索计算机科学、数学、统计学、心理学和应用领域的知识，提升跨学科能力，可从多个角度思考和解决问题。&lt;/li>
&lt;li>&lt;strong>协作沟通等软技能&lt;/strong>：能够和团队成员、机器智能，协同合作，有效传达和交流想法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>两个思维：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>创造力和创新思维&lt;/strong>：解决问题、提出创新解决方案、设计新系统的能力。&lt;/li>
&lt;li>&lt;strong>伦理和社会责任意识&lt;/strong>：对AI应用的伦理问题理解、对隐私的关注、对AI的社会影响的认识。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3人才培养途径变革">(3)人才培养途径变革&lt;/h2>
&lt;p>为达成以上培养目标，需要拓宽当前计算机人才培养的途径，建议采取如下措施：&lt;/p>
&lt;ul>
&lt;li>将&lt;strong>计算机科学转为必修课&lt;/strong>，而且是基础课。&lt;/li>
&lt;li>通过&lt;strong>学科交叉学习和实践&lt;/strong>，让计算机专业和非计算机专业的学生都可以具备解决跨学科问题的能力，匹配未来人才结构需求。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605204013658.png" alt="image-20240605204013658">&lt;/p>
&lt;ul>
&lt;li>AI辅助编程 + AI辅助专业知识学习，培养&lt;strong>兼具AI技能和专业能力&lt;/strong>的人才。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605204409806.png" alt="image-20240605204409806">&lt;/p>
&lt;h1 id="3计算机人才培养实施路径">3.计算机人才培养实施路径&lt;/h1>
&lt;h2 id="31ai辅助教学">3.1.AI辅助教学&lt;/h2>
&lt;p>随着2023年生成式AI的发展，以及各大厂商在AI辅助研发领域的投入，AI辅助研发、AI for SE有了飞速发展：&lt;/p>
&lt;ul>
&lt;li>通过AI，代码生成、错误检测、自动重构优化、自动化测试，都成为了可能。&lt;/li>
&lt;/ul>
&lt;p>在AI辅助研发取得长足进步的背景下，AI辅助教学也成为了可能：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>内容创作&lt;/strong>：AI自动生成教学案例、总结教学内容、生成教学视频。&lt;/li>
&lt;li>&lt;strong>个性化教学&lt;/strong>：根据不同学生特点，推荐个性化教学内容和习题。&lt;/li>
&lt;li>&lt;strong>编程开发&lt;/strong>：自动生成代码和测试用例，帮助学生关注高层设计、避免陷入低层次细节。&lt;/li>
&lt;li>&lt;strong>互动体验&lt;/strong>：数字导师和AI助教，应用于教学与答疑。&lt;/li>
&lt;li>&lt;strong>游戏化教学&lt;/strong>：生成游戏环境、角色和情节，提升教学趣味性。&lt;/li>
&lt;li>&lt;strong>智能评价&lt;/strong>：用AI对学生作业和项目进行综合评价，总结优缺点及改进建议。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605172637375.png" alt="image-20240605172637375">&lt;/p>
&lt;p>在这样的背景下，教学模式&lt;strong>从循证教学法向AI辅助教学法转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>循证教学法&lt;/strong>的&lt;strong>本质&lt;/strong>：传统的教学模式，先教语言语法，再进行大量的代码片段练习。&lt;/li>
&lt;li>&lt;strong>AI辅助教学法&lt;/strong>的&lt;strong>本质&lt;/strong>：是利用大模型来处理低层次任务(将语法的细节延后)，让学生更聚焦于高层次的思维与算法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605172715134.png" alt="image-20240605172715134">&lt;/p>
&lt;h2 id="32实践1新质教育系统">3.2.实践1：新质教育系统&lt;/h2>
&lt;h3 id="1ai生成数字化学习资源">(1)AI生成数字化学习资源&lt;/h3>
&lt;ul>
&lt;li>通过AI，生成MOOC、实战实训、数字教材、图文专栏、导学项目等线上资源。&lt;/li>
&lt;li>通过AI，生成课堂教学、PPT课件、印刷教材等线下资源。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605174417159.png" alt="image-20240605174417159">&lt;/p>
&lt;h3 id="2ai生成个性化教学案例">(2)AI生成个性化教学案例&lt;/h3>
&lt;ul>
&lt;li>通过AI，生成个性化教学案例。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605175600964.png" alt="image-20240605175600964">&lt;/p>
&lt;h3 id="3ai生成作业和项目设计">(3)AI生成作业和项目设计&lt;/h3>
&lt;ul>
&lt;li>通过AI，以及提示词技术，生成课程作业、项目设计稿。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605175528506.png" alt="image-20240605175528506">&lt;/p>
&lt;h3 id="4ai数字教师数字助教">(4)AI数字教师、数字助教&lt;/h3>
&lt;ul>
&lt;li>基于AI数字人，实现数字导师、数字助教。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180234845.png" alt="image-20240605180234845">&lt;/p>
&lt;h3 id="5智能评价">(5)智能评价&lt;/h3>
&lt;ul>
&lt;li>利用AI，从不同维度对学生的输出进行评价。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180723165.png" alt="image-20240605180723165">&lt;/p>
&lt;h2 id="33实践2生成式探究教学">3.3.实践2：生成式探究教学&lt;/h2>
&lt;h3 id="1生成式探究教学的要素">(1)生成式探究教学的要素&lt;/h3>
&lt;p>生成式探究教学倡导：学生&lt;strong>主动探索&lt;/strong>，强调通过&lt;strong>创造性生成和构建&lt;/strong>深入探索和理解知识，鼓励学生&lt;strong>自发提出问题&lt;/strong>、独立思考并&lt;strong>创造性地解决问题&lt;/strong>。&lt;/p>
&lt;p>生成式探究教学的要素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学生主导&lt;/strong>：学生在学习过程中处于主导地位，学生提出问题、制定计划、选择学习资源、协作探索。&lt;/li>
&lt;li>&lt;strong>创造性生成&lt;/strong>：学生被鼓励创造性生成，如：设计产品、编写代码、创作艺术作品等。&lt;/li>
&lt;li>&lt;strong>深入探索&lt;/strong>：学生被孤立批判性思维、思考知识的本质、寻找创新解决问题的方法。&lt;/li>
&lt;li>&lt;strong>反思和分享&lt;/strong>：学生在实践过程中进行反思，总结经验教训，与他人分享见解和成果。&lt;/li>
&lt;li>&lt;strong>教练而不是教师&lt;/strong>：老师的角色是支持者，提供必要的指导、资源、反馈，已促进学生的学习和发展。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605221417594.png" alt="image-20240605221417594">&lt;/p>
&lt;h3 id="2生成式探究教学的实施过程">(2)生成式探究教学的实施过程&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>提出教学目标&lt;/strong>：教学目标从知识向批判思维迁移。&lt;/li>
&lt;li>&lt;strong>采用开放性教学资源&lt;/strong>：而不仅仅是教科书。&lt;/li>
&lt;li>&lt;strong>提出开放性问题&lt;/strong>：而不是有标准答案的问题。&lt;/li>
&lt;li>&lt;strong>鼓励探索&lt;/strong>：鼓励学生主动探索，老师与学生一起探索。&lt;/li>
&lt;li>&lt;strong>分享成果&lt;/strong>：让学生分享自己的探索成果。&lt;/li>
&lt;li>&lt;strong>评价&lt;/strong>：采用多元性、过程性、差异性评价方式。&lt;/li>
&lt;li>&lt;strong>教练式辅导&lt;/strong>：过程中进行必要的指导和讲解。&lt;/li>
&lt;/ul>
&lt;h3 id="3生成式探究教学案例">(3)生成式探究教学案例&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>项目介绍和目标设定&lt;/strong>——&lt;strong>提出教学目标&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>2048小游戏的基本规则和玩法。&lt;/li>
&lt;li>项目目标and预期结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>学生探究&lt;/strong>——&lt;strong>鼓励探索、提出开放性问题&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>游戏逻辑和规则如何实现？&lt;/li>
&lt;li>游戏界面如何设计？&lt;/li>
&lt;li>如何处理用户输入和游戏状态的更新？&lt;/li>
&lt;li>如何判断游戏胜利或失败的调价？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>指导和讲解&lt;/strong>——&lt;strong>教练式辅导&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>讲解基础知识和技能&lt;/li>
&lt;li>通过AI，学习各模块的需求。&lt;/li>
&lt;li>如何实现游戏的基本功能，如：移动方块、合并方块、更新游戏状态等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>编码实践&lt;/strong>——&lt;strong>鼓励探索&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>根据自己的理解和AI交互编写游戏代码&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>调试和测试&lt;/strong>——&lt;strong>鼓励探索&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>根据游戏的功能和逻辑，与AI交互进行调试和测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>展示和反思&lt;/strong>——&lt;strong>分享成果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>展示2048小游戏，分享学习心得和经验。&lt;/li>
&lt;li>思考在开发过程中遇到的挑战和解决方案是什么？&lt;/li>
&lt;li>思考游戏的改进和扩展方向是什么？&lt;/li>
&lt;li>如何将本案例的编程知识、业务技能，举一反一地应用到其它项目中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="34ai辅助教学的优势与挑战">3.4.AI辅助教学的优势与挑战&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>优势&lt;/strong>：降低了老师的教学成本、降低了学生的学习成本。&lt;/li>
&lt;li>&lt;strong>挑战&lt;/strong>：
&lt;ul>
&lt;li>传统OJ测评方法失效。&lt;/li>
&lt;li>过程性评估。&lt;/li>
&lt;li>AI算法的局限性。&lt;/li>
&lt;li>评价标准缺失。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4总结">4.总结&lt;/h1>
&lt;p>智能化时代对高等教育提出了新的要求，也为教育的创新和发展提供了广阔的空间。&lt;/p>
&lt;p>AIGC时代已经到来，教育者和学习者都需要懂AI、用AI以适应和拥抱变化。&lt;/p>
&lt;p>通过智能化技术的赋能，我们可以期待一个更加开放、灵活和高效的教育未来。&lt;/p></description></item><item><title>【chatGPT】学习笔记48-AiDD 2024参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Fri, 24 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>&lt;p>本文记录笔者参加AiDD 2024(AI+研发数字峰会)相关的议题，方便各位小伙伴快速了解最新AI理论研究和行业应用情况。&lt;/p>
&lt;h1 id="1aidd-2024概览">1.AiDD 2024概览&lt;/h1>
&lt;h2 id="1会议简介">(1)会议简介&lt;/h2>
&lt;ul>
&lt;li>AI+研发数字峰会(AiDD)专注“&lt;strong>AI技术和软件研发融合&lt;/strong>” ，AiDD 2024在上海举办，旨在&amp;rdquo;&amp;quot;&lt;strong>帮助企业借助AI技术，推动研发全面进入数智化时代&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>大会特邀&lt;strong>100+业界大咖&lt;/strong>分享行业洞察及专家见解，通过&lt;strong>60+创新案例&lt;/strong>展示全新研发思路，设置&lt;strong>15+个分论坛&lt;/strong>涵盖了软件研发全流程。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524133916685.png" alt="image-20240524133916685">&lt;/p>
&lt;h2 id="2议题分布">(2)议题分布&lt;/h2>
&lt;p>AiDD设置了15个分论坛，覆盖AI+研发全流程。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编程Copilot&lt;/strong>&lt;/li>
&lt;li>&lt;strong>算法与模型训练&lt;/strong>&lt;/li>
&lt;li>&lt;strong>知识工程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI工具链与工程平台&lt;/strong>&lt;/li>
&lt;li>&lt;strong>智能运维&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI算力与优化&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI原生应用开发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>智能需求工程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI驱动产品创新&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI人才培养&lt;/strong>&lt;/li>
&lt;li>&lt;strong>数据智能&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI赋能测试&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI智能体&lt;/strong>&lt;/li>
&lt;li>&lt;strong>领域大模型&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI对齐&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>接下来，我们从&lt;strong>3个方向&lt;/strong>总结相关会议议题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础模型及Agent&lt;/strong>&lt;/li>
&lt;li>&lt;strong>基础设施&lt;/strong>&lt;/li>
&lt;li>&lt;strong>LLM应用落地&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h1 id="2方向1基础模型及agent">2.方向1：基础模型及Agent&lt;/h1>
&lt;p>通过本次会议可以观察到2点趋势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>趋势1&lt;/strong>：2023年的研究热点还是&lt;strong>LLM基础理论&lt;/strong>，2024年的热点逐步转向到&lt;strong>VLM及多模态模型&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>趋势2&lt;/strong>：LLM基础模型的成熟与落地，促成了&lt;strong>Agent技术&lt;/strong>成为2024年的行业热点。&lt;/li>
&lt;/ul>
&lt;h2 id="议题1多模态大语言模型中的上下文学习">议题1：多模态大语言模型中的上下文学习&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>： 杨旭，南洋理工博士，东南大学副教授。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>价值&lt;/strong>：通过ICL(上下文学习)，提升多模态大模型的字幕生成能力、视觉问答能力(VQA)、解决视觉语言任务。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>探索不同的上下文配置对图像字幕生成任务中的视觉-语言(VL)模型的影响，提出了四种图像选择策略和四种字幕分配策略。&lt;/li>
&lt;li>探索在视觉问答(VQA)任务中，如何配置有效的上下文序列以增强大型视觉语言模型(LVLMs)的上下文学习(ICL)性能。为了提高ICL的性能，作者设计了多种检索方法，并采用不同的策略来操作检索到的示例。&lt;/li>
&lt;li>通过在视觉问答(VQA)和图像字幕(IC)任务中的实验验证了使用语言模型配置ICD的方法的可行性，并通过全面的消融研究进一步探讨了数据集构建和ICD-LM开发设置对结果的影响。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524163938055.png" alt="image-20240524163938055">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164044361.png" alt="image-20240524164044361">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164420747.png" alt="image-20240524164420747">&lt;/p>
&lt;h2 id="议题2基于多模态大语言模型的gui智能体">议题2：基于多模态大语言模型的GUI智能体&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：张驰，腾讯研究科学家，入选斯坦福大学发布的《2023全球前2%顶尖科学家榜单》&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>产品&lt;/strong>：AppAgent是由腾讯公司开发的一种高级多模态代理框架，主要用于智能手机应用程序的操作和管理。它基于大型语言模型（LLM），能够通过直观的点击、滑动等手势与应用程序进行交互，模仿类似人类的动作。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>多模态交互&lt;/strong>：AppAgent结合了视觉和文本输入，使得代理能够理解和执行基于视觉信息的任务，这在以往的文本中心的Agent模型中是不可能的。&lt;/li>
&lt;li>&lt;strong>无需系统后端访问&lt;/strong>：与传统的智能助手(如Siri)不同，AppAgent通过模拟用户在图形用户界面（GUI）上的操作，如点击和滑动，而不是依赖于系统后端访问，从而实现更高的灵活性和安全性。&lt;/li>
&lt;li>&lt;strong>自主学习能力&lt;/strong>：AppAgent通过预定义操作与手机app交互来学习，也可以通过观察人类演示来学习。这些观察被记录成文档，供后续使用&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524165245515.png" alt="image-20240524165245515">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164953565.png" alt="image-20240524164953565">&lt;/p>
&lt;h2 id="议题3多场景下智能体应用构建技巧">议题3：多场景下智能体应用构建技巧&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：卢建晖，微软高级云技术布道师&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>产品&lt;/strong>：微软的Semantic Kernel构建智能体是一个轻量级的软件开发工具包，旨在帮助开发者更有效地将大型语言模型集成到他们的应用程序中。&lt;/li>
&lt;li>&lt;strong>价值&lt;/strong>：Semantic Kernel的价值在于它为开发者提供了一个高效的方式来利用大型语言模型解决复杂的问题，通过定义插件并自动协调这些插件与AI，开发者可以快速实现特定的功能或解决特定的问题。还允许开发者在应用程序中充分利用与Copilot和Bing相同的人工智能协调模式，从而增强应用程序的功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170304878.png" alt="image-20240524170304878">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170402852.png" alt="image-20240524170402852">&lt;/p>
&lt;h2 id="议题4aiagent认知框架与案例实践">议题4：AIAgent认知框架与案例实践&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：黄佳，新加坡科技研究局AI研究员、技术作家。主攻方向LLM的开发和应用、AI for FinTech、AI for MedTech、持续学习等。著有多部AI相关畅销书。&lt;/li>
&lt;/ul>
&lt;p>个人感觉黄佳老师的议题比较出彩的一个，他不愧为技术作家，议题讲解也带有很强的技术科普风格，具体内容如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI应用的五个层次&lt;/strong>：分L1~L5，目前很多AI助手停留在L3，但业界都在向L4，即Agent方向努力。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170728959.png" alt="image-20240524170728959">&lt;/li>
&lt;li>&lt;strong>Agent方法论&lt;/strong>：黄老师的这个总结非常精彩。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171055835.png" alt="image-20240524171055835">&lt;/li>
&lt;li>&lt;strong>Agent认知框架的四种设计模式&lt;/strong>：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171224406.png" alt="image-20240524171224406">&lt;/li>
&lt;li>&lt;strong>Agent认知框架选型方法&lt;/strong>：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171326589.png" alt="image-20240524171326589">&lt;/li>
&lt;li>&lt;strong>Agent研究综述&lt;/strong>：黄佳老师介绍了Agent研究综述，非常值得学习的一篇综述性论文。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171839884.png" alt="image-20240524171839884">&lt;/li>
&lt;/ul>
&lt;h2 id="议题5个性化智能体价值观与社交能力的评估与对齐">议题5：个性化智能体价值观与社交能力的评估与对齐&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：高星，通义星尘算法负责人。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>通过基于专家指导原则的自我对齐，有效提升了人类价值观对齐的效果。&lt;/li>
&lt;li>通过多阶段迭代训练、CycleAlign方法、大小模型协同等，提升个性化角色的模型能力，打造类人智能体。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524172521205.png" alt="image-20240524172521205">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524172501644.png" alt="image-20240524172501644">&lt;/p>
&lt;h1 id="3方向2基础设施">3.方向2：基础设施&lt;/h1>
&lt;h2 id="议题1构建agi时代的推理基础设施">议题1：构建AGI时代的推理基础设施&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>： 单一舟，南洋理工博士，东南大学副教授。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍华为云构建满足AGI需求的推理基础设施的解决方案。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>以存代算，降低首字时延&lt;/li>
&lt;li>分离式内存弹性伸缩，降低推理集群成本&lt;/li>
&lt;li>资源感知调度，提升推理集群利用率&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142746428.png" alt="image-20240525142746428">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142816516.png" alt="image-20240525142816516">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142834332.png" alt="image-20240525142834332">&lt;/p>
&lt;h2 id="议题2向量数据库大模型时代的基础设施构建">议题2：向量数据库大模型时代的基础设施构建&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：刘力，Zilliz首席工程师&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了Zilliz Cloud产品，以及其关键特性，如：向量数据库即服务、Saas架构、Logic Clusters、分层存储、冷热数据分离、Zilliz Cloud Pipeline、Cardinal极致性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525143445461.png" alt="image-20240525143445461">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525145559234.png" alt="image-20240525145559234">&lt;/p>
&lt;h2 id="议题3构建云原生算力基础设施驱动大模型创新实践">议题3：构建云原生算力基础设施驱动大模型创新实践&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：王羽中，杭州谐云科技有限公司技术总监&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了支撑大模型的云原生算力基础设施解决方案，阐述了多项关键技术，如：跨算力中心的纳管和调度、异构资源纳管和调度、算力超分和优先级调度、算力资源共享和隔离、算力资源动态共享、多卡共享、精细化计费等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525150032505.png" alt="image-20240525150032505">&lt;/p>
&lt;h2 id="议题4ai原生应用开发工具链详解">议题4：AI原生应用开发工具链详解&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：罗义云，阿里云资深技术专家、PAI平台工程负责人&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍阿里云PAI的整体架构，阐述了模型微调工具链的模型微调、模型评测Eval-Scope、实验管理、量化压缩、BladeLLM等特性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525150748022.png" alt="image-20240525150748022">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525151401633.png" alt="image-20240525151401633">&lt;/p>
&lt;h1 id="4方向3llm应用落地">4.方向3：LLM应用落地&lt;/h1>
&lt;h2 id="议题1大模型加持如何改变需求工程任务">议题1：大模型加持如何改变需求工程任务&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：金芝，北京大学教授，IEEE/CCF/AAIA Fellow，高可信软件技术教育部重点实验室常务副主任。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：介绍大模型对需求工程的改变
&lt;ul>
&lt;li>&lt;strong>需求相关的提示模式&lt;/strong>：《Requirements Engineering using Generative AI：Prompts and Prompting Patterns》阐述了5种提示模式。&lt;/li>
&lt;li>&lt;strong>人机协作架构设计&lt;/strong>：《Towards Human-Bot Collaborative Software Architecting with ChatGPT》展示了人类和ChatGPT协同开展架构设计的实践。&lt;/li>
&lt;li>&lt;strong>AI Agent协同目标建模&lt;/strong>：《MAPE-K Loop-based Goal Model Generation Using Generative AI》论述了一种由AI Agent扮演需求分析师、领域业务专家的角色，共同开展设计建模的方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153313639.png" alt="image-20240525153313639">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153706831.png" alt="image-20240525153706831">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153909763.png" alt="image-20240525153909763">&lt;/p>
&lt;h2 id="议题2智能化研发在百度的落地">议题2：智能化研发在百度的落地&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：张立理，百度前端架构师，百度前端CMC主席，技术组织委员会 Web方向负责人。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>如何构建强力的代码模型&lt;/strong>：构造高质量代码数据集、多种代码生成模式、降低模型对算力的依赖提升计算速度。&lt;/li>
&lt;li>&lt;strong>如何有效服务产品用户&lt;/strong>：多语言多IDE覆盖、提供丰富的编码器能力、基于研发现场知识增强、构建与实践紧密结合的模型训练飞轮。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154247326.png" alt="image-20240525154247326">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154737151.png" alt="image-20240525154737151">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154838530.png" alt="image-20240525154838530">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154855168.png" alt="image-20240525154855168">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154920249.png" alt="image-20240525154920249">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154950695.png" alt="image-20240525154950695">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525155007958.png" alt="image-20240525155007958">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525155020779.png" alt="image-20240525155020779">&lt;/p>
&lt;h2 id="议题3阿里云服务器智能异常调度系统及llmops构建与实践">议题3：阿里云服务器智能异常调度系统及LLMOPS构建与实践&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：郭红科，阿里云高级开发工程师&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了阿里云服务器的AI for Ops解决方案，分享了query重写/意图识别、AI Agent等关键技术实践的经验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160135574.png" alt="image-20240525160135574">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160335459.png" alt="image-20240525160335459">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160407489.png" alt="image-20240525160407489">&lt;/p>
&lt;h2 id="议题4aiops在线评测基准系统">议题4：AIOps在线评测基准系统&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：聂晓辉，必示科技产品部总监、算法研究员，清华大学计算机系博士, 研究领域为智能运维 (AIOps)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>介绍了必示科技的AIOps平台，展示了AI在IT运维领域的一些能力，如：异常检测、告警分析、故障处理、成本效率、混沌工程、可观测等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160625980.png" alt="image-20240525160625980">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>AiDD 2024会议的收获如下：&lt;/p>
&lt;ul>
&lt;li>基础模型的研究热点是多模态大模型、AI Agent，需要进一步跟踪探索。&lt;/li>
&lt;li>各大厂商已经在布局基础设施建设，AI算力基础设施是近几年重要的营收点。&lt;/li>
&lt;li>在AI for SE方面，各厂商的AI辅助研发逐渐在落地，同时也在启动AIOps的探索。&lt;/li>
&lt;/ul></description></item><item><title>【读书】李娟-我的阿勒泰11</title><link>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E8%AF%BB%E4%B9%A6%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/</link><pubDate>Mon, 20 May 2024 00:00:30 +0800</pubDate><guid>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E8%AF%BB%E4%B9%A6%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/</guid><description>&lt;p>迷茫有一阵子了，感觉应该慢下来，那就读书吧。&lt;/p>
&lt;p>耐不下性子，就用AI来帮我读吧。&lt;/p>
&lt;h1 id="我的阿勒泰想表达什么">《我的阿勒泰》想表达什么？&lt;/h1>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520000811635.png" alt="image-20240520000811635">&lt;/p>
&lt;h1 id="这本书的目录">这本书的目录？&lt;/h1>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520000905619.png" alt="image-20240520000905619">&lt;/p>
&lt;h1 id="我所能带给你们的事物">我所能带给你们的事物&lt;/h1>
&lt;p>写这篇文章的时候，我的手机还在播放着马斯克的星舰IFT-4飞行测试。&lt;/p>
&lt;p>一半原始、一半现代。让我去阿勒泰旅行可以，但很难想象生活在阿勒泰。&lt;/p>
&lt;p>在那种原始的状态，为什么人类可以保持希望和期待？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520001118886.png" alt="image-20240520001118886">&lt;/p>
&lt;h1 id="属于我的马">属于我的马&lt;/h1>
&lt;p>原来马是自由的隐喻。经历太多，我能理解生命的脆弱，一直想知道生命的意义和死亡的意义。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520001249488.png" alt="image-20240520001249488">&lt;/p>
&lt;h1 id="小鸟牌香烟">小鸟牌香烟&lt;/h1>
&lt;p>香烟隐喻了时间的流逝？挺新奇的说法。很佩服你们这些感性的人——任何一个小小的器物就能帮你赋予生活的意义。&lt;/p>
&lt;p>但我习惯了量化和计划，我很迷惑，目前一知半解地认为康德想告诉我——生命的意义就是生命的意义、良善的实践就是人类自由意志的实现。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520002142006.png" alt="image-20240520002142006">&lt;/p>
&lt;h1 id="打电话">打电话&lt;/h1>
&lt;p>对文中小伙子和恋人打电话那个扭捏片段，i人不仅是对恋人，即使是面对稍有好感的人，也会语无伦次。&lt;/p>
&lt;p>i人在酒吧都是自己玩，从不和人搭讪。有一次和朋友打赌去要一位陌生女生的联系方式，鼓了半天气请那女生喝了杯酒(还是委托调酒师转告)，结果还是不敢过去。最后，那位女生临走之前，很cute地过来说谢谢你啊，我应该是全身僵住、支支吾吾。。。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520002635384.png" alt="image-20240520002635384">&lt;/p>
&lt;p>睡了，明天再继续。&lt;/p></description></item><item><title>【chatGPT】学习笔记47-LLM微调技术之P-Tuning V2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</link><pubDate>Sat, 11 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</guid><description>&lt;p>上篇专栏我们讲到，&lt;code>P-Tuning V1&lt;/code>通过在预训练模型的输入层加入可训练的连续提示，有效提升了训练效果。但其在复杂NLU任务和小参数模型上表现并不理想。&lt;/p>
&lt;p>&lt;code>P-Tuning V2&lt;/code>是对&lt;code>P-Tuning V1&lt;/code>的改进，使其能在不同规模的模型和各种NLU任务中都能与全量微调相媲美。&lt;/p>
&lt;p>本文解读论文**《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》**，探究&lt;code>Prompt Tuning V2&lt;/code>技术的原理。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112022603.png" alt="image-20240511112022603">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>的最大问题是不具备普适性(&lt;strong>a lack of universality&lt;/strong>)。
&lt;ul>
&lt;li>&lt;strong>不同规模模型的微调效果不稳定&lt;/strong>：Lack of universality across scales。模型规模超过10B时，&lt;strong>P-Tuning v1&lt;/strong>和&lt;strong>Fine Tuning&lt;/strong>水平相当。模型规模在0.1B到1B时，&lt;strong>P-Tuning v1&lt;/strong>的效果远不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>不同下游任务的微调效果不稳定&lt;/strong>：Lack of universality across tasks。实验证明，针对某些下游任务进行&lt;strong>P-Tuning v1&lt;/strong>，效果远差于&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>解决方案&lt;/strong>：论文提出&lt;strong>P-Tuning v2&lt;/strong>技术，采用&lt;strong>Deep Prompt Tuning&lt;/strong>方法，同时针对NLU任务做了一定适配和优化。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：实验证明&lt;strong>P-Tuning v2&lt;/strong>，在不同规模的模型上、在不同下游任务上都可获得较高的稳定性。是一种对&lt;strong>P-Tuning v1&lt;/strong>更好的替代方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112132956.png" alt="image-20240511112132956">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>在不同规模的模型下、不同下游任务中，微调效果不稳定。&lt;/p>
&lt;ul>
&lt;li>如：当模型大小不大，特别是少于10B参数时，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114454639.png" alt="image-20240511114454639">&lt;/li>
&lt;li>如：抽取式问答(extractive question answering)，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114429201.png" alt="image-20240511114429201">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>P-Tuning v2采用&lt;strong>Deep P-Tuning&lt;/strong>的优化方式，用于探索垂域知识。&lt;/li>
&lt;li>Deep表现于在预训练模型的每一层注意力层增加了一个小模型，作用于每一层的输入。而P-Tuning v1仅在第一层增加了一个LSTM小模型。&lt;/li>
&lt;li>这种方法的本质是：不同规模的模型对于&lt;strong>P-Tuning v1&lt;/strong>在第一层增加的前缀向量的特征提取能力不同。越大的模型特征提取越强，后续各层都能感知注意到这个前缀向量的特征。反之，小模型特征提取能力弱，后续各层无法感知注意到前缀向量的特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511115216844.png" alt="image-20240511115216844">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>对300M到10B参数的模型上实验，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;li>以抽取式问答和命名实体识别为代表的下游任务上，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>&lt;strong>P-Tuning v1&lt;/strong>提出了将&lt;strong>数学意义上的离散提示词转换为连续可微的提示词&lt;/strong>，但存在的问题还有2个：&lt;/p>
&lt;ul>
&lt;li>不同规模的模型微调效果不稳定。&lt;/li>
&lt;li>不同下游任务的模型微调效果不稳定。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>离散到连续&lt;/strong>是&lt;strong>Soft Prompt&lt;/strong>技术分支的&lt;strong>重要思想、重要里程碑&lt;/strong>，但&lt;strong>P-Tuning v1&lt;/strong>已经做到连续可微了，还有什么改进空间呢？&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>为了寻找突破口，我们还是进行数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>V、M、e&lt;/strong>：V表示模型M的词汇表，e表示模型M的词嵌入层。&lt;/li>
&lt;li>&lt;strong>离散到连续的转换&lt;/strong>：假定离散提示词为序列**[h&lt;sub>0&lt;/sub>, &amp;hellip;, h&lt;sub>i&lt;/sub>]**，经过**P-Tuning v1**的**Prompt Encoder**模块转换为向量序列**[e(x), e(h&lt;sub>0&lt;/sub>), &amp;hellip;, e(h&lt;sub>i&lt;/sub>)]**。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511144119291.png" alt="image-20240511144119291">&lt;/li>
&lt;/ul>
&lt;p>通过问题建模，我们可以看到&lt;strong>数学意义上&lt;/strong>的&lt;strong>离散提示&lt;/strong>已经表示为&lt;strong>连续提示&lt;/strong>。那么不同规模的模型、不同下游任务的微调效果不稳定，很可能源于&lt;strong>P-Tuning v1&lt;/strong>在输入层添加的前缀向量没有起到有效作用，从逻辑上，我们可以有如下猜测：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型规模对前缀向量的影响&lt;/strong>：不同规模的模型对前缀向量的特征提取能力是不同的，小模型特征提取不足，导致后续预训练模型的各层无法感知注意到前缀向量。&lt;/li>
&lt;li>&lt;strong>下游任务类型对前缀向量的影响&lt;/strong>：从离散提示词看，不同下游任务的提示词是不同的。同理，不同下游任务的连续提示词应该也是不同的。&lt;/li>
&lt;/ul>
&lt;p>因此：&lt;/p>
&lt;ul>
&lt;li>从数学上，&lt;strong>P-Tuning v1&lt;/strong>的连续可微前缀向量没有太多改进空间。&lt;/li>
&lt;li>从模型结构上，
&lt;ul>
&lt;li>&lt;strong>可以在Transformer的各层添加前缀向量&lt;/strong>，以抵消小模型对前缀向量的特征提取不足的局限。&lt;/li>
&lt;li>&lt;strong>可以改变前缀向量的长度&lt;/strong>，以实现不同下游任务有不同的前缀向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>为了更好地阐述论文的改进思路，我们列出相关源码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>DebertaPrefixModelForQuestionAnswering&lt;/strong>类，是针对QA下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511150051142.png" alt="image-20240511150051142">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RobertaPrefixForSequenceClassification&lt;/strong>类，是针对序列分类下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151145088.png" alt="image-20240511151145088">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>覆写Deberta的注意力层，支持在Deberta各注意力层都增加了前缀向量：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151537379.png" alt="image-20240511151537379">&lt;/p>
&lt;p>最后，在train方法中，将上述对模型结构的改进串联起来：&lt;/p>
&lt;ul>
&lt;li>创建前缀向量编码器对象，根据本下游任务指定的提示长度，生成前缀向量。&lt;/li>
&lt;li>前向传播时，将前缀向量传入本下游任务对应的各注意力层，实现不同层都能提取到前缀向量特征。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151852070.png" alt="image-20240511151852070">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1across-scales">(1)实验结果1：Across Scales&lt;/h2>
&lt;p>针对不同规模的模型，&lt;strong>P-Tuning v2&lt;/strong>的微调效果比较稳定。&lt;/p>
&lt;ul>
&lt;li>在四种&lt;strong>参数小于10B的模型&lt;/strong>上，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511152631012.png" alt="image-20240511152631012">&lt;/p>
&lt;h2 id="2实验结果2across-tasks">(2)实验结果2：Across Tasks&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>CoNLL03、OntoNotes5.0、CoNLL04、SQuAD1.1dev、SQuAD2.0dev、CoNLL12、CoNLL05 WSJ、CoNLL05 Brown&lt;/strong>八种下游任务中，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153327159.png" alt="image-20240511153327159">&lt;/p>
&lt;h2 id="3其它重要发现实现prompt-encoder的神经网络结构的选择技巧">(3)其它重要发现：实现Prompt Encoder的神经网络结构的选择技巧&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>P-Tuning v1&lt;/strong>中，采用&lt;strong>LSTM+MLP&lt;/strong>或&lt;strong>MLP&lt;/strong>，其中MLP采用2层线性层、ReLU作为激活函数。&lt;/li>
&lt;li>在&lt;strong>P-Tuning v2&lt;/strong>中，通过超参数针对不同下游任务选择不同神经网络，其中MLP的一种实现可以采用2层线性层、tanh作为激活函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153852283.png" alt="image-20240511153852283">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning v1&lt;/strong>的局限性：不同的下游任务、不同规模的模型，微调结果不稳定。&lt;/li>
&lt;li>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：修改模型结构，在各层注意力层增加前缀处理器网络以抵消小模型对前缀向量特征提取不足的局限，支持不同下游任务选择不同前缀提示长度、选择不同前缀编码器神经网络结构。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2110.07602&lt;/p></description></item><item><title>【chatGPT】学习笔记46-LLM微调技术之P-Tuning V1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</link><pubDate>Wed, 01 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</guid><description>&lt;p>前面给大家分享了&lt;code>Soft Prompt&lt;/code>技术分支下的&lt;code>Prefix-Tuning&lt;/code>和&lt;code>Prompt Tuning&lt;/code>，在这个技术分支下，还有一项需要重点了解的微调技术——&lt;code>P-Tuning&lt;/code>。&lt;/p>
&lt;p>&lt;code>P-Tuning&lt;/code>是清华大学和MIT于2021年联合发布的一项微调技术，在NLU(自然语言理解)任务上有重大突破。&lt;/p>
&lt;p>本文解读论文**《GPT Understands, Too》**，我们一起来学习一下&lt;code>P-Tuning&lt;/code>技术的原理。&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Discrete Prompts&lt;/strong>(离散提示词)会导致大模型性能不稳定。&lt;/p>
&lt;ul>
&lt;li>比如：修改提示词中的一个单词，都可能导致大模型的性能大幅下降。&lt;/li>
&lt;li>本质：根据自然语言形式的提示词进行预测，对于大模型本身&lt;strong>从数学上是不可微的&lt;/strong>(这就是数学意义上的&lt;strong>离散性&lt;/strong>)——不可微就意味着AI无法高效、稳定地&lt;strong>提特征&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>P-Tuning&lt;/code>技术，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。将离散提示词向量化为可训练的连续提示词(&lt;strong>trainable continuous prompt embeddings&lt;/strong>)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning&lt;/strong>通过连续提示词向量，降低了不同离散提示之间的差距，进而提升了模型的稳定性。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>在LAMA、SuperGLUE等NLU任务上，显著提高了模型性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426094803830.png" alt="image-20240426094803830">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：离散提示会导致大模型的稳定性问题。
&lt;ul>
&lt;li>&lt;strong>以手动离散提示为例&lt;/strong>：提示中改变一个单词可能会导致显著的性能下降，存在很大的不稳定性。&lt;/li>
&lt;li>&lt;strong>一些优化尝试&lt;/strong>：
&lt;ul>
&lt;li>调整语言模型本身，不稳定性问题有所缓解，但不同提示之间的性能差异仍然很大(特别是在少样本场景下)。&lt;/li>
&lt;li>&lt;strong>自动提示法(automatic prompting)&lt;/strong>：试图为给定任务搜索更好的提示，但这些方法并没有改变离散提示的不稳定本质。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426114420388.png" alt="image-20240426114420388">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>prompt encoder&lt;/strong>：论文提到通过&lt;strong>prompt encoder(提示词编码器)&lt;/strong>，将输入的离散提示Token，和连续提示Embedding连接起来后，输入给大语言模型。其中，&lt;strong>prompt encoder&lt;/strong>可采用LSTM或MLP来实现。&lt;/li>
&lt;li>&lt;strong>backpropagation to optimize&amp;hellip;&lt;/strong>：可以通过反向传播，优化连续提示词，进而将离散提示转变为可微的连续提示。&lt;/li>
&lt;li>&lt;strong>P-Tuning的本质&lt;/strong>：该技术的本质打破离散提示的限制——离散则不便于&lt;strong>提特征&lt;/strong>，连续可微则可学习——因此P-Tuning抵消了离散提示中微小变化对稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在LAMA基准测试中，使用P-Tuning，比手动离散提示(manual discrete prompts)提升了20多分，比搜索提示(searched prompts)提升了9分。&lt;/li>
&lt;li>在SuperGLUE基准测试中，在全监督和少样本下都优于PET的最佳离散提示(the best discrete prompts)。&lt;/li>
&lt;li>实验还证明，在更广泛的任务中，P-Tuning降低了不同离散提示之间的差异，进而提升了模型的稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426113823515.png" alt="image-20240426113823515">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>提示词是大家耳熟能详的激发LLM能力的技术手段，但是&lt;strong>从数学上具有极大的局限性&lt;/strong>——就是它是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;p>论文作者举了这样的一个例子：&lt;/p>
&lt;ul>
&lt;li>表格第三行和表格第四行的两个提示词，只是少了一个单词&lt;strong>In&lt;/strong>，AI猜出来X和Y填什么的准确度就下降了20分。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501144315842.png" alt="image-20240501144315842">&lt;/p>
&lt;p>在本论文发表前，业界还有一些自动化搜索离散提示的优化尝试：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>mining the training corpus&lt;/strong>：挖掘训练语料库。&lt;/li>
&lt;li>&lt;strong>gradient-based searching&lt;/strong>：基于梯度的搜索。&lt;/li>
&lt;li>&lt;strong>using pretrained generative model&lt;/strong>：使用预训练的生成模型。&lt;/li>
&lt;/ul>
&lt;p>这些优化方法的本质就是&lt;strong>自动生成提示词&lt;/strong>，但是用自然语言表示的提示词依然还是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>M、V、h&lt;/strong>：M表示预训练模型， 词表大小V，隐藏层大小h。&lt;/li>
&lt;li>&lt;strong>{(x&lt;sub>i&lt;/sub>, y&lt;sub>j&lt;/sub>)}&lt;sub>i&lt;/sub>&lt;/strong>：表示在NLU任务中的数据集。x&lt;sub>0:n&lt;/sub>={x&lt;sub>0&lt;/sub>, x&lt;sub>1&lt;/sub>, &amp;hellip;, x&lt;sub>n&lt;/sub>}是一系列离散Token组成的输入，y∈Y表示标签。&lt;/li>
&lt;li>&lt;strong>f&lt;sub>M&lt;/sub>(x)=p(y|x)&lt;/strong>：表示预训练模型M的任务，就是预测分类的条件概率。&lt;/li>
&lt;li>&lt;strong>[Di]&lt;/strong>：表示离散提示的Token，每一个离散提示都可以表示为T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;/ul>
&lt;p>通俗一点说，上面这一通数学建模，就是描述了一个填字游戏：&lt;/p>
&lt;ul>
&lt;li>比如：The capital of &lt;input checked="" disabled="" type="checkbox"> is [y]。&lt;/li>
&lt;li>如果x=Britain，则希望AI输出y=London。&lt;/li>
&lt;li>如果x=中国，则希望AI输出y=北京。&lt;/li>
&lt;/ul>
&lt;p>离散提示&lt;strong>T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}&lt;strong>会被Embedding为&lt;/strong>{e(D&lt;sub>0&lt;/sub>)&amp;hellip;e(D&lt;sub>i&lt;/sub>), e(x&lt;sub>0&lt;/sub>), &amp;hellip;, e(x&lt;sub>n&lt;/sub>), &amp;hellip;, e(D&lt;sub>k&lt;/sub>)}&lt;/strong>，其中e ∈
R&lt;sup>|V|×d&lt;/sup>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501151933103.png" alt="image-20240501151933103">&lt;/p>
&lt;p>&lt;strong>P-Tuning的实现怎么表达呢&lt;/strong>？如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[P&lt;sub>i&lt;/sub>]&lt;/strong>：表示第i个连续提示Embedding，注意论文的表述——&lt;strong>连续的提示词嵌入(continuous prompt embedding)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}&lt;/strong>：基于[P&lt;sub>i&lt;/sub>]的含义，那么任意一个提示词都能表达为T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;li>&lt;strong>f: [P&lt;sub>i&lt;/sub>]-&amp;gt;h&lt;sub>i&lt;/sub>&lt;/strong>：一个词嵌入函数，用来将T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}转换为{h&lt;sub>0&lt;/sub>, e(x), h&lt;sub>i+1&lt;/sub>, &amp;hellip;, h&lt;sub>j&lt;/sub>, e(y), h&lt;sub>j+1&lt;/sub>, &amp;hellip;, h&lt;sub>k&lt;/sub>}。&lt;/li>
&lt;li>&lt;strong>{P&lt;sub>i&lt;/sub>}&lt;sup>k&lt;/sup>&lt;sub>i=1&lt;/sub>&lt;/strong>：表示P-Tuning的目标——反向传播，优化损失值，在预训练模型之前学习到提示词的特征。&lt;/li>
&lt;/ul>
&lt;p>不严谨地理解一下P-Tuning的玩法——就是加了个新的神经网络，不断地在学习如下提示词：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>吾饥矣&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹中空空&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹鸣如鼓&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;/ul>
&lt;p>它会发现&lt;strong>吾饥矣、吾腹中空空、吾腹鸣如鼓&lt;/strong>的特征，都是在说&lt;strong>肚子饿了&lt;/strong>，于是在调用大语言模型之前，它就把自然语言形态的离散提示词都转变为：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>我饿了&lt;/strong>，你就要说：xxx。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153355161.png" alt="image-20240501153355161">&lt;/p>
&lt;p>最后，我们来完整地对比一下&lt;strong>自动化搜索离散提示&lt;/strong>法和&lt;strong>P-Tuning&lt;/strong>的差别：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自动搜索离散提示&lt;/strong>用的是&lt;strong>Prompt Generator&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>是用&lt;strong>Prompt Encoder&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>将数学意义上的&lt;strong>离散提示词&lt;/strong>转换为了&lt;strong>连续可微提示词&lt;/strong>，帮助AI更好地提特征。&lt;/li>
&lt;li>其实两种思路本质都一样，都是很巧妙的想法。&lt;/li>
&lt;li>论文还提到Prompt Encoder的实现采用了&lt;strong>LSTM、MLPs、identity mapping function(恒等映射函数)&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153925852.png" alt="image-20240501153925852">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1knowledge-probing">(1)实验结果1：Knowledge Probing&lt;/h2>
&lt;p>在知识探索(Knowledge Probing)型任务上，实验可以评估出AI获得现实世界知识量。&lt;/p>
&lt;p>LAMA数据集创建了三元组形式的完型填空，来实施知识探索评估。&lt;/p>
&lt;p>从实验结果上看，P-Tuning显著提高了知识探测的效果。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LAMA-34k数据集&lt;/strong>：从43.3%提高到50.6%&lt;/li>
&lt;li>&lt;strong>LAMA-29k数据集&lt;/strong>：从45.2%提高到64.2%&lt;/li>
&lt;li>&lt;strong>相较于离散提示搜索方法&lt;/strong>：P-Tuning优于离散提示搜索方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161239613.png" alt="image-20240501161239613">&lt;/p>
&lt;h2 id="2实验结果2fully-supervised-learning">(2)实验结果2：Fully-supervised Learning&lt;/h2>
&lt;p>实验采用了SuperGLUE基准测试，测试了7个自然语言理解任务(NLU)。包括：&lt;/p>
&lt;ul>
&lt;li>问答、MultiRC、文本蕴含、RTE、共指消解、因果推理、词义消歧。&lt;/li>
&lt;/ul>
&lt;p>实验使用了四个版本的预训练模型：&lt;/p>
&lt;ul>
&lt;li>GPT2-Base&lt;/li>
&lt;li>GPT2-medium&lt;/li>
&lt;li>BERT-Base&lt;/li>
&lt;li>BERT-Large&lt;/li>
&lt;/ul>
&lt;p>实验证明&lt;strong>P-Tuning可以提高 BERT和GPT上的全监督学习性能&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在 BERT-Base上，P-Tuning在5/7任务上实现了最佳性能。&lt;/li>
&lt;li>在 BERT-Large上，P-Tuning在4/7任务上超越了其他方法。&lt;/li>
&lt;li>在 GPT2-Base和 GPT2-Medium上，P-Tuning 在所有任务上始终是最佳性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161948829.png" alt="image-20240501161948829">&lt;/p>
&lt;h2 id="3实验结果3few-shot-learning">(3)实验结果3：Few-Shot Learning&lt;/h2>
&lt;p>实验使用了少样本SuperGLUE基准测试，就是FewGLUE数据集。&lt;/p>
&lt;p>实验证明了P-Tuning有一定的提升：&lt;/p>
&lt;ul>
&lt;li>ALBERT上，比PET平均高出1个点、比Prompt Tuning高出13个点。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501162327022.png" alt="image-20240501162327022">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>离散提示的问题&lt;/strong>：数学上离散，不便于AI提特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning的核心思想&lt;/strong>：将离散提示词转换为连续可微提示词，微调的目标是用LSTM这类网络学习提示词特征。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2103.10385&lt;/p></description></item><item><title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</link><pubDate>Wed, 03 Apr 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</guid><description>&lt;p>紧接着Stanford的&lt;code>Prefix Tuning&lt;/code>论文，Google迅速发表了&lt;code>Prompt Tuning&lt;/code>技术论文。Google声称该技术比&lt;code>Prefix Tuning&lt;/code>更易上手且成本更低，因此该技术随后也成为了微调技术中的一个重要分支。&lt;/p>
&lt;p>本文解读论文**《The Power of Scale for Parameter-Efficient Prompt Tuning》**，与大家共同感受&lt;code>Prompt Tuning&lt;/code>技术的奇妙之处。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403122759085.png" alt="image-20240403122759085">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Prompt Tuning&lt;/strong>与&lt;strong>Prefix Tuning&lt;/strong>一样，都是以任务为中心的思路解决问题。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>以任务为中心&lt;/strong>：它们都在试图解决&lt;strong>FFT&lt;/strong>针对不同的下游任务都需产生一个新的微调后大模型而导致的成本效率问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prompt Tuning&lt;/code>，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。统一不同下游任务的训练数据格式，并将这些不同下游任务的训练数据汇总成一个乱序的数据集，微调预训练模型，最终获得一个能处理不同下游任务的大模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在小参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>略差于FFT性能。&lt;/li>
&lt;li>在中参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>快速接近于FFT性能。&lt;/li>
&lt;li>在大参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>与FFT性能持平。&lt;/li>
&lt;li>因此，&lt;strong>Prompt Tuning&lt;/strong>在大参数规模的模型上，更具成本效率优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403092613314.png" alt="image-20240403092613314">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：论文中提到的&lt;strong>Prompt Design&lt;/strong>可以理解为大家耳熟能详的&lt;strong>提示词及提示词工程&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>经过工程实践，大家都知道提示词工程有一定的效果，但效果远不及FFT。为什么呢？论文中总结了提示词工程的两个短板：&lt;strong>the discrete space of words(离散空间的单词)&lt;strong>和&lt;/strong>requires human involvement(人类的投入)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>the discrete space of words(离散空间的单词)&lt;/strong>：提示词工程中的提示词是数学意义上的&lt;strong>离散&lt;/strong>，大语言模型不能很好地学习其隐含的特征。隐含特征不是提示词内容本身的显式特征，而是诸如提示词的句式、问法、潜台词等隐式特征。业界针对提取离散空间词汇也提出了一些算法(如：&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)，但效果也非常限。&lt;/li>
&lt;li>&lt;strong>requires human involvement(人类的投入)&lt;/strong>：提示词工程依赖有经验的提示词工程师，针对不同下游任务设计提示词，工作量巨大，并且对大模型推理能力的提升又极其有限。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：论文中提到的&lt;strong>Model Tuning&lt;/strong>和&lt;strong>Model Tuning(Multi Task)&lt;/strong>，可以理解为&lt;strong>FFT&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Model Tuing(Multi Task)&lt;/strong>：&lt;strong>Model Tuning(Multi Task)&lt;strong>是针对每个下游任务都微调出一个大模型，而&lt;/strong>Model Tuning&lt;/strong>是将N个下游任务都微调到一个大模型中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>统一下游任务的数据格式&lt;/strong>：论文中提到&lt;code>an additional k tunable tokens per downstream task to be prepended to the input text&lt;/code>，就是为了达成统一下游任务的数据格式。如：['[CLS]&amp;rsquo;, &amp;lsquo;中&amp;rsquo;, &amp;lsquo;国&amp;rsquo;, &amp;lsquo;的&amp;rsquo;, &amp;lsquo;春&amp;rsquo;, &amp;lsquo;节&amp;rsquo;, &amp;lsquo;是&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;。&amp;rsquo;, &amp;lsquo;[SEP]']。&lt;/li>
&lt;li>&lt;strong>合并下游任务的数据集合&lt;/strong>：当我们统一了下游任务的数据格式，就可以将这些下游任务数据集合混合在一起。&lt;/li>
&lt;li>&lt;strong>LLM具备学习数据集合隐式特征的能力&lt;/strong>：论文假设LLM是具备学习上述数据格式隐式特征的能力，并通过实验验证了这个假设。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的本质&lt;/strong>：该技术的本质是LLM的核心能力之一就是&lt;strong>提特征&lt;/strong>。如果特征很明显，LLM就可以低成本提取。如果特征很隐晦，LLM无法低成本提取、甚至无法提取，&lt;strong>Prompt Tuning&lt;/strong>就是改变数据集，将隐式特征转为显式特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;li>因此，在大规模模型上，&lt;strong>Promp Tuning&lt;/strong>具备巨大的成本优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101305842.png" alt="image-20240403101305842">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1下游任务数据格式归一化的理论基础">(1)下游任务数据格式归一化的理论基础&lt;/h2>
&lt;p>论文的实验对象是T5，因为T5有一个有趣的观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Following the “text-to-text” approach of T5 (Raffelet al., 2020), we cast all tasks as text generation&lt;/strong>：所有的下游任务都可以等效于文本生成。这个观点就可以支撑&lt;strong>Prompt Tuning&lt;/strong>将所有下游任务的训练数据格式统一起来。&lt;/li>
&lt;li>如：翻译下游任务，可以将训练数据构造为：&amp;ldquo;translate English to German: hello world!&amp;rdquo;&lt;/li>
&lt;li>如：摘要下游任务，可以将训练数据构造为：&amp;ldquo;summarize: xxxxxxxxxxxxxxxxxxx&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403112458776.png" alt="image-20240403112458776">&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>：在不同下游任务的训练数据可归一化的前提下，大语言模型可被建模为&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>，X是用户输入的Tokens，Y是在X发生的概率下模型的输出。&lt;/li>
&lt;li>&lt;strong>Prompt Design&lt;/strong>的短板：提示词工程需要人类不断尝试寻找出合适提示词，这种不断尝试方法可能是人工寻找的，也可能采用了非可微的搜索方法(如：前文提到的&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>Pr&lt;sub>θ;θ&lt;sub>P&lt;/sub>&lt;/sub> (Y|[P; X])&lt;/strong>：这个公式表达了Prompt Tuning的核心思想——在训练数据中植入特殊Token，大模型除了学习训练数据中的显式特征外，还能学习Prompt形式训练数据的隐式特征。对于Prompt隐式特征的学习最终影响的不是预训练模型的参数θ，而是在修正θ&lt;sub>P&lt;/sub>。&lt;/li>
&lt;li>&lt;strong>[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>] ∈ R&lt;sup>(p+n)×e&lt;/sup>&lt;/strong>：Prompt中的普通标记被大模型嵌入后得到&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)，Prompt中的特殊标记被大模型嵌入后得到&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)。[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>]则表示输入给大模型后续神经网络层的高维向量。训练的影响并不会修正&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数，只会修正&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115838249.png" alt="image-20240403115838249">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115900991.png" alt="image-20240403115900991">&lt;/p>
&lt;p>因此，实验的关注点如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的初始值&lt;/strong>：和Prefix Tuning一样，都需要关注软提示的初始值，以提升训练速度和效果。&lt;/li>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的长度&lt;/strong>：和Prefix Tuning一样，也需要关注软提示的长度，以降低训练成本。
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的参数成本=E*P&lt;/strong>：E是普通标记的向量维数，P是特殊标记的长度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3如何消减特殊标记的影响">(3)如何消减特殊标记的影响&lt;/h2>
&lt;p>由于增加了特殊标记，大模型学习的内容就不再是原汁原味的&amp;quot;人类自然语言&amp;quot;了。这样就可能导致大模型无法用自然语言作答——这就好像你在训练大模型鸟语但又期待它能说人话、你在用中文教英语最后学会的是Chinglish。&lt;/p>
&lt;p>论文中提出了**Span Corruption(跨度损失)**的概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Span Corruption(跨度损失)&lt;/strong>：比如，训练数据&lt;code>Thank you [X] me to your party [Y] week&lt;/code>，&lt;input checked="" disabled="" type="checkbox"> 、[Y]就是特殊标记，将这种特殊标记植入自然语言的目的是&amp;quot;问题模式等隐式特征的显性化&amp;rdquo;，但弊端就是让大语言模型学会了非人类的自然语言。&lt;/li>
&lt;li>&lt;strong>论文提出了三种解决方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>Span Corruption法&lt;/strong>：啥都不做，任由大语言模型输出特殊标记，忽略这种影响。&lt;/li>
&lt;li>&lt;strong>Span Corruption+Sentinel法&lt;/strong>：在大语言模型中增加Sentinel，一定程度地降低这种影响。&lt;/li>
&lt;li>&lt;strong>LM Adaptation法&lt;/strong>：采用Raffel提出的一个小模型，纠正大语言模型输出特殊标记的倾向，最终输出纯粹的自然语言。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115956245.png" alt="image-20240403115956245">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文阐述了详细的实验过程、实验数据，最终的实验结果如前文所述：&lt;/p>
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120117025.png" alt="image-20240403120117025">&lt;/p>
&lt;ul>
&lt;li>当模型规模逐渐变大，&lt;strong>Promp Tuning&lt;/strong>涉及的参数相较于&lt;strong>Prefix Tuning&lt;/strong>更少，但微调效果持平，因此&lt;strong>Prompt Tuning具备巨大的成本优势&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120213717.png" alt="image-20240403120213717">&lt;/p>
&lt;h2 id="42重要发现">4.2.重要发现&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;p>论文从可解释性方面发现了&lt;strong>语义聚合现象&lt;/strong>，进一步证明了Prompt形式的数据更有利于大语言模型学习其隐式特征：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>语义聚合现象&lt;/strong>：观测被大语言模型嵌入后的特殊标记和普通标记，可以发现出现了物以类聚的现象：
&lt;ul>
&lt;li>如：Technology / technology / Technologies / technological / technologies相关的训练数据，向量相似度发生了语义聚合。&lt;/li>
&lt;li>语义聚合的出现，说明了大语言模型学习到了Prompt形式的训练数据中的隐式特征，因此可以举一反三地处理为见过的下游任务相关输入。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文还证明了&lt;strong>Prompt Ensembling(提示集成能力)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>基于Prompt Tuning的技术思想，可以做到数据格式统一、不同下游任务的训练数据混合训练，进而达到&amp;rdquo;&lt;strong>一个大模型支持多种不同下游任务&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>这种思想可以在超大规模的模型上极大地降低训练成本、使用成本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;p>在工程实践方面，论文也给出了&lt;strong>Prompt长度、Prompt初始值&lt;/strong>的相关推荐：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt长度的影响&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在中小参数规模的模型上，Prompt长度越长，提示效果越好，但过犹不及(实验长度的临界值是150)——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;li>在大参数规模的模型上，Prompt长度反而没有什么影响了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt的初始值选择的影响&lt;/strong>：随机初始化Prompt的效果远差于用下游任务相关联提示词做初始值的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121146629.png" alt="image-20240403121146629">&lt;/p>
&lt;p>最后，论文还通过消融实验，补充了消减&lt;strong>Span Corruption&lt;/strong>的建议：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LM Adaptation&lt;/strong>：在中小规模模型上，采用LM Adaptation，对大语言模型的纠正效果更好。LM Adaptation增加步数会达到更好的纠正效果。&lt;/li>
&lt;li>在大规模模型上，Span Corruption的影响也可以忽略不计了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121223051.png" alt="image-20240403121223051">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：通过归一化不同下游任务的训练数据，并将隐式特征显性化，帮助大语言模型学习。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prompt形式的训练数据有助于LLM学习隐式特征。&lt;/li>
&lt;li>采用Prompt Tuning可用一套模型搞定多个下游任务。&lt;/li>
&lt;li>对于大规模参数的模型，Prompt长度和初始化影响很小。&lt;/li>
&lt;li>对于中小规模参数的模型，Prompt长度和初始值可参考Prefix Tuning的实践。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2104.08691.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</link><pubDate>Fri, 29 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</guid><description>&lt;p>&lt;code>Prefix Tuning&lt;/code>是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。&lt;/p>
&lt;p>本文通过解读论文**《Prefix-Tuning: Optimizing Continuous Prompts for Generation》**，与小伙伴们一起学习理解&lt;code>Prefix Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329153821295.png" alt="image-20240329153821295">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：**FFT(全参数微调)**针对不同的下游任务都需要产生一个新的微调后大模型，存在成本效率等诸多工程问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prefix Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。针对不同下游任务创建不同的&lt;strong>&lt;code>Prefix(前缀向量模块)&lt;/code>&lt;/strong>，这样不同下游任务只需要在一套预训练大模型上加载不同&lt;strong>Prefix小模型&lt;/strong>即可。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在GPT-2的&lt;strong>&lt;code>Table-To-Text(表格生成文本)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占GPT-2参数的0.1%，即可达到GPT-2同等水平。&lt;/li>
&lt;li>在BART的&lt;strong>&lt;code>Sumarization(摘要)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占BART参数的0.1%，即可达到BART同等水平。&lt;/li>
&lt;li>在上述两种实验中，额外还观察到一定的泛化涌现能力，&lt;code>Prefix Tuning&lt;/code>可外推到训练期间未见过的任务主题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329111747247.png" alt="image-20240329111747247">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>是向预训练模型中增加新的小模型，仅微调小模型参数以达到高效微调的目的。实验效果证明仅需微调2%~4%的参数即可达到全参数微调的效果。&lt;/p>
&lt;ul>
&lt;li>Adapter Tuning的详细解读可参见本技术专栏这篇文章《【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：GPT-3的一个重要贡献就是&lt;strong>Context Learning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>，GPT-3是一套统一的大语言模型，用户不需要针对下游任务单独微调，直接通过提示词和上下文，影响GPT-3输出的答案。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：&lt;strong>Prefix Tuning&lt;/strong>借鉴了&lt;strong>Adapter Tuning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>的思想：&lt;/p>
&lt;ul>
&lt;li>Prefix Tuning也额外&lt;strong>增加了N个小模型&lt;/strong>，这些小模型外挂于同一套预训练模型上，&lt;strong>不同小模型解决不同的下游任务&lt;/strong>。&lt;/li>
&lt;li>Prefix Tuning增加的这些小模型的作用类似&lt;strong>Prompt(提示词)&lt;/strong>，它们会在用户输入的文本前额外增加针对不同下游任务的&lt;strong>提示词前缀Prefix&lt;/strong>。这些前缀不是自然语言，而是Transformer架构中向量形式的Token，这种Token叫做&lt;strong>虚拟Token&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GPT-2的Table-To-Text&lt;/strong>和&lt;strong>BART的Sumarization&lt;/strong>的测试效果：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPT-2的Table-To-Text&lt;/strong>：在完整数据集上训练时，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>在表格到文本的下游任务的性能相当。&lt;/li>
&lt;li>&lt;strong>BART的Sumarization&lt;/strong>：在摘要方面，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>性能略有下降。&lt;/li>
&lt;li>&lt;strong>Low Data Settting&lt;/strong>：在数据量少的数据集上，&lt;strong>Prefix Tuning&lt;/strong>能够克服数据集样本不足、提炼数据特征困难的问题，表现出了泛化涌现能力。在上述两项任务中性能表现优于FFT。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329113337615.png" alt="image-20240329113337615">&lt;/p>
&lt;h1 id="3prefix-tuning">3.Prefix Tuning&lt;/h1>
&lt;h2 id="31problem-statement问题陈述">3.1.Problem Statement(问题陈述)&lt;/h2>
&lt;p>Prefix Tuning采用了严谨的数学描述来阐述待解决的问题，这也是算法工程中的算法建模环节，我们尽量通俗地理解这个问题模型，也感受一下算法工程师的思维模式。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>x和y&lt;/strong>：x是大语言模型的输入，y是大语言模型的输出。&lt;/p>
&lt;ul>
&lt;li>如下图右侧上方所示，在摘要型下游任务中，人类输入的原始文本就是x，大语言模型输出的总结结果就是y。&lt;/li>
&lt;li>如下图右侧下方所示，在表格转文本的下游任务中，人类输入的结构化的表格字符串是x，大语言模型输出的表格描述就是y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>自回归语言模型的问题子域&lt;/strong>(如下图左上所示)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>pφ(y | x)&lt;/strong>：根据Transformer这种自回归模型的网络结构，其本质可抽象为&lt;strong>pφ(y | x)&lt;/strong>，φ为大语言模型的参数，p就是在**&amp;ldquo;人类输入字符串x的条件下，大语言模型输出y的概率分布&amp;rdquo;**。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;/strong>：z被定义为x和y的序列，其中X&lt;sub>idx&lt;/sub>表示了x的索引，Y&lt;sub>idx&lt;/sub>表示y的索引。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>&lt;/strong>：h&lt;sub>i&lt;/sub>表示&lt;strong>时间步i的激活(activation)&lt;/strong>，h&lt;sub>i&lt;/sub>又是由第i个时间步中的n层激活组成的序列(即h&lt;sub>i&lt;/sub>= [h&lt;sub>i&lt;/sub>&lt;sup>(1)&lt;/sup>;···;h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>])，其中h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>表示在Transformer架构中的第i个时间步的第n层的激活。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：此数学公式表示——根据z&lt;sub>i&lt;/sub>，以及h&lt;sub>1&lt;/sub>~h&lt;sub>i-1&lt;/sub>，计算当前的h&lt;sub>i&lt;/sub>。展开解释一下就是，Transformer模型会根据第i个时间步的x、y，以及第1个时间步~第i-1个时间步计算的各时间步计算的各层激活向量，计算当前时间步下Transformer各层的激活。&lt;/li>
&lt;li>&lt;strong>p&lt;sub>φ&lt;/sub>(z&lt;sub>i+1&lt;/sub> | h&lt;sub>≤i&lt;/sub>) = softmax(W&lt;sub>φ&lt;/sub>h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>)&lt;/strong>：最后一层的h&lt;sub>i&lt;/sub>是Transformer训练后获得的概率分布，用来根据当前Token预测下一个Token。其中，W&lt;sub>φ&lt;/sub>是一个用于将h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>映射到词汇表上logits的预训练矩阵。&lt;/li>
&lt;li>上述数学描述中，包含很多AI相关术语(如：自回归模型、时间步i的激活(activation)、各层激活向量、词汇表上logits等)，可参见本技术专栏**《NLP底层原理》篇**的系列文章。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Encoder-Decoder模型的问题子域&lt;/strong>(如下图左下所示)：&lt;/p>
&lt;ul>
&lt;li>与自回归语言模型的问题子域大部分数学描述相同。&lt;/li>
&lt;li>不同点在于基于Encoder-Decoder架构的结构特点，x和y被拆分开了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的微调求解目标&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>max&lt;sub>φ&lt;/sub> log p&lt;sub>φ&lt;/sub>(y | x) = Σ&lt;sub>i∈Y&lt;sub>idx&lt;/sub>&lt;/sub> log p&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub> | h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：这个看着复杂的公式，就是在表达微调的最终目标就是在各个时间步的各时间步激活的条件下x和y序列的概率分布求和。说人话就是，&lt;strong>微调后的模型能够根据x预测最大概率应该输出y&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329125952458.png" alt="image-20240329125952458">&lt;/p>
&lt;h2 id="32intuition直觉">3.2.Intuition(直觉)&lt;/h2>
&lt;p>论文在正式阐述&lt;strong>Prefix Tuning&lt;/strong>的原理之前，描述了研究员的灵感来源：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Based on intuition from prompting&lt;/strong>：提示词被证明是可以影响大语言模型输出，提示词的思想本质是——大语言模型像一个什么都知道的老人，人类的输入需要使用一定的提示技巧唤醒老人的记忆，从而帮助他输出正确的答案。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;strong>和&lt;/strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：再看3.1问题陈述章节的这两个数学公式，提示词有两个作用：
&lt;ul>
&lt;li>&lt;strong>作用1&lt;/strong>：提示词作为人类输入x的一部分，它起到了&lt;strong>影响大语言模型关注x的哪些Token&lt;/strong>的作用。&lt;/li>
&lt;li>&lt;strong>作用2&lt;/strong>：在作用1的驱动下，影响了大语言模型各层的激活计算结果h&lt;sub>i&lt;/sub>，进而&lt;strong>影响了在x条件下应该接什么y的概率&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>but fail for most pretrained LMs&lt;/strong>：虽然提示词从理论上可以影响大语言模型的输出，但论文进一步阐述了提示词的局限性——做过提示词工程的小伙伴应该知道，仅仅通过自然语言形式的提示词，对大语言模型输出的效果提升很有限。&lt;/li>
&lt;li>&lt;strong>continuous word embeddings&lt;/strong>：论文根据上述头脑实验，准备新增1个Prefix模型，在人类输入x词嵌入为向量后，在这个词嵌入向量前增加1个&lt;strong>Prefix Token(前缀向量)&lt;/strong>(这种向量不同于离散向量，不会引发计算的困难)。为了保证Prefix Token有足够的提示性，论文在Transformer的所有层都增加了这种前缀向量。&lt;/li>
&lt;/ul>
&lt;h2 id="33methodparametrization-of-psubθsub原理">3.3.Method/Parametrization of P&lt;sub>θ&lt;/sub>(原理)&lt;/h2>
&lt;p>论文至此，正式阐述了&lt;strong>Prefix Tuning&lt;/strong>的实现方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>z=[x;y]到z=[prefix;x;y]&lt;/strong>：给自回归模型和Encoder-Decoder模型的z向量增加了前缀&lt;strong>PREFIX&lt;/strong>向量。其中P&lt;sub>idx&lt;/sub>表示了前缀的索引。(如：)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329142053466.png" alt="image-20240329142053466">&lt;/p>
&lt;ul>
&lt;li>基于前缀向量，h&lt;sub>i&lt;/sub>的数学公式表示为各层的激活包含前缀向量的计算和大语言模型预测的概率分布。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329143101780.png" alt="image-20240329143101780">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>|P&lt;sub>idx&lt;/sub>| × dim(h&lt;sub>i&lt;/sub>)&lt;/strong>：P&lt;sub>θ&lt;/sub>是Prefix Tuning新增的小模型的参数，参数的维数必然是有限的，因为它的维数等于前缀向量个数乘以h&lt;sub>i&lt;/sub>的维度。这也解释了为什么Prefix Tuning新增的小模型参数规模仅占预训练模型的0.1%。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P&lt;sub>θ&lt;/sub>[i, :] = MLP&lt;sub>θ&lt;/sub>(P ′&lt;sub>θ&lt;/sub>[i, :])&lt;/strong>：论文还发现直接微调P&lt;sub>θ&lt;/sub>会导致训练效果不好，于是采用了重参数化方法，引入了前馈神经网络MLP&lt;sub>θ&lt;/sub>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文的实验结论：&lt;strong>Prefix Tuning&lt;/strong>在Table-To-Text下游任务表现良好、在外推涌现方面表现良好，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Table-To-Text实验&lt;/strong>：用GPT2-Medium和GPT2-Large对比FFT和Prefix Tuning，Prefix Tuning都达到了SOTA水平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150136166.png" alt="image-20240329150136166">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Low-Data-Setting&lt;/strong>实验：在少数据测试中，Prefix Tuning的性能表现和训练稳定度优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150354433.png" alt="image-20240329150354433">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Summarization实验&lt;/strong>：使用XSUM数据集，Prefix Tuning性能略低于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150524132.png" alt="image-20240329150524132">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Extrapolation实验&lt;/strong>：外推涌现实验中，使用XSUM数据集，Prefix Tuning效果优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150541755.png" alt="image-20240329150541755">&lt;/p>
&lt;h2 id="42intrinsic-evaluation重要发现">4.2.Intrinsic Evaluation(重要发现)&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix长度的影响&lt;/strong>：
&lt;ul>
&lt;li>不同下游任务需要增加不同长度的前缀向量。&lt;/li>
&lt;li>Prefix长度越长，提示效果越好，但过犹不及——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151553754.png" alt="image-20240329151553754">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>在哪些层做Prefix的影响&lt;/strong>：仅在词嵌入层做Prefix的效果远差于在Transformer各层做Prefx的效果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151830368.png" alt="image-20240329151830368">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix和Infix的影响&lt;/strong>：用前缀法的性能效果优于中缀法。
&lt;ul>
&lt;li>研究员猜测，前缀法可能影响x和y，中缀法可能只影响y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152257513.png" alt="image-20240329152257513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix的初始值选择的影响&lt;/strong>：随机初始化Prefix向量的效果远差于与下游任务相关联的Prefix向量初始化的效果。
&lt;ul>
&lt;li>这种现象可能由于用与下游任务不相关的提示词向量，会导致更长时间的前缀神经网络的收敛(甚至不收敛)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152826581.png" alt="image-20240329152826581">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：增加一个新的具备提示能力的前缀向量小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prefix长度不宜过长或过短，需根据下游任务实验获得。&lt;/li>
&lt;li>对Transformer做全层的Prefix效果更好。&lt;/li>
&lt;li>Prefix会影响x和y，效果优于Infix。&lt;/li>
&lt;li>Prefix的初始值需选择与下游任务相关的提示向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2101.00190.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</link><pubDate>Fri, 22 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</guid><description>&lt;p>&lt;code>Adapter Tuning&lt;/code>是LLM微调技术中一个重要的技术分支，于2019年由Google的Neil Houlsby等研究员提出。&lt;/p>
&lt;p>&lt;code>Adapter Tuning&lt;/code>方法证明了：微调&lt;strong>少量参数&lt;/strong>即可获得与&lt;strong>全参数微调&lt;/strong>接近的大模型性能。&lt;/p>
&lt;p>本文解读Neil Houlsby的论文**《Parameter-Efficient Transfer Learning for NLP》**，与小伙伴们一起学习理解&lt;code>Adapter Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322130214258.png" alt="image-20240322130214258">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：基于迁移学习思想，需要针对特定的下游任务，对预训练模型进行&lt;strong>全参数微调&lt;/strong>。但针对每个下游任务都要做一次全参数微调，&lt;strong>成本高效率低&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Adapter Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>进行迁移学习的方法。&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>仅需要微调少量参数，就可以支持不同的下游任务。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GLUE基准测试&lt;/strong>中，用&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>方法仅需在预训练模型基础上&lt;strong>增加并微调3.6%的参数&lt;/strong>，即可达到BERT Transformer模型全参数微调的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>通过上述摘要的内容，我们可以想象一下，在2019年绝大多数人还在采用全参数微调这种高成本方案时，&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>仅需微调3.6%的少量参数，会产生多大的&lt;strong>生产效率差异&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322103351046.png" alt="image-20240322103351046">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>背景技术1&lt;/strong>：&lt;strong>基于特征的迁移和微调&lt;/strong>(&lt;code>feature-based transfer and fine-tuning&lt;/code>)是迁移学习思想中的重要工程方法，它也是BERT模型的重要理论基础。论文中：&lt;code>Fine-tuning involves copying the weights from a pre-trained network and tuning them on the downstream task&lt;/code>，表达了BERT模型的训练范式——复用1个BERT的预训练模型的参数(&lt;strong>基于特征的迁移&lt;/strong>)，再针对不同下游任务进行微调(&lt;strong>基于特征的微调&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>背景技术2&lt;/strong>：历史上，已证明针对预训练模型的网络结构中的高层(&lt;code>top layer&lt;/code>)进行&lt;strong>基于特征的微调&lt;/strong>，相较于&lt;strong>基于特征的全参数微调&lt;/strong>，更具有性价比。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：对比&lt;strong>Adapter Tuning&lt;/strong>、&lt;strong>Top Layer Fine Tuing&lt;/strong>、&lt;strong>Full Fine Tuning&lt;/strong>在多任务微调场景下的效果：
&lt;ul>
&lt;li>&lt;strong>在小参数模型上的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可达到&lt;strong>Full Fine Tuning&lt;/strong>的效果，&lt;strong>Top Layer Fine Tuing&lt;/strong>达不到。&lt;/li>
&lt;li>&lt;strong>在稳定性方面的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>的训练效果很稳定，&lt;strong>Top Layer Fine Tuing&lt;/strong>在小参数模型上波动大、只有在大参数模型上才能保证稳定的训练效果。&lt;/li>
&lt;li>&lt;strong>在性价比方面的对比&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可以微调少量参数，即可达到&lt;strong>Full Fine Tuning&lt;/strong>的训练效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322115129249.png" alt="image-20240322115129249">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>基于特征的迁移和微调&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, f(w))&lt;/code>，微调的过程是不断学习修改参数&lt;code>w&lt;/code>和&lt;code>v&lt;/code>，这样就导致预训练模型的参数&lt;code>w&lt;/code>被修改，进而导致极高的训练成本。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, w)&lt;/code>，微调的过程是不断学习修改参数&lt;code>v&lt;/code>，直接复用预训练模型的参数&lt;code>w&lt;/code>而不是修改它，又因为参数&lt;code>v&lt;/code>的数量级远小于参数&lt;code>w&lt;/code>，因此训练成本极低。另外，针对新的&lt;code>下游任务n&lt;/code>只需要增加新的Adapter，训练对应的参数&lt;code>vn&lt;/code>。&lt;/li>
&lt;li>&lt;code>g(v, w)&lt;/code>的具体代码实现等效于，在原有预训练模型的网络结构中，插入一些&lt;strong>Adapter层&lt;/strong>，预训练模型参数&lt;code>w&lt;/code>作为Adapter层的入参，&lt;strong>训练的目标是学习并修改Adapter层的参数&lt;code>v&lt;/code>&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322120647325.png" alt="image-20240322120647325">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>容易与Adapter Tuning混淆的其它训练方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>多任务学习&lt;/strong>：multi-task learning，也会在预训练模型的网络结构中增加新层，最终也是修改了新层的参数。但多任务学习的训练，是将所有下游任务作为训练新层参数的输入。&lt;/li>
&lt;li>&lt;strong>持续学习&lt;/strong>：contiuanl learning，是将N个下游任务组成任务流后逐一学习，这样就要求训练&lt;code>任务m&lt;/code>时，预训练模型的网络结构能够记住之前已经训练过的&lt;code>任务1~任务m-1&lt;/code>得到的参数。这将对预训练模型的记忆能力产生巨大的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322122001309.png" alt="image-20240322122001309">&lt;/p>
&lt;h1 id="3adapter-tuning原理">3.Adapter Tuning(原理)&lt;/h1>
&lt;p>Adatper Tuning具体是如何实现的呢？论文中详细解释了Adapter层的网络结构，以及如何在原始的预训练模型上插入这些Adapter层：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的插入位置&lt;/strong>：在Transformer的多头注意力+前馈网络层之后，2x前馈网络层之后，分别插入了&lt;strong>Adapter层&lt;/strong>。另外，在每个Adapter层之后还插入了一个Layer Norm层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124748365.png" alt="image-20240322124748365">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的内部结构&lt;/strong>：Adapter层包含3层
&lt;ul>
&lt;li>&lt;strong>前馈网络的向量降维层&lt;/strong>：用于将前一层预训练模型输出的高维向量，降维为低维向量。&lt;/li>
&lt;li>&lt;strong>非线性处理层&lt;/strong>：对下游任务微调时，学习参数&lt;code>v&lt;/code>。&lt;/li>
&lt;li>&lt;strong>前馈网络的向量升维层&lt;/strong>：用于将Adapter层输出的低维向量，升维为高维向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Adapter层的参数数量计算公式&lt;/strong>：&lt;code>count(v)=2md+d+m&lt;/code>
&lt;ul>
&lt;li>&lt;strong>d&lt;/strong>：前一层预训练模型输出的高维向量的维数。&lt;/li>
&lt;li>&lt;strong>m&lt;/strong>：Adapter层降维后的低维向量维数。&lt;/li>
&lt;li>&lt;strong>实践经验&lt;/strong>：当m远小于d时，Adapter层的参数量会很小。论文给出的经验数据是可以通过控制m的数值，将Adapter层的参数量控制为预训练大模型参数量的0.5%~8%。这样，可以精准控制微调成本。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124814661.png" alt="image-20240322124814661">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;p>论文至此就一个实验结论：&lt;strong>Adapter Tuning&lt;/strong>就是香，具体如下：&lt;/p>
&lt;ul>
&lt;li>在GLUE基准测试和其他17个公共文本分类任务上，适配器调优效果，优于全参数微调。&lt;/li>
&lt;li>适配器调优在参数数量大幅减少的情况下，仍能保持与全参数微调相近的性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125615723.png" alt="image-20240322125615723">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125621882.png" alt="image-20240322125621882">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的价值&lt;/strong>：追求微调少量参数，仍能达到全参数微调效果。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：增加一个新的小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的具体实现&lt;/strong>：改变预训练模型的网络结构，通过高维向量到低维向量的转换，训练不同下游任务的Adapter层。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/1902.00751.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记42-LLM微调技术概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 15 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</guid><description>&lt;p>笔者在使用大语言模型开展具体业务领域的任务时，遇到了如下问题：&lt;/p>
&lt;ul>
&lt;li>LLM的预训练模型不具备垂域知识，无法很好回答垂域的问题；&lt;/li>
&lt;li>希望用各种提示词技巧让模型理解专业问题，但效果有限，而且提高了使用门槛和模型推理成本；&lt;/li>
&lt;li>微调词嵌入模型成本低、速度快，但容易出现过拟合或泛化不足，也是治标不治本。&lt;/li>
&lt;/ul>
&lt;p>经过不断尝试总结，最终落脚点还是回到了大模型自身 &amp;ndash; 大模型的微调才是关键。&lt;/p>
&lt;p>什么是大模型微调？微调如何做？本篇开始，我们将为大家一步步揭开大模型微调的神秘面纱。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240314173850356.png" alt="image-20240314173850356">&lt;/p>
&lt;h1 id="1大语言模型的技术金字塔">1.大语言模型的技术金字塔&lt;/h1>
&lt;p>大语言模型相关技术可分为四层，其中：&lt;/p>
&lt;ul>
&lt;li>预训练的难度和成本最高，通常只有巨型公司(如谷歌、微软、OpenAI)才能承担其成本。&lt;/li>
&lt;li>提示词工程难度和成本最低，普通人就可以掌握。&lt;/li>
&lt;li>大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315141452986.png" alt="image-20240315141452986">&lt;/p>
&lt;p>以下是各层技术的详细阐述：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示词工程&lt;/strong>
&lt;ul>
&lt;li>技术原理：通过设计合适的提示或上下文来引导LLM生成期望的输出，侧重于用提示来激活预训练模型的能力，如总结摘要、翻译转换等。适用于文本处理、机器翻译等场景。&lt;/li>
&lt;li>技术特征：技术门槛低，终端用户即可掌握。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Agents&lt;/strong>：
&lt;ul>
&lt;li>技术原理：让LLM来决策一系列的动作，这些动作可以是让LLM分步解决问题，也可以是调用工具查询外部信息。这些动作形成一个工作流，最终完成任务目标。适用于相对复杂的用户交互应用，如智能客服。&lt;/li>
&lt;li>技术特征：有技术门槛，需要专业的LLM应用开发人员，并且了解大模型的基础原理，熟悉其领域的业务逻辑和流程。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>大模型微调&lt;/strong>：
&lt;ul>
&lt;li>技术原理：在预训练模型的基础上，针对特定任务的参数做调整。通过少量的数据训练即可提升模型在特定任务上的能力，同时保留模型原已学到的知识。适用于语义理解、垂域的各类应用。&lt;/li>
&lt;li>技术特征：技术门槛相对高，需要垂域有自己的LLM应用研发团队，具有数据处理、模型训练的能力和经验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>预训练技术&lt;/strong>：
&lt;ul>
&lt;li>技术原理：比如训练出一个&amp;quot;GPT3.5&amp;rdquo;，或者&amp;quot;GLM3.0&amp;rdquo;。&lt;/li>
&lt;li>技术特征：百G甚至千G的GPU资源需求，大量的大模型研究人员、数据科学家投入，超出了大部分公司的能力。一旦功成，效果显著。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>由于大模型微调的实用性，本专栏后续会展开介绍大模型微调技术。&lt;/p>
&lt;h1 id="2大模型微调的可行性及理论基础">2.大模型微调的可行性及理论基础&lt;/h1>
&lt;h2 id="1in-context-learning">(1)In-Context Learning&lt;/h2>
&lt;p>OpenAI在预训练过程中，发现LLM能够挖掘训练数据中的潜在特征和通用范式，进而习得训练数据之外的新能力。研究者把这一发现称之为In-Context Learning(基于上下文的学习)。&lt;/p>
&lt;p>基于In-Context Learning思想，衍生出了两种训练微调技术：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Prompt-tuning&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Instruction-tuning&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In-Context Learning的本质就是举一反三，基于已知的知识和训练数据可以挑战从没做过工作(下游任务)。预训练模型所蕴含的知识、预训练模型所使用的数据是广义上的&lt;code>context&lt;/code>。正因为In-Context Learning的有效性，所以对预训练模型进行微调是业界公认可行的技术路线。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315152229685.png" alt="image-20240315152229685">&lt;/p>
&lt;h2 id="2transfer-learning">(2)Transfer Learning&lt;/h2>
&lt;p>Transfer Learning(迁移学习)是人工智能发展过程中的一个重要思想，它的目标是将已经训练好的模型所包含的知识、推理能力迁移到未经训练的新模型上。比如对模型进行压缩、复用开源模型已经具备的能力来新模型，都是基于迁移学习思想。&lt;/p>
&lt;p>迁移学习包含很多具体的工程方法：&lt;/p>
&lt;ul>
&lt;li>Conservative Training&lt;/li>
&lt;li>Multi-task Learning&lt;/li>
&lt;li>Progressive Neural Network&lt;/li>
&lt;li>Domain-adversarial training&lt;/li>
&lt;li>Zero Shot Learning&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>无论有多少种工程方法，其核心思想是：&lt;strong>已训练的数据和任务与未训练的数据和任务存在因果、关联等逻辑关系&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315151715205.png" alt="image-20240315151715205">&lt;/p>
&lt;h2 id="3bert的优秀实践">(3)Bert的优秀实践&lt;/h2>
&lt;p>Bert充分发挥了Fine-tuning的技术优势和特点，在已经训练好的Bert模型基础上，加入少量的task-specific parameters。&lt;/p>
&lt;ul>
&lt;li>如分类任务，只需要在Bert模型上加一层softmax网络，然后对softmax网络进行微调。&lt;/li>
&lt;li>再如情感分析任务，取第一个token的输出表示，喂给一个softmax层得到分类结果输出。&lt;/li>
&lt;/ul>
&lt;p>微调Bert之所以成功，其本质原因是由于In-context Learning和Transfer Learning的有效性。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315153411365.png" alt="image-20240315153411365">&lt;/p>
&lt;h1 id="3大模型微调的技术全景图">3.大模型微调的技术全景图&lt;/h1>
&lt;h2 id="1大模型微调技术全景">(1)大模型微调技术全景&lt;/h2>
&lt;p>大模型微调技术从大的流派上可分为两类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Full Fine-Tuning&lt;/strong>：简称FFT，全量微调，即全参数微调。&lt;/li>
&lt;li>&lt;strong>Parameter-Efficient Fine-Tuning&lt;/strong>: 简称PEFT，高效微调，即通过某些技术手段选择部分参数微调。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315160640710.png" alt="image-20240315160640710">&lt;/p>
&lt;p>与FFT相比，PEFT是当前业界主流的技术路线，其原因主要是FFT存在如下缺陷：&lt;/p>
&lt;ul>
&lt;li>训练成本过高&lt;/li>
&lt;li>灾难性遗忘&lt;/li>
&lt;/ul>
&lt;h2 id="2peft技术分支">(2)PEFT技术分支&lt;/h2>
&lt;p>PEFT从微调目标方面可分为两类：&lt;/p>
&lt;ul>
&lt;li>Supervised Fine-Tuning: 简称SFT，属于有监督的微调。&lt;/li>
&lt;li>Reinforce Learning Human Feedback: 简称RLHF，属于利用人类反馈的强化学习。&lt;/li>
&lt;/ul>
&lt;p>其中SFT又具有诸多工程实践，因此产生了很多技术分支：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Additive&lt;/strong>: 增量派，在原有模型上增加额外小模型和少量参数。&lt;/li>
&lt;li>&lt;strong>Selective&lt;/strong>: 选择派，从原有模型的海量参数中，选择与下游任务相关的少量参数。&lt;/li>
&lt;li>&lt;strong>Reparametrization-based&lt;/strong>: 数学派，基于重参数化方法，将原有模型参数低秩化，获得小参数矩阵。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315161446294.png" alt="image-20240315161446294">&lt;/p>
&lt;p>在上述流派中，每个流派都有自己的代表方法，目前在业界广泛使用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sparse Adapter&lt;/strong>: 稀疏适配器。新增小模型，并从原有大模型中选择一部分参数，对这两部分参数进行微调。&lt;/li>
&lt;li>&lt;strong>Prefix-Tuning&lt;/strong>: 模块化轻量微调。增加prefix模块(Prefix模块会在用户输入前增加虚拟token)，训练prefix模块的参数。&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>: Low-Rank Adaption，低秩适配微调。它是目前业界在大语言模型、大视觉模型、多模态模型微调中的热门技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315162656941.png" alt="image-20240315162656941">&lt;/p>
&lt;p>上述技术分支的关系错综复杂，我们可以抽象的概括一下它们的技术思想：&lt;/p>
&lt;ul>
&lt;li>基于数据的技术分支：其核心思想是对进入预训练模型的训练语料进行前置处理。
&lt;ul>
&lt;li>如soft prompts中的Prompt-Tuning、Prefix-Tuning、P-Tuning，都是属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>基于模型的技术分支：其核心思想是在预训练模型基础上增加额外的模型分层，仅针对增量模型的参数和预训练模型的少量参数进行微调和变换。
&lt;ul>
&lt;li>如Adapter模型属于此技术分支。&lt;/li>
&lt;li>如LoRA、QLoRA、AdaLoRA也属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文从宏观上介绍了大模型微调技术的全景图，重点梳理了PEFT微调技术的众多技术分支，旨在帮助大家了解大模型微调技术的全貌。本专栏后续文章会展开阐述PEFT的各类微调技术。&lt;/p>
&lt;ul>
&lt;li>大语言模型的技术金字塔
&lt;ul>
&lt;li>大语言模型相关技术可分为四层：提示词工程、Agents、大模型微调、预训练技术。&lt;/li>
&lt;li>预训练的难度和成本最高；提示词工程难度和成本最低；大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的可行性及理论基础
&lt;ul>
&lt;li>In-Context Learning&lt;/li>
&lt;li>Transfer Learning&lt;/li>
&lt;li>Bert的优秀实践&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的技术全景图
&lt;ul>
&lt;li>Full Fine-Tuning&lt;/li>
&lt;li>Parameter-Efficient Fine-Tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记41-多模态-Sora浅析</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</link><pubDate>Mon, 26 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</guid><description>&lt;p>Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。&lt;/p>
&lt;p>本篇基于OpenAI发布的技术报告对Sora的技术特点和原理进行解读。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226144755598.png" alt="image-20240226144755598">&lt;/p>
&lt;h1 id="1sora是什么">1.Sora是什么？&lt;/h1>
&lt;p>Sora是一个文生视频的AI模型，可以根据文本信息生成真实且富有想象力的视频内容。主要特点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>以自然语言为输入(提示词)，生成符合提示词描述的视频&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>可生成1分钟内容连贯的视频，视频尺寸/分辨率可调整，目前只有Sora做到&lt;/strong>。其它模型只能生成4秒以内、256x256固定尺寸的视频。&lt;/li>
&lt;li>&lt;strong>真实世界的模拟器&lt;/strong>，不仅理解物理实体(如人、猫、狗、&amp;hellip;)，还懂得物理规律(如光照、碰撞、粒子、&amp;hellip;)。&lt;/li>
&lt;/ul>
&lt;p>Sora出道即颠峰，让一众顶级文生视频模型望尘莫及。我们来直观感受下Sora生成的视频的震撼效果：&lt;/p>
&lt;p>提示词如下：&lt;/p>
&lt;blockquote>
&lt;p>Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field.&lt;/p>
&lt;/blockquote>
&lt;p>视频如下：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=1301071173&amp;bvid=BV1zu4m1c7gg&amp;cid=1452188766&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="2sora原理浅析和技术优势">2.Sora原理浅析和技术优势&lt;/h1>
&lt;p>OpenAI没有公开Sora的模型细节，本文后续分析依据OpenAI的&amp;quot;Technic Report&amp;quot;推测所得。&lt;/p>
&lt;h2 id="21sora是否采用了新的模型架构技术">2.1.Sora是否采用了新的模型架构&amp;amp;技术？&lt;/h2>
&lt;p>答案是没有。&lt;/p>
&lt;p>文生视频可能涉及的模型架构如下：&lt;/p>
&lt;ul>
&lt;li>RRN (循环神经网络)&lt;/li>
&lt;li>GAN (生成式对抗网络)&lt;/li>
&lt;li>自回归Transformer&lt;/li>
&lt;li>Diffusion (扩散模型)&lt;/li>
&lt;/ul>
&lt;p>Sora采用的是：&lt;/p>
&lt;ul>
&lt;li>自回归Transformer&lt;/li>
&lt;li>Diffusion (扩散模型)&lt;/li>
&lt;/ul>
&lt;h2 id="22sora对现有模型架构技术的创新">2.2.Sora对现有模型架构&amp;amp;技术的创新&lt;/h2>
&lt;p>Sora架构是结合了Diffusion扩散模型和Transformer架构的创新设计。&lt;/p>
&lt;h3 id="1用transformer架构学习视频特征">(1)用Transformer架构，学习视频特征&lt;/h3>
&lt;p>灵感来自于Tranformer架构在GPT中的成功应用，OpenAI把自然语言的特征表示方法引入到了视频处理中：&lt;/p>
&lt;ul>
&lt;li>通过编码器，把视频的每一帧转换为有时序的向量，若干帧形成了&lt;strong>向量矩阵&lt;/strong>，Sora称之为&lt;strong>Turning visual data into patches&lt;/strong>。(大语言模型是把文字转换成一个个token，Sora则是把视频转换成一个个patch。)&lt;/li>
&lt;li>与大语言模型中token线性序列不同，由patch组成的高维向量矩阵包含了时序、分辨率、高宽比等丰富信息。&lt;/li>
&lt;li>和大语言模型一样(文本信息压缩网络)，Sora模型本质是一个视频压缩网络&lt;strong>Video compression network&lt;/strong>，使用该视频压缩网络把高维向量矩阵压缩成单维向量序列。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240223161152675.png" alt="image-20240223161152675">&lt;/p>
&lt;h3 id="2用结合了transformer的diffusion模型学习还原视频的特征">(2)用结合了Transformer的Diffusion模型，学习还原视频的特征&lt;/h3>
&lt;h4 id="--什么是扩散模型diffusion-model">- 什么是扩散模型Diffusion Model&lt;/h4>
&lt;p>Diffusion Model的基本原理是将原始图片逐渐加入噪声(Noise)，让原本清晰的图片逐渐变成全是噪声的状态。&lt;/p>
&lt;p>Diffusion Model主要有两个过程：前向处理和反向处理。图解如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>前向处理&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>AI训练过程中的1次噪声扩散&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226131740185.png" alt="image-20240226131740185">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AI训练过程中，进行N次噪声扩散&lt;/p>
&lt;ul>
&lt;li>进行N次噪声扩散，将图片变成全是噪点的图像&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132019285.png" alt="image-20240226132019285">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>后向处理&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>AI训练过程中，进行N次噪声降噪，变成清晰图片&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132143014.png" alt="image-20240226132143014">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>经过如上扩散处理，我们得到了&lt;strong>噪声与图像的关系。&lt;/strong>&lt;/p>
&lt;p>利用transformer模型，既可以理解噪声的特性向量，也可以理解自然语言的特征向量，那么就可以建立&lt;strong>噪声与提示词的关系&lt;/strong>。基于这种机制，模型就能够根据提示词来生成图片了。图片是视频的一帧，文生视频也可以用同样的原理来生成。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240227084715255.png" alt="image-20240227084715255">&lt;/p>
&lt;h4 id="--sora的diffusion-transformer模型">- Sora的Diffusion Transformer模型&lt;/h4>
&lt;p>在Diffusion Model的基础上，引入Transformer技术方法：&lt;/p>
&lt;ul>
&lt;li>通过编码器，将每一帧噪声转换为向量，若干帧形成了&lt;strong>噪声向量矩阵&lt;/strong>，Sora称之为&lt;strong>Noisy patches&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104456184.png" alt="image-20240226104456184">&lt;/p>
&lt;ul>
&lt;li>Sora的创新点：&lt;strong>一次性生成&lt;/strong>Noise Vector Cude，用以保证帧与帧之间的逻辑关系。从而生成时间和空间更流畅、更连贯的视频。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104436506.png" alt="image-20240226104436506">&lt;/p>
&lt;h2 id="23sora的技术优势">2.3.Sora的技术优势&lt;/h2>
&lt;p>虽然OpenAI没有公布Sora模型的训练细节，但即使公布了，其他厂家可能也很难复制或追赶。&lt;/p>
&lt;h3 id="1sora的video-compression-network依托于强大的算力">(1)Sora的Video compression network依托于强大的算力&lt;/h3>
&lt;p>模型成功的一个重要因素是海量的训练数据，训练需要消耗大量算力。Sora采用高维向量矩阵模式的视频处理方式，意味着更大算力消耗。中信证券曾简单估算，一个6~8秒的视频（约60帧）需要约6万个Patches，如果去噪步数是20的话，相当于要生成120万个Tokens，这是相当大的计算量。那么Sora生成1分钟视频需要的算力可想而知。&lt;/p>
&lt;p>同一个提示词，算力越高，生成视频效果越好。Sora给出基础算力、4倍算力、32倍算力下的效果展示：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1uC411s7y9&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1ey421q72G&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h3 id="2gpt4加速孵化多模态大模型多模态大模型反哺llm">(2)GPT4加速孵化多模态大模型，多模态大模型反哺LLM&lt;/h3>
&lt;ul>
&lt;li>遥遥领先的自然语言理解能力：
&lt;ul>
&lt;li>文生视频模型的训练离不开大量带有文字标注的视频，OpenAI基于GPT4专门训练一个高度描述性的字幕模型，使用它来为训练数据集中的所有视频生成文本字幕。这样进行训练极大的保证了文本保真度以及视频的整体质量。&lt;/li>
&lt;li>同时，GPT优秀的文本扩展能力，可以丰富用户输入的提示词，让文本描述更丰富、更细致，从而生成丰富、细腻的视频。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226141517838.png" alt="image-20240226141517838">&lt;/p>
&lt;h1 id="3sora有哪些创意玩法">3.Sora有哪些创意玩法&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>Animating DALL·E images&lt;/strong>：图生视频
&lt;ul>
&lt;li>左边是图片和提示词，右边是生成的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1pZ421y7RF&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Extending generated videos&lt;/strong>：视频续写
&lt;ul>
&lt;li>基于原视频，向前或向后续写新的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1oK42187Kc&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Video-to-video editing&lt;/strong>：编辑视频
&lt;ul>
&lt;li>左边是原视频，右边是根据提示词修改的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1Vu4m1w7QY&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Connecting videos&lt;/strong>：连接视频
&lt;ul>
&lt;li>通过左侧视频和右侧视频，生成中间过渡视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV19w4m1f7dr&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Image generation capabilities&lt;/strong>：生成高质量图片
&lt;ul>
&lt;li>生成各种尺寸的图片，分辨率最高可达 2048x2048&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226173623135.png" alt="image-20240226173623135">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>卓越的仿真能力&lt;/strong>：生成现实世界仿真视频&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>3D consistency&lt;/strong>：空间一致性&lt;/p>
&lt;ul>
&lt;li>模拟摄像机镜头旋转和运行，画面中的人和场景元素在三维空间中也一致的移动。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV11K421t75e&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Long-range coherence and object permanence&lt;/strong>：时间一致性
&lt;ul>
&lt;li>Sora能够有效地为短期和长期依赖关系建模。例如，模型可以保存人物、动物和物体，即使它们被遮挡或离开了框架(下左视频)。同样，可以基于单个样本生成同一角色的多个镜头，并在整个视频中保持其外观(下右视频)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1Av421k7vD&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Interacting with the world&lt;/strong>：模拟真实世界的动作和结果
&lt;ul>
&lt;li>随着画笔的移动，画面中增加了花瓣；人吃汉堡，汉堡留下咬痕。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1vF4m157Hm&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="4sora对视频相关领域的影响">4.Sora对视频相关领域的影响&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>内容创作与媒体行业&lt;/strong>：&lt;/p>
&lt;p>在内容创作领域，Sora将缩短创作周期，降低制作成本。&lt;/p>
&lt;p>导演、视频编辑等岗位将直面Sora的影响。&lt;/p>
&lt;p>抖音类短视频App是否会面临挑战？&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>教育和培训：&lt;/strong>
教育工作者可以利用Sora创建生动的教学材料，提高课程互动性和学习效果。
但Sora仍然面临垂域微调的难题——我们尝试生成一个排序算法的Demo，得到的视频不尽如人意。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>营销和广告&lt;/strong>
广告设计师和品牌经理可以通过Sora来快速生成视频广告内容。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>游戏开发与动画制作&lt;/strong>
游戏设计师和动画制作人员可能会使用Sora快速原型制作和动画创建。
这不仅能够加快项目开发速度，还能使得复杂场景的测试和迭代变得更为高效。
不过，这也可能引发对传统动画和建模技艺的重新评估。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>上下游基础设施：&lt;/strong>
上游厂商：AI服务器、AI芯片、通信行业、云厂商。
下游应用：大量的短/中/长视频应用和服务需求。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="5参考">5.参考&lt;/h1>
&lt;blockquote>
&lt;p>&lt;a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记40-LLM应用-如何构建RAG数据集</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</link><pubDate>Mon, 05 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</guid><description>&lt;p>2023年是基础大模型的爆发元年，专家预测2024年将是AI应用的爆发元年。&lt;/p>
&lt;p>因此，本专栏希望通过一系列文章，和大家探讨AI应用的规划、落地、实践等问题：&lt;/p>
&lt;blockquote>
&lt;p>如何在千行百业寻找AI应用的落地点？如何用AI为客户带来真实的价值？&lt;/p>
&lt;p>如何开发高质量AI应用？如何评估和控制AI应用的开发成本？&lt;/p>
&lt;/blockquote>
&lt;p>RAG是目前AI应用落地的主要技术领域，本文首先来探讨RAG相关的产品实践。&lt;/p>
&lt;h1 id="1开发ai应用前我们应该考虑什么">1.开发AI应用前，我们应该考虑什么？&lt;/h1>
&lt;h2 id="1我们真的需要ai应用吗">(1)我们真的需要AI应用吗？&lt;/h2>
&lt;p>相信大家有一种感觉：AI很强大，但问及AI解决什么具体的行业问题？又无从回答。&lt;/p>
&lt;p>这是一个产品规划问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>行业痛点&lt;/strong>：我的行业领域有什么Gap点？&lt;/li>
&lt;li>&lt;strong>业务设计&lt;/strong>：AI能解决这些Gap点吗？&lt;/li>
&lt;li>&lt;strong>价值呈现&lt;/strong>：这些问题解决后能给客户带来什么价值？&lt;/li>
&lt;/ul>
&lt;p>我们不能叶公好龙式地迷失在AI浪潮中，还是要落脚于&lt;strong>解决客户痛点、呈现产品价值&lt;/strong>。&lt;/p>
&lt;p>以&lt;strong>Khan Academy&lt;/strong>在AI赋能教育的实践为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Khan Academy是一家具备全球影响力的在线教育平台，愿景是为提供&lt;strong>普惠教育&lt;/strong>(为无法享受基础教育的学生提供覆盖学科广泛、内容专业的在线课程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>行业痛点&lt;/strong>：Khan Academy的普惠教育理想，意味着既要&lt;strong>高教学质量、千人千面&lt;/strong>，又要&lt;strong>控制教学成本&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>业务设计&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>怎么学&lt;/strong>：通过AI对不同学生的画像，自动化(成本)、专业化(高质量)、针对性(千人千面)地给出学习地图。&lt;/li>
&lt;li>&lt;strong>学什么&lt;/strong>：通过AI自主挖掘发现学习资源，自动化(成本)、针对性(千人千面)地为学生推荐学习资源。&lt;/li>
&lt;li>&lt;strong>监督引导&lt;/strong>：通过AI，自动化(成本)、有耐心(高质量)地监督与调整学生进度及学习习惯。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>价值呈现&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>自主学习、个性化学习是普惠教育的基石。&lt;/li>
&lt;li>&lt;strong>AI从教学质量和成本上使得自主学习、个性化学习成为了可能&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206095448606.png" alt="image-20240206095448606">&lt;/p>
&lt;h2 id="2解决问题的手段只有锤子吗">(2)解决问题的手段只有锤子吗？&lt;/h2>
&lt;p>AI智能问答是一把锤子(也是目前国内AI应用的主要形态)，但它只是AI能力的很小一部分。&lt;/p>
&lt;p>为了更好地选择合适的AI能力解决客户问题，我们需要理解AI能力的全集。&lt;/p>
&lt;p>AI能力可以分为&lt;strong>基础能力&lt;/strong>和&lt;strong>综合能力&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础能力&lt;/strong>：AI的听说读写能力，这些能力可以让AI将&amp;quot;文字&amp;rdquo;、&amp;ldquo;语音&amp;rdquo;、&amp;ldquo;图像及视频&amp;quot;进行相互转换。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240203083813459.png" alt="image-20240203083813459">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>综合能力&lt;/strong>：综合能力依托于AI基础能力，进而解决客户的业务问题。
&lt;ul>
&lt;li>&lt;strong>RAG能力&lt;/strong>：AI可以通过外挂形式，进行垂域知识问答。&lt;/li>
&lt;li>&lt;strong>ReAct能力&lt;/strong>：可以发挥AI具备的一定的推理能力，分解任务，自动执行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206101931437.png" alt="image-20240206101931437">&lt;/p>
&lt;p>面向不同类型的客户问题，我们选择的AI能力不同：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>检索类问题适合采用RAG&lt;/strong>。
&lt;ul>
&lt;li>如：了解工作流程、学习专业知识等检索类客户问题，适合使用AI的RAG综合能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>任务执行类问题适合采用ReAct&lt;/strong>。
&lt;ul>
&lt;li>如：挖掘海量互联网学习资源、根据学生学习情况监督调整学习计划等任务执行类问题，适合采用ReAct综合能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="2如何构建rag数据集">2.如何构建RAG数据集？&lt;/h1>
&lt;p>在确定采用RAG技术解决客户问题后，构建高质量的RAG数据集是RAG成功的关键(否则就是&amp;quot;垃圾进，垃圾出&amp;quot;的结果)。&lt;/p>
&lt;p>我们接下来讨论如何构建高质量RAG数据集。&lt;/p>
&lt;h2 id="21问题分类">2.1.问题分类&lt;/h2>
&lt;p>在制作数据集前，要理清垂域中的&lt;strong>垂域用户问题&lt;/strong>和&lt;strong>垂域知识&lt;/strong>的&lt;strong>特点&lt;/strong>。&lt;/p>
&lt;p>垂域用户问题有如下特点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>高频问题易识别&lt;/strong>: 常见问题和重复性问题出现的频率较高，垂域专家清楚高频问题是什么。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>针对性强&amp;amp;辨识度高&lt;/strong>：通常针对特定的知识进行提问，问题描述包含垂域专业术语，不闲聊。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>兼具广度和深度&lt;/strong>：可能是问结构纲领级的问题，也可能是针对某个具体细节的问题。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>垂域知识有以下特点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>独特性&lt;/strong>：垂域知识有其独特的知识体系和术语，需要具备一定的专业背景和知识储备才能理解和应用。&lt;/li>
&lt;li>&lt;strong>结构明确&lt;/strong>：垂域知识通常有明确的结构和层次，各层知识间不存在重复和二义性。垂域知识更新很快，但知识结构变化相对小。&lt;/li>
&lt;/ul>
&lt;p>基于以上特点，可以将垂域用户向AI提问的&lt;strong>问题类型分为2类&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高频问题&lt;/strong>：适合采用FAQ问答技术，一则可使&lt;strong>答案准确可控&lt;/strong>，二则减少AI的资源消耗。&lt;/li>
&lt;li>&lt;strong>知识问答&lt;/strong>：适合采用文档问答技术，一则文档所覆盖的垂域知识深度与广度兼备，二则构建成本相对较低。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206103355976.png" alt="image-20240206103355976">&lt;/p>
&lt;h2 id="22faq问答数据集构建">2.2.FAQ问答数据集构建&lt;/h2>
&lt;h3 id="1关注构建成本">(1)关注构建成本&lt;/h3>
&lt;p>FAQ问答数据数据集构建时，可能存在一种误区：&lt;strong>问题不够加问题&lt;/strong>？&lt;/p>
&lt;ul>
&lt;li>问题不是不够，而是AI的理解能力有限，无法理解同一个问题五花八门的表达形式。&lt;/li>
&lt;li>问题不够加问题，本质就是穷举问题表达形式，但自然语言的表达形式穷举成本极高甚至不可穷举，因此会导致数据集构建成本不可承受。&lt;/li>
&lt;/ul>
&lt;p>如果上述方法行不通，则说明我们需要：用&lt;strong>有限的问答对&lt;/strong>覆盖&lt;strong>多样的问题表达形式&lt;/strong>，进而才能控制FAQ数据集的构建成本。&lt;/p>
&lt;p>如下是我们的工程实践：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从知识的覆盖度上构建问答对，而不是从问题的表达多样性上构建问答对——这样&lt;strong>问答对的数量就是有限的&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>覆盖多样的问题表达形式，可以采用关键词、扩展问等技巧。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这个过程必须投入业务专家，根据上述两点工程实践进行把关和输出，业务专家需要保证。&lt;/p>
&lt;ul>
&lt;li>每个问题要保证&lt;strong>语义唯一性&lt;/strong>。&lt;/li>
&lt;li>所有问题构成的语义集合要保证&lt;strong>业务覆盖性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h3 id="2关注关键词提取">(2)关注关键词提取&lt;/h3>
&lt;p>关于前文提到的关键词技巧，也存在一个误区：直接将问题中的主谓宾作为关键词。&lt;/p>
&lt;ul>
&lt;li>这里举个例子：&lt;code>Java&lt;/code>是&amp;quot;如何提高Java的调试与定位能力？&amp;ldquo;这个问题的关键词吗？&lt;/li>
&lt;li>显然，从自然语言角度&lt;code>Java&lt;/code>是关键词，但从业务角度&lt;code>Java&lt;/code>不是关键词。
&lt;ul>
&lt;li>因为用户对Java领域的常见问题，都会带有Java这个单词。&lt;/li>
&lt;li>如果将Java作为本问题的关键词，那么所有的垂域问题都会被AI认为与本问题有关。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们的实践经验是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>每个关键词必须具备&lt;strong>独特性&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>上述例子中，&lt;code>Java&lt;/code>没有独特性，失去了特征，而&lt;code>调试与定位能力&lt;/code>作为这个问题的关键词更为合适，因其具有独特性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>关键词集合必须具备&lt;strong>丰富性&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>从业务角度，&amp;ldquo;调试与定位能力&amp;rdquo;，也有&amp;quot;调试定位&amp;rdquo;、&amp;ldquo;调试与定位&amp;quot;这种&lt;strong>惯用语&lt;/strong>，因此关键词集合可以丰富为[&amp;ldquo;调试与定位能力&amp;rdquo;, &amp;ldquo;调试与定位&amp;rdquo;, &amp;ldquo;调试定位&amp;rdquo;]。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="23文档问答数据集构建">2.3.文档问答数据集构建&lt;/h2>
&lt;h3 id="1关注文档质量">(1)关注文档质量&lt;/h3>
&lt;p>文档问答是AI对垂域文档进行学习理解(向量化)。这个过程类似老师(AI工程师)教小孩(AI)学习一本教材(垂域文档)。&lt;/p>
&lt;p>需要充分理解AI的特点(&lt;strong>因材施教&lt;/strong>)，设计出AI更容易理解垂域文档(&lt;strong>好教材&lt;/strong>)，是构建文档问答数据集的关键技术。&lt;/p>
&lt;p>构建垂域文档，有如下实践：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>知识组织形式&lt;/strong>会影响召回率：
&lt;ul>
&lt;li>通过实测和尝试，将垂域知识构建为树状结构，比较易于当前国内LLM理解和学习。&lt;/li>
&lt;li>不要在垂域知识树的节点之间产生关联关系形成有向图，可以通过搬移树节点的形式用线性的形式表达知识节点的关系。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206113907749.png" alt="image-20240206113907749">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>每个知识块节点的&lt;strong>粒度适中&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>知识块不宜过大&lt;/strong>：将整个垂域文档设计为一个知识块，会导致问啥问题都返回这个知识块。&lt;/li>
&lt;li>&lt;strong>知识块不宜过小&lt;/strong>：将垂域文档设计为一句话一个知识块，会导致知识点太碎，知识点之间存在复杂的逻辑关联，远超出现有LLM的推理能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>每个知识块节点内容&lt;strong>避免重复&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个知识块节点内容&lt;strong>避免矛盾&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="24ai辅助构建数据集">2.4.AI辅助构建数据集&lt;/h2>
&lt;p>前述FAQ数据集构建、文档问答数据集构建的过程，都可以采用AI、自动化工具等方式辅助构建。&lt;/p>
&lt;p>我们会在后续文章中分享AI辅助构建数据集的工程方法与实践。&lt;/p>
&lt;p>但，无论采用怎样的数据集构建过程，还是要关注数据集的内容本身，是否满足上述工程实践的要求和原则。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>构建RAG数据集，需要考虑一系列实践方法，建立标注规范，确保数据的质量和有效性。具体实践经验如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>AI应用的规划与设计：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>要用产品规划方法进行业务设计，落脚于&lt;strong>解决客户痛点、呈现产品价值&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>要选择合适的AI能力解决业务问题，可用的AI能力包括&lt;strong>基础能力(听说读写)&lt;strong>和&lt;/strong>综合能力(RAG、ReAct)&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>构建RAG数据集方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题分类：&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>高频问题&lt;/strong>：适合采用FAQ问答技术，一则可使&lt;strong>答案准确可控&lt;/strong>，二则减少AI的资源消耗。&lt;/li>
&lt;li>&lt;strong>知识问答&lt;/strong>：适合采用文档问答技术，一则文档所覆盖的垂域知识深度与广度兼备，二则构建成本相对较低。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>FAQ问答数据集构建方法：
&lt;ul>
&lt;li>每个问题要保证&lt;strong>语义唯一性&lt;/strong>。&lt;/li>
&lt;li>所有问题构成的语义集合要保证&lt;strong>业务覆盖性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>文档问答数据集构建方法：
&lt;ul>
&lt;li>&lt;strong>知识组织形式&lt;/strong>会影响召回率：将垂域知识构建为树状结构，可以通过搬移树节点的形式用线性的形式表达知识节点的关系。&lt;/li>
&lt;li>每个知识块节点的&lt;strong>粒度适中&lt;/strong>：知识块不宜过大，也不宜过小。&lt;/li>
&lt;li>每个知识块节点内容&lt;strong>避免重复&lt;/strong>。&lt;/li>
&lt;li>每个知识块节点内容&lt;strong>避免矛盾&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可采用AI辅助构建数据集，但关键还是&lt;strong>要关注数据集的内容本身符合上述工程实践的要求和原则&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记39-LangChain解读-LCEL语言之领域功能(5)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD5/</link><pubDate>Thu, 01 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD5/</guid><description>&lt;p>在LLM应用程序中，不可避免的会存在大量可知或不可知的故障点，比如模型API调用异常、链组合集成的问题、自定义的组件运行出错等。如果针对这些情况准备了fallback备用方法，就可以让程序更稳健。&lt;/p>
&lt;p>本篇我们来看下在LCEL中如何实现&amp;quot;fallback&amp;rdquo;。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202010923281.png" alt="image-20240202010923281">&lt;/p>
&lt;h1 id="1-lcel的fallback方法">1. LCEL的Fallback方法&lt;/h1>
&lt;p>LCEL在runnable中定义了fallback方法&lt;code>withfallback&lt;/code>，runnable组件及其组成的链都可以直接使用这个方法。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>当调用方出现异常，则执行fallback中预置的备用方法。&lt;/li>
&lt;li>通常用于预防LLM API错误，也可用于其它runnable组件，或者工作链。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>withfallbacks&lt;/code>的第一个参数是&lt;strong>备用方法&lt;/strong>，放在方括号&lt;code>[]&lt;/code>中。&lt;/li>
&lt;li>第二个参数&lt;code>exceptions_to_handle&lt;/code>为可选，用于&lt;strong>指定要处理的错误&lt;/strong>——当发生这些错误时才执行备用方法。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> runnable对象.withfallback&lt;span class="o">(&lt;/span>
&lt;span class="o">[&lt;/span>备用方法&lt;span class="o">]&lt;/span>,
&lt;span class="nv">exceptions_to_handle&lt;/span>&lt;span class="o">=(&lt;/span>KeyboardInterrupt, ...,&lt;span class="o">)&lt;/span>
&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以智能客服场景为例，模拟LLM模型发生异常的情况，使用fallback来预防这些错误，提升用户体验。&lt;/p>
&lt;h2 id="step1-构建故障链">STEP1: 构建故障链&lt;/h2>
&lt;ul>
&lt;li>构造一个工作链，专门负责回答物流问题。通过设置不存在的模型来模拟模型故障。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003427497.png" alt="image-20240202003427497">&lt;/p>
&lt;ul>
&lt;li>运行链，返回关于模型的错误信息。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003628125.png" alt="image-20240202003628125">&lt;/p>
&lt;h2 id="step2-构建备用链">STEP2: 构建备用链&lt;/h2>
&lt;ul>
&lt;li>构建默认回复链，该链使用正常模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003914611.png" alt="image-20240202003914611">&lt;/p>
&lt;h2 id="step3-构建fallback链">STEP3: 构建Fallback链&lt;/h2>
&lt;ul>
&lt;li>使用&lt;code>with_fallbacks&lt;/code>方法，构造工作链service_chain，把默认回复链作为物流链的后备方法。&lt;/li>
&lt;li>运行具有fallback能力service_chain，顺利得到回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003948900.png" alt="image-20240202003948900">&lt;/p>
&lt;ul>
&lt;li>除了上述基础用法，我们还可以指定fallback服务于特殊的异常类型。如下示例中，指定故障链一旦发生了KeyboardInterrupt异常，才执行备用链。&lt;/li>
&lt;li>具体如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202004341255.png" alt="image-20240202004341255">&lt;/p>
&lt;h2 id="step4-构建fallback链自定义备用方法">STEP4: 构建Fallback链：自定义备用方法&lt;/h2>
&lt;ul>
&lt;li>假设LLM模型全部失效，把默认回复链的模型也改成无效的模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005006014.png" alt="image-20240202005006014">&lt;/p>
&lt;ul>
&lt;li>自定义一个备用方法，来兜底保证给用户及时回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005208000.png" alt="image-20240202005208000">&lt;/p>
&lt;ul>
&lt;li>构造新的工作链new_service_chain，把兜底回复作为后备方法。&lt;/li>
&lt;li>运行链，顺利得到回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005233635.png" alt="image-20240202005233635">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL的fallback方法&lt;code>withfallbacks&lt;/code>，可以有效解决LLM API、模型输出不佳以及其他集成问题带来的问题。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>&lt;code>withfallbacks&lt;/code>方法&lt;/strong>：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LCEL在runnable中定义了fallback方法&lt;code>withfallbacks&lt;/code>，runnable组件及其组成的链都可以直接使用这个方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>withfallbacks&lt;/code>的第一个参数是&lt;strong>备用方法&lt;/strong>，第二个参数&lt;code>exceptions_to_handle&lt;/code>为可选，用于&lt;strong>指定要处理的错误&lt;/strong>——当发生这些错误时才执行备用方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使用上述方法，构建了一个具有fallback机制的客服问答工作链。模拟LLM模型发生异常时，通过预置的后备方法及时回复用户消息。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记38-LangChain解读-LCEL语言之领域功能(4)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD4/</link><pubDate>Sun, 21 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD4/</guid><description>&lt;p>langchain基础的链包括三个组件：提示词、LLM模型、输出解析器，但实际LLM应用开发会复杂很多，可能需要更多组件。&lt;/p>
&lt;p>本篇我们介绍LCEL提供的几个常用方法，可以帮助开发者构建自定义runnable组件，进而组合成功能更强大的链。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120130624759.png" alt="">&lt;/p>
&lt;h1 id="1-lcel组件构建方法">1. LCEL组件构建方法&lt;/h1>
&lt;p>LCEL在Runnable协议中定义了如下方法，帮助开发者灵活构建runnable组件：&lt;/p>
&lt;ul>
&lt;li>RunnablePassthrough&lt;/li>
&lt;li>RunnableLambda&lt;/li>
&lt;li>RunnableBranch&lt;/li>
&lt;/ul>
&lt;h2 id="1runnablepassthrough">(1)RunnablePassthrough:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>透传用户输入。通常与一个键组成键值对，用于构造下一个组件的输入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，prompt组件需要的输入是以topic为键的字典，但实际用户输入是字符串&amp;quot;袋鼠&amp;rdquo;。&lt;/li>
&lt;li>利用&lt;code>RunnablePassthrough&lt;/code>透传用户输入，并与topic组成键值对，构造了promt所需的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">prompt&lt;/span> &lt;span class="o">=&lt;/span> ChatPromptTemplate.from_template&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;请讲一个关于{topic}的笑话。&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="nv">output_parser&lt;/span> &lt;span class="o">=&lt;/span> StrOutputParser&lt;span class="o">()&lt;/span>
&lt;span class="nv">model&lt;/span> &lt;span class="o">=&lt;/span> ChatOpenAI&lt;span class="o">()&lt;/span>
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">{&lt;/span>&lt;span class="s2">&amp;#34;topic&amp;#34;&lt;/span>: RunnablePassthrough&lt;span class="o">()}&lt;/span> &lt;span class="p">|&lt;/span> prompt&lt;span class="p">|&lt;/span> model &lt;span class="p">|&lt;/span> output_parser
chain.invoke&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;袋鼠&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="2runnablelambda">(2)RunnableLambda:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>&lt;code>RunnableLambda&lt;/code>可以把自定义函数转换成Runnable对象，以便在组件中使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，自定义函数my_function，然后通过RunnableLambda(my_function)转换为runnable组件。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">def my_function&lt;span class="o">()&lt;/span>
...
&lt;span class="k">return&lt;/span> xxxx
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> RunnableLambda&lt;span class="o">(&lt;/span>my_function&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> 其它组件&lt;span class="p">|&lt;/span> ...
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="3runnablebranch">(3)RunnableBranch&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>类似路由功能，可以根据条件选择要运行的分支，即当条件满足时，则执行该分支。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，&lt;code>RunnableBranch&lt;/code>的参数是&amp;quot;条件、分支&amp;quot;对 + 默认分支。&lt;/li>
&lt;li>当某个条件满足时，则执行对应分支；如果条件都不满足，则执行默认分支。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">branch&lt;/span> &lt;span class="o">=&lt;/span> RunnableBranch&lt;span class="o">(&lt;/span>
&lt;span class="o">(&lt;/span>条件1, 分支1&lt;span class="o">)&lt;/span>,
...,
&lt;span class="o">(&lt;/span>条件n, 分支n&lt;span class="o">)&lt;/span>,
默认分支,
&lt;span class="o">)&lt;/span>
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> branch &lt;span class="p">|&lt;/span> 其它组件&lt;span class="p">|&lt;/span> ...
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以智能客服场景为例，我们使用上述三个方法来组成一个相对复杂的工作链：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>利用LLM判断问题类别、感情色彩&lt;/p>
&lt;/li>
&lt;li>
&lt;p>自定义函数对LLM输出结果格式化，以便后续组件使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据问题类别，构造不同的客服分支，组建客服路由链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>组成完整工作链，根据问题类别回答问题&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="step1-构建问题分类链">STEP1: 构建问题分类链&lt;/h2>
&lt;ul>
&lt;li>构建问题分类链topic_chain，任务是判断问题属于&amp;quot;产品质量&amp;rdquo; or &amp;ldquo;物流&amp;rdquo;，情绪是&amp;quot;积极&amp;rdquo;、&amp;ldquo;消极&amp;rdquo; or &amp;ldquo;中立&amp;rdquo;。&lt;/li>
&lt;li>使用&lt;code>RunnablePassthrough&lt;/code>透传用户问题，并与&amp;quot;question&amp;quot;组成键值对，作为prompt组件的输入。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120115926742.png" alt="">&lt;/p>
&lt;h2 id="step2-自定义runnable组件格式化输出结果">STEP2: 自定义Runnable组件：格式化输出结果&lt;/h2>
&lt;ul>
&lt;li>LangChain自带的输出解析器&lt;code>StrOutputParser&lt;/code>输出的是字符串，但我们下个组件的输入要求是字典。所以自己写个函数format_func，把字符串转成字典格式。&lt;/li>
&lt;li>使用&lt;code>RunnableLambda&lt;/code>把自定义函数format_func转换成runnable，并作为组件加入到工作链service_chain中。&lt;/li>
&lt;li>使用相同的用户问题调用工作链service_chain，输出结果为字典。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120121343812.png" alt="">&lt;/p>
&lt;h2 id="step3-构建路由链由3个分支链组成物流产品和默认">STEP3: 构建路由链：由3个分支链组成(物流、产品和默认)&lt;/h2>
&lt;ul>
&lt;li>先创建3个分支链，用于回答产品质量、物流等不同类别的问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120122633665.png" alt="">&lt;/p>
&lt;ul>
&lt;li>利用&lt;code>RunnableBranch&lt;/code>构造客服路由链anwser_routing_chain：
&lt;ul>
&lt;li>当用户问题类别为产品质量时，走产品客服链prod_response&lt;/li>
&lt;li>当用户问题类别为物流时，走物流客服链logistic_response链&lt;/li>
&lt;li>当用户问题类别不是上面两类时，走默认链general_response&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120123827219.png" alt="">&lt;/p>
&lt;h2 id="step4-组成完整工作链">STEP4: 组成完整工作链&lt;/h2>
&lt;ul>
&lt;li>现在，把工作链service_chain增加客服路由组件，组成完整工作链。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120124737826.png" alt="">&lt;/p>
&lt;ul>
&lt;li>使用不同的问题来测试，不同类别的问题都得到了对应领域客服的回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120124807112.png" alt="">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL提供的几种常用方法，可以帮助开发者灵活构建自己的Runnable组件，以支撑开发复杂的LLM应用。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnablePassthrough: 透传用户输入。通常与一个键组成键值对，用于构造下一个组件的输入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnableLambda: 把自定义函数转换成Runnable对象，以便在组件中使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnableBranch: 类似路由功能，可以根据条件选择要运行的分支，即当条件满足时，则执行该分支。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>使用上述方法，构建了一个客服问答工作链，自动判断用户问题类别并路由到对应领域的客服。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记37-LangChain解读-LCEL语言之领域功能(3)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD3/</link><pubDate>Mon, 08 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD3/</guid><description>&lt;p>LCEL 可以轻松的构建复杂的链，并灵活调用它们，是因为LCEL基于&lt;code>Runnable&lt;/code>协议封装了一系列组合方法和调用接口。&lt;/p>
&lt;p>之前我们介绍了LCEL的核心调用接口&lt;code>invoke/ainvoke&lt;/code>、&lt;code>stream/astream&lt;/code>、&lt;code>batch/abatch&lt;/code>，本篇开始我们来学习LCEL的组合方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240105205805572.png" alt="">&lt;/p>
&lt;h1 id="1-lcel的组合原语">1. LCEL的组合原语&lt;/h1>
&lt;p>LCEL的主要组合原语有两种：**RunnableSequence(串行组合)**和 &lt;strong>RunnableParallel( 并行组合)&lt;/strong>。&lt;/p>
&lt;h2 id="1runnablesequence-串行组合">(1)RunnableSequence: 串行组合&lt;/h2>
&lt;p>&lt;strong>方法简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableSequence可以将多个链组成一个串行工作流，各链串行执行。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个链的输出是下个链的输入，最后一个链的输出就是该整个工作流的输出。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>组合语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableSequence的原始调用语法如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableSequence对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableSequence&lt;span class="o">(&lt;/span>&lt;span class="nv">first&lt;/span>&lt;span class="o">=&lt;/span>组件1, &lt;span class="nv">middle&lt;/span>&lt;span class="o">=[&lt;/span>组件2, 组件3, ...&lt;span class="o">]&lt;/span>, &lt;span class="nv">last&lt;/span>&lt;span class="o">=&lt;/span>组件n&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableSequence在构建串行工作流的示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">串行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableSequence&lt;span class="o">(&lt;/span>&lt;span class="nv">first&lt;/span>&lt;span class="o">=&lt;/span>chain1, &lt;span class="nv">middle&lt;/span>&lt;span class="o">=[&lt;/span>chain2, chain3, ...&lt;span class="o">]&lt;/span>, &lt;span class="nv">last&lt;/span>&lt;span class="o">=&lt;/span>chainn&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>LCEL实现了用管道符&lt;code>|&lt;/code>来做串行组合的连接符。我们之前的案例都是在用这种方式来做串行组合。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">串行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> chain1 &lt;span class="p">|&lt;/span> chain2 &lt;span class="p">|&lt;/span> ... &lt;span class="p">|&lt;/span> chainn&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="2runnableparallel-并行组合">(2)RunnableParallel: 并行组合&lt;/h2>
&lt;p>&lt;strong>方法简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>RunnableParallel可以把多个链组成并行工作流，各链并行执行。&lt;/li>
&lt;li>各链的输入是相同的，整个工作流的输出是用各链的输出构造的字典。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>组合语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableParallel的调用语法如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableParallel对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">({&lt;/span>&lt;span class="s2">&amp;#34;Key1&amp;#34;&lt;/span>: 组件1, &lt;span class="s2">&amp;#34;Key2&amp;#34;&lt;/span>: 组件2, ..., &lt;span class="s2">&amp;#34;Keyn&amp;#34;&lt;/span>: 组件n&lt;span class="o">})&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>由于&lt;code>Runnable&lt;/code>支持类型转换，所以也可以这样调用：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableParallel对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">(&lt;/span>&lt;span class="nv">Key1&lt;/span>&lt;span class="o">=&lt;/span>组件1, &lt;span class="nv">Key2&lt;/span>&lt;span class="o">=&lt;/span>组件2, ..., &lt;span class="nv">Keyn&lt;/span>&lt;span class="o">=&lt;/span>组件n&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableParallel的输出是字典格式，是由原键+组件执行结果组成的键值对，示例如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">{&lt;/span>&lt;span class="s1">&amp;#39;Key1&amp;#39;&lt;/span>: 组件1的结果, &lt;span class="s1">&amp;#39;Key2&amp;#39;&lt;/span>: 组件2的结果, ..., &lt;span class="s1">&amp;#39;Keyn&amp;#39;&lt;/span>: 组件n的结果&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableParallel构建工作流的示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">并行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">({&lt;/span>&lt;span class="s2">&amp;#34;Key1&amp;#34;&lt;/span>: chain1, &lt;span class="s2">&amp;#34;Key2&amp;#34;&lt;/span>: chain2, ..., &lt;span class="s2">&amp;#34;Keyn&amp;#34;&lt;/span>: chainn&lt;span class="o">})&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="3runnablesequence和runnableparallel的组合使用">(3)RunnableSequence和RunnableParallel的组合使用&lt;/h2>
&lt;p>遵循LCEL &lt;code>Runnable&lt;/code>协议的组件都可以使用上述两种方式组合。同时，由于组合成的链也是一个Runnable对象，这就意味着在一个链里可以混合使用RunnableSequence和RunnableParallel两种组合方式，进而实现了串并结合的工作流。&lt;/p>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>下面通过一个实例来看下RunnableSequence和RunnableParallel的用法。&lt;/p>
&lt;p>示例场景：利用LLM做病情摘要，然后提供治疗建议，最后把结果翻译成英文和日语。&lt;/p>
&lt;p>我们用4个链实现上述功能，然后用RunnableSequence和RunnableParallel组合成完整工作流。&lt;/p>
&lt;h2 id="step1-构建chain1病情简介">STEP1: 构建chain1：病情简介&lt;/h2>
&lt;ul>
&lt;li>引入相关类库，构建第一个链chain1，任务是根据疾病标题和患者年龄生成病情简介。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195544772.png" alt="">&lt;/p>
&lt;ul>
&lt;li>先执行chain1观察下输出结果，输出内容符合预期。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195656025.png" alt="">&lt;/p>
&lt;h2 id="step2-构建chain2治疗方案">STEP2: 构建chain2：治疗方案&lt;/h2>
&lt;ul>
&lt;li>构建第二个链chain2，任务是根据病情简介生成治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108202230067.png" alt="">&lt;/p>
&lt;ul>
&lt;li>执行chain2观察输出结果，LLM给出了治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195753143.png" alt="">&lt;/p>
&lt;h2 id="step3构建chain3chain4文本翻译">STEP3:构建Chain3、Chain4：文本翻译&lt;/h2>
&lt;ul>
&lt;li>构建Chain3、Chain4，任务是把指定的文本内容分别翻译成英文、日语&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108192549849.png" alt="">&lt;/p>
&lt;h2 id="step4-构建工作流链medical_chain组合chain1chain4">STEP4: 构建工作流链medical_chain：组合chain1~chain4&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>基于上面4个任务链，工作流设计为：首先生成病情简介，然后根据病情生成治疗方案，最后把治疗方案同时翻译成英语、日语。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>因此，工作流链medical_chain组合如下：&lt;/p>
&lt;ul>
&lt;li>chain1、chain2与后面的翻译链(chain3、chain4)是串行组合&lt;/li>
&lt;li>chain3、chain4使用RunnableParallel组合，把chain2的输出作为输入，并行执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108200421256.png" alt="">&lt;/p>
&lt;ul>
&lt;li>执行medical_chain观察结果，得到了英语和日语的治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108200458628.png" alt="">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL的两种主要的组合原语：**RunnableSequence(串行组合)**和 &lt;strong>RunnableParallel( 并行组合)&lt;/strong>。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>RunnableSequence&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>可以将多个链组成一个串行工作流，各链串行执行。支持用管道符&lt;code>|&lt;/code>组合。&lt;/li>
&lt;li>每个链的输出是下个链的输入，最后一个链的输出就是该整个工作流的输出。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RunnableParallel&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>可以把多个链组成并行工作流，各链并行执行。&lt;/li>
&lt;li>各链的输入是相同的，整个工作流的输出是用各链的输出构造的字典。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>用RunnableSequence和RunnableParallel构建了一个诊疗工作流。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记36-LangChain解读-LCEL语言之领域功能(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD2/</link><pubDate>Wed, 03 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD2/</guid><description>&lt;p>并发是LLM应用常见的使用场景，LCEL提供了非常简便的方式来实现并发处理。&lt;/p>
&lt;p>本篇我们就来学习LCEL这一核心特性——batch/abatch。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104090138975.png" alt="">&lt;/p>
&lt;h1 id="1-lcel并发接口-batchabatch">1. LCEL并发接口-batch/abatch&lt;/h1>
&lt;h2 id="1接口介绍">(1)接口介绍&lt;/h2>
&lt;p>batch和abatch是两个非常实用的接口，它们允许并发处理多个用户输入，从而提高程序的执行效率。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>batch&lt;/strong>: 开发LLM应用时，有可能需要同时处理多个用户输入(如：多个提示词)，batch内部通过线程池并行处理多个用户输入(每个线程都是在调用上一篇介绍的&lt;code>invoke&lt;/code>接口)，最终汇总结果。&lt;/li>
&lt;li>&lt;strong>abatch&lt;/strong>: batch的非阻塞接口。&lt;/li>
&lt;/ul>
&lt;h2 id="2接口语法">(2)接口语法&lt;/h2>
&lt;p>batch/abatch是LCEL在Runnable协议中定义的标准接口，LangChain的组件和链(Runnable对象)可以直接调用该方法。&lt;/p>
&lt;p>以链chain为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>chain.batch([&amp;ldquo;input1&amp;rdquo;, &amp;ldquo;input2&amp;rdquo;, &amp;hellip;])&lt;/p>
&lt;/li>
&lt;li>
&lt;p>await chain.abatch([&amp;ldquo;input1&amp;rdquo;, &amp;ldquo;input2&amp;rdquo;, &amp;hellip;])&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以利用LLM做试题解析为例，用batch/abatch功能来批量处理多个试题。&lt;/p>
&lt;ul>
&lt;li>构造任务链&lt;/li>
&lt;li>使用batch方法处理&lt;/li>
&lt;li>使用abatch方法处理&lt;/li>
&lt;/ul>
&lt;h2 id="step1-构造任务链">STEP1: 构造任务链&lt;/h2>
&lt;p>任务链的构造方法不再赘述，见下面代码。&lt;/p>
&lt;ul>
&lt;li>链的任务是判断题目是否正确。&lt;/li>
&lt;li>设置列表变量topics，存放3个题目。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084034633.png" alt="">&lt;/p>
&lt;h2 id="step2-使用batch方法处理">STEP2: 使用batch方法处理&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.batch(topics)&lt;/li>
&lt;li>执行效果：任务启动后，马上得到答案，没有因为问题多导致过多等待。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084838085.png" alt="">&lt;/p>
&lt;h2 id="step3-使用abatch方法处理">STEP3: 使用abatch方法处理&lt;/h2>
&lt;ul>
&lt;li>调用语法：await.chain.abatch(topics)&lt;/li>
&lt;li>执行效果：任务启动后，马上得到答案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084749830.png" alt="">&lt;/p>
&lt;h1 id="3batch-vs-invoke">3.Batch vs Invoke&lt;/h1>
&lt;p>看到这里，我们再来回顾一下本文的&lt;code>batch&lt;/code>和前文的&lt;code>invoke&lt;/code>的区别：&lt;/p>
&lt;ul>
&lt;li>&lt;code>batch&lt;/code>不仅实现了批量处理多个输入，而且是多线程并行调用LLM，很好的提升了处理效率。&lt;/li>
&lt;li>而&lt;code>invoke&lt;/code>更适合需要串行执行。&lt;/li>
&lt;/ul>
&lt;p>具体看前文的例子，我们发现耗时的差别如下：&lt;/p>
&lt;ul>
&lt;li>使用invoke处理单个题目，耗时1.13s&lt;/li>
&lt;li>使用batch处理3个题目，耗时1.3s&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104010153222.png" alt="">&lt;/p>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>Batch/abatch 是LCEL中用于实现并发请求的重要工具，也为开发人员带来高效、简洁的编程体验。&lt;/p>
&lt;p>本文关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>LCEL并发接口-batch/abatch&lt;/p>
&lt;ul>
&lt;li>输入为列表，内部实际是使用线程池执行器&lt;strong>并行&lt;/strong>执行invoke()。&lt;/li>
&lt;li>LangChain的Runnable对象可以直接调用batch/abatch方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>以让LLM来批改试卷为例，演示batch/abatch语法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记35-LangChain解读-LCEL语言之领域功能(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD1/</link><pubDate>Wed, 27 Dec 2023 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD1/</guid><description>&lt;p>LCEL是专门为方便开发LLM应用而设计的编程语言，它提供了一系列直观、好用的功能和语法。&lt;/p>
&lt;p>本篇我们来学习提升LLM应用体验的一个功能——stream流式输出。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227115947187.png" alt="image-20231227115947187">&lt;/p>
&lt;h1 id="1-lcel流式输出接口-stream">1. LCEL流式输出接口-stream&lt;/h1>
&lt;h2 id="1为什么要用流式输出">(1)为什么要用流式输出&lt;/h2>
&lt;p>在使用LLM应用时，用户总是期望立即得到答案，不管他的问题有多么复杂，或者让LLM生成的文本有多么长。&lt;/p>
&lt;p>&amp;ldquo;&lt;strong>流式输出&lt;/strong>&amp;ldquo;这时就派上用场了，它能够&lt;strong>使LLM在生成文本时逐步提供结果&lt;/strong>，而不是等到整个文本生成完成后再一次性返回给用户。&lt;/p>
&lt;p>流式输出的方式有以下几个优点：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>实时性&lt;/strong>：不用等待整个文本生成完毕再返回结果，减少响应时间，用户获得实时响应体验。&lt;/li>
&lt;li>&lt;strong>交互性&lt;/strong>：生成结果逐步提供，可匹配用户阅读速度，增强交互感。&lt;/li>
&lt;li>&lt;strong>节省资源和带宽&lt;/strong>：逐步返回生成的结果可以减少对网络带宽的需求。&lt;/li>
&lt;li>&lt;strong>支持长文本生成&lt;/strong>：生成长文本时无需担心超出内存限制。&lt;/li>
&lt;/ol>
&lt;p>总之，流式输出可以让用户得到更好的体验，让用户觉得你的LLM程序&amp;rdquo;&lt;strong>性能&lt;/strong>&amp;ldquo;很棒。&lt;/p>
&lt;h2 id="2流式输出接口的使用">(2)流式输出接口的使用&lt;/h2>
&lt;p>LangChain在Runnable协议中定义了流式输出方法&lt;strong>stream&lt;/strong>，以及它的异步方法&lt;strong>astream&lt;/strong>。&lt;/p>
&lt;p>LangChain的很多组件都是遵循Runnable的对象，可以直接调用stream/astream方法。以之前专栏多次构建过的链chain为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>chain.stream()&lt;/p>
&lt;/li>
&lt;li>
&lt;p>chain.astream()&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以让LLM生成一首诗为例，对比下直接输出、流式输出的差异效果。&lt;/p>
&lt;p>任务链的构建不再赘述，如下代码，我们构建了名为&lt;code>chain&lt;/code>的链，它的功能是根据输入的主题生成一首诗。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227125844993.png" alt="image-20231227125844993">&lt;/p>
&lt;h2 id="1使用invoke方法直接调用">(1)使用invoke方法直接调用&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.invoke(&amp;ldquo;犀牛&amp;rdquo;)&lt;/li>
&lt;li>执行效果：任务启动后，没有立即得到响应；大概6秒后，生成结果一次性闪现。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/invoke.gif" alt="invoke">&lt;/p>
&lt;h2 id="2使用stream方法流式输出">(2)使用stream方法流式输出&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.stream(&amp;ldquo;犀牛&amp;rdquo;)
&lt;ul>
&lt;li>注意：由于stream()是迭代输出，因此用for语句来循环遍历输出结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>执行效果：任务启动后，立即得到响应，以一定的字频逐行输出结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/stream.gif" alt="stream">&lt;/p>
&lt;ul>
&lt;li>如果想用异步方式astream，则调用语法如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227145757283.png" alt="image-20231227145757283">&lt;/p>
&lt;h2 id="3-对比总结-invoke-vs-stream">(3) 对比总结 Invoke vs Stream&lt;/h2>
&lt;p>对于一个交互式的LLM应用程序，stream流式输出的响应实时性和交互体验都更好。&lt;/p>
&lt;p>在 Python 中，没有名为&amp;quot;stream&amp;rdquo; 的内置方法，但是有类似于流式操作的机制，比如生成器(yield)、迭代器(iterator)。所以，如果没有LCEL提供的stream方法，而是要我们自己来实现的话，将是一件费脑筋的事。&lt;/p>
&lt;p>如下是不使用LCEL实现流式输出的示例代码，可以跟上面简单的一行方法调用对比下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227124359421.png" alt="image-20231227124359421">&lt;/p>
&lt;p>LangChain的大牛们用python实现了与Java &amp;ldquo;Stream API&amp;quot;类似的流式处理方法，极大的方便了开发人员。&lt;/p>
&lt;p>某种意义上来说，LangChain不仅仅是定义了一套Runnable协议，也是对Python语法的重构，构建了自己的语法——LCEL。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain LCEL语法中提升性能的一种方法——stream流式输出。关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>流式输出接口&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;流式输出&amp;quot;使LLM在生成文本时逐步提供结果，提升LLM应用程序的性能，让用户得到更好的体验。&lt;/li>
&lt;li>LangChain的Runnable对象可以直接调用stream/astream方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对比invoke直接输出和stream流式输出的效果差异。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记34-Show一下我们的语音克隆技术</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B034-show%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%AD%E9%9F%B3%E5%85%8B%E9%9A%86%E6%8A%80%E6%9C%AF/</link><pubDate>Sun, 24 Dec 2023 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B034-show%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%AD%E9%9F%B3%E5%85%8B%E9%9A%86%E6%8A%80%E6%9C%AF/</guid><description>&lt;p>小刚刚同学是小伙伴中的TTS专家，他训练的语音克隆模型已经初见雏形，口音、语速、情绪都还不错。&lt;/p>
&lt;p>小刚刚的AI模型克隆了他自己的声音之后为我们念出了如下文章，听到人工智能的声音，激动且开心，为小刚刚的AI儿子点赞！&lt;/p>
&lt;ul>
&lt;li>&lt;strong>这是AI的念稿的声音&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.bilibili.com/audio/au4209312?type=1">B站链接&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>如下为AI念的稿件&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>接下来，我给大家介绍一下TTS原理：&lt;/p>
&lt;h2 id="1-引言">1. 引言&lt;/h2>
&lt;p>文字到语音（TTS，Text-to-Speech）技术是将人类语言文本转换为人类语音输出的技术。随着人工智能、自然语言处理等技术的快速发展，TTS技术在智能语音助手、虚拟主播、教育、娱乐等领域得到了广泛应用。本文将介绍TTS技术的原理及其发展历程，并探讨其在未来的发展趋势。&lt;/p>
&lt;h2 id="2-tts原理">2. TTS原理&lt;/h2>
&lt;h3 id="21-语音合成">2.1 语音合成&lt;/h3>
&lt;p>语音合成是将文本转换为语音的过程，主要包括以下几个步骤：&lt;/p>
&lt;ol>
&lt;li>音素到状态的转换：将输入的音素序列转换为声道状态序列。&lt;/li>
&lt;li>声道状态到声码元的转换：将声道状态序列转换为声码元序列。&lt;/li>
&lt;li>声码元到语音的转换：将声码元序列转换为语音信号。&lt;/li>
&lt;/ol>
&lt;h3 id="22-语音合成模型">2.2 语音合成模型&lt;/h3>
&lt;p>目前主流的语音合成模型主要包括以下几种：&lt;/p>
&lt;ol>
&lt;li>参数模型：将语音合成看作是一个参数估计问题，通过训练模型来获得参数值。&lt;/li>
&lt;li>统计模型：基于统计学原理，通过概率模型来生成语音。&lt;/li>
&lt;li>深度学习模型：利用深度神经网络模型进行语音合成。&lt;/li>
&lt;/ol>
&lt;h3 id="23-声学模型">2.3 声学模型&lt;/h3>
&lt;p>声学模型是TTS技术中的关键部分，其主要任务是模拟人类听觉系统，通过声学模型可以计算出每个音素的声学特征，并将其用于语音合成。目前主流的声学模型包括线性预测编码（LPC）、高斯混合模型（GMM）等。&lt;/p>
&lt;h2 id="3-tts发展历程">3. TTS发展历程&lt;/h2>
&lt;p>TTS技术的发展历程可以分为以下几个阶段：&lt;/p>
&lt;ol>
&lt;li>基于规则的方法：早期的TTS技术采用基于规则的方法，通过手动设计规则来生成语音。&lt;/li>
&lt;li>基于模板的方法：基于模板的方法通过预先定义的语音模板来生成语音，效率较低。&lt;/li>
&lt;li>基于统计的方法：基于统计的方法采用概率模型来生成语音，效果较好，但需要大量的训练数据。&lt;/li>
&lt;li>基于深度学习的方法：基于深度学习的方法利用神经网络模型进行语音合成，效果最好，但需要大量的训练数据和计算资源。&lt;/li>
&lt;/ol>
&lt;h2 id="4-tts未来发展趋势">4. TTS未来发展趋势&lt;/h2>
&lt;p>随着人工智能、自然语言处理等技术的不断发展，TTS技术在未来将会呈现出以下发展趋势：&lt;/p>
&lt;ol>
&lt;li>更高的语音质量：通过改进声学模型和语音合成算法，提高语音质量。&lt;/li>
&lt;li>更自然的发音：通过改进语音合成算法，使生成的语音更加自然。&lt;/li>
&lt;li>更丰富的语言支持：通过扩大语言模型和语音合成模型的训练数据集，支持更多的语言。&lt;/li>
&lt;li>更广泛的应用：通过改进TTS技术，使其在更多的领域得到应用，如智能客服、智能家居等。&lt;/li>
&lt;/ol>
&lt;h2 id="5-结论">5. 结论&lt;/h2>
&lt;p>TTS技术是将文本转换为语音的技术，其原理主要包括语音合成、语音合成模型、声学模型等。随着人工智能、自然语言处理等技术的不断发展，TTS技术在未来将会呈现出更高的语音质量、更自然的发音、更丰富的语言支持和更广泛的应用等特点。&lt;/p></description></item><item><title>【chatGPT】学习笔记33-LangChain解读-LCEL语言之基础语法(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%952/</link><pubDate>Sat, 23 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%952/</guid><description>&lt;p>LCEL让我们可以用&lt;code>|&lt;/code>管道符把不同的组件或链组合起来，这都得益于这些组件都实现了Runnable接口。&lt;/p>
&lt;p>本篇我们就带着Runnable的概念继续学习下LCEL的基础用法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231222104906413.png" alt="image-20231222104906413">&lt;/p>
&lt;h1 id="1-lcel核心接口runnable">1. LCEL核心接口Runnable&lt;/h1>
&lt;h2 id="1runnable接口">(1)Runnable接口&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Langchain定义了“Runnable”协议，协议中实现了一系列方法，比如对管道运算符&lt;code>|&lt;/code>的支持、invoke方法等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这里介绍下Runnable定义的一组标准调用接口（其中后三个是前面三个异步方式）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#stream">&lt;code>stream&lt;/code>&lt;/a>：流式输出&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#invoke">&lt;code>invoke&lt;/code>&lt;/a>：基于单一输入调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#batch">&lt;code>batch&lt;/code>&lt;/a>：基于列表输入调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-stream">&lt;code>astream&lt;/code>&lt;/a>：异步流式输出&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-invoke">&lt;code>ainvoke&lt;/code>&lt;/a>：基于单一输入异步调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-batch">&lt;code>abatch&lt;/code>&lt;/a>：基于列表输入异步调用链&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>LangChain的很多组件(对象)都应用了Runnable协议，可以把他们称为Runnable对象，这些对象都实现了上述接口，因此这些对象组成的链也都支持上述调用方法。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2常用runnable对象的输入输出类型">(2)常用Runnable对象的输入输出类型&lt;/h2>
&lt;p>LCEL中使用&lt;code>|&lt;/code>将前一个Runnable对象的输出传递给下一个Runnable对象作为输入，因此需要保证管道两端的型号一致。&lt;/p>
&lt;p>LangChain常用组件的输入、输出数据类型如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>组件&lt;/strong>&lt;/th>
&lt;th>&lt;strong>输入类型&lt;/strong>&lt;/th>
&lt;th>&lt;strong>输出类型&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Prompt (提示词)&lt;/td>
&lt;td>字典&lt;/td>
&lt;td>PromptValue&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatModel (聊天模型)&lt;/td>
&lt;td>单个字符串、聊天消息列表或 PromptValue&lt;/td>
&lt;td>ChatMessage&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLM (非聊天模型)&lt;/td>
&lt;td>单个字符串、聊天消息列表或 PromptValue&lt;/td>
&lt;td>String&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OutputParser (输出解析器)&lt;/td>
&lt;td>LLM 或 ChatModel 的输出&lt;/td>
&lt;td>取决于解析器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Retriever (检索器 )&lt;/td>
&lt;td>单个字符串&lt;/td>
&lt;td>文档清单&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tool (工具)&lt;/td>
&lt;td>单个字符串或字典，具体取决于工具&lt;/td>
&lt;td>取决于工具&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="3-runnablepassthrough在管道中的作用">(3) RunnablePassthrough在管道中的作用&lt;/h2>
&lt;p>在使用LCEL构建链时，原始用户输入可能不仅要传给第一个组件，还要传给后续组件，这时可以用RunnablePassthrough。RunnablePassthrough可以透传用户输入。&lt;/p>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>下面我们通过一个包含4个组件的RAG检索链来再次体验LCEL的便捷。&lt;/p>
&lt;p>示例场景是基于给定的文本回答用户问题。&lt;/p>
&lt;h2 id="step1环境准备">STEP1.环境准备&lt;/h2>
&lt;ul>
&lt;li>注意：由于示例的是检索本地文本，所以安装了向量数据库及其所需的库。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223004634709.png" alt="image-20231223004634709">&lt;/p>
&lt;h2 id="step2构建检索器组件">STEP2.构建检索器组件&lt;/h2>
&lt;ul>
&lt;li>为方便演示，本地文本简单的放一句话：”大鱼吃小鱼，小鱼吃虾米“。&lt;/li>
&lt;li>用langchain的检索器&lt;code>as_retriever&lt;/code>，它可以根据输入查询向量库并返回相关文本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223005234058.png" alt="image-20231223005234058">&lt;/p>
&lt;h2 id="step3构建prompt提示词组件">STEP3.构建Prompt提示词组件&lt;/h2>
&lt;ul>
&lt;li>提示词设计的是根据给定的文字回答问题。&lt;/li>
&lt;li>提示词中共2个变量：&lt;code>question&lt;/code>用来接收用户问题，&lt;code>context&lt;/code>来接收检索器的查询结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223010525145.png" alt="image-20231223010525145">&lt;/p>
&lt;h2 id="step4构建llm组件">STEP4.构建llm组件&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011307090.png" alt="image-20231223011307090">&lt;/p>
&lt;h2 id="step5组合成链">STEP5.组合成链&lt;/h2>
&lt;ul>
&lt;li>把检索器、提示词、模型、输出解析器四个组件串起来，命名为检索链&lt;code>retrieval_chain&lt;/code>。&lt;/li>
&lt;li>这一步注意两点：
&lt;ul>
&lt;li>用户输入的问题，不止组件1的检索器要用，组件2也要用它来构建提示词，因此组件1使用RunnablePassthrough方法把原始输入透传给下一步。&lt;/li>
&lt;li>由于组件2 prompt的输入要求是字典类型，所以组件1把检索器和用户问题写成字典格式，并用组件2的变量作为键。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011350292.png" alt="image-20231223011350292">&lt;/p>
&lt;h2 id="step6运行链">STEP6.运行链&lt;/h2>
&lt;ul>
&lt;li>传入用户问题，得到期望结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011640703.png" alt="image-20231223011640703">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain LCEL及其Runnable基础语法，关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Runnable接口&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Runnable是Langchain表达式语言的核心接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Runnable提供了一组标准调用接口：stream、invoke、batch及对应的异步方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Langchain很多组件都遵循Runnable协议，因此可以方便的组合、调用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>常用Runnable对象的输入输出类型要求，用管道&lt;code>|&lt;/code>方式组合时需注意管道两端的类型一致。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnablePassthrough在管道中的作用。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>构建了包含四个组件的检索链。&lt;/li>
&lt;li>通过实操关注了管道前后数据类型的匹配、RunnablePassthrough的作用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记32-LangChain解读-LCEL语言之基础语法(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%951/</link><pubDate>Wed, 13 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%951/</guid><description>&lt;p>在《【chatGPT】学习笔记29-LangChain解读1-快速入门》中，我们学习了LangChain的入门文档，了解到LangChain的模块化组件和链让开发LLM应用变得简单。&lt;/p>
&lt;p>同时，LangChain还推出了自己的语法——LCEL(LangChain表达式语言)，让复杂的组合链变得更简单。&lt;/p>
&lt;p>本篇我们跟着官方文档来学习LCEL的概念及如何使用。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231208175706824.png" alt="image-20231208175706824">&lt;/p>
&lt;h1 id="1-lcel概览">1. LCEL概览&lt;/h1>
&lt;h2 id="1什么是lcel">(1)什么是LCEL&lt;/h2>
&lt;p>LCEL是LangChain表达式语言(LangChain Expression Language)的缩写，是LangChain官方推出的一种新的语法，它提供了一种声明式的方法(而不是编写普通代码)来组合链，简化了构建复杂LLM应用的过程。&lt;/p>
&lt;h2 id="2为什么要用lcel">(2)为什么要用LCEL&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>声明式编程&lt;/strong>：由于LLM强大的理解和生成能力，LLM应用开发侧重业务逻辑的编排，LCEL提供的声明式编程方法让这类操作更高效。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>统一接口&lt;/strong>：LCEL中实现了“Runnable”协议，每个LCEL 对象都应用了“Runnable”接口。&lt;/p>
&lt;ul>
&lt;li>该接口定义了一组通用的调用方法（&lt;code>invoke&lt;/code>、&lt;code>batch&lt;/code>、&lt;code>stream&lt;/code>、&lt;code>ainvoke&lt;/code>&amp;hellip;），因此采用LCEL构建的任何链都将自动支持流、同步、异步和批处理等能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>组合原语&lt;/strong>：LCEL 提供了许多原语，可以轻松组合链、并行化组件、添加回退、动态配置链内部等。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为了更好地理解 LCEL，我们将分期介绍这些能力和实际应用效果。本篇我们先从看看如何用LCEL的声明式方法来组合业务组件和链。&lt;/p>
&lt;h1 id="2快速上手lcel">2.快速上手LCEL&lt;/h1>
&lt;p>以让LLM生成文本为例，来看看如何使用LCEL来完成任务。&lt;/p>
&lt;blockquote>
&lt;p>环境准备请参考《【chatGPT】学习笔记29-LangChain解读1-快速入门》。&lt;/p>
&lt;/blockquote>
&lt;h2 id="step1构造三组件">STEP1.构造三组件&lt;/h2>
&lt;p>我们在LangChain快速入门中学习过，一个常见的链包括三个组件：&lt;strong>提示词模板、模型、输出解析器&lt;/strong>。&lt;/p>
&lt;p>使用LangChain的方法构建如下——任务是给某个人或物写一句表扬的话：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210112756607.png" alt="image-20231210112756607">&lt;/p>
&lt;h2 id="step2构建自定义链">STEP2.构建自定义链&lt;/h2>
&lt;p>使用LCEL语法，用上面的3个组件组合成链，代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210113216116.png" alt="image-20231210113216116">&lt;/p>
&lt;p>你会发现，LCEL用&lt;code>|&lt;/code>把不同组件链接在一起，该符号类似于unix的管道运算符，将一个组件的输出作为下一个组件的输入。&lt;/p>
&lt;h2 id="step3调用链">STEP3.调用链&lt;/h2>
&lt;p>LCEL这种表达式方法是有效呢，我们调用一下看看。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210113642196.png" alt="image-20231210113642196">&lt;/p>
&lt;p>让链工作的方法也很简单，使用&lt;code>Runnable&lt;/code>接口的&lt;code>invoke&lt;/code>方法，传入主题字符串，任务就完成了。&lt;/p>
&lt;p>从上面的示例，我们可以看到LCEL的运作流程如下：&lt;/p>
&lt;ul>
&lt;li>传入用户输入，为提示词模板中的变量赋值。本例中是山姆奥特曼，则格式为{&amp;ldquo;topic&amp;rdquo;: &amp;ldquo;山姆奥特曼&amp;rdquo;}。&lt;/li>
&lt;li>&lt;code>prompt&lt;/code>获取用户输入，并使用&amp;quot;topic&amp;quot;构建提示词。&lt;/li>
&lt;li>&lt;code>model&lt;/code>组件拿到生成的提示词，并传递给LLM模型进行处理。模型生成的输出是一个&lt;code>ChatMessage&lt;/code>对象。&lt;/li>
&lt;li>最后，&lt;code>output_parser&lt;/code>组件接收&lt;code>ChatMessage&lt;/code>并将其转换为 Python 字符串，该字符串通过 invoke 方法返回。&lt;/li>
&lt;/ul>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain官方文档的“LangChain Expression Language (LCEL)”的部分章节，关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>LCEL概览&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>LCEL是LangChain官方推出的一种声明式编程语法，让构建复杂LLM应用变得更简单。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LCEL的主要价值点：&lt;/p>
&lt;ul>
&lt;li>声明式编程方法，便于组件和链的编排。&lt;/li>
&lt;li>统一接口，每个LCEL 对象都应用了“Runnable”接口。&lt;/li>
&lt;li>组合原语。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>构造链的组件。&lt;/li>
&lt;li>LCEL使用&lt;code>|&lt;/code>把不同的组件组合成链。&lt;/li>
&lt;li>使用invoke方法调用链，完成任务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item></channel></rss>