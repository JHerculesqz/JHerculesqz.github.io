<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 10 Jan 2025 11:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记59-AgentScope解读-提示工程</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B059-agentscope%E8%A7%A3%E8%AF%BB-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/</link><pubDate>Fri, 10 Jan 2025 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B059-agentscope%E8%A7%A3%E8%AF%BB-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/</guid><description>&lt;p>提示工程(Prompt Engineering)是与大语言模型交互的一项关键技术，作为以大语言模型为大脑的智能体，提示工程同样扮演着重要角色。&lt;/p>
&lt;p>本文为小伙伴们介绍AgentScope中的提示工程。&lt;/p>
&lt;h1 id="1提示工程">1.提示工程&lt;/h1>
&lt;h2 id="11提示工程在智能体中的作用">1.1.提示工程在智能体中的作用&lt;/h2>
&lt;p>在智能体中，提示工程用于帮助大模型理解复杂问题，给出准确的答案，以支撑智能体完成各类任务。&lt;/p>
&lt;p>提示工程的主要作用可归纳为以下几点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>明确任务和指令&lt;/strong>：针对具体任务，将用户需求和预期输出转化为明确的任务描述和指令。&lt;/li>
&lt;li>&lt;strong>上下文提供&lt;/strong>：提供必要的上下文信息，帮忙模型理解和处理复杂问题。&lt;/li>
&lt;li>&lt;strong>思维链推理&lt;/strong>：对于复杂问题，可以通过提示词进行逐步推理（Chain-of-Thought Prompting），即通过逐步生成中间推理步骤来解决问题。&lt;/li>
&lt;li>&lt;strong>控制输出格式&lt;/strong>：提示可以规定输出的具体格式，如列表、表格、段落等，使得生成的内容更加规范、易于理解和使用。&lt;/li>
&lt;/ul>
&lt;h2 id="12多智能体构建提示工程的难点">1.2.多智能体构建提示工程的难点&lt;/h2>
&lt;p>在多智能体系统中，由于任务的复杂性和需要基础的模型的多样性，构建提示工程面临挑战：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>复杂多样&lt;/strong>：在多智能体系统中，每个智能体可能具有不同的特性、能力和目标。需要考虑到每个智能体的独特性和差异性，增加了提示工程的复杂性。&lt;/li>
&lt;li>&lt;strong>部分模型不支持角色扮演&lt;/strong>：大多数语言模型都是为聊天场景设计的，角色只支持&lt;code>&amp;quot;user&amp;quot;&lt;/code>和&lt;code>&amp;quot;assistant&amp;quot;&lt;/code>，不支持角色扮演。&lt;/li>
&lt;li>&lt;strong>对话管理差异&lt;/strong>：某些模型规定发言必须在&lt;code>&amp;quot;user&amp;quot;&lt;/code>和&lt;code>&amp;quot;assistant&amp;quot;&lt;/code>之间交替进行，且序列的首尾都应由&lt;code>&amp;quot;user&amp;quot;&lt;/code>发言。这导致当单一代理需要连续扮演多个角色时，构建对话变得困难。&lt;/li>
&lt;/ul>
&lt;h2 id="13agentscope的提示工程构建策略">1.3.AgentScope的提示工程构建策略&lt;/h2>
&lt;p>针对上述难点，是否有统一的提示工程构建方案来普适不同模型呢？根据工程实践，答案是没有，&lt;strong>不存在&amp;quot;适用于所有模型API&amp;quot;的提示词构建方案&lt;/strong>。&lt;/p>
&lt;p>AgentScope的解决方案是，针对不同的模型API，提供对应的内置提示构建策略。这些策略在对应Model Wrapper类的&lt;code>format&lt;/code>函数中实现的，它接受&lt;code>Msg&lt;/code>对象的列表作为输入，并重新组织成一个&lt;code>Msg&lt;/code>对象的列表。AgentScope为以下模型API提供了内置的提示构建策略：&lt;/p>
&lt;ul>
&lt;li>OpenAIChatWrapper&lt;/li>
&lt;li>DashScopeChatWrapper&lt;/li>
&lt;li>DashScopeMultiModalWrapper&lt;/li>
&lt;li>OllamaChatWrapper&lt;/li>
&lt;li>OllamaGenerationWrapper&lt;/li>
&lt;li>GeminiChatWrapper&lt;/li>
&lt;li>ZhipuAIChatWrapper&lt;/li>
&lt;/ul>
&lt;p>这些策略可以帮助开发者快速开始使用AgentScope，简化了多智能体对话的构建过程。&lt;/p>
&lt;h1 id="2agentscope提示工程构建策略详解">2.AgentScope提示工程构建策略详解&lt;/h1>
&lt;p>下面，我们以OpenAI聊天模型API为例，详细看下AgentScope的提示构建策略。&lt;/p>
&lt;h2 id="21内置策略-openaichatwrapper">2.1.内置策略-OpenAIChatWrapper&lt;/h2>
&lt;p>AgentScope针对OpenAI聊天模型封装的提示构建接口是&lt;code>OpenAIChatWrapper&lt;/code>。&lt;/p>
&lt;p>&lt;code>OpenAIChatWrapper&lt;/code>封装了OpenAI聊天API，它以字典列表作为输入，其中字典必须遵循以下规则：&lt;/p>
&lt;ul>
&lt;li>需要&lt;code>role&lt;/code>和&lt;code>content&lt;/code>字段，以及一个可选的&lt;code>name&lt;/code>字段。&lt;/li>
&lt;li>&lt;code>role&lt;/code>字段必须是&lt;code>&amp;quot;system&amp;quot;&lt;/code>、&lt;code>&amp;quot;user&amp;quot;&lt;/code>或&lt;code>&amp;quot;assistant&amp;quot;&lt;/code>之一。&lt;/li>
&lt;/ul>
&lt;h2 id="211非视觉模型的提示构建">2.1.1.非视觉模型的提示构建&lt;/h2>
&lt;p>&lt;code>OpenAIChatWrapper&lt;/code>提供&lt;code>format&lt;/code>方法来构建多角色的对话提示。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>要点1&lt;/strong>：&lt;code>format&lt;/code>方法的参数很简单：
&lt;ul>
&lt;li>System角色：Msg对象，要求是包含&lt;code>role&lt;/code>、&lt;code>content&lt;/code>和&lt;code>name&lt;/code>三个字段的字典。&lt;/li>
&lt;li>非System角色：Msg对象的列表，即上述格式字典的列表。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>要点2&lt;/strong>：使用&lt;code>name&lt;/code>字段区分对话中的不同发言者。&lt;/li>
&lt;/ul>
&lt;p>示例代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.models&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAIChatWrapper&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.message&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Msg&lt;/span>
&lt;span class="c1"># 初始化model wrapper&lt;/span>
&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAIChatWrapper&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">config_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">model_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;gpt-4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 实例化prompt提示&lt;/span>
&lt;span class="n">prompt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">format&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;You&amp;#39;re a helpful assistant&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1"># Msg对象&lt;/span>
&lt;span class="p">[&lt;/span> &lt;span class="c1"># Msg对象的列表&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Bob&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Hi.&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistant&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Alice&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Nice to meet you!&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistant&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="p">],&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="212视觉模型的提示构建策略">2.1.2.视觉模型的提示构建策略&lt;/h2>
&lt;p>对于支持视觉的模型(如gpt-4o)，同样还是用&lt;code>format&lt;/code>方法来构建提示，使用url字段承载图片信息。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>要点1&lt;/strong>：&lt;code>format&lt;/code>方法的参数：
&lt;ul>
&lt;li>System角色：Msg对象，要求是包含&lt;code>role&lt;/code>、&lt;code>content&lt;/code>和&lt;code>name&lt;/code>三个字段的字典。&lt;/li>
&lt;li>非System角色：Msg对象的列表，每个Msg对象的要求是包含&lt;code>role&lt;/code>、&lt;code>content&lt;/code>、&lt;code>name&lt;/code>和&lt;code>url&lt;/code>四个字段的字典。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>要点2&lt;/strong>：&lt;code>url&lt;/code>可以是图片网络地址或本地地址的字符串，也可以是多个图片的地址字符串的列表。
&lt;ul>
&lt;li>如果是网络图片url，将直接传递给OpenAI Chat API；&lt;/li>
&lt;li>如果本地图片url，将被转换为base64格式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>示例代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.models&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAIChatWrapper&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.message&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Msg&lt;/span>
&lt;span class="c1"># 初始化model wrapper&lt;/span>
&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAIChatWrapper&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">config_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">model_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;gpt-4o&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 实例化prompt提示&lt;/span>
&lt;span class="n">prompt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">format&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;You&amp;#39;re a helpful assistant&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1"># Msg 对象&lt;/span>
&lt;span class="p">[&lt;/span> &lt;span class="c1"># Msg 对象的列表&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Describe this image&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;https://xxx.png&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1"># 图片网络地址&lt;/span>
&lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;And these images&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;/Users/xxx/test.png&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;/Users/xxx/test.mp3&amp;#34;&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="c1"># 图片url地址&lt;/span>
&lt;span class="p">],&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="c1"># 打印提示内容&lt;/span>
&lt;span class="k">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prompt&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;system&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;You are a helpful assistant&amp;#34;&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;text&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;text&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Describe this image&amp;#34;&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;image_url&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;image_url&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;url&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;https://xxx.png&amp;#34;&lt;/span> &lt;span class="c1"># 网络图片，将url直接传递给模型&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;text&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;text&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;And these images&amp;#34;&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;image_url&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;image_url&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;url&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;data:image/png;base64,YWJ...&amp;#34;&lt;/span> &lt;span class="c1"># 本地图片，将图片的base64信息传递给模型&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>智能体系统中提示工程的作用和构建难点：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AgentScope的提示工程构建策略：&lt;/p>
&lt;ul>
&lt;li>不存在&amp;quot;适用于所有模型API&amp;quot;的提示词构建方案。&lt;/li>
&lt;li>AgentScope针对不同的模型API，提供对应的内置提示构建策略。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>参考：https://arxiv.org/pdf/2402.14034&lt;/p></description></item><item><title>【chatGPT】学习笔记58-AgentScope解读-流式输出</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B058-agentscope%E8%A7%A3%E8%AF%BB-%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/</link><pubDate>Fri, 27 Dec 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B058-agentscope%E8%A7%A3%E8%AF%BB-%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/</guid><description>&lt;p>流式输出是大语言模型的重要功能，可以提高模型输出效率、提升用户交互体验。&lt;/p>
&lt;p>本文为小伙伴们介绍AgentScope中流式输出的用法。&lt;/p>
&lt;h1 id="1agentscope流式输出">1.AgentScope流式输出&lt;/h1>
&lt;h2 id="11-流式输出的概念">1.1 流式输出的概念&lt;/h2>
&lt;p>流式输出是指大模型在生成文本或其他类型的数据时，生成时即刻发送，而不是等待所有数据生成完毕后一次性发送。&lt;/p>
&lt;p>流式输出适用于生成较长文本或需要即时反馈的场景，可以减少等待时间，提高用户体验，并允许系统在数据生成过程中进行实时处理。&lt;/p>
&lt;h2 id="12流式输出和非流式输出的应用场景">1.2.流式输出和非流式输出的应用场景&lt;/h2>
&lt;p>流式输出适合需要实时反馈和处理连续数据的应用，而非流式输出适合批量处理和结果完整性要求较高的场景。&lt;/p>
&lt;p>在多智能体（Multi-Agent）系统开发过程中，可根据业务需求和系统设计选择合适的输出方式：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>流式输出&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实时交互应用&lt;/strong>：需要实时反馈的应用，如在线聊天机器人或实时监控系统。&lt;/li>
&lt;li>&lt;strong>长任务处理&lt;/strong>：需要长时间处理的任务，流式输出可以让用户或系统在处理过程中逐步获取结果，而不是等待整个任务完成。&lt;/li>
&lt;li>&lt;strong>数据流处理&lt;/strong>：处理连续的数据流时，如传感器数据或实时日志。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>非流式输出&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>批量处理任务&lt;/strong>：不需要实时反馈的批量处理任务，如夜间批处理作业。&lt;/li>
&lt;li>&lt;strong>结果完整性要求&lt;/strong>：需要确保所有输出数据的完整性和一致性时，非流式输出可以确保在所有数据处理完成后一次性输出结果。&lt;/li>
&lt;li>&lt;strong>复杂决策过程&lt;/strong>：需要综合所有输入数据才能做出决策的场景中，非流式输出可以确保在所有数据被处理和分析后，再输出最终的决策结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="13agentscope如何支持流式输出">1.3.AgentScope如何支持流式输出&lt;/h2>
&lt;p>AgentScope支持集成不同厂家的大模型服务，对各家模型的流式输出也做了统一封装：&lt;/p>
&lt;ul>
&lt;li>通过&lt;code>stream&lt;/code>参数设置流式输出(参数值为&lt;code>True&lt;/code>表示流式输出)，这个参数会覆盖模型中的&lt;code>stream&lt;/code>参数。&lt;/li>
&lt;/ul>
&lt;p>目前，AgentScope支持以下大模型 API 的流式输出：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>OpenAI API&lt;/strong>：对话（Chat）&lt;/li>
&lt;li>&lt;strong>DashScope API&lt;/strong>：对话（Chat）&lt;/li>
&lt;li>&lt;strong>Gemini API&lt;/strong>：对话（Chat）&lt;/li>
&lt;li>&lt;strong>ZhipuAi API&lt;/strong>：对话（Chat）&lt;/li>
&lt;li>&lt;strong>Ollama API&lt;/strong>：对话（Chat）&lt;/li>
&lt;li>&lt;strong>LiteLLM API&lt;/strong>：对话（Chat）&lt;/li>
&lt;/ul>
&lt;h1 id="2agentscope流式输出的配置和调用">2.AgentScope流式输出的配置和调用&lt;/h1>
&lt;h2 id="21流式输出的配置">2.1.流式输出的配置&lt;/h2>
&lt;p>AgentScope支持两种方式配置流式输出：&lt;/p>
&lt;ul>
&lt;li>在配置文件中
&lt;ul>
&lt;li>在&lt;code>model_config&lt;/code>模型配置中，将&lt;code>stream&lt;/code>参数设置为&lt;code>True&lt;/code>即表示启用流式输出模式：&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">model_config&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s2">&amp;#34;config_name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;xxx&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;model_type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;xxx&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s2">&amp;#34;stream&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>在python模型调用时
&lt;ul>
&lt;li>在python脚本中调用模型时，将&lt;code>stream&lt;/code>参数设置为&lt;code>True&lt;/code>，启用模型的流式输出模式：&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">MyAgent&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">AgentBase&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">reply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Msg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Sequence&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Msg&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;span class="n">response&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">prompt&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">stream&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="22流式输出的调用">2.2.流式输出的调用&lt;/h2>
&lt;h3 id="1流式打印">(1)流式打印&lt;/h3>
&lt;ul>
&lt;li>启用流式输出后，模型输出的&lt;code>stream&lt;/code>字段是生成器，&lt;code>text &lt;/code>字段为 None。&lt;/li>
&lt;li>使用speak方法打印流式输出，参数是&lt;code>stream&lt;/code>。&lt;/li>
&lt;li>访问&lt;code>text &lt;/code>字段，生成器将被迭代以生成完整的文本并存入&lt;code>text &lt;/code>字段。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">reply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Msg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Sequence&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Msg&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># response.steam 为 生成器，response.text 为 None&lt;/span>
&lt;span class="n">response&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prompt&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 在 terminal 和 AgentScope Studio 中流式打印文本&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">speak&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stream&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 生成器被迭代时，产生的文本将自动被存储在 response.text 中，因此用户可以直接使用 response.text 处理响应文本&lt;/span>
&lt;span class="n">msg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistant&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">msg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">msg&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2自定义流式输出">(2)自定义流式输出&lt;/h3>
&lt;ul>
&lt;li>AgentScope提供了&lt;code>log_stream_msg&lt;/code>函数，用户使用此函数自行处理生成器类型的消息，比如上例中的&lt;code>response.steam&lt;/code>:
&lt;ul>
&lt;li>&lt;code>stream&lt;/code> 字段中的生成器将生成一个布尔值和字符串的二元组。布尔值表示当前是否是最后一段文本，字符串是到目前为止的响应文本。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="c1"># ...&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GeneratorType&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># 流式消息必须共享相同的 id 才能在 AgentScope Studio 中显示，因此这里通过同一条消息切换 content 字段来实现&lt;/span>
&lt;span class="n">msg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistant&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">last&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">text_chunk&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">msg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">text_chunk&lt;/span>
&lt;span class="n">log_stream_msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">msg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">last&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">last&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>流式输出是大模型在生成数据时即时响应的一种方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>流式输出、非流式输出有各自的应用场景：&lt;/p>
&lt;ul>
&lt;li>流式输出适合需要实时反馈和处理连续数据的应用，而非流式输出适合批量处理和结果完整性要求较高的场景。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>AgentScope支持多种模型的流式输出，包括OpenAI、DashScope、Gemini、ZhipuAi、Ollama、LiteLLM。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AgentScope流式输出的配置：&lt;/p>
&lt;ul>
&lt;li>&lt;code>model_config&lt;/code>的&lt;code>stream&lt;/code>参数。&lt;/li>
&lt;li>脚本中的模型调用参数&lt;code>stream&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>AgentScope流式输出的调用：&lt;/p>
&lt;ul>
&lt;li>模型输出的stream字段是个生成器。&lt;/li>
&lt;li>使用speak方法直接流式打印。&lt;/li>
&lt;li>使用log_stream_msg方法自定义流式打印。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>参考：https://arxiv.org/pdf/2402.14034&lt;/p></description></item><item><title>【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/</link><pubDate>Thu, 14 Nov 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/</guid><description>&lt;p>在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精准的模型事后模型量化(Post-Training Quantization)。&lt;/p>
&lt;blockquote>
&lt;p>PS：理解GPTQ，就需要理解前两篇专栏介绍的&lt;strong>模型量化基本概念&lt;/strong>、&lt;strong>模型剪枝及OBD/OBS&lt;/strong>，不熟悉的小伙伴可以翻一下前面的专栏。&lt;/p>
&lt;/blockquote>
&lt;h1 id="1gptq相关技术">1.GPTQ相关技术&lt;/h1>
&lt;h2 id="11量化方法分类">1.1.量化方法分类&lt;/h2>
&lt;p>从量化时机看，有两类量化方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>QAT&lt;/strong>：Quantization During Training，训练期量化。&lt;/li>
&lt;li>&lt;strong>PTQ&lt;/strong>：Post-Training Quantization，训练后量化。&lt;/li>
&lt;/ul>
&lt;p>这两类量化方法由于量化时机不同，所以训练方法也有所不同。&lt;/p>
&lt;h3 id="1quantization-during-training">(1)Quantization During Training&lt;/h3>
&lt;p>训练期量化(Quantization During Training)，也称为&lt;strong>量化感知训练&lt;/strong>，是在模型训练的过程中进行量化，使得模型在训练时就适应量化带来的误差。&lt;/p>
&lt;p>QAT算法的主要步骤如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>STEP1.模拟量化&lt;/strong>：在训练过程中，权重和激活值会被临时量化到较低精度(如：8bit)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>STEP2.反量化&lt;/strong>：量化后的值会在后续的计算中被反量化回浮点数，用于计算梯度和更新权重。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>STEP3.梯度传递&lt;/strong>：量化和反量化的过程会引入额外的误差，模型进一步学习如何最小化这些额外误差对最终性能的影响。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>STEP4.微调&lt;/strong>：在训练的最后阶段会移除量化操作，对模型进行微调以恢复由于量化而可能损失的精度。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>QAT的优点是可以更好地保持模型的精度，使得模型在训练过程中适应量化带来的变化。&lt;/p>
&lt;p>QAT的缺点是增加训练复杂度和训练成本。&lt;/p>
&lt;h3 id="2post-traning-quantization">(2)Post-Traning Quantization&lt;/h3>
&lt;p>训练后量化(Post-Training Quantization)，是在模型训练完成后，再对模型权重、激活值进行量化。&lt;/p>
&lt;p>PTQ算法的主要步骤如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>STEP1.离线校准&lt;/strong>：分析模型在特定数据集上的激活值分布来确定量化参数(如：比例因子)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>STEP2.量化&lt;/strong>：使用STEP1离线校准得到的量化参数，将模型的权重和激活值量化到较低的精度。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>STEP3.转换&lt;/strong>：将量化后的模型转换为可以在目标硬件上运行的形式。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>PTQ的的优点是简单且计算成本较低，它不需要在训练过程中模拟量化操作&lt;/p>
&lt;p>PTQ的缺点是可能不如训练期间量化那样能够保持模型的精度，因为它没有给模型机会去适应量化带来的误差。&lt;/p>
&lt;h3 id="3qat-vs-ptq">(3)QAT vs PTQ&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>从精度看&lt;/strong>：QAT通常能够更好地保持模型的精度，因为它允许模型在训练过程中适应量化操作。&lt;/li>
&lt;li>&lt;strong>从计算成本看&lt;/strong>：PTQ的计算成本较低，因为它避免了在训练过程中模拟量化的额外计算。&lt;/li>
&lt;li>&lt;strong>从灵活性看&lt;/strong>：PTQ提供了更大的灵活性，因为它允许在不同的硬件和部署环境中使用相同的量化模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>GPTQ属于PTQ量化方法，GPTQ就是G-PTQ，G表示GPT系列大语言模型。&lt;/strong>&lt;/p>
&lt;h2 id="12模型剪枝技术回顾">1.2.模型剪枝技术回顾&lt;/h2>
&lt;p>在PTQ(训练后量化)的思想下，模型剪枝技术得以发展与应用。这些技术更集中应用于视觉模型，但在大模型上应用面临诸多挑战。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>OBD&lt;/strong>：Opimal Brain Damage&lt;/li>
&lt;li>&lt;strong>OBS&lt;/strong>：Optimal Brain Surgeon&lt;/li>
&lt;li>&lt;strong>OBQ&lt;/strong>：Optimal Brain Quantization&lt;/li>
&lt;/ul>
&lt;h3 id="1optimal-brain-damage">(1)Optimal Brain Damage&lt;/h3>
&lt;p>OBD是一种早期的剪枝算法(由LeCun提出)，其核心思想是通过Loss二阶导制定参数的影响，从而在剪枝过程中自动化地找出并裁剪&amp;quot;无影响&amp;quot;的参数。&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>剪枝自动化&lt;/strong>：寻找到参数与Loss关系的数学方法，使剪枝过程自动化&lt;/li>
&lt;li>&lt;strong>计算简单&lt;/strong>：从数学意义上，证明了可以忽略参数间的相互作用等因素，极大降低计算复杂度。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>局部最优&lt;/strong>：OBD可能只找到局部最优解(由于忽略了参数间相互作用，可能会在剪枝过程中在错误的方向上越走越远)，而不是全局最优解。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>关于OBD详细内容，可阅读本专栏《【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)》相关章节&lt;/p>
&lt;/blockquote>
&lt;h3 id="2optimal-brain-surgeon">(2)Optimal Brain Surgeon&lt;/h3>
&lt;p>OBS是OBD的改进版，它建立了模型参数间的关联关系。&lt;/p>
&lt;p>OBS的核心思想是在剪枝过程中&lt;strong>找到对模型误差影响最小的权重&lt;/strong>，&lt;strong>并更新其他权重以补偿剪枝带来的影响&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>更精确&lt;/strong>：考虑了权重之间的交叉项，可以更准确地评估剪枝的影响。&lt;/li>
&lt;li>&lt;strong>全局最优&lt;/strong>：相比于OBD，OBS更可能找到全局最优解。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>计算复杂&lt;/strong>：由于需要考虑交叉项，计算复杂度较高，尤其是在大规模网络中。&lt;/li>
&lt;li>&lt;strong>资源消耗&lt;/strong>：对于大规模网络，计算和存储资源消耗较大。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>关于OBD详细内容，可阅读本专栏《【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)》相关章节&lt;/p>
&lt;/blockquote>
&lt;h3 id="3optimal-brain-quantization">(3)Optimal Brain Quantization&lt;/h3>
&lt;p>OBQ是将OBS的思想应用到量化问题中，它处理的是将浮点数权重转换为整数表示，而不是直接将权重设置为零。&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>精度保持&lt;/strong>：相比于直接剪枝，量化可以保持模型的数值精度，减少信息损失。&lt;/li>
&lt;li>&lt;strong>硬件友好&lt;/strong>：量化后的模型更适合在低精度硬件上运行，如移动设备和嵌入式系统。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量化策略选择&lt;/strong>：需要精心设计量化策略，以平衡模型性能和计算效率。&lt;/li>
&lt;li>&lt;strong>后处理需求&lt;/strong>：量化后可能需要额外的步骤来微调模型，以恢复性能。&lt;/li>
&lt;/ul>
&lt;p>关于OBD、OBS、OBQ的简述以及GPTQ与之的关联，详见论文Related Work章节。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114083900824.png" alt="image-20241114083900824">&lt;/p>
&lt;h2 id="13大模型量化技术">1.3.大模型量化技术&lt;/h2>
&lt;p>&lt;strong>大模型量化技术&lt;/strong>：Large-model Quantization，专门针对大模型实施模型量化的技术。&lt;/p>
&lt;ul>
&lt;li>大型模型量化技术的实验对象通常是BLOOM和OPT-175B这样的大型语言模型，&lt;/li>
&lt;li>论文中列举的量化方法有：ZeroQuant、LLM.int8()和nuQmm。
&lt;ul>
&lt;li>ZeroQuant提出了逐层知识蒸馏，但只能应用于最多13亿参数的模型，且计算时间较长。&lt;/li>
&lt;li>LLM.int8()关注到激活值中的异常值会破坏大型模型的量化，并提出通过保持这些维度的高精度来解决。&lt;/li>
&lt;li>nuQmm则开发了针对特定二进制编码量化方案的高效GPU内核。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>而GPTQ的实验证明，它能在更短的时间内量化更大的模型。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114091053556.png" alt="image-20241114091053556">&lt;/p>
&lt;h2 id="14关键技术1层级量化">1.4.关键技术1：层级量化&lt;/h2>
&lt;p>在宏观层面，GPTQ遵循层级量化思想(Layer-Wise Quantization)，层级量化属于PTQ方法，通过逐层量化、逐层重构。层级量化的目标是找到量化权重矩阵，这个矩阵用来保障最小化各层的平方误差。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数学表达&lt;/strong>：W表示线性层的权重，X表示线性层的一小批数据点。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114102155494.png" alt="image-20241114102155494">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量化目标&lt;/strong>：找到一组量化权重 W^，使得与全精度层输出的平方误差最小化。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114102506034.png" alt="image-20241114102506034">&lt;/p>
&lt;h2 id="15关键技术2obq">1.5.关键技术2：OBQ&lt;/h2>
&lt;p>OBQ是GPTQ的数学方法基础：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量化目标&lt;/strong>：OBQ独立处理每一行权重，一次量化一个权重，同时更新其它未量化的权重，以最小化该层的输出误差。&lt;/li>
&lt;li>&lt;strong>数学表达&lt;/strong>：w&lt;sub>q&lt;/sub>表示需要量化的权重，quant(w&lt;sub>q&lt;/sub>)是w&lt;sub>q&lt;/sub>的量化值，H&lt;sub>F&lt;/sub>&lt;sup>-1&lt;/sup>表示海森矩阵的逆。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114103135744.png" alt="image-20241114103135744">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效更新海森矩阵&lt;/strong>：在量化过程中，Hessian矩阵用来表征剩余全精度权重的集合。如下公式表示了OBQ更新海森矩阵的逻辑，OBQ在量化一个权重后，移除已量化权重的行和列，避免后续量化中的重复计算，从而避免了更新海森矩阵对算力的巨大消耗。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114140940163.png" alt="image-20241114140940163">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量化并行&lt;/strong>：OBQ方法并行处理W的多行，实验中单GPU上消耗4小时量化OPT-175B模型(1750亿参数)，相较于其它量化方法提升了3个数量级的速度。&lt;/li>
&lt;/ul>
&lt;p>GPTQ就是在OBQ的基础上进行了重大修改，使其能够扩展到大型语言模型，使得在保持模型性能的同时，显著减少模型的存储和计算需求，使得大型模型的部署和推理变得更加可行。&lt;/p>
&lt;h1 id="2gptq原理">2.GPTQ原理&lt;/h1>
&lt;h2 id="21gptq对obq的改进">2.1.GPTQ对OBQ的改进&lt;/h2>
&lt;h3 id="1arbitrary-order-insight">(1)Arbitrary Order Insight&lt;/h3>
&lt;p>OBQ量化权重的顺序是按照贪婪法进行量化的，即总是选择当前量化误差最小的权重进行量化。然而GPTQ实验发现，这种量化顺序的策略在大模型中效果不佳。&lt;/p>
&lt;p>GPTQ按照相同顺序量化所有行的权重相反能获得不错的效果，其数学原理是海森矩阵H&lt;sub>F&lt;/sub>仅依赖于层输入X&lt;sub>F&lt;/sub>，而不依赖于权重。因此可以对所有行的H&lt;sub>F&lt;/sub>&lt;sup>-1&lt;/sup>进行相同的更新，而替代了对每个权重单独更新。&lt;/p>
&lt;blockquote>
&lt;p>PS：Arbitrary Order Insight，我刚开始一脸懵逼地直译为&amp;quot;任意顺序洞察&amp;rdquo;，按照上述数学逻辑，应该翻译为“块量化”更为合适，正因为海森矩阵的特性才实现了块量化。&lt;/p>
&lt;/blockquote>
&lt;h3 id="2lazy-batch-updates">(2)Lazy Batch-Updates&lt;/h3>
&lt;p>Lazy Batch-Updates，批量更新，是一种工程化方法。量化在GPU内非常快，但是更新剩余未量化的权重的内存访问将成为QPTQ的性能瓶颈。&lt;/p>
&lt;ul>
&lt;li>GPTQ的解决思路是每次处理128列，只更新这些列对应的子块(即，&lt;strong>B * B block of H&lt;sup>-1&lt;/sup>&lt;/strong>)。只有一个块完全处理后，才会驱动如下两个公式，对整个H&lt;sup>-1&lt;/sup>和W矩阵进行全局更新。其中，Q表示一组索引，H&lt;sup>-1&lt;/sup>&lt;sub>-Q&lt;/sub>表示移除了相应行和列的逆矩阵。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114161325271.png" alt="image-20241114161325271">&lt;/p>
&lt;ul>
&lt;li>上述更新方法，本质上解决的是内存吞吐量瓶颈。&lt;/li>
&lt;/ul>
&lt;h3 id="3cholesky-reformulation">(3)Cholesky Reformulation&lt;/h3>
&lt;p>Cholesky Reformulation，切尔斯基分解。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在量化过程中，可能会出现数值不准确性，进而导致海森矩阵的逆变为奇异(即不可逆) 。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数值稳定性&lt;/strong>：GPTQ利用了Cholesky分解的数值稳定性，通过预先计算和更新Cholesky分解，避免直接计算和更新Hessian矩阵的逆。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="22gptq算法的全流程">2.2.GPTQ算法的全流程&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.块量化(Arbitrary Order Insight)&lt;/strong>：选择一个连续块，如下图中粗线包含的区域。&lt;/li>
&lt;li>&lt;strong>STEP2.Cholesky分解(Cholesky Reformulation)&lt;/strong>：利用Cholesky分解得到海森矩阵的逆，进而量化STEP1选择的块。橙色为已量化的权重，白色为正在量化的权重，蓝色为未量化的权重。&lt;/li>
&lt;li>&lt;strong>STEP3.权重批量更新(Lazy Batch-Updates)&lt;/strong>：在当前块量化的最后，就更新所有剩余未量化的权重。&lt;/li>
&lt;li>完成STEP1~STEP3后，&lt;strong>递归量化&lt;/strong>下一个块。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114164012033.png" alt="image-20241114164012033">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114163908213.png" alt="image-20241114163908213">&lt;/p>
&lt;h2 id="23gptq核心代码解读">2.3.GPTQ核心代码解读&lt;/h2>
&lt;p>通过对GPTQ的论文解读，我们了解了GPTQ的相关技术，以及算法流程。&lt;/p>
&lt;p>最后我们再来看一下GPTQ的核心代码，以加深对论文的理解：&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/AutoGPTQ/AutoGPTQ/blob/main/auto_gptq/quantization/gptq.py">https://github.com/AutoGPTQ/AutoGPTQ/blob/main/auto_gptq/quantization/gptq.py&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h3 id="1关闭cuda的tf32精度">(1)关闭CUDA的TF32精度&lt;/h3>
&lt;ul>
&lt;li>TF32是一种混合精度计算，关闭它可以保证计算精确性。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165123240.png" alt="image-20241114165123240">&lt;/p>
&lt;h3 id="2gptq类的构造函数">(2)GPTQ类的构造函数&lt;/h3>
&lt;ul>
&lt;li>构造函数的输入是神经网络中的一个层。&lt;/li>
&lt;li>复制层的权重，得到W。&lt;/li>
&lt;li>根据层类型(Conv1D、Conv2d)，对权重矩阵的形状进行调整。&lt;/li>
&lt;li>H就是海森矩阵，初始为零矩阵。&lt;/li>
&lt;li>quantizer是量化器对象。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165410715.png" alt="image-20241114165410715">&lt;/p>
&lt;h3 id="3add_batch">(3)add_batch&lt;/h3>
&lt;ul>
&lt;li>处理输入和输出的batch&lt;/li>
&lt;li>根据层类型，调整输入的形状&lt;/li>
&lt;li>根据调整好的输入形状，更新海森矩阵H&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165844565.png" alt="image-20241114165844565">&lt;/p>
&lt;h3 id="4fasterquant">(4)fasterquant&lt;/h3>
&lt;ul>
&lt;li>克隆并调整当前层的权重矩阵的形状。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114170835985.png" alt="image-20241114170835985">&lt;/p>
&lt;ul>
&lt;li>根据块分组进行量化，逐步地计算损失并更新权重。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114170950349.png" alt="image-20241114170950349">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114171103060.png" alt="image-20241114171103060">&lt;/p>
&lt;ul>
&lt;li>最终返回量化参数&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114171215236.png" alt="image-20241114171215236">&lt;/p>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文介绍了：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPTQ相关技术&lt;/strong>：QAT和PTQ、OBD/OBS/OBQ、大模型量化技术
&lt;ul>
&lt;li>其中，层级量化和OBQ是GPTQ的关键技术基础&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>GPTQ的技术原理&lt;/strong>：
&lt;ul>
&lt;li>块量化&lt;/li>
&lt;li>批量更新&lt;/li>
&lt;li>Cholesky分解&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>GPTQ的算法流程及代码解读&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>论文地址：https://arxiv.org/abs/2210.17323&lt;/p></description></item><item><title>【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/</link><pubDate>Fri, 18 Oct 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/</guid><description>&lt;p>笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Optimal Brain Damage》，以及1992年的《Second order derivatives for network pruning: Optimal Brain Surgeon》，来理解一下大神们在90年代如何解决模型剪枝问题的。&lt;/p>
&lt;h1 id="1模型剪枝">1.模型剪枝&lt;/h1>
&lt;h2 id="1什么是模型剪枝">(1)什么是模型剪枝？&lt;/h2>
&lt;p>模型剪枝是一种模型优化技术，通过移除神经网络中的冗余连接或神经元，达到降低模型复杂度的目的。&lt;/p>
&lt;p>降低模型复杂度，除了通过降低模型计算量提高模型的推理速度，还能在一定程度上提升模型的泛化能力。&lt;/p>
&lt;h2 id="2模型剪枝的分类">(2)模型剪枝的分类&lt;/h2>
&lt;p>模型剪枝主要分两大类：结构化剪枝和非结构化剪枝。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>非结构化剪枝&lt;/strong>：这种剪枝方式涉及单个权重或神经元的移除，结果是权重矩阵中出现不规则的稀疏模式。&lt;/p>
&lt;ul>
&lt;li>由于这种剪枝方式不遵循特定的规则，因此压缩后的模型可能需要特殊的处理才能被有效地存储和计算。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>结构化剪枝&lt;/strong>：这种剪枝方式会按照一定的规则移除权重，保持模型原有的结构。&lt;/p>
&lt;ul>
&lt;li>这种方法实现成本低，剪枝后的模型更容易被深度学习框架1支持。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3模型剪枝的流程">(3)模型剪枝的流程&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.预训练&lt;/strong>：首先在大量数据集上预训练一个深度学习模型。&lt;/li>
&lt;li>&lt;strong>STEP2.剪枝&lt;/strong>：根据一定的剪枝准则(如：权重的绝对值、激活值、梯度信息等)，选择并剪除那些对模型性能影响较小的连接或神经元。&lt;/li>
&lt;li>&lt;strong>STEP3.微调&lt;/strong>：剪枝后，在剩余的数据集上对剪枝后模型进行微调，以恢复或提升模型的性能。&lt;/li>
&lt;li>&lt;strong>STEP4.评估&lt;/strong>：最后对剪枝后的模型进行评估，以确保其性能满足特定任务的要求。&lt;/li>
&lt;/ul>
&lt;h2 id="4模型剪枝的收益">(4)模型剪枝的收益&lt;/h2>
&lt;ul>
&lt;li>减少模型的计算复杂度和存储空间。&lt;/li>
&lt;li>改善模型的泛化能力。&lt;/li>
&lt;li>移除不必要的参数，使得模型的结构更加清晰，更易于理解和解释，提高模型的可解释性。&lt;/li>
&lt;li>适应不同的任务和环境，特别是在资源受限的设备上部署模型时尤为重要&lt;/li>
&lt;/ul>
&lt;h2 id="5模型剪枝的挑战">(5)模型剪枝的挑战&lt;/h2>
&lt;ul>
&lt;li>模型剪枝时，剪哪里、怎么剪，需要对模型的深度理解和数学能力，如：本文即将解读的OBD、OBS就是给出了两种剪枝方法及数学推导。&lt;/li>
&lt;li>剪枝不当可能消耗大量算力后却损失了信息，反而导致模型性能下降。&lt;/li>
&lt;li>剪枝后模型还需要进一步调整和优化，以适应不同更广泛的场景。&lt;/li>
&lt;/ul>
&lt;h1 id="2optimal-brain-damage">2.Optimal Brain Damage&lt;/h1>
&lt;h2 id="1obd的推导过程">(1)OBD的推导过程&lt;/h2>
&lt;p>&lt;strong>首先，我们了解一下OBD解决什么问题：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>这篇论文由Lecun发表，Optimal Brain Damage (OBD)是一种神经网络模型剪枝技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018144556744.png" alt="image-20241018144556744">&lt;/p>
&lt;ul>
&lt;li>在论文的开篇，Lecun就提出了问题：模型剪枝就是删除没用的权重，那么如何衡量&amp;quot;这个权重没用？&amp;quot;，如何&amp;quot;删除权重，清零吗？&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018154321755.png" alt="image-20241018154321755">&lt;/p>
&lt;ul>
&lt;li>因此，Lecun提出了Optimal Brain Damage (OBD)，通过选择性地删除网络中的权重来减少模型的大小。&lt;/li>
&lt;li>“选择性”的核心思想是：利用目标函数的二阶导数信息来评估每个权重的重要性，即通过海森矩阵(Hessian matrix)来确定权重的“贡献度”或“saliency”。
&lt;ul>
&lt;li>权重的贡献度：指删除该权重后对目标函数(通常是损失函数)的影响程度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>OBD的目标是找到一个参数集合，使得删除这些参数后目标函数的增加最小，从而在减少网络复杂度的同时尽量保持模型性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018154349795.png" alt="image-20241018154349795">&lt;/p>
&lt;p>&lt;strong>然后，我们再看看推导过程的关键细节：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>目标函数E：比如目标函数E就是损失函数。&lt;/li>
&lt;li>泰勒展开：我们对目标函数E进行泰勒展开，就得到&amp;quot;参数向量影响δU对目标函数E的影响公式&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018155414749.png" alt="image-20241018155414749">&lt;/p>
&lt;ul>
&lt;li>g&lt;sub>i&lt;/sub>是U对E的梯度，h&lt;sub>ij&lt;/sub>是U对E的海森矩阵的元素。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018155355488.png" alt="image-20241018155355488">&lt;/p>
&lt;ul>
&lt;li>但即使是泰勒展开了，公式(1)依然有很大的计算量，Lecun做了三个近似简化：
&lt;ul>
&lt;li>数学意义上对角线近似简化上述公式的第3项，业务意义上就是证明了删除权重a导致的损失函数的Loss增大和删除权重b导致的Loss增大是独立的。&lt;/li>
&lt;li>数学意义上通过极值点简化上述公式的第1项，业务意义上就是证明了如果神经网络已经训练收敛了，U对E就达到了局部最小值，即U对E的梯度等于0。&lt;/li>
&lt;li>数学意义上假设E是二次函数简化上述公式的第4项。&lt;/li>
&lt;li>综合上述3个简化，公式(1)简化为公式(3)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018160514805.png" alt="image-20241018160514805">&lt;/p>
&lt;h2 id="2obd的执行流程">(2)OBD的执行流程&lt;/h2>
&lt;p>有了上述公式，OBD的执行流程如下：&lt;/p>
&lt;ul>
&lt;li>STEP1.选择1个神经网络结构&lt;/li>
&lt;li>STEP2.训练这个神经网络&lt;/li>
&lt;li>STEP3.计算每个权重的二阶导h&lt;sub>kk&lt;/sub>&lt;/li>
&lt;li>STEP4.计算每个参数对目标函数的影响s&lt;sub>k&lt;/sub>&lt;/li>
&lt;li>STEP5.删除最低影响的参数&lt;/li>
&lt;li>STEP6.回到STEP2&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018160651088.png" alt="image-20241018160651088">&lt;/p>
&lt;h1 id="3optimal-brain-surgeon">3.Optimal Brain Surgeon&lt;/h1>
&lt;h2 id="1obs的推导过程">(1)OBS的推导过程&lt;/h2>
&lt;p>&lt;strong>首先，我们也了解一下OBS解决什么问题：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>这篇论文是对OBD的优化，Optimal Brain Surgeon(OBS)也是一种神经网络模型剪枝技术。&lt;/li>
&lt;li>Optimal Brain Damage(OBD)衡量了某个权重对贡献度并删除贡献度最低的权重，但一旦删除了某个错误的权重，反而会导致在错误的剪枝方向上越走越远。&lt;/li>
&lt;li>OBS的目标就是允许比其他方法剪枝更多的权重，在测试数据上产生更好的泛化。其核心思想就是从训练数据和网络的结构信息中计算逆Hessian矩阵H的逆。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018161259795.png" alt="image-20241018161259795">&lt;/p>
&lt;ul>
&lt;li>OBS本质是考虑的&amp;quot;局部最优&amp;rdquo;，即训练得到局部最小误差。那么我们将相对权重扰动的Loss函数进行泰勒展开：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018161832293.png" alt="image-20241018161832293">&lt;/p>
&lt;ul>
&lt;li>其中，H是Loss对权重的海森矩阵，依然对公式(1)进行近似简化，忽略公式(1)的第1项和第3项。&lt;/li>
&lt;li>那么，删除权重w&lt;sub>q&lt;/sub>的操作可以表示为公式(2)，其中e&lt;sub>q&lt;/sub>是权重空间w中相对于w&lt;sub>q&lt;/sub>的单位向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162056961.png" alt="image-20241018162056961">&lt;/p>
&lt;ul>
&lt;li>那么，OBS的目标就是求解公式(3)的最优化问题：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162249685.png" alt="image-20241018162249685">&lt;/p>
&lt;ul>
&lt;li>转换为拉格朗日乘法：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162354240.png" alt="image-20241018162354240">&lt;/p>
&lt;ul>
&lt;li>求解出δw，即剪切w&lt;sub>q&lt;/sub>的同时，调整了其它权重，并保证Loss变动最小。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018162518230.png" alt="image-20241018162518230">&lt;/p>
&lt;h2 id="2obs为什么优于obd">(2)OBS为什么优于OBD？&lt;/h2>
&lt;p>论文基于上述推导，解释了为什么OBS优于OBD：&lt;/p>
&lt;ul>
&lt;li>基于OBD，
&lt;ul>
&lt;li>从w&lt;sup>*&lt;/sup>处的局部最小值开始，基于贡献度误删除了权重2(假设：上帝知道权重2不应该被删掉)。&lt;/li>
&lt;li>通过再次训练，权重1将增加，权重1将在错误的方向上越走越远，直到达到平衡，但此时不是局部最优。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>基于OBS，
&lt;ul>
&lt;li>从w&lt;sup>*&lt;/sup>处的局部最小值开始，删除权重2的同时，权重1也会被修正。&lt;/li>
&lt;li>通过再次训练，由于局部最优的约束，会尽快达到稳态。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(2)/image-20241018163243850.png" alt="image-20241018163243850">&lt;/p>
&lt;h1 id="4总结">4.总结&lt;/h1>
&lt;p>为了解读QLoRA的原理，我们扩展阅读了模型剪枝技术，并解读了经典的两篇论文OBD和OBS：&lt;/p>
&lt;ul>
&lt;li>模型剪枝：通过移除神经网络中的冗余连接或神经元，达到降低模型复杂度的目的。&lt;/li>
&lt;li>关键问题：删什么权重，怎么删除？&lt;/li>
&lt;li>OBD：Optimal Brain Damage (OBD)，利用目标函数的二阶导数信息来评估每个权重的重要性，即通过海森矩阵(Hessian matrix)来确定权重的“贡献度”或“saliency”。&lt;/li>
&lt;li>OBS：Optimal Brain Surgeon(OBS)，利用相对权重扰动的Loss函数进行泰勒展开，剪切w&lt;sub>q&lt;/sub>的同时，调整了其它权重，并保证Loss变动最小。&lt;/li>
&lt;/ul>
&lt;p>论文链接1：https://arxiv.org/pdf/2303.10512&lt;/p>
&lt;p>论文链接2：https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html&lt;/p></description></item><item><title>【chatGPT】学习笔记56-AgentScope解读-模型</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-agentscope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 18 Oct 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-agentscope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p>模型是智能体的&amp;quot;大脑&amp;rdquo;，帮助智能体实现复杂的自然语言交互和精准的推理决策。&lt;/p>
&lt;p>本文为小伙伴介绍AgentScope中模型的部署和调用。&lt;/p>
&lt;h1 id="1agentscope模型服务">1.AgentScope模型服务&lt;/h1>
&lt;h2 id="11支持的模型">1.1.支持的模型&lt;/h2>
&lt;p>构建一个多智能体系统，需要根据任务的复杂性和不同子任务的专业领域，用到多种类型的模型和不同厂商的模型。&lt;/p>
&lt;p>从模型类型看，多智能体系统可能需要综合运用大语言模型和多模态模型：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>大语言模型&lt;/strong>：用于处理复杂的语言任务，促进智能体之间的交流和任务指示的解释。&lt;/li>
&lt;li>&lt;strong>多模态模型&lt;/strong>：处理图像、视频和语音等多种信息，帮助智能体理解和响应非文本输入。&lt;/li>
&lt;/ul>
&lt;p>从厂商看，多智能体系统针对不同下游任务可能选择不同厂商的模型：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>OpenAI&lt;/strong>：在推理和任务分解上可能需要调用OpenAI的模型，以获得更好的任务分解效果。&lt;/li>
&lt;li>&lt;strong>Qwen&lt;/strong>：在多模态相关任务上可能需要调用QwenVL，以平衡推理成本和推理性能。&lt;/li>
&lt;/ul>
&lt;p>基于上述使用场景，AgentScope内置了以下模型服务API，支撑自然语言、多模态等各类任务的应用。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>OpenAI API&lt;/strong>：对话（Chat）、图片生成（DALL-E）和文本嵌入（Embedding）。&lt;/li>
&lt;li>&lt;strong>DashScope API&lt;/strong>：对话（Chat）、图片生成（Image Synthesis）和文本嵌入（Text Embedding）。&lt;/li>
&lt;li>&lt;strong>Gemini API&lt;/strong>：对话（Chat）和嵌入（Embedding）。&lt;/li>
&lt;li>&lt;strong>ZhipuAi API&lt;/strong>：对话（Chat）和嵌入（Embedding）。&lt;/li>
&lt;li>&lt;strong>Ollama API&lt;/strong>：对话（Chat）、嵌入（Embedding）和生成（Generation）。&lt;/li>
&lt;li>&lt;strong>LiteLLM API&lt;/strong>：对话（Chat），兼容多种模型的API。&lt;/li>
&lt;li>&lt;strong>Post请求API&lt;/strong>：基于Post请求实现的模型推理服务，包括Huggingface/ModelScope Inference API和各种符合Post请求格式的API。&lt;/li>
&lt;/ul>
&lt;h2 id="12模型服务">1.2.模型服务&lt;/h2>
&lt;h3 id="1模型服务集成的难点">(1)模型服务集成的难点&lt;/h3>
&lt;p>多智能体系统需要集成多种类型的模型和不同厂商的模型，但不同模型提供的API风格迥异，因此多智能体系统的模型服务层面临多适配的难点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>API风格差异&lt;/strong>：不同的模型服务由不同的团队或厂家开发，采用不同的设计理念和技术栈，API的风格差异大，如参数命名、请求格式、响应结构等。&lt;/li>
&lt;li>&lt;strong>功能特性差异&lt;/strong>：不同的模型服务专注于不同的功能或应用场景，需要根据功能需求来设计API。例如，一些模型专注于文本生成，而另一些可能专注于图像识别。&lt;/li>
&lt;li>&lt;strong>性能模式差异&lt;/strong>：不同的模型服务可能针对不同的性能要求进行优化。例如，一些模型可能需要更快的响应时间，而另一些可能需要处理更大规模的数据。这些性能差异可能导致API设计上的不同。&lt;/li>
&lt;li>&lt;strong>安全性和权限管理差异&lt;/strong>：不同的模型服务可能有不同的安全要求和权限管理机制。例如，一些服务可能需要更复杂的认证流程，而另一些可能提供更细粒度的访问控制。这些安全差异也会影响API的设计。&lt;/li>
&lt;li>&lt;strong>版本迭代导致的差异&lt;/strong>：模型服务版本迭代和更新的过程中，API可能会产生变化，这导致不同版本的API之间存在差异。&lt;/li>
&lt;/ul>
&lt;h3 id="2-agentscope模型服务的解耦设计">(2) AgentScope模型服务的解耦设计&lt;/h3>
&lt;p>AgentScope通过解耦设计，让模型的集成和维护变得灵活、简单：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>统一的接口抽象&lt;/strong>：AgentScope定义了一个统一的接口抽象，即&lt;code>ModelWrapper&lt;/code>基类。所有的模型服务API都必须实现这个基类，从而确保了接口的一致性。这样，无论底层API如何变化，上层的应用代码都可以保持不变。&lt;/li>
&lt;li>&lt;strong>模型配置的灵活性&lt;/strong>：AgentScope允许用户通过模型配置来指定使用的模型服务。这些配置可以是字典、字典列表，或者是指向模型配置文件的路径。这种灵活性使得用户可以轻松地切换不同的模型服务，而不需要修改代码。&lt;/li>
&lt;li>&lt;strong>子类化和多态&lt;/strong>：AgentScope通过子类化和多态来支持不同的模型服务API。每个模型服务都有一个对应的&lt;code>ModelWrapper&lt;/code>子类，这些子类实现了与特定模型服务交互的具体逻辑。当用户通过配置指定使用某个模型服务时，AgentScope会动态地创建相应的&lt;code>ModelWrapper&lt;/code>子类实例。&lt;/li>
&lt;li>&lt;strong>自定义模型包装器&lt;/strong>：AgentScope允许开发者自定义自己的模型包装器。开发者可以通过继承&lt;code>ModelWrapperBase&lt;/code>类并实现必要的方法来创建自定义的&lt;code>ModelWrapper&lt;/code>子类。这样，即使AgentScope原生不支持某个模型服务，开发者也可以通过自定义包装器来实现支持。&lt;/li>
&lt;li>&lt;strong>脚本和工具的支持&lt;/strong>：AgentScope提供了一些脚本来帮助开发者快速搭建和部署模型服务。这些脚本和工具可以简化部署过程，使得开发者可以更容易地集成不同的模型服务。&lt;/li>
&lt;/ul>
&lt;h3 id="3agentscope-model-wrapper解耦设计的优势">(3)AgentScope Model Wrapper解耦设计的优势&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>易用性&lt;/strong>：用户不需要了解模型服务的具体实现，只需通过配置和统一的接口来使用模型。&lt;/li>
&lt;li>&lt;strong>灵活性&lt;/strong>：如果需要更换模型服务，用户只需要修改配置文件，而不需要修改调用模型的代码。&lt;/li>
&lt;li>&lt;strong>可维护性&lt;/strong>：模型服务的更新和维护在&lt;code>ModelWrapper&lt;/code>内部进行，无论底层模型服务如何变化，只要&lt;code>ModelWrapper&lt;/code>接口保持不变，用户代码就不需要修改。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-AgentScope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/image-20241025145705780.png" alt="image-20241025145705780">&lt;/p>
&lt;h1 id="2搭建自己的模型服务">2.搭建自己的模型服务&lt;/h1>
&lt;p>在上一篇“AgentScope解读之概述”中，我们已经尝试过用AgentScope的API快速构建一套模型服务。&lt;/p>
&lt;p>如果想使用已有的本地模型搭建自己的模型服务，如何操作呢？下面我们就动手实操一下。&lt;/p>
&lt;p>AgentScope内置了一些脚本来帮助开发者快速搭建自己的模型服务，在AgentScope的&lt;code>scripts&lt;/code>目录下，包括：&lt;/p>
&lt;ul>
&lt;li>CPU推理引擎ollama&lt;/li>
&lt;li>基于Flask + Transformers的模型服务&lt;/li>
&lt;li>基于Flask + ModelScope的模型服务&lt;/li>
&lt;li>FastChat推理引擎&lt;/li>
&lt;li>vllm推理引擎&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-AgentScope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/image-20241025151841025.png" alt="image-20241025151841025">&lt;/p>
&lt;p>我们以vllm为例，来搭建一套本地模型服务。&lt;/p>
&lt;h2 id="step1安装vllm">STEP1.安装vllm&lt;/h2>
&lt;p>在python环境中安装vllm，可通过2种方式：&lt;/p>
&lt;ul>
&lt;li>pip直接安装：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">pip install vllm
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>从源码编译安装(如果pip安装遇到问题)：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="step2启动大模型">STEP2.启动大模型&lt;/h2>
&lt;ul>
&lt;li>本示例使用Llama模型，从 ModelScope 平台下载模型文件，上传到服务器&amp;quot;meta-llama/Llama-2-7b-chat-hf&amp;quot;目录下。&lt;/li>
&lt;li>直接使用AgentScope提供脚本，命令如下：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">./vllm/vllm_setup.sh -m meta-llama/Llama-2-7b-chat-hf -p 8000
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="step3agentscope的模型配置">STEP3.AgentScope的模型配置&lt;/h2>
&lt;p>在AgentScope中，模型的配置是通过&lt;code>model_configs&lt;/code>参数来指定的。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>用户通过提供模型配置来指定要使用的模型。这些配置可以是字典、字典列表，或者是指向模型配置文件的路径。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>模型配置中包含了模型的类型（&lt;code>model_type&lt;/code>）、名称（&lt;code>model_name&lt;/code>）、API密钥（&lt;code>api_key&lt;/code>）、组织名称（&lt;code>organization&lt;/code>）以及其他可能的参数（如生成参数&lt;code>generate_args&lt;/code>）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在vllm/model_config.json中，配置模型服务信息：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;model_type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;openai_chat&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;config_name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;vllm_llama2-7b-chat-hf&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;model_name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;meta-llama/Llama-2-7b-chat-hf&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;api_key&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;EMPTY&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;client_args&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;base_url&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;http://127.0.0.1:8000/v1/&amp;#34;&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="nt">&amp;#34;generate_args&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;temperature&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">0.5&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="step4初始化agentscope模型服务">STEP4.初始化AgentScope模型服务&lt;/h2>
&lt;ul>
&lt;li>使用agentscope.init初始化模型服务，模型配置使用上一步配好的文件。&lt;/li>
&lt;li>然后就可以使用该模型服务创建agent去执行任务了。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">agentscope&lt;/span>
&lt;span class="c1"># 读取模型配置&lt;/span>
&lt;span class="n">agentscope&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_configs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;./model_configs.json&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文要点如下：&lt;/p>
&lt;ul>
&lt;li>AgentScope内置支持多种模型服务API，包括OpenAI、DashScope、Gemini、ZhipuAi、Ollama、LiteLLM和Post请求API等。&lt;/li>
&lt;li>AgentScope通过&lt;code>ModelWrapper&lt;/code>实现模型的部署和调用解耦，实现对不同模型的灵活部署和调用。&lt;/li>
&lt;li>AgentScope支持自定义模型服务，并内置了服务部署脚本。&lt;/li>
&lt;/ul>
&lt;p>参考：https://arxiv.org/pdf/2402.14034&lt;/p></description></item><item><title>【chatGPT】学习笔记54-AgentScope解读-概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-agentscope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 18 Oct 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-agentscope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/</guid><description>&lt;p>人类一直试图构建一个能够自主完成任务的代理或实体，即&lt;strong>智能体 (AI Agents 或 Agents)&lt;/strong>。大型语言模型（LLM）的引入为智能体的构建提供了新的核心组件，让智能体能够理解复杂指令，具有了更广泛的应用场景，智能体也不再局限于处理单一任务的单智能体，能够处理复杂任务的多智能体已成为当前热点。&lt;/p>
&lt;p>本文为小伙伴介绍阿里多智能体框架——AgentScope.&lt;/p>
&lt;h1 id="1agentscope简介">1.AgentScope简介&lt;/h1>
&lt;h2 id="11agentscope是什么">1.1.AgentScope是什么&lt;/h2>
&lt;p>AgentScope是阿里巴巴于2024年2月开源的一款&lt;strong>多智能体(Multi-Agent)开发平台&lt;/strong>，旨在使开发者能够轻松地构建基于大语言模型的多智能体应用程序。AgentScope提供了一系列的开发工具和组件，支持多模态数据，并引入了分布式机制(通过集中式编程实现复杂的分布式工作流)。&lt;/p>
&lt;h2 id="12agentscope的关键概念">1.2.AgentScope的关键概念&lt;/h2>
&lt;p>AgentScope框架包含如下四个关键概念：&lt;/p>
&lt;h3 id="1消息message">(1)消息(Message)&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>消息是多智能体对话中的&lt;strong>信息交换载体&lt;/strong>，承载着信息的来源和内容。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在AgentScope中，消息字典类型，包括&lt;code>name&lt;/code>和&lt;code>content&lt;/code>字段，多模态消息还会有&lt;code>url&lt;/code>字段。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AgentScope官方示例：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-AgentScope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/image-20241017072749521.png" alt="image-20241017072749521">&lt;/p>
&lt;h3 id="2智能体agent">(2)智能体(Agent)&lt;/h3>
&lt;ul>
&lt;li>Agent是能够与环境和其他智能体交互，并采取行动改变环境的自主实体。&lt;/li>
&lt;li>在AgentScope中， 智能体以消息作为输入，并生成相应的响应消息。&lt;/li>
&lt;li>AgentScope官方示例：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-AgentScope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/image-20241017072835706.png" alt="image-20241017072835706">&lt;/p>
&lt;h3 id="3服务service">(3)服务(Service)&lt;/h3>
&lt;ul>
&lt;li>服务是可被智能体调用，帮助智能体执行特定任务的功能性API。&lt;/li>
&lt;li>在AgentScope中，服务分为模型API服务(用于使用大语言模型)和通用API服务(提供各种工具函数)。&lt;/li>
&lt;/ul>
&lt;h3 id="4工作流workflow">(4)工作流(Workflow)&lt;/h3>
&lt;ul>
&lt;li>工作流描述了多智能体执行的流程序列，它是消息交换的有序序列，用于协作多智能体的工作过程。&lt;/li>
&lt;li>通俗理解，工作流类似于TensorFlow中的计算图，也类似于LangChain的LCEL工作流。&lt;/li>
&lt;/ul>
&lt;h2 id="13agentscope的特点">1.3.AgentScope的特点&lt;/h2>
&lt;p>多智能体的应用系统面临众多工程化挑战：&lt;/p>
&lt;ul>
&lt;li>支持创建和管理多个智能体&lt;/li>
&lt;li>每个智能体可以调用来自不同厂商的大模型&lt;/li>
&lt;li>多智能体支持分布式部署与运行&lt;/li>
&lt;/ul>
&lt;p>因此，多智能体框架在易用性、兼容性、性能等方面有极高的要求。&lt;/p>
&lt;p>AgentScope作为多智能体框架，具有如下特性：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>易用性&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>AgentScope注重开发者体验，提供了灵活易用的语法糖、开箱即用的组件和预构建的multi-agent样例。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>鲁棒性&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>集成了服务级重试机制和规则性修正工具，确保了对多种模型和APIs的容错性。&lt;/li>
&lt;li>提供了可定制的容错配置，使开发者能够通过参数来自定义容错机制。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>多模态数据支持&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>支持多模态数据（如文件、图像、音频和视频）的对话展示、消息传输和数据存储。&lt;/li>
&lt;li>基于统一的URL属性解耦多模态数据的传输和存储，最大限度地减少了消息在每个智能体的内存使用量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>分布式部署&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>引入了actor模式，实现了复杂分布式工作流的集中式编程。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="14agentscope的逻辑架构">1.4.AgentScope的逻辑架构&lt;/h2>
&lt;p>AgentScope的设计重点在于易用性和鲁棒性，其架构包括三个层级：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Utility Layer 实用程序层&lt;/strong> ：单个智能体的基本和高级功能
&lt;ul>
&lt;li>提供基本服务来支持代理的核心功能。&lt;/li>
&lt;li>抽象了底层的复杂操作，如模型 API 调用和服务功能（包括代码执行和数据库操作）。&lt;/li>
&lt;li>提供内置的自主重试机制，用于针对意外中断的异常和错误处理。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Manager and Wrapper Layer 管理器和包装层&lt;/strong>：资源和运行时管理
&lt;ul>
&lt;li>管理资源和API 服务，确保资源的高可用性并防止来自LLMs的不良响应。&lt;/li>
&lt;li>提供用于容错控制的可定制接口。&lt;/li>
&lt;li>负责维护代理的操作完整性，提升LLMs在不同条件下的一致性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Agent Layer 智能体层&lt;/strong>：智能体级到工作流级的编程接口
&lt;ul>
&lt;li>Agent是负责交互和通信的主要实体。&lt;/li>
&lt;li>集成简化的语法和工具，减轻了开发者的编程负担。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-AgentScope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/image-20241016083605408.png" alt="image-20241016083605408">&lt;/p>
&lt;h1 id="2agentscope应用实例">2.AgentScope应用实例&lt;/h1>
&lt;p>下面，我们来部署一套AgentScope环境，并通过一个简单的对话示例来介绍AgentScope的基本用法。&lt;/p>
&lt;h2 id="21搭建环境">2.1.搭建环境&lt;/h2>
&lt;ul>
&lt;li>创建python虚拟环境：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1">## 创建虚拟环境&lt;/span>
conda create -n agentscope &lt;span class="nv">python&lt;/span>&lt;span class="o">=&lt;/span>3.10
&lt;span class="c1"># 激活虚拟环境&lt;/span>
conda activate agentscope
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>安装AgentScope和依赖：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1">## AgentScope 支持可选依赖如下：&lt;/span>
ollama: Ollama API
litellm: Litellm API
zhipuai: Zhipuai API
gemini: Gemini API
service: 不同工具函数的依赖
distribute: 分布式模式的依赖
full: 一次性安装上述所有的依赖，可能耗时较长
&lt;span class="c1">## 安装命令格式如下，可选择安装所有依赖：&lt;/span>
pip install agentscope&lt;span class="o">[&lt;/span>full&lt;span class="o">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="22配置模型">2.2.配置模型&lt;/h2>
&lt;p>Agentscope内置多种大模型服务API，用户可以通过设定模型配置来指定模型服务。&lt;/p>
&lt;ul>
&lt;li>模型配置：配置模型的名称、类型、路径等信息&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">model_config = {
&amp;#34;config_name&amp;#34;: &amp;#34;{config_name}&amp;#34;,
&amp;#34;model_type&amp;#34;: &amp;#34;openai_chat&amp;#34;,
&amp;#34;model_name&amp;#34;: &amp;#34;{model_name}&amp;#34;,
&amp;#34;api_key&amp;#34;: &amp;#34;xxx&amp;#34;,
&amp;#34;organization&amp;#34;: &amp;#34;xxx&amp;#34;,
}
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>多智能体使用不同模型，则可配置多个模型&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="c1"># 一次性初始化多个模型配置&lt;/span>
&lt;span class="n">openai_cfg_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="n">modelscope_cfg_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="c1"># ...&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="23构建智能体">2.3.构建智能体&lt;/h2>
&lt;ul>
&lt;li>初始化AgentScope，定义智能体(指定模型、设定角色)
&lt;ul>
&lt;li>下例中定义了2个智能体：dialogAgent、userAgent&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">agentscope&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.agents&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">DialogAgent&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">UserAgent&lt;/span>
&lt;span class="c1"># 读取模型配置&lt;/span>
&lt;span class="n">agentscope&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_configs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;./model_configs.json&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 创建一个对话智能体和一个用户智能体&lt;/span>
&lt;span class="n">dialogAgent&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">DialogAgent&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistant&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">model_config_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;gpt-4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sys_prompt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;You are a helpful ai assistant&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">userAgent&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">UserAgent&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="24执行智能体">2.4.执行智能体&lt;/h2>
&lt;ul>
&lt;li>Message 消息&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">agentscope.message&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Msg&lt;/span>
&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Alice&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Hi!&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Msg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Bob&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;What about this picture I took?&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/path/to/picture.jpg&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>开始对话&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="bp">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dialogAgent&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">userAgent&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 如果用户输入&amp;#34;exit&amp;#34;，则终止对话&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;exit&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Exiting the conversation.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">break&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>AgentScope是一款多智能体(Multi-Agent)开发平台&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>AgentScope的特点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>易用性&lt;/li>
&lt;li>鲁棒性&lt;/li>
&lt;li>多模态数据支持&lt;/li>
&lt;li>分布式部署&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>AgentScope的架构&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Utility Layer 实用程序层&lt;/strong> ：单个智能体的基本和高级功能&lt;/li>
&lt;li>&lt;strong>Manager and Wrapper Layer 管理器和包装层&lt;/strong>：资源和运行时管理&lt;/li>
&lt;li>&lt;strong>Agent Layer 智能体层&lt;/strong>：智能体级到工作流级的编程接口&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>参考：https://arxiv.org/pdf/2402.14034&lt;/p></description></item><item><title>【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/</link><pubDate>Fri, 20 Sep 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/</guid><description>&lt;p>我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后续3~4篇文章来详细理解一下QLoRA。今天我先简单地介绍一下模型量化技术。&lt;/p>
&lt;h1 id="1模型量化技术概览">1.模型量化技术概览&lt;/h1>
&lt;h2 id="11模型参数与显存占用计算方法">1.1.模型参数与显存占用计算方法&lt;/h2>
&lt;p>在模型训练、模型推理的过程中，需要考虑GPU的显存是否满足要求，那么模型参数与GPU显存具体怎么估算呢？&lt;/p>
&lt;ul>
&lt;li>&lt;strong>总参数量&lt;/strong>：已Qwen2-7B-Instruct为例，7B就是参数量为7Billion，即70亿参数。&lt;/li>
&lt;li>&lt;strong>每个参数的显存占用&lt;/strong>：假设Qwen2-7B-Instruct的dtype选择为Float16，那么每个参数占2个字节，即16bit。&lt;/li>
&lt;li>&lt;strong>总显存量&lt;/strong>：&lt;font color=red>总参数量 = 每个参数的显存占用 * 总参数量&lt;/font>，因此：
&lt;ul>
&lt;li>QWen2-7B-Instruct总显存占用量 = 2字节 * 70亿 = 140亿字节 = 14 * 10&lt;sup>9&lt;/sup>字节&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>总显存量换算&lt;/strong>：&lt;font color=red>1GB = 2&lt;sup>30&lt;/sup>B ≈ 10&lt;sup>9&lt;/sup>字节&lt;/font>，因此：
&lt;ul>
&lt;li>Qwen2-7B-Instruct总显存占用量 = 14GB&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们将Qwen2-7B-Instruct以Float16加载到GPU中，观察一下GPU的显存占用大致也是14GB。&lt;/p>
&lt;p>根据上述原理，我们可以提出一个问题：&lt;strong>&lt;font color=red>如果降低每个参数的显存占用，是不是就可以降低GPU的总显存量？&lt;/font>&lt;/strong>&lt;/p>
&lt;h2 id="12模型量化解决的问题及关键要素">1.2.模型量化解决的问题及关键要素&lt;/h2>
&lt;p>&lt;strong>模型量化(Model Quantization)&lt;/strong>，就是聚焦于用更少位数的信息表示法，降低模型每个参数的显存占用量，进而降低模型的总显存占用量。&lt;/p>
&lt;ul>
&lt;li>如：模型权重以Float32存储(即每个参数的显存占用量为32bit)，假设总显存占用量M。&lt;/li>
&lt;li>如果将每个参数量化为Float16(即每个参数的显存占用量为16bit)，则只需要&lt;strong>二分之一&lt;/strong>的总显存占用量M。&lt;/li>
&lt;li>如果将每个参数量化为Int8(即每个参数的显存占用量为8bit)，则只需要&lt;strong>四分之一&lt;/strong>的总显存占用量M。&lt;/li>
&lt;li>如果将每个参数量话为NF4(即每个菜单树的显存占用量为4bit)，则只需要&lt;strong>八分之一&lt;/strong>的总显存占用量M。&lt;/li>
&lt;/ul>
&lt;p>因此，我们可以很自然地想到模型量化技术的&lt;strong>关键要素之一&lt;/strong>——&lt;strong>量化精度&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>用更少位数表示更高位数的数字，精度不会丢失吗？丢失的精度会给模型训练和模型推理带来多少误差？&lt;/li>
&lt;li>简单地将模型的参数向量的每个元素认为是一个用&lt;strong>32bit表达的小数转为用4bit表达&lt;/strong>，直觉上就会带来极大地误差。&lt;/li>
&lt;/ul>
&lt;p>另外，还有其它2个关键要素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型架构&lt;/strong>&lt;/li>
&lt;li>&lt;strong>硬件设备&lt;/strong>：GPU、NPU、TPU等&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723154227521.png" alt="image-20240723154227521">&lt;/p>
&lt;h1 id="2量化三步曲">2.量化三步曲&lt;/h1>
&lt;p>量化过程包括：&lt;font color=red>&lt;strong>向量量化、算子量化、模型量化&lt;/strong>&lt;/font>&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723160736130.png" alt="image-20240723160736130">&lt;/p>
&lt;h2 id="21向量量化">2.1.向量量化&lt;/h2>
&lt;p>向量量化是所有量化的基础，量化过程包括&lt;strong>量化函数&lt;/strong>和&lt;strong>反量化函数&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>假设&lt;strong>量化函数&lt;/strong>为：&lt;strong>X&lt;/strong>表示原始向量，&lt;strong>INT8&lt;/strong>表示&lt;strong>量化后向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162534164.png" alt="image-20240723162534164">&lt;/p>
&lt;ul>
&lt;li>假设&lt;strong>反量化函数&lt;/strong>为：&lt;strong>INT8&lt;/strong>表示&lt;strong>量化后向量&lt;/strong>，&lt;strong>QX&lt;/strong>表示通过&lt;strong>量化后向量&lt;/strong>，还原的&lt;strong>还原向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162633015.png" alt="image-20240723162633015">&lt;/p>
&lt;ul>
&lt;li>假设&lt;strong>误差函数&lt;/strong>为：&lt;strong>原始向量X&lt;/strong>和&lt;strong>还原向量QX&lt;/strong>可能存在误差，误差当然越小越好，衡量这个误差的函数就是误差函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162954824.png" alt="image-20240723162954824">&lt;/p>
&lt;h3 id="1int4">(1)INT4&lt;/h3>
&lt;p>以&lt;strong>INT4的向量量化&lt;/strong>过程为例：&lt;/p>
&lt;ul>
&lt;li>假设&lt;strong>向量系数&lt;/strong>为：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165432046.png" alt="image-20240723165432046">&lt;/p>
&lt;ul>
&lt;li>假设&lt;strong>原始向量&lt;/strong>为：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165524411.png" alt="image-20240723165524411">&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>原始向量&lt;/strong>进行计算：
&lt;ul>
&lt;li>求每个分量的绝对值的最大值，结果为1.52。&lt;/li>
&lt;li>将每个分量除以1.52，得到&lt;strong>过程向量&lt;/strong>[0.18, 0.38, -0.49, 1.00]。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165559900.png" alt="image-20240723165559900">&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>过程向量&lt;/strong>进行计算：
&lt;ul>
&lt;li>在量化系数中，找到最近的值，得到&lt;strong>最近值向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165922698.png" alt="image-20240723165922698">&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>最近值向量&lt;/strong>进行计算，得到最终的&lt;strong>量化后向量&lt;/strong>：
&lt;ul>
&lt;li>将最近值向量的每个分量用量化系数的索引进行替换，得到最终的&lt;strong>量化后向量&lt;/strong>[9, 10, 4, 15]。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170126385.png" alt="image-20240723170126385">&lt;/p>
&lt;p>再来看看&lt;strong>INT4的向量反量化&lt;/strong>过程：&lt;/p>
&lt;ul>
&lt;li>将&lt;strong>量化后向量&lt;/strong>，根据索引还原为&lt;strong>过程向量&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170439886.png" alt="image-20240723170439886">&lt;/p>
&lt;ul>
&lt;li>将&lt;strong>过程向量&lt;/strong>乘以稀疏1.52，得到&lt;strong>还原向量&lt;/strong>：
&lt;ul>
&lt;li>还原向量为[0.304, 0.5016, -0.7144, 1.52]。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170605292.png" alt="image-20240723170605292">&lt;/p>
&lt;ul>
&lt;li>我们可以看到，&lt;strong>原始向量&lt;/strong>和&lt;strong>还原向量&lt;/strong>存在一定的&lt;strong>误差&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170731983.png" alt="image-20240723170731983">&lt;/p>
&lt;p>从数据分布上看，INT4量化方法更适合如下数据分布特征(中段数据过于一致，此量化方法有局限性)：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240724095119629.png" alt="image-20240724095119629">&lt;/p>
&lt;h3 id="2fp4">(2)FP4&lt;/h3>
&lt;p>FP4的计算过程不赘述，可参考：&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#fp4-precision-in-a-few-words">https://huggingface.co/blog/4bit-transformers-bitsandbytes#fp4-precision-in-a-few-words&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920110352774.png" alt="image-20240920110352774">&lt;/p>
&lt;h3 id="3nf4">(3)NF4&lt;/h3>
&lt;p>NF4 Quant是QLoRA提出的一种新的数据类型(4-bit NormalFloat)，量化过程中保留了零点，使用2&lt;sup>k&lt;/sup>位表示k位数据类型。&lt;/p>
&lt;p>NF4通过估计&lt;strong>两个范围的分位数q&lt;sup>i&lt;/sup>&lt;/strong>，创建了一个非对称的数据类型。第一个范围表示负数部分[-1, 0]的2&lt;sup>k-1&lt;/sup>，第二个范围表示正数部分[0, 1]的2&lt;sup>k-1&lt;/sup>+1.&lt;/p>
&lt;p>这种量化方法在每个量化bin中都具备相等的期望值数量，这种以零为中心的正态分布数据在信息论中是最优的。&lt;/p>
&lt;p>上述数学方法到底解决什么问题呢？预训练的权重通常具有以零为中心的正态分布(标准差为σ)，缩放σ可以使得权重分布适应NF的范围，量化时的数据类型和神经网络权重的分位数最终都会被归一化到[-1, 1]的范围内。计算分位数的公式：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920112507633.png" alt="image-20240920112507633">&lt;/p>
&lt;p>更详细的代码参考：&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/csrc/kernels.cu">https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/csrc/kernels.cu&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="n">__device__&lt;/span> &lt;span class="k">static&lt;/span> &lt;span class="kt">float&lt;/span> &lt;span class="n">nf4_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">16&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.6961928009986877&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.5250730514526367&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.39491748809814453&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.28444138169288635&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.18477343022823334&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.09105003625154495&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.07958029955625534&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.16093020141124725&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.24611230194568634&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.33791524171829224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.44070982933044434&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.5626170039176941&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.7229568362236023&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="22算子量化">2.2.算子量化&lt;/h2>
&lt;p>在数学领域，一个函数空间到函数空间上的映射O：X→Y，称为算子。广义上，对任何函数进行某一项操作都是一个算子：&lt;/p>
&lt;ul>
&lt;li>如：微分算子&lt;/li>
&lt;li>如：不定积分算子&lt;/li>
&lt;/ul>
&lt;p>在深度学习中，各种网络结构，都是由若干的计算单元组成，这些计算单元就是算子(Operator)。&lt;/p>
&lt;ul>
&lt;li>如：卷积层(Convolution Layer)是一个算子&lt;/li>
&lt;li>如：全连接层(Fully-connected Layer， FC layer)中的权值求和过程，也是一个算子。&lt;/li>
&lt;/ul>
&lt;p>如下图所示，Conv1、Pool1、Conv2都是网络中的算子，Conv1和Conv2都是做卷积运算的算子(Convolution)。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920133118786.png" alt="image-20240920133118786">&lt;/p>
&lt;blockquote>
&lt;p>参考：https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/devguide/opdevg/tbeaicpudevg/atlasopdev_10_0006.html&lt;/p>
&lt;/blockquote>
&lt;p>算子量化如何开展呢？我们可以看到算子的前端有输入，算子的后端有输出。因此，算子量化就是将输入向量进行量化，对输出向量进行量化，而不修改算子的逻辑。但算子量化也有一些特殊情况：有的算子不能量化输入，有的算子不能量化输出。&lt;/p>
&lt;h2 id="23网络量化">2.3.网络量化&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>关闭冗余量化信息&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在量化神经网络过程中，需要有量化控制信息向量TQC。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在神经网络中，算子Conv连接到算子Reshape上，那么就会出现两个一模一样的TQC。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这种量化控制信息就是多余的，我们就需要消除掉Reshape侧的量化控制信息。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920134759671.png" alt="image-20240920134759671">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>图融合&lt;/strong>
&lt;ul>
&lt;li>本质是将N个标准的小算子进行融合，如：对Conv后的激活函数进行算子融合。&lt;/li>
&lt;li>算子融合后得到了一个新的非标的大算子，进而减少了量化控制信息。&lt;/li>
&lt;li>如下图：为了实现FlashAttention，将图中的MatMul算子(Cube）、Scale算子(Vector)、Mask算子(Vector)、SoftMax算子(Vector)融合为一个大的算子Flash Attention。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920135602648.png" alt="image-20240920135602648">&lt;/p>
&lt;blockquote>
&lt;p>参考：https://www.hiascend.com/doc_center/source/zh/canncommercial/80RC2/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0041.html&lt;/p>
&lt;/blockquote>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>为了解读QLoRA的原理，我们简要地介绍了模型量化技术的概念和基本原理：&lt;/p>
&lt;ul>
&lt;li>模型量化的第一层是向量量化，即针对向量量化，有量化函数和反量化函数。&lt;/li>
&lt;li>模型量化的第二层是算子量化，即针对算子的输入向量和输出向量进行量化。&lt;/li>
&lt;li>模型量化的第三层是网络量化，即针对网络去除冗余的量化控制信息、将小算子融合为大算子进行整体量化处理。&lt;/li>
&lt;/ul>
&lt;p>有了本文的模型量化基础知识，我们就可以进一步阅读QLoRA的论文了。&lt;/p>
&lt;p>论文链接：https://arxiv.org/pdf/2303.10512&lt;/p></description></item><item><title>【chatGPT】学习笔记52-麦肯锡《生成式AI与美国的未来工作》报告解读</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/</link><pubDate>Mon, 15 Jul 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/</guid><description>&lt;p>近期萝卜快跑成为了热点话题，人工智能对就业产生多大的影响成为社会关注的焦点。&lt;/p>
&lt;p>我们试图通过解读麦肯锡《生成式AI与美国的未来工作》研究报告，为大家展示美国的就业形式、生成式AI对美国劳动力市场的影响、以及美国社会和美国政府的观点及解决方案，以他山之石为我们提供一些思考。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240708090201905.png" alt="image-20240708090201905">&lt;/p>
&lt;h1 id="1核心观点1就业结构正在发生变化">1.核心观点1：就业结构正在发生变化&lt;/h1>
&lt;p>报告指出：&lt;strong>人力需求结构已发生变革&lt;/strong>——过去三年美国劳动市场已经历&lt;strong>860万职业转换&lt;/strong>，到2030年将再发生&lt;strong>1200万个职业转换&lt;/strong>，比两年前预测的&lt;strong>高出25%&lt;/strong>。&lt;/p>
&lt;p>调查报告通过数据分析，给我们展示了一张&lt;strong>宏观的就业结构图&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从AI技术对各行业的影响看&lt;/strong>：教育行业、金融行业、法律行业、艺术创作领域、STEM(科学技术)领域，AI技术将极大地提升这些行业和领域的自动化程度。&lt;/li>
&lt;li>&lt;strong>从职业转换的程度看&lt;/strong>：医疗行业新增岗位最多，建筑、运输、机械、房地产行业岗位变化大，制造、餐饮行业岗位需求量小且略有下跌，客服、办公服务(如：文员)需求量大幅下降。&lt;strong>预计在2030年，将产生1200万个职业转换&lt;/strong>。
&lt;ul>
&lt;li>我们可以发现，自动化程度很高的行业，对人的招聘需求会越来越低。&lt;/li>
&lt;li>如：在制造领域，AI和具身智能的渗透率非常高，导致这个行业自动化程度很高，因此对人的岗位需求出现了负增长。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：职业转换表示某个岗位消失，导致打工人需要转岗。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240715145303000.png" alt="image-20240715145303000">&lt;/p>
&lt;p>在就业结构发生变化的情况下，社会不同阶层的就业有哪些变化呢？&lt;/p>
&lt;ul>
&lt;li>&lt;strong>低工资职业的就业人数下降&lt;/strong>：年收入低于3.5万美元的就业人数下降3.7%，这种趋势会持续下去。&lt;/li>
&lt;li>&lt;strong>中等工资职业的就业人数保持稳定&lt;/strong>：年收入3.5万至5.7万美元的就业人数保持相对稳定，微幅增长0.1%。&lt;/li>
&lt;li>&lt;strong>高工资职业的就业人数增加&lt;/strong>：年收入超过5.7万美元的就业人数增加了9%，同时新增了约350万个就业岗位。&lt;/li>
&lt;/ul>
&lt;p>通过以上数据可以看到，&lt;strong>低端岗位在逐步减少，高端岗位在急剧增加&lt;/strong>。预计到2030年，最低工资阶层的工作将减少110万个，而最高工资阶层的工作将急剧增加380万个。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>可能增加的工作岗位：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>医疗保健&lt;/p>
&lt;/li>
&lt;li>
&lt;p>STEM(科学、技术、工程和数学)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>建筑&lt;/p>
&lt;/li>
&lt;li>
&lt;p>商业法律专业人员&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>可能消失的工作岗位&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>办公支持&lt;/li>
&lt;li>客户服务&lt;/li>
&lt;li>餐饮服务&lt;/li>
&lt;li>产业工人&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707232142776.png" alt="image-20240707232142776">&lt;/p>
&lt;p>但随着低端岗位在逐步减少以及高端岗位在急剧增加，也&lt;strong>造成了劳动力短缺&lt;/strong>的现象：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>劳动力短缺&lt;/strong>：2022年初~2023年上半年，出现了约1000万个职位空缺。&lt;/li>
&lt;li>&lt;strong>低端岗位产生了空缺&lt;/strong>：美国社会出现了打工人宁可不工作，也不愿意从事低端岗位的情况。&lt;/li>
&lt;li>&lt;strong>高端岗位产生了空缺&lt;/strong>：随着AI替代了某些人类岗位，打工人需要转型从事新的岗位，转型需要的时间导致了劳动力短缺。&lt;/li>
&lt;/ul>
&lt;p>职位空缺预示着&lt;strong>人力需求结构&lt;/strong>问题，也凸显了确保关键岗位被填补，以及为人们提供获得新岗位技能的紧迫性。&lt;/p>
&lt;p>最后，麦肯锡的研究报告归纳了3个结论：&lt;/p>
&lt;ul>
&lt;li>就业结构在AI等科技进步下发生了巨大变化。&lt;/li>
&lt;li>生成式AI将使部分&lt;strong>基础、低薪工作岗位消失&lt;/strong>，但&lt;strong>高端工作岗位会大量增加&lt;/strong>。&lt;/li>
&lt;li>劳动力短缺凸显了&lt;strong>人才需求结构失衡&lt;/strong>的情况。&lt;/li>
&lt;/ul>
&lt;h1 id="2核心观点2生成式ai是就业结构变化的重要因素">2.核心观点2：生成式AI是就业结构变化的重要因素&lt;/h1>
&lt;p>疫情和美国政府投资向基建倾斜等因素，导致了美国就业结构的变化。但生成式AI的出现以及新一代人工智能的爆发，进一步加速了就业结构的改变。&lt;/p>
&lt;ul>
&lt;li>生成式AI大大&lt;strong>加快所有场景的自动化进程&lt;/strong>：
&lt;ul>
&lt;li>预计到2030年，目前工种30%的工作量将被AI自动化替代。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707232511264.png" alt="image-20240707232511264">&lt;/p>
&lt;ul>
&lt;li>在生成式AI的加持下，所有场景的&lt;strong>自动化程度会显著提升&lt;/strong>：
&lt;ul>
&lt;li>没有生成式AI，可使用自动化替代人类工作的小时数，占美国经济目前工作小时数的21.5%。&lt;/li>
&lt;li>有了生成式AI，预计到2030年，这一比例将提升至29.5%。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707232558539.png" alt="image-20240707232558539">&lt;/p>
&lt;p>生成式AI提升了自动化率和自动化程度，不仅影响了低薪岗位，也影响了传统高薪岗位。这些传统高薪岗位包括：律师、计算机程序员、科学研究人员、市场研究人员、翻译员和金融顾问等——这些传统高薪岗位30%的工作任务都可以被借助生成式AI实现自动化，比如：&lt;/p>
&lt;ul>
&lt;li>律师需要花大量的时间搜索案例、修改法律模板文件，现在可以用生成式AI来做。&lt;/li>
&lt;li>土木工程师可以使用生成式AI来设计复杂的机械、电气和管道系统，加快设计过程。生成式AI可以更高效的考虑所有建筑规范、找到最节能或使用最少材料的方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707232643540.png" alt="image-20240707232643540">&lt;/p>
&lt;p>由于生成式AI技术的重要影响力，与AI相关的技术岗位、科学研究岗位需求也大幅增加：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>STEM(科学、技术、工程和数学)领域的工作预计到2030年将&lt;strong>增长23%&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>增长最快的职业包括&lt;strong>软件开发人员、计算机系统分析师和数据科学家&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>各行各业对数字化的持续扩大需求，推动对STEM人才需求的增加。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707232002157.png" alt="image-20240707232002157">&lt;/p>
&lt;h1 id="3核心观点3打工人企业主应该如何应对变化">3.核心观点3：打工人、企业主应该如何应对变化&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>打工人：拥抱AI技术，将AI作为生产力工具，打造超级个体。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>报告指出：&lt;strong>生成式AI带来的技术革命不可阻挡&lt;/strong>，变革的步伐还在加速。传统岗位会非常快地被AI所替代，因此&lt;strong>每个人都会面临职业转换&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>打工人们应该将这些AI工具视为工作增强器，而不是替代人类工作的破坏者。&lt;strong>当机器接管了枯燥或不愉快的任务时，人们可以留下更多需要创造力、解决问题能力和与他人合作的有趣工作。&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E3%80%8A%E7%94%9F%E6%88%90%E5%BC%8FAI%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E3%80%8B%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/image-20240707234640601.png" alt="image-20240707234640601">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>企业主：需要创新招聘策略，思考人机协同新场景下的劳动制度。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>报告指出：企业主将面临高端岗位招聘困难，因此企业主需要有一个持续的实施策略，包括对人才和生态系统的补充投资。&lt;/li>
&lt;li>创新性的招聘策略包括：提高工作质量、薪酬、福利，扩大招聘范围——生成式AI降低了专业门槛和技术难度，因此可以引入跨专业人才、低学历人才。&lt;/li>
&lt;li>生成式AI工具将成为人类的提效降本助手，因此企业主需要制定&lt;strong>符合人机协同新场景下的劳动制度&lt;/strong>。&lt;/li>
&lt;li>企业主还应该考虑提高AI可替代岗位的自动化率和自动化程度，这将为企业带来更大的长期价值。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>政府：制定宏观政策，为工人提供学习机会以适应变革。&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>随着数以百万计的工作可能因自动化而被淘汰，以及更多在需要不同技能的领域中被创造出来，政府需要雇主、教育机构、非营利组织、工会和行业团体联合起来，应对这一挑战。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>大学前：学校中设立与未来工作(STEM)相关的课程，以激励孩子们在STEM领域的学习和未来职业选择。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>大学：鼓励企业与大学和社区学院合作，建立高度针对性的职业管道和学术计划，以满足高需求领域。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>职业教育：企业与教育机构合作，创建专业课程和职业发展通道，以满足特定行业的需求。&lt;/p>
&lt;ul>
&lt;li>面向应聘者：教育机构根据本地产业需求定制课程，企业为学生提供实践机会和就业机会，如纽约城市大学在数据分析和网络安全课程上的修订。&lt;/li>
&lt;li>面向失业人群：组织可以建立培训计划和路径，将失业者引入需求高的工作岗位。例如，确定新员工所需的基本技能，然后将这些技能提炼成短期的四到十二周的技术培训课程，并伴有导师指导和与合作雇主的安置服务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4思考">4.思考&lt;/h1>
&lt;p>上述就是对《生成式AI与美国未来工作》调查报告的解读，从报告中可以看到AI技术对未来就业与劳动力的巨大影响和挑战：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>科技进步带来的影响不可阻挡&lt;/strong>：
&lt;ul>
&lt;li>报告中委婉地提出&lt;strong>职业转换&lt;/strong>这个概念，本质就是AI可能造成人类的失业。&lt;/li>
&lt;li>但，科技进步是历史潮流，科技带来的影响也不可阻挡。&lt;/li>
&lt;li>打工人、企业主、政府需要共同面对科技进步带来的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>AI竞争是残酷的&lt;/strong>：
&lt;ul>
&lt;li>发展中国家依靠中低端产业和廉价劳动力推动经济发展，与发达国家形成差异化竞争。&lt;/li>
&lt;li>但，生成式AI带来的自动化能力提升，AI和机器人的劳动力成本将远低于人类的劳动力成本。&lt;/li>
&lt;li>这样，发展中国家必须积极追赶AI技术发展的步伐，降低和发达国家科技上的差距——这是一个不进则退的科技竞争游戏。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>政府需要做好&lt;strong>顶层设计&lt;/strong>、企业主需要&lt;strong>提升自动化程度&amp;amp;制定长远招聘策略&lt;/strong>、打工人需要&lt;strong>掌握AI工具终身学习&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h1 id="5参考">5.参考&lt;/h1>
&lt;p>原文链接：https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america&lt;/p></description></item><item><title>【chatGPT】学习笔记51-LLM微调技术之AdaLoRA</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/</link><pubDate>Wed, 26 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/</guid><description>&lt;p>今天我们通过解读论文《&lt;strong>ADALORA: ADAPTIVE BUDGET ALLOCATION FOR&lt;/strong>
&lt;strong>PARAMETER-EFFICIENT FINE-TUNING&lt;/strong>》来学习一下AdaLoRA。&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：&lt;strong>AdaLoRA&lt;/strong>也是解决全参数微调参数量过大、成本过高的问题。&lt;/li>
&lt;li>&lt;strong>解决方案&lt;/strong>：论文提出了&lt;strong>AdaLoRA&lt;/strong>微调技术，是对&lt;strong>LoRA&lt;/strong>的一种改进，&lt;strong>LoRA&lt;/strong>没有考虑不同权重参数的重要性不同。AdaLoRA以奇异值分解的形式参数化增量更新，有效地修剪不重要更新的奇异值，避免了密集的精确SVD计算。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：在自然语言处理、问答和自然语言生成等多个预训练模型上的实验表明，AdaLoRA在低预算设置下，微调效果有显著的改进。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621144023004.png" alt="image-20240621144023004">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：LoRA有哪些局限性呢？论文给出了一个测试。
&lt;ul>
&lt;li>&lt;strong>左图&lt;/strong>：针对不同模块进行LoRA微调，效果是不一样的。针对前馈网络FFN进行LoRA微调的效果要远优于自注意力模块的LoRA微调。&lt;/li>
&lt;li>&lt;strong>右图&lt;/strong>：针对不同层进行LoRA微调，效果也是不一样的。10~12层的微调效果要远优于1~3层的微调效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621164711429.png" alt="image-20240621164711429">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>AdaLoRA的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>通过上述实验可以看到：在理想状态下，&lt;strong>微调关键模块/关键层的权重矩阵是最有效的&lt;/strong>，而微调不太重要的模块/层的权重矩阵不仅毫无意义，甚至有负面的影响。&lt;/li>
&lt;li>AdaLoRA针对关键矩阵设置为高秩，对于不重要的矩阵修剪为低秩。设置为高秩的增量矩阵可以捕获更细粒度的特征。&lt;/li>
&lt;li>AdaLoRA如何实现上述这种&lt;strong>对秩的调整&lt;/strong>呢？答案是数学工具&lt;strong>SVD(矩阵奇异值分解)&lt;/strong>，SVD是将高维矩阵拆分为三个矩阵，即&lt;strong>∆=PΛQ&lt;/strong>。P和Q分别表示∆的左/右奇异向量，Λ表示∆的对角矩阵。SVD在数据科学、机器学习中应用广泛，常用于降维、数据压缩、噪声过滤、文本数据的潜在语义结构提取等。&lt;/li>
&lt;li>AdaLoRA通过对某层、某模块的重要性评分，动态调整&lt;strong>∆=PΛQ&lt;/strong>的秩。AdaLoRA构造了N个三元组，每个三元组G&lt;sub>i&lt;/sub>=(P, Λ, Q)，AdaLoRA设计了一种重要性度量方法，重要性得分高的三元组被赋予高秩。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>AdaLoRA的实验效果：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>针对DeBERTaV3-base进行自然语言理解(GLUE)、问题(SQuADv1)进行了评估，AdaLoRA有了更好的性能表现。&lt;/li>
&lt;li>针对BART-large进行自然语言生成(XSum)进行了评估，AdaLoRA始终优于基线的性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>对Transformer的建模&lt;/strong>：Transformer模型有L个堆叠的块组成，每个块包含2个子模块——MHA(多头注意力)、FFN(全连接前馈网络)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>MHA函数&lt;/strong>：给定输入序列X∈R&lt;sup>n×d&lt;/sup>，MHA执行了h个多头注意力函数表示为&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621184305094.png" alt="image-20240621184305094">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>FFN函数&lt;/strong>：前馈网络由两个线性变换组成，中间有一个ReLU激活函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI拾遗/【chatGPT】学习笔记51-LLM微调技术之AdaLoRA/image-20240621185258000.png" alt="image-20240621185258000" style="zoom:80%;" />&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>对LoRA建模&lt;/strong>：LoRA的本质是用两个小矩阵替代一个大矩阵。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621185936493.png" alt="image-20240621185936493">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>LoRA模型的问题&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>LoRA的低秩分解方法选择了相同的秩r&lt;/li>
&lt;li>不同层的矩阵参数都采用相同的AB分解方法吗？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>SVD&lt;/strong>：SVD是对高维矩阵进行降维的行之有效的数学工具。
&lt;ul>
&lt;li>&lt;strong>SVD的目标&lt;/strong>：将1个高维矩阵，转换为3个低维矩阵，并且可以通过调整秩r来控制对角矩阵Λ(公式3)。&lt;/li>
&lt;li>&lt;strong>Λ的数学性质&lt;/strong>：对角矩阵Λ的特征值逐步递减，不严谨地说特征值越高，这个维度的特征越重要。&lt;/li>
&lt;li>&lt;strong>Λ的近似&lt;/strong>：当我们用秩r舍弃对角矩阵Λ较小的特征值，就实现了降维的同时又不丢失有效的信息。&lt;/li>
&lt;li>&lt;strong>PQ的数学性质&lt;/strong>：在SVD中，PQ要满足正交性，因此论文中引入了对P和Q的正则损失约束(公式4)。&lt;/li>
&lt;li>SVD的详细原理和数学推导本文不赘述，可以参考机器学习的PCA方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626162821750.png" alt="image-20240626162821750">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626170654390.png" alt="image-20240626170654390">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>重要性评估&lt;/strong>：某一层的某个参数矩阵对于整个Transformer到底是不是重要的？&lt;/p>
&lt;ul>
&lt;li>&lt;strong>G&lt;sub>k,i&lt;/sub>&lt;/strong>： 假设G这个三元组表示训练过程中对P、Λ、Q的更新。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626170842830.png" alt="image-20240626170842830">&lt;/li>
&lt;li>&lt;strong>C(P, E, Q)&lt;/strong>：假设C表示对P、Λ、Q的更新代价。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171118350.png" alt="image-20240626171118350">&lt;/li>
&lt;li>&lt;strong>R(P&lt;sub>k&lt;/sub>, Q&lt;sub>k&lt;/sub>)&lt;/strong>：假设R表示P和Q满足公式4R(P, Q)正交性的代价。&lt;/li>
&lt;li>&lt;strong>L(P, E, Q)&lt;/strong>：则，L表示训练目标是要保证C和R都尽量小，才能得到满足训练效果的最优训练成本。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171425594.png" alt="image-20240626171425594">&lt;/li>
&lt;li>&lt;strong>Λ&lt;sup>(t)&lt;/sup>&lt;sub>k&lt;/sub>&lt;/strong>：则，Λ&lt;sup>(t)&lt;/sup>&lt;sub>k&lt;/sub>表示采用梯度更新的计算过程(公式5)。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171551548.png" alt="image-20240626171551548">&lt;/li>
&lt;li>但是我们知道对每轮迭代、对每一层的高维参数矩阵进行SVD，存在巨大的计算成本，因此论文终于从逻辑上引出了重要性评估：&lt;/li>
&lt;li>&lt;strong>SVD剪枝&lt;/strong>：假设S是一个重要性评估的函数，那么Λ&lt;sup>(t)&lt;/sup>&lt;sub>k&lt;/sub>和S&lt;sup>(t)&lt;/sup>&lt;sub>k&lt;/sub>共同决定了Λ&lt;sup>(t+1)&lt;/sup>&lt;sub>k&lt;/sub>是否还有必要进行更新(公式6)。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171655605.png" alt="image-20240626171655605">&lt;/li>
&lt;li>&lt;strong>S&lt;sup>t&lt;/sup>&lt;sub>k&lt;/sub>&lt;/strong>：重要性评估函数考虑P、Λ、Q三个矩阵重要性的影响(公式7)，其中**s(·)**表示具体的重要性计算方法。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172223409.png" alt="image-20240626172223409">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>s(·)的三种计算方法&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>不考虑P、Q的影响&lt;/strong>：对角矩阵Λ中特征值越大越重要，这是传统的SVD判定重要性的方法，这种方法忽略了P和Q对各层参数矩阵的重要性影响。&lt;/li>
&lt;li>&lt;strong>考虑P、Q影响，但不考虑重要性函数的平滑&lt;/strong>：(公式8)。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172632400.png" alt="image-20240626172632400">&lt;/li>
&lt;li>考虑P、Q影响，但考虑重要性函数的平滑：什么情况会导致重要性不平滑呢？通俗地说，模型对于某一批次的数据已经训练地很好了，那么可能会导致认为当前的参数矩阵对整个模型并不重要。3个公式相当于增加了二级平滑的函数，保证了考虑P和Q的同时，也能相对客观地评估它们的重要性(公式9、公式10、公式11)。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172937743.png" alt="image-20240626172937743">&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172945391.png" alt="image-20240626172945391">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果">(1)实验结果&lt;/h2>
&lt;p>从整体的实验结果来看，AdaLoRA的微调效果全面优于LoRA。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174406600.png" alt="image-20240626174406600">&lt;/p>
&lt;h2 id="2关键发现">(2)关键发现&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>增加计算预算&lt;/strong>，AdaLoRA的训练效果会显著优于LoRA。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626173949733.png" alt="image-20240626173949733">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>SVD降维、保证PQ矩阵的正交性&lt;/strong>，对AdaLoRA的训练效果都有影响。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174221946.png" alt="image-20240626174221946">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Transformer中，&lt;strong>FNN的参数矩阵重要性要大于自注意力层的参数矩阵&lt;/strong>、&lt;strong>顶层的参数矩阵重要性大于低层的参数矩阵重要性&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174532657.png" alt="image-20240626174532657">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AdaLoRA的核心思想&lt;/strong>：采用SVD、正则损失、二次平滑的重要性评估，精准地控制每一层的参数矩阵降维。&lt;/li>
&lt;li>&lt;strong>AdaLoRA的关键发现&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>SVD降维、保证PQ矩阵的正交性&lt;/strong>，对AdaLoRA的训练效果都有影响。&lt;/li>
&lt;li>Transformer中，&lt;strong>FNN的参数矩阵重要性要大于自注意力层的参数矩阵&lt;/strong>、&lt;strong>顶层的参数矩阵重要性大于低层的参数矩阵重要性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>阅读AdaLoRA最大的感触就是数学的奇妙，AdaLoRA是一个主要依赖数学公式推导解决算法问题的典型代表&lt;/strong>。&lt;/p>
&lt;p>论文链接：https://arxiv.org/pdf/2303.10512&lt;/p></description></item><item><title>【chatGPT】学习笔记50-LLM微调技术之LoRA</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/</link><pubDate>Mon, 10 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/</guid><description>&lt;p>前面的专栏我们介绍了&lt;strong>Adapt Tuning&lt;/strong>、&lt;strong>Soft Prompt Tuning&lt;/strong>等微调技术，本文让我们跟随着论文《&lt;strong>LORA: LOW-RANK ADAPTATION OF LARGE LAN-&lt;/strong>
&lt;strong>GUAGE MODELS&lt;/strong>》，来看一下&lt;strong>LoRA微调技术&lt;/strong>。&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：&lt;strong>LoRA&lt;/strong>也是解决全参数微调参数量过大、成本过高的问题。&lt;/li>
&lt;li>&lt;strong>解决方案&lt;/strong>：论文提出了&lt;strong>LoRA&lt;/strong>微调技术，冻结预训练模型权重，在Transformer架构的每一层注入可训练的低秩矩阵。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：LoRA大幅减少了训练参数量，与使用Adam微调的GPT-3 175B相比，LoRA将训练参数数量减少了10000倍，GPU需求减少3倍。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240610131206816.png" alt="image-20240610131206816">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：全参数微调成本过高，现有高效微调技术扩展了模型深度导致了推理延迟or减少了模型可用序列长度。&lt;/p>
&lt;ul>
&lt;li>全参微调的主要缺点是新模型包含的参数量与原始模型一样多，进而导致成本巨大。如：GPT-3拥有1750亿参数，全参微调成本巨大。&lt;/li>
&lt;li>Houlsby、Rebuffi等专家提出的高效微调技术，增加了模型深度，也会导致推理延迟，也可能导致减少了模型可用序列长度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>LoRA的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>可以共享一个预训练模型，在其基础上构建许多用于不同任务的小LoRA模块。&lt;/li>
&lt;li>LoRA模块用两个低秩矩阵A和B，替换原有的参数。利用低秩矩阵的数学特性，减少参数量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240610132355705.png" alt="image-20240610132355705">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LoRA的优势：&lt;/strong>
&lt;ul>
&lt;li>LoRA的这种设计使训练更加高效，将GPU计算量降低了3倍。&lt;/li>
&lt;li>LoRA模块对Transformer的改变是线性的，因此不会由于加深网络结构导致推理延迟。&lt;/li>
&lt;li>LoRA的设计和现有高效微调技术是正交的，可以和它们结合使用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>对全量微调建模&lt;/strong>：在全量微调下，预训练模型参数的初始值为Φ&lt;sub>0&lt;/sub>，随着梯度更新为Φ&lt;sub>0&lt;/sub>+∆Φ，则找到∆Φ的任务可定义为：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611040221623.png" alt="image-20240611040221623">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>对全量微调缺点建模&lt;/strong>：∆Φ的维度等于|Φ&lt;sub>0&lt;/sub>|，当模型很大时，梯度更新Φ&lt;sub>0&lt;/sub>将导致巨大的计算成本。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>LoRA是这样寻找突破口的：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LoRA建模&lt;/strong>：∆Φ=∆Φ(Θ)，|Θ|维度&amp;laquo;|Φ&lt;sub>0&lt;/sub>|，则找到∆Φ的任务可转换为对Θ的优化：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611041007236.png" alt="image-20240611041007236">&lt;/li>
&lt;/ul>
&lt;p>如何做到∆Φ=∆Φ(Θ)呢？其本质就是&lt;strong>降维&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>低秩矩阵&lt;/strong>：低秩矩阵的概念本文不会展开描述，小伙伴们可以复习一下线性代数。但低秩矩阵的本质是2个低阶矩阵的运算可以&lt;strong>约等于&lt;/strong>1个高阶矩阵。&lt;/li>
&lt;li>&lt;strong>低秩矩阵与深度学习的结合&lt;/strong>：假设预训练模型的参数矩阵为W&lt;sub>0&lt;/sub>，则梯度更新可认为W&lt;sub>0&lt;/sub>+∆W。如果将∆W分解为2个低秩矩阵B和A，即W&lt;sub>0&lt;/sub>+∆W=W&lt;sub>0&lt;/sub>+BA&lt;sub>x&lt;/sub>。&lt;/li>
&lt;li>&lt;strong>低秩矩阵与Transformer的结合&lt;/strong>：在Transformer架构中，W&lt;sub>q&lt;/sub>、W&lt;sub>k&lt;/sub>、W&lt;sub>v&lt;/sub>，W&lt;sub>o&lt;/sub>分别表示Query、Key、Value、输出投影矩阵。W&lt;sub>q&lt;/sub>、W&lt;sub>k&lt;/sub>、W&lt;sub>v&lt;/sub>是单一维度为dmodel*dmodel的矩阵。W&lt;sub>q&lt;/sub>+∆W、W&lt;sub>k&lt;/sub>+∆W、W&lt;sub>v&lt;/sub>+∆W中的∆W分解为低秩矩阵B和A，则也将问题转换为∆W小矩阵问题。&lt;/li>
&lt;/ul>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果">(1)实验结果&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>在GLUE基准测试中使用RoBERTa(基础和大型)和DeBERTa 1.5B获得了与全参数微调相当或更优的结果，同时只训练和存储了一小部分参数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611072021335.png" alt="image-20240611072021335">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在GPT-2上，LoRA与全参数微调和其他高效调整方法和前缀调整相比具有优势。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611072244925.png" alt="image-20240611072244925">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2关键发现">(2)关键发现&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>LoRA与Transformer结合，&lt;strong>应该如何划分低秩矩阵&lt;/strong>？&lt;/p>
&lt;ul>
&lt;li>实验证明：r=8或r=4时，适用于GPT-3(175B模型)的96层注意力层。&lt;/li>
&lt;li>实验证明：不要仅仅只刷新∆W&lt;sub>q&lt;/sub>或∆W&lt;sub>v&lt;/sub>，同时刷新它们会产生更好的性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>∆W和W强相关，∆W增强了W学习到的一些特征。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611092100515.png" alt="image-20240611092100515">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LoRA的核心思想&lt;/strong>：LoRA模块用两个低秩矩阵A和B，替换原有的参数。利用低秩矩阵的数学特性，减少参数量。。&lt;/li>
&lt;li>&lt;strong>LoRA的优势&lt;/strong>：LoRA的这种设计使训练更加高效，且不会由于加深网络结构导致推理延迟。&lt;/li>
&lt;li>&lt;strong>LoRA实验的关键发现&lt;/strong>：
&lt;ul>
&lt;li>LoRA与Transformer结合，要考虑如何划分低秩矩阵。&lt;/li>
&lt;li>∆W和W强相关，∆W增强了W学习到的一些特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/abs/2106.09685&lt;/p></description></item><item><title>【chatGPT】学习笔记49-AiDD 2024_AI人才培养分论坛参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Wed, 05 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>&lt;p>在AIGC时代，AI对高等教育及人才培养有怎样的变革？如何培养适应时代发展需求的计算机人才？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240606075433249.png" alt="image-20240606075433249">&lt;/p>
&lt;h1 id="1aigc时代下的人才需求趋势">1.AIGC时代下的人才需求趋势&lt;/h1>
&lt;h2 id="1时代背景">(1)时代背景&lt;/h2>
&lt;p>生成式人工智能(AIGC)的快速发展已悄然推动&amp;quot;第四次工业革命&amp;rdquo;，AI能力从感知理解世界跃迁到生成创造世界，工业应用的焦点将快速从自动化全面转变为智能化。Gartner预测，到2026年将有超过80%的企业将使用生成式人工智能。&lt;/p>
&lt;p>与历次工业革命一样，智能化不仅仅是技术和经济层面的变革，更是一场认知和思维的革命。作为塑造未来、培养创新人才的教育行业，必然要拥抱AIGC引发教育和教学变革。我国教育部也一直在大力推动“工业化教育模式”向“智能化教育模式”转变。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240603090307442-17176300245711.png" alt="image-20240603090307442">&lt;/p>
&lt;h2 id="2社会对人才需求及人力结构的转变">(2)社会对人才需求及人力结构的转变&lt;/h2>
&lt;p>在AIGC时代，社会对人才的需求和人力结构会发展巨大转变。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>人力结构转变&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>初级专业技术岗位将大幅减少，高级技术和创新型岗位将激增。&lt;/strong>
&lt;ul>
&lt;li>麦肯锡《生成式人工智能和美国工作的未来》报告提出2030年美国&lt;strong>工资最低的岗位将减少110万个&lt;/strong>，但&lt;strong>工资最高的岗位可能增加 380万个&lt;/strong>。&lt;/li>
&lt;li>世界经济论坛发布《未来就业报告2023》显示，未来5年内，&lt;strong>人工智能、商业智能分析师、数据科学&lt;/strong>等大数据相关职位的需求增长最快。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>人力需求转变&lt;/strong>：
&lt;ul>
&lt;li>**培养创新性人才：**创造性、分析性思维，技术素养、好奇心与学习能力、韧性、灵活性与敏捷性等通识素养是未来最需要培养的技能。&lt;/li>
&lt;li>**培养跨专业人才：**学科壁垒不断消融，数字技术、人工智能等科创技术能力与人文素养、通识教育并重，跨专业人才需求紧俏。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这意味着，AIGC时代人力需求不减反增，但需要的是具有创造力和深度思考能力，并且具备人工智能技术的人。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240606081142441.png" alt="image-20240606081142441">&lt;/p>
&lt;h2 id="3社会对计算机人才需求及人力结构的转变">(3)社会对计算机人才需求及人力结构的转变&lt;/h2>
&lt;p>计算机学科的人才们创造了AIGC，那AIGC对未来计算机专业人才的需求有哪些转变呢？&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>人力结构转变：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>那些会消失的岗位：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础业务代码编写与维护&lt;/strong>：会逐步消失，简单的编码任务、有标准代码模板的任务、修复常见的编程错误、基础业务逻辑代码，都可能被AI取代。&lt;/li>
&lt;li>&lt;strong>初级测试&lt;/strong>：AI会自动生成常规测试用例，保证基本测试质量。&lt;/li>
&lt;li>&lt;strong>简单的数据分析&lt;/strong>：也会逐步消失，AI已经可以对于数据集进行初步分析了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>那些会新增的岗位&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型训练&amp;amp;AI应用开发&lt;/strong>：设计、训练、微调、部署AI模型，在垂直领域开发AI应用，将成为新兴的岗位。&lt;/li>
&lt;li>&lt;strong>人机交互设计师&lt;/strong>：为了提高AI应用的可用性和用户体验，专注于人机交互的角色会非常重要。&lt;/li>
&lt;li>&lt;strong>AI解决方案工程师&lt;/strong>：对特定行业、垂直领域设计定制化解决方案。&lt;/li>
&lt;li>&lt;strong>AI伦理和审计&lt;/strong>：处理伦理、合规、隐私、AI决策透明度，开发和监督AI应用的使用准则。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>人力需求转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>与AIGC相关的工作岗位将占到甚至超过总体人力需求的50%&lt;/li>
&lt;li>AIGC将促成大量非计算机专业交叉学科的计算机人才，也使&amp;quot;超级个体&amp;quot;的产生变得更容易。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180823443.png" alt="image-20240605180823443">&lt;/p>
&lt;h1 id="2计算机人才培养目标变革">2.计算机人才培养目标变革&lt;/h1>
&lt;h2 id="1教育理念变革">(1)教育理念变革&lt;/h2>
&lt;p>以&lt;strong>布鲁姆学习目标分类体系、斯金纳程序教学模式&lt;/strong>为代表的传统教育体系，在第一次工业革命建立起来后，其教育理念表现为：&lt;/p>
&lt;ul>
&lt;li>强调&lt;strong>结构化和标准化教学&lt;/strong>&lt;/li>
&lt;li>一定程度地&lt;strong>限制了创新能力和批判性思维&lt;/strong>的培养。&lt;/li>
&lt;/ul>
&lt;p>而在AIGC时代背景下，亟需在新的教育理念指引下，尽快转变为&amp;quot;融合AI的超级个体&amp;quot;教学模式。教育理念有如下变革：&lt;/p>
&lt;ul>
&lt;li>**知识观：**智能时代知识更新速度快、知识获取难度低，&lt;strong>知识不是教学的核心&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>教学观：&lt;/strong>
&lt;ul>
&lt;li>教学目标改变：&lt;strong>培养综合能力和高阶思维&lt;/strong>。&lt;/li>
&lt;li>教学内容改变：&lt;strong>构建软知识的思路和框架&lt;/strong>。&lt;/li>
&lt;li>教学方法改变：&lt;strong>发现式教学和分享式教学&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>学习观：&lt;/strong>
&lt;ul>
&lt;li>学习目标改变：&lt;strong>综合能力提升和批判性思维&lt;/strong>。&lt;/li>
&lt;li>学习内容改变：&lt;strong>以问题求解为中心&lt;/strong>。&lt;/li>
&lt;li>学习方式改变：&lt;strong>&amp;ldquo;人-机&amp;quot;合作式学习&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>人才观：&lt;strong>智能时代人类主要承担创造性工作，要&lt;/strong>培养创新型人才和跨领域人才，培养善于与AI共同工作的人才&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605193942764.png" alt="image-20240605193942764">&lt;/p>
&lt;h2 id="2培养目标变革">(2)培养目标变革&lt;/h2>
&lt;p>AIGC时代需要具备AI技能且善于思考创新的人才，结合学科知识体系，可分解为如下六项培养目标：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>四项能力：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础和全流程能力&lt;/strong>：除了编程能力，还强调基础算法和数据结构、机器学习、数据科学的理解与应用。&lt;/li>
&lt;li>&lt;strong>终身学习和适应能力&lt;/strong>：学习新知识、掌握新技术、适应新环境的能力。&lt;/li>
&lt;li>&lt;strong>解决跨学科问题的能力&lt;/strong>：探索计算机科学、数学、统计学、心理学和应用领域的知识，提升跨学科能力，可从多个角度思考和解决问题。&lt;/li>
&lt;li>&lt;strong>协作沟通等软技能&lt;/strong>：能够和团队成员、机器智能，协同合作，有效传达和交流想法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>两个思维：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>创造力和创新思维&lt;/strong>：解决问题、提出创新解决方案、设计新系统的能力。&lt;/li>
&lt;li>&lt;strong>伦理和社会责任意识&lt;/strong>：对AI应用的伦理问题理解、对隐私的关注、对AI的社会影响的认识。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3人才培养途径变革">(3)人才培养途径变革&lt;/h2>
&lt;p>为达成以上培养目标，需要拓宽当前计算机人才培养的途径，建议采取如下措施：&lt;/p>
&lt;ul>
&lt;li>将&lt;strong>计算机科学转为必修课&lt;/strong>，而且是基础课。&lt;/li>
&lt;li>通过&lt;strong>学科交叉学习和实践&lt;/strong>，让计算机专业和非计算机专业的学生都可以具备解决跨学科问题的能力，匹配未来人才结构需求。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605204013658.png" alt="image-20240605204013658">&lt;/p>
&lt;ul>
&lt;li>AI辅助编程 + AI辅助专业知识学习，培养&lt;strong>兼具AI技能和专业能力&lt;/strong>的人才。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605204409806.png" alt="image-20240605204409806">&lt;/p>
&lt;h1 id="3计算机人才培养实施路径">3.计算机人才培养实施路径&lt;/h1>
&lt;h2 id="31ai辅助教学">3.1.AI辅助教学&lt;/h2>
&lt;p>随着2023年生成式AI的发展，以及各大厂商在AI辅助研发领域的投入，AI辅助研发、AI for SE有了飞速发展：&lt;/p>
&lt;ul>
&lt;li>通过AI，代码生成、错误检测、自动重构优化、自动化测试，都成为了可能。&lt;/li>
&lt;/ul>
&lt;p>在AI辅助研发取得长足进步的背景下，AI辅助教学也成为了可能：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>内容创作&lt;/strong>：AI自动生成教学案例、总结教学内容、生成教学视频。&lt;/li>
&lt;li>&lt;strong>个性化教学&lt;/strong>：根据不同学生特点，推荐个性化教学内容和习题。&lt;/li>
&lt;li>&lt;strong>编程开发&lt;/strong>：自动生成代码和测试用例，帮助学生关注高层设计、避免陷入低层次细节。&lt;/li>
&lt;li>&lt;strong>互动体验&lt;/strong>：数字导师和AI助教，应用于教学与答疑。&lt;/li>
&lt;li>&lt;strong>游戏化教学&lt;/strong>：生成游戏环境、角色和情节，提升教学趣味性。&lt;/li>
&lt;li>&lt;strong>智能评价&lt;/strong>：用AI对学生作业和项目进行综合评价，总结优缺点及改进建议。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605172637375.png" alt="image-20240605172637375">&lt;/p>
&lt;p>在这样的背景下，教学模式&lt;strong>从循证教学法向AI辅助教学法转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>循证教学法&lt;/strong>的&lt;strong>本质&lt;/strong>：传统的教学模式，先教语言语法，再进行大量的代码片段练习。&lt;/li>
&lt;li>&lt;strong>AI辅助教学法&lt;/strong>的&lt;strong>本质&lt;/strong>：是利用大模型来处理低层次任务(将语法的细节延后)，让学生更聚焦于高层次的思维与算法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605172715134.png" alt="image-20240605172715134">&lt;/p>
&lt;h2 id="32实践1新质教育系统">3.2.实践1：新质教育系统&lt;/h2>
&lt;h3 id="1ai生成数字化学习资源">(1)AI生成数字化学习资源&lt;/h3>
&lt;ul>
&lt;li>通过AI，生成MOOC、实战实训、数字教材、图文专栏、导学项目等线上资源。&lt;/li>
&lt;li>通过AI，生成课堂教学、PPT课件、印刷教材等线下资源。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605174417159.png" alt="image-20240605174417159">&lt;/p>
&lt;h3 id="2ai生成个性化教学案例">(2)AI生成个性化教学案例&lt;/h3>
&lt;ul>
&lt;li>通过AI，生成个性化教学案例。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605175600964.png" alt="image-20240605175600964">&lt;/p>
&lt;h3 id="3ai生成作业和项目设计">(3)AI生成作业和项目设计&lt;/h3>
&lt;ul>
&lt;li>通过AI，以及提示词技术，生成课程作业、项目设计稿。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605175528506.png" alt="image-20240605175528506">&lt;/p>
&lt;h3 id="4ai数字教师数字助教">(4)AI数字教师、数字助教&lt;/h3>
&lt;ul>
&lt;li>基于AI数字人，实现数字导师、数字助教。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180234845.png" alt="image-20240605180234845">&lt;/p>
&lt;h3 id="5智能评价">(5)智能评价&lt;/h3>
&lt;ul>
&lt;li>利用AI，从不同维度对学生的输出进行评价。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605180723165.png" alt="image-20240605180723165">&lt;/p>
&lt;h2 id="33实践2生成式探究教学">3.3.实践2：生成式探究教学&lt;/h2>
&lt;h3 id="1生成式探究教学的要素">(1)生成式探究教学的要素&lt;/h3>
&lt;p>生成式探究教学倡导：学生&lt;strong>主动探索&lt;/strong>，强调通过&lt;strong>创造性生成和构建&lt;/strong>深入探索和理解知识，鼓励学生&lt;strong>自发提出问题&lt;/strong>、独立思考并&lt;strong>创造性地解决问题&lt;/strong>。&lt;/p>
&lt;p>生成式探究教学的要素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学生主导&lt;/strong>：学生在学习过程中处于主导地位，学生提出问题、制定计划、选择学习资源、协作探索。&lt;/li>
&lt;li>&lt;strong>创造性生成&lt;/strong>：学生被鼓励创造性生成，如：设计产品、编写代码、创作艺术作品等。&lt;/li>
&lt;li>&lt;strong>深入探索&lt;/strong>：学生被孤立批判性思维、思考知识的本质、寻找创新解决问题的方法。&lt;/li>
&lt;li>&lt;strong>反思和分享&lt;/strong>：学生在实践过程中进行反思，总结经验教训，与他人分享见解和成果。&lt;/li>
&lt;li>&lt;strong>教练而不是教师&lt;/strong>：老师的角色是支持者，提供必要的指导、资源、反馈，已促进学生的学习和发展。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-AiDD2024_AI%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240605221417594.png" alt="image-20240605221417594">&lt;/p>
&lt;h3 id="2生成式探究教学的实施过程">(2)生成式探究教学的实施过程&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>提出教学目标&lt;/strong>：教学目标从知识向批判思维迁移。&lt;/li>
&lt;li>&lt;strong>采用开放性教学资源&lt;/strong>：而不仅仅是教科书。&lt;/li>
&lt;li>&lt;strong>提出开放性问题&lt;/strong>：而不是有标准答案的问题。&lt;/li>
&lt;li>&lt;strong>鼓励探索&lt;/strong>：鼓励学生主动探索，老师与学生一起探索。&lt;/li>
&lt;li>&lt;strong>分享成果&lt;/strong>：让学生分享自己的探索成果。&lt;/li>
&lt;li>&lt;strong>评价&lt;/strong>：采用多元性、过程性、差异性评价方式。&lt;/li>
&lt;li>&lt;strong>教练式辅导&lt;/strong>：过程中进行必要的指导和讲解。&lt;/li>
&lt;/ul>
&lt;h3 id="3生成式探究教学案例">(3)生成式探究教学案例&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>项目介绍和目标设定&lt;/strong>——&lt;strong>提出教学目标&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>2048小游戏的基本规则和玩法。&lt;/li>
&lt;li>项目目标and预期结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>学生探究&lt;/strong>——&lt;strong>鼓励探索、提出开放性问题&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>游戏逻辑和规则如何实现？&lt;/li>
&lt;li>游戏界面如何设计？&lt;/li>
&lt;li>如何处理用户输入和游戏状态的更新？&lt;/li>
&lt;li>如何判断游戏胜利或失败的调价？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>指导和讲解&lt;/strong>——&lt;strong>教练式辅导&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>讲解基础知识和技能&lt;/li>
&lt;li>通过AI，学习各模块的需求。&lt;/li>
&lt;li>如何实现游戏的基本功能，如：移动方块、合并方块、更新游戏状态等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>编码实践&lt;/strong>——&lt;strong>鼓励探索&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>根据自己的理解和AI交互编写游戏代码&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>调试和测试&lt;/strong>——&lt;strong>鼓励探索&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>根据游戏的功能和逻辑，与AI交互进行调试和测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>展示和反思&lt;/strong>——&lt;strong>分享成果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>展示2048小游戏，分享学习心得和经验。&lt;/li>
&lt;li>思考在开发过程中遇到的挑战和解决方案是什么？&lt;/li>
&lt;li>思考游戏的改进和扩展方向是什么？&lt;/li>
&lt;li>如何将本案例的编程知识、业务技能，举一反一地应用到其它项目中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="34ai辅助教学的优势与挑战">3.4.AI辅助教学的优势与挑战&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>优势&lt;/strong>：降低了老师的教学成本、降低了学生的学习成本。&lt;/li>
&lt;li>&lt;strong>挑战&lt;/strong>：
&lt;ul>
&lt;li>传统OJ测评方法失效。&lt;/li>
&lt;li>过程性评估。&lt;/li>
&lt;li>AI算法的局限性。&lt;/li>
&lt;li>评价标准缺失。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4总结">4.总结&lt;/h1>
&lt;p>智能化时代对高等教育提出了新的要求，也为教育的创新和发展提供了广阔的空间。&lt;/p>
&lt;p>AIGC时代已经到来，教育者和学习者都需要懂AI、用AI以适应和拥抱变化。&lt;/p>
&lt;p>通过智能化技术的赋能，我们可以期待一个更加开放、灵活和高效的教育未来。&lt;/p></description></item><item><title>【chatGPT】学习笔记48-AiDD 2024参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Fri, 24 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>&lt;p>本文记录笔者参加AiDD 2024(AI+研发数字峰会)相关的议题，方便各位小伙伴快速了解最新AI理论研究和行业应用情况。&lt;/p>
&lt;h1 id="1aidd-2024概览">1.AiDD 2024概览&lt;/h1>
&lt;h2 id="1会议简介">(1)会议简介&lt;/h2>
&lt;ul>
&lt;li>AI+研发数字峰会(AiDD)专注“&lt;strong>AI技术和软件研发融合&lt;/strong>” ，AiDD 2024在上海举办，旨在&amp;rdquo;&amp;quot;&lt;strong>帮助企业借助AI技术，推动研发全面进入数智化时代&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>大会特邀&lt;strong>100+业界大咖&lt;/strong>分享行业洞察及专家见解，通过&lt;strong>60+创新案例&lt;/strong>展示全新研发思路，设置&lt;strong>15+个分论坛&lt;/strong>涵盖了软件研发全流程。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524133916685.png" alt="image-20240524133916685">&lt;/p>
&lt;h2 id="2议题分布">(2)议题分布&lt;/h2>
&lt;p>AiDD设置了15个分论坛，覆盖AI+研发全流程。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编程Copilot&lt;/strong>&lt;/li>
&lt;li>&lt;strong>算法与模型训练&lt;/strong>&lt;/li>
&lt;li>&lt;strong>知识工程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI工具链与工程平台&lt;/strong>&lt;/li>
&lt;li>&lt;strong>智能运维&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI算力与优化&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI原生应用开发&lt;/strong>&lt;/li>
&lt;li>&lt;strong>智能需求工程&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI驱动产品创新&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI人才培养&lt;/strong>&lt;/li>
&lt;li>&lt;strong>数据智能&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI赋能测试&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI智能体&lt;/strong>&lt;/li>
&lt;li>&lt;strong>领域大模型&lt;/strong>&lt;/li>
&lt;li>&lt;strong>AI对齐&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>接下来，我们从&lt;strong>3个方向&lt;/strong>总结相关会议议题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础模型及Agent&lt;/strong>&lt;/li>
&lt;li>&lt;strong>基础设施&lt;/strong>&lt;/li>
&lt;li>&lt;strong>LLM应用落地&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h1 id="2方向1基础模型及agent">2.方向1：基础模型及Agent&lt;/h1>
&lt;p>通过本次会议可以观察到2点趋势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>趋势1&lt;/strong>：2023年的研究热点还是&lt;strong>LLM基础理论&lt;/strong>，2024年的热点逐步转向到&lt;strong>VLM及多模态模型&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>趋势2&lt;/strong>：LLM基础模型的成熟与落地，促成了&lt;strong>Agent技术&lt;/strong>成为2024年的行业热点。&lt;/li>
&lt;/ul>
&lt;h2 id="议题1多模态大语言模型中的上下文学习">议题1：多模态大语言模型中的上下文学习&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>： 杨旭，南洋理工博士，东南大学副教授。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>价值&lt;/strong>：通过ICL(上下文学习)，提升多模态大模型的字幕生成能力、视觉问答能力(VQA)、解决视觉语言任务。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>探索不同的上下文配置对图像字幕生成任务中的视觉-语言(VL)模型的影响，提出了四种图像选择策略和四种字幕分配策略。&lt;/li>
&lt;li>探索在视觉问答(VQA)任务中，如何配置有效的上下文序列以增强大型视觉语言模型(LVLMs)的上下文学习(ICL)性能。为了提高ICL的性能，作者设计了多种检索方法，并采用不同的策略来操作检索到的示例。&lt;/li>
&lt;li>通过在视觉问答(VQA)和图像字幕(IC)任务中的实验验证了使用语言模型配置ICD的方法的可行性，并通过全面的消融研究进一步探讨了数据集构建和ICD-LM开发设置对结果的影响。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524163938055.png" alt="image-20240524163938055">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164044361.png" alt="image-20240524164044361">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164420747.png" alt="image-20240524164420747">&lt;/p>
&lt;h2 id="议题2基于多模态大语言模型的gui智能体">议题2：基于多模态大语言模型的GUI智能体&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：张驰，腾讯研究科学家，入选斯坦福大学发布的《2023全球前2%顶尖科学家榜单》&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>产品&lt;/strong>：AppAgent是由腾讯公司开发的一种高级多模态代理框架，主要用于智能手机应用程序的操作和管理。它基于大型语言模型（LLM），能够通过直观的点击、滑动等手势与应用程序进行交互，模仿类似人类的动作。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>多模态交互&lt;/strong>：AppAgent结合了视觉和文本输入，使得代理能够理解和执行基于视觉信息的任务，这在以往的文本中心的Agent模型中是不可能的。&lt;/li>
&lt;li>&lt;strong>无需系统后端访问&lt;/strong>：与传统的智能助手(如Siri)不同，AppAgent通过模拟用户在图形用户界面（GUI）上的操作，如点击和滑动，而不是依赖于系统后端访问，从而实现更高的灵活性和安全性。&lt;/li>
&lt;li>&lt;strong>自主学习能力&lt;/strong>：AppAgent通过预定义操作与手机app交互来学习，也可以通过观察人类演示来学习。这些观察被记录成文档，供后续使用&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524165245515.png" alt="image-20240524165245515">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524164953565.png" alt="image-20240524164953565">&lt;/p>
&lt;h2 id="议题3多场景下智能体应用构建技巧">议题3：多场景下智能体应用构建技巧&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：卢建晖，微软高级云技术布道师&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>产品&lt;/strong>：微软的Semantic Kernel构建智能体是一个轻量级的软件开发工具包，旨在帮助开发者更有效地将大型语言模型集成到他们的应用程序中。&lt;/li>
&lt;li>&lt;strong>价值&lt;/strong>：Semantic Kernel的价值在于它为开发者提供了一个高效的方式来利用大型语言模型解决复杂的问题，通过定义插件并自动协调这些插件与AI，开发者可以快速实现特定的功能或解决特定的问题。还允许开发者在应用程序中充分利用与Copilot和Bing相同的人工智能协调模式，从而增强应用程序的功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170304878.png" alt="image-20240524170304878">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170402852.png" alt="image-20240524170402852">&lt;/p>
&lt;h2 id="议题4aiagent认知框架与案例实践">议题4：AIAgent认知框架与案例实践&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：黄佳，新加坡科技研究局AI研究员、技术作家。主攻方向LLM的开发和应用、AI for FinTech、AI for MedTech、持续学习等。著有多部AI相关畅销书。&lt;/li>
&lt;/ul>
&lt;p>个人感觉黄佳老师的议题比较出彩的一个，他不愧为技术作家，议题讲解也带有很强的技术科普风格，具体内容如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI应用的五个层次&lt;/strong>：分L1~L5，目前很多AI助手停留在L3，但业界都在向L4，即Agent方向努力。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524170728959.png" alt="image-20240524170728959">&lt;/li>
&lt;li>&lt;strong>Agent方法论&lt;/strong>：黄老师的这个总结非常精彩。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171055835.png" alt="image-20240524171055835">&lt;/li>
&lt;li>&lt;strong>Agent认知框架的四种设计模式&lt;/strong>：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171224406.png" alt="image-20240524171224406">&lt;/li>
&lt;li>&lt;strong>Agent认知框架选型方法&lt;/strong>：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171326589.png" alt="image-20240524171326589">&lt;/li>
&lt;li>&lt;strong>Agent研究综述&lt;/strong>：黄佳老师介绍了Agent研究综述，非常值得学习的一篇综述性论文。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524171839884.png" alt="image-20240524171839884">&lt;/li>
&lt;/ul>
&lt;h2 id="议题5个性化智能体价值观与社交能力的评估与对齐">议题5：个性化智能体价值观与社交能力的评估与对齐&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：高星，通义星尘算法负责人。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>通过基于专家指导原则的自我对齐，有效提升了人类价值观对齐的效果。&lt;/li>
&lt;li>通过多阶段迭代训练、CycleAlign方法、大小模型协同等，提升个性化角色的模型能力，打造类人智能体。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524172521205.png" alt="image-20240524172521205">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240524172501644.png" alt="image-20240524172501644">&lt;/p>
&lt;h1 id="3方向2基础设施">3.方向2：基础设施&lt;/h1>
&lt;h2 id="议题1构建agi时代的推理基础设施">议题1：构建AGI时代的推理基础设施&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>： 单一舟，南洋理工博士，东南大学副教授。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍华为云构建满足AGI需求的推理基础设施的解决方案。&lt;/li>
&lt;li>&lt;strong>成果&lt;/strong>：
&lt;ul>
&lt;li>以存代算，降低首字时延&lt;/li>
&lt;li>分离式内存弹性伸缩，降低推理集群成本&lt;/li>
&lt;li>资源感知调度，提升推理集群利用率&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142746428.png" alt="image-20240525142746428">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142816516.png" alt="image-20240525142816516">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525142834332.png" alt="image-20240525142834332">&lt;/p>
&lt;h2 id="议题2向量数据库大模型时代的基础设施构建">议题2：向量数据库大模型时代的基础设施构建&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：刘力，Zilliz首席工程师&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了Zilliz Cloud产品，以及其关键特性，如：向量数据库即服务、Saas架构、Logic Clusters、分层存储、冷热数据分离、Zilliz Cloud Pipeline、Cardinal极致性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525143445461.png" alt="image-20240525143445461">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525145559234.png" alt="image-20240525145559234">&lt;/p>
&lt;h2 id="议题3构建云原生算力基础设施驱动大模型创新实践">议题3：构建云原生算力基础设施驱动大模型创新实践&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：王羽中，杭州谐云科技有限公司技术总监&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了支撑大模型的云原生算力基础设施解决方案，阐述了多项关键技术，如：跨算力中心的纳管和调度、异构资源纳管和调度、算力超分和优先级调度、算力资源共享和隔离、算力资源动态共享、多卡共享、精细化计费等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525150032505.png" alt="image-20240525150032505">&lt;/p>
&lt;h2 id="议题4ai原生应用开发工具链详解">议题4：AI原生应用开发工具链详解&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：罗义云，阿里云资深技术专家、PAI平台工程负责人&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍阿里云PAI的整体架构，阐述了模型微调工具链的模型微调、模型评测Eval-Scope、实验管理、量化压缩、BladeLLM等特性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525150748022.png" alt="image-20240525150748022">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525151401633.png" alt="image-20240525151401633">&lt;/p>
&lt;h1 id="4方向3llm应用落地">4.方向3：LLM应用落地&lt;/h1>
&lt;h2 id="议题1大模型加持如何改变需求工程任务">议题1：大模型加持如何改变需求工程任务&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>报告人&lt;/strong>：金芝，北京大学教授，IEEE/CCF/AAIA Fellow，高可信软件技术教育部重点实验室常务副主任。&lt;/li>
&lt;li>&lt;strong>内容小结&lt;/strong>：介绍大模型对需求工程的改变
&lt;ul>
&lt;li>&lt;strong>需求相关的提示模式&lt;/strong>：《Requirements Engineering using Generative AI：Prompts and Prompting Patterns》阐述了5种提示模式。&lt;/li>
&lt;li>&lt;strong>人机协作架构设计&lt;/strong>：《Towards Human-Bot Collaborative Software Architecting with ChatGPT》展示了人类和ChatGPT协同开展架构设计的实践。&lt;/li>
&lt;li>&lt;strong>AI Agent协同目标建模&lt;/strong>：《MAPE-K Loop-based Goal Model Generation Using Generative AI》论述了一种由AI Agent扮演需求分析师、领域业务专家的角色，共同开展设计建模的方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153313639.png" alt="image-20240525153313639">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153706831.png" alt="image-20240525153706831">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525153909763.png" alt="image-20240525153909763">&lt;/p>
&lt;h2 id="议题2智能化研发在百度的落地">议题2：智能化研发在百度的落地&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：张立理，百度前端架构师，百度前端CMC主席，技术组织委员会 Web方向负责人。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>如何构建强力的代码模型&lt;/strong>：构造高质量代码数据集、多种代码生成模式、降低模型对算力的依赖提升计算速度。&lt;/li>
&lt;li>&lt;strong>如何有效服务产品用户&lt;/strong>：多语言多IDE覆盖、提供丰富的编码器能力、基于研发现场知识增强、构建与实践紧密结合的模型训练飞轮。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154247326.png" alt="image-20240525154247326">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154737151.png" alt="image-20240525154737151">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154838530.png" alt="image-20240525154838530">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154855168.png" alt="image-20240525154855168">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154920249.png" alt="image-20240525154920249">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525154950695.png" alt="image-20240525154950695">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525155007958.png" alt="image-20240525155007958">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525155020779.png" alt="image-20240525155020779">&lt;/p>
&lt;h2 id="议题3阿里云服务器智能异常调度系统及llmops构建与实践">议题3：阿里云服务器智能异常调度系统及LLMOPS构建与实践&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：郭红科，阿里云高级开发工程师&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>主题&lt;/strong>：介绍了阿里云服务器的AI for Ops解决方案，分享了query重写/意图识别、AI Agent等关键技术实践的经验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160135574.png" alt="image-20240525160135574">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160335459.png" alt="image-20240525160335459">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160407489.png" alt="image-20240525160407489">&lt;/p>
&lt;h2 id="议题4aiops在线评测基准系统">议题4：AIOps在线评测基准系统&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>报告人&lt;/strong>：聂晓辉，必示科技产品部总监、算法研究员，清华大学计算机系博士, 研究领域为智能运维 (AIOps)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>内容小结&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>介绍了必示科技的AIOps平台，展示了AI在IT运维领域的一些能力，如：异常检测、告警分析、故障处理、成本效率、混沌工程、可观测等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-AiDD2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/image-20240525160625980.png" alt="image-20240525160625980">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>AiDD 2024会议的收获如下：&lt;/p>
&lt;ul>
&lt;li>基础模型的研究热点是多模态大模型、AI Agent，需要进一步跟踪探索。&lt;/li>
&lt;li>各大厂商已经在布局基础设施建设，AI算力基础设施是近几年重要的营收点。&lt;/li>
&lt;li>在AI for SE方面，各厂商的AI辅助研发逐渐在落地，同时也在启动AIOps的探索。&lt;/li>
&lt;/ul></description></item><item><title>【读书】李娟-我的阿勒泰11</title><link>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E8%AF%BB%E4%B9%A6%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/</link><pubDate>Mon, 20 May 2024 00:00:30 +0800</pubDate><guid>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E8%AF%BB%E4%B9%A6%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/</guid><description>&lt;p>迷茫有一阵子了，感觉应该慢下来，那就读书吧。&lt;/p>
&lt;p>耐不下性子，就用AI来帮我读吧。&lt;/p>
&lt;h1 id="我的阿勒泰想表达什么">《我的阿勒泰》想表达什么？&lt;/h1>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520000811635.png" alt="image-20240520000811635">&lt;/p>
&lt;h1 id="这本书的目录">这本书的目录？&lt;/h1>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520000905619.png" alt="image-20240520000905619">&lt;/p>
&lt;h1 id="我所能带给你们的事物">我所能带给你们的事物&lt;/h1>
&lt;p>写这篇文章的时候，我的手机还在播放着马斯克的星舰IFT-4飞行测试。&lt;/p>
&lt;p>一半原始、一半现代。让我去阿勒泰旅行可以，但很难想象生活在阿勒泰。&lt;/p>
&lt;p>在那种原始的状态，为什么人类可以保持希望和期待？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520001118886.png" alt="image-20240520001118886">&lt;/p>
&lt;h1 id="属于我的马">属于我的马&lt;/h1>
&lt;p>原来马是自由的隐喻。经历太多，我能理解生命的脆弱，一直想知道生命的意义和死亡的意义。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520001249488.png" alt="image-20240520001249488">&lt;/p>
&lt;h1 id="小鸟牌香烟">小鸟牌香烟&lt;/h1>
&lt;p>香烟隐喻了时间的流逝？挺新奇的说法。很佩服你们这些感性的人——任何一个小小的器物就能帮你赋予生活的意义。&lt;/p>
&lt;p>但我习惯了量化和计划，我很迷惑，目前一知半解地认为康德想告诉我——生命的意义就是生命的意义、良善的实践就是人类自由意志的实现。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520002142006.png" alt="image-20240520002142006">&lt;/p>
&lt;h1 id="打电话">打电话&lt;/h1>
&lt;p>对文中小伙子和恋人打电话那个扭捏片段，i人不仅是对恋人，即使是面对稍有好感的人，也会语无伦次。&lt;/p>
&lt;p>i人在酒吧都是自己玩，从不和人搭讪。有一次和朋友打赌去要一位陌生女生的联系方式，鼓了半天气请那女生喝了杯酒(还是委托调酒师转告)，结果还是不敢过去。最后，那位女生临走之前，很cute地过来说谢谢你啊，我应该是全身僵住、支支吾吾。。。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E8%AF%BB%E4%B9%A6%E3%80%91%E6%9D%8E%E5%A8%9F-%E6%88%91%E7%9A%84%E9%98%BF%E5%8B%92%E6%B3%B01/image-20240520002635384.png" alt="image-20240520002635384">&lt;/p>
&lt;p>睡了，明天再继续。&lt;/p></description></item><item><title>【chatGPT】学习笔记47-LLM微调技术之P-Tuning V2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</link><pubDate>Sat, 11 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</guid><description>&lt;p>上篇专栏我们讲到，&lt;code>P-Tuning V1&lt;/code>通过在预训练模型的输入层加入可训练的连续提示，有效提升了训练效果。但其在复杂NLU任务和小参数模型上表现并不理想。&lt;/p>
&lt;p>&lt;code>P-Tuning V2&lt;/code>是对&lt;code>P-Tuning V1&lt;/code>的改进，使其能在不同规模的模型和各种NLU任务中都能与全量微调相媲美。&lt;/p>
&lt;p>本文解读论文**《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》**，探究&lt;code>Prompt Tuning V2&lt;/code>技术的原理。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112022603.png" alt="image-20240511112022603">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>的最大问题是不具备普适性(&lt;strong>a lack of universality&lt;/strong>)。
&lt;ul>
&lt;li>&lt;strong>不同规模模型的微调效果不稳定&lt;/strong>：Lack of universality across scales。模型规模超过10B时，&lt;strong>P-Tuning v1&lt;/strong>和&lt;strong>Fine Tuning&lt;/strong>水平相当。模型规模在0.1B到1B时，&lt;strong>P-Tuning v1&lt;/strong>的效果远不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>不同下游任务的微调效果不稳定&lt;/strong>：Lack of universality across tasks。实验证明，针对某些下游任务进行&lt;strong>P-Tuning v1&lt;/strong>，效果远差于&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>解决方案&lt;/strong>：论文提出&lt;strong>P-Tuning v2&lt;/strong>技术，采用&lt;strong>Deep Prompt Tuning&lt;/strong>方法，同时针对NLU任务做了一定适配和优化。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：实验证明&lt;strong>P-Tuning v2&lt;/strong>，在不同规模的模型上、在不同下游任务上都可获得较高的稳定性。是一种对&lt;strong>P-Tuning v1&lt;/strong>更好的替代方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112132956.png" alt="image-20240511112132956">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>在不同规模的模型下、不同下游任务中，微调效果不稳定。&lt;/p>
&lt;ul>
&lt;li>如：当模型大小不大，特别是少于10B参数时，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114454639.png" alt="image-20240511114454639">&lt;/li>
&lt;li>如：抽取式问答(extractive question answering)，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114429201.png" alt="image-20240511114429201">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>P-Tuning v2采用&lt;strong>Deep P-Tuning&lt;/strong>的优化方式，用于探索垂域知识。&lt;/li>
&lt;li>Deep表现于在预训练模型的每一层注意力层增加了一个小模型，作用于每一层的输入。而P-Tuning v1仅在第一层增加了一个LSTM小模型。&lt;/li>
&lt;li>这种方法的本质是：不同规模的模型对于&lt;strong>P-Tuning v1&lt;/strong>在第一层增加的前缀向量的特征提取能力不同。越大的模型特征提取越强，后续各层都能感知注意到这个前缀向量的特征。反之，小模型特征提取能力弱，后续各层无法感知注意到前缀向量的特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511115216844.png" alt="image-20240511115216844">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>对300M到10B参数的模型上实验，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;li>以抽取式问答和命名实体识别为代表的下游任务上，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>&lt;strong>P-Tuning v1&lt;/strong>提出了将&lt;strong>数学意义上的离散提示词转换为连续可微的提示词&lt;/strong>，但存在的问题还有2个：&lt;/p>
&lt;ul>
&lt;li>不同规模的模型微调效果不稳定。&lt;/li>
&lt;li>不同下游任务的模型微调效果不稳定。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>离散到连续&lt;/strong>是&lt;strong>Soft Prompt&lt;/strong>技术分支的&lt;strong>重要思想、重要里程碑&lt;/strong>，但&lt;strong>P-Tuning v1&lt;/strong>已经做到连续可微了，还有什么改进空间呢？&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>为了寻找突破口，我们还是进行数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>V、M、e&lt;/strong>：V表示模型M的词汇表，e表示模型M的词嵌入层。&lt;/li>
&lt;li>&lt;strong>离散到连续的转换&lt;/strong>：假定离散提示词为序列**[h&lt;sub>0&lt;/sub>, &amp;hellip;, h&lt;sub>i&lt;/sub>]**，经过**P-Tuning v1**的**Prompt Encoder**模块转换为向量序列**[e(x), e(h&lt;sub>0&lt;/sub>), &amp;hellip;, e(h&lt;sub>i&lt;/sub>)]**。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511144119291.png" alt="image-20240511144119291">&lt;/li>
&lt;/ul>
&lt;p>通过问题建模，我们可以看到&lt;strong>数学意义上&lt;/strong>的&lt;strong>离散提示&lt;/strong>已经表示为&lt;strong>连续提示&lt;/strong>。那么不同规模的模型、不同下游任务的微调效果不稳定，很可能源于&lt;strong>P-Tuning v1&lt;/strong>在输入层添加的前缀向量没有起到有效作用，从逻辑上，我们可以有如下猜测：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型规模对前缀向量的影响&lt;/strong>：不同规模的模型对前缀向量的特征提取能力是不同的，小模型特征提取不足，导致后续预训练模型的各层无法感知注意到前缀向量。&lt;/li>
&lt;li>&lt;strong>下游任务类型对前缀向量的影响&lt;/strong>：从离散提示词看，不同下游任务的提示词是不同的。同理，不同下游任务的连续提示词应该也是不同的。&lt;/li>
&lt;/ul>
&lt;p>因此：&lt;/p>
&lt;ul>
&lt;li>从数学上，&lt;strong>P-Tuning v1&lt;/strong>的连续可微前缀向量没有太多改进空间。&lt;/li>
&lt;li>从模型结构上，
&lt;ul>
&lt;li>&lt;strong>可以在Transformer的各层添加前缀向量&lt;/strong>，以抵消小模型对前缀向量的特征提取不足的局限。&lt;/li>
&lt;li>&lt;strong>可以改变前缀向量的长度&lt;/strong>，以实现不同下游任务有不同的前缀向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>为了更好地阐述论文的改进思路，我们列出相关源码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>DebertaPrefixModelForQuestionAnswering&lt;/strong>类，是针对QA下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511150051142.png" alt="image-20240511150051142">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RobertaPrefixForSequenceClassification&lt;/strong>类，是针对序列分类下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151145088.png" alt="image-20240511151145088">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>覆写Deberta的注意力层，支持在Deberta各注意力层都增加了前缀向量：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151537379.png" alt="image-20240511151537379">&lt;/p>
&lt;p>最后，在train方法中，将上述对模型结构的改进串联起来：&lt;/p>
&lt;ul>
&lt;li>创建前缀向量编码器对象，根据本下游任务指定的提示长度，生成前缀向量。&lt;/li>
&lt;li>前向传播时，将前缀向量传入本下游任务对应的各注意力层，实现不同层都能提取到前缀向量特征。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151852070.png" alt="image-20240511151852070">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1across-scales">(1)实验结果1：Across Scales&lt;/h2>
&lt;p>针对不同规模的模型，&lt;strong>P-Tuning v2&lt;/strong>的微调效果比较稳定。&lt;/p>
&lt;ul>
&lt;li>在四种&lt;strong>参数小于10B的模型&lt;/strong>上，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511152631012.png" alt="image-20240511152631012">&lt;/p>
&lt;h2 id="2实验结果2across-tasks">(2)实验结果2：Across Tasks&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>CoNLL03、OntoNotes5.0、CoNLL04、SQuAD1.1dev、SQuAD2.0dev、CoNLL12、CoNLL05 WSJ、CoNLL05 Brown&lt;/strong>八种下游任务中，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153327159.png" alt="image-20240511153327159">&lt;/p>
&lt;h2 id="3其它重要发现实现prompt-encoder的神经网络结构的选择技巧">(3)其它重要发现：实现Prompt Encoder的神经网络结构的选择技巧&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>P-Tuning v1&lt;/strong>中，采用&lt;strong>LSTM+MLP&lt;/strong>或&lt;strong>MLP&lt;/strong>，其中MLP采用2层线性层、ReLU作为激活函数。&lt;/li>
&lt;li>在&lt;strong>P-Tuning v2&lt;/strong>中，通过超参数针对不同下游任务选择不同神经网络，其中MLP的一种实现可以采用2层线性层、tanh作为激活函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153852283.png" alt="image-20240511153852283">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning v1&lt;/strong>的局限性：不同的下游任务、不同规模的模型，微调结果不稳定。&lt;/li>
&lt;li>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：修改模型结构，在各层注意力层增加前缀处理器网络以抵消小模型对前缀向量特征提取不足的局限，支持不同下游任务选择不同前缀提示长度、选择不同前缀编码器神经网络结构。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2110.07602&lt;/p></description></item><item><title>【chatGPT】学习笔记46-LLM微调技术之P-Tuning V1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</link><pubDate>Wed, 01 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</guid><description>&lt;p>前面给大家分享了&lt;code>Soft Prompt&lt;/code>技术分支下的&lt;code>Prefix-Tuning&lt;/code>和&lt;code>Prompt Tuning&lt;/code>，在这个技术分支下，还有一项需要重点了解的微调技术——&lt;code>P-Tuning&lt;/code>。&lt;/p>
&lt;p>&lt;code>P-Tuning&lt;/code>是清华大学和MIT于2021年联合发布的一项微调技术，在NLU(自然语言理解)任务上有重大突破。&lt;/p>
&lt;p>本文解读论文**《GPT Understands, Too》**，我们一起来学习一下&lt;code>P-Tuning&lt;/code>技术的原理。&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Discrete Prompts&lt;/strong>(离散提示词)会导致大模型性能不稳定。&lt;/p>
&lt;ul>
&lt;li>比如：修改提示词中的一个单词，都可能导致大模型的性能大幅下降。&lt;/li>
&lt;li>本质：根据自然语言形式的提示词进行预测，对于大模型本身&lt;strong>从数学上是不可微的&lt;/strong>(这就是数学意义上的&lt;strong>离散性&lt;/strong>)——不可微就意味着AI无法高效、稳定地&lt;strong>提特征&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>P-Tuning&lt;/code>技术，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。将离散提示词向量化为可训练的连续提示词(&lt;strong>trainable continuous prompt embeddings&lt;/strong>)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning&lt;/strong>通过连续提示词向量，降低了不同离散提示之间的差距，进而提升了模型的稳定性。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>在LAMA、SuperGLUE等NLU任务上，显著提高了模型性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426094803830.png" alt="image-20240426094803830">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：离散提示会导致大模型的稳定性问题。
&lt;ul>
&lt;li>&lt;strong>以手动离散提示为例&lt;/strong>：提示中改变一个单词可能会导致显著的性能下降，存在很大的不稳定性。&lt;/li>
&lt;li>&lt;strong>一些优化尝试&lt;/strong>：
&lt;ul>
&lt;li>调整语言模型本身，不稳定性问题有所缓解，但不同提示之间的性能差异仍然很大(特别是在少样本场景下)。&lt;/li>
&lt;li>&lt;strong>自动提示法(automatic prompting)&lt;/strong>：试图为给定任务搜索更好的提示，但这些方法并没有改变离散提示的不稳定本质。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426114420388.png" alt="image-20240426114420388">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>prompt encoder&lt;/strong>：论文提到通过&lt;strong>prompt encoder(提示词编码器)&lt;/strong>，将输入的离散提示Token，和连续提示Embedding连接起来后，输入给大语言模型。其中，&lt;strong>prompt encoder&lt;/strong>可采用LSTM或MLP来实现。&lt;/li>
&lt;li>&lt;strong>backpropagation to optimize&amp;hellip;&lt;/strong>：可以通过反向传播，优化连续提示词，进而将离散提示转变为可微的连续提示。&lt;/li>
&lt;li>&lt;strong>P-Tuning的本质&lt;/strong>：该技术的本质打破离散提示的限制——离散则不便于&lt;strong>提特征&lt;/strong>，连续可微则可学习——因此P-Tuning抵消了离散提示中微小变化对稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在LAMA基准测试中，使用P-Tuning，比手动离散提示(manual discrete prompts)提升了20多分，比搜索提示(searched prompts)提升了9分。&lt;/li>
&lt;li>在SuperGLUE基准测试中，在全监督和少样本下都优于PET的最佳离散提示(the best discrete prompts)。&lt;/li>
&lt;li>实验还证明，在更广泛的任务中，P-Tuning降低了不同离散提示之间的差异，进而提升了模型的稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426113823515.png" alt="image-20240426113823515">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>提示词是大家耳熟能详的激发LLM能力的技术手段，但是&lt;strong>从数学上具有极大的局限性&lt;/strong>——就是它是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;p>论文作者举了这样的一个例子：&lt;/p>
&lt;ul>
&lt;li>表格第三行和表格第四行的两个提示词，只是少了一个单词&lt;strong>In&lt;/strong>，AI猜出来X和Y填什么的准确度就下降了20分。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501144315842.png" alt="image-20240501144315842">&lt;/p>
&lt;p>在本论文发表前，业界还有一些自动化搜索离散提示的优化尝试：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>mining the training corpus&lt;/strong>：挖掘训练语料库。&lt;/li>
&lt;li>&lt;strong>gradient-based searching&lt;/strong>：基于梯度的搜索。&lt;/li>
&lt;li>&lt;strong>using pretrained generative model&lt;/strong>：使用预训练的生成模型。&lt;/li>
&lt;/ul>
&lt;p>这些优化方法的本质就是&lt;strong>自动生成提示词&lt;/strong>，但是用自然语言表示的提示词依然还是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>M、V、h&lt;/strong>：M表示预训练模型， 词表大小V，隐藏层大小h。&lt;/li>
&lt;li>&lt;strong>{(x&lt;sub>i&lt;/sub>, y&lt;sub>j&lt;/sub>)}&lt;sub>i&lt;/sub>&lt;/strong>：表示在NLU任务中的数据集。x&lt;sub>0:n&lt;/sub>={x&lt;sub>0&lt;/sub>, x&lt;sub>1&lt;/sub>, &amp;hellip;, x&lt;sub>n&lt;/sub>}是一系列离散Token组成的输入，y∈Y表示标签。&lt;/li>
&lt;li>&lt;strong>f&lt;sub>M&lt;/sub>(x)=p(y|x)&lt;/strong>：表示预训练模型M的任务，就是预测分类的条件概率。&lt;/li>
&lt;li>&lt;strong>[Di]&lt;/strong>：表示离散提示的Token，每一个离散提示都可以表示为T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;/ul>
&lt;p>通俗一点说，上面这一通数学建模，就是描述了一个填字游戏：&lt;/p>
&lt;ul>
&lt;li>比如：The capital of &lt;input checked="" disabled="" type="checkbox"> is [y]。&lt;/li>
&lt;li>如果x=Britain，则希望AI输出y=London。&lt;/li>
&lt;li>如果x=中国，则希望AI输出y=北京。&lt;/li>
&lt;/ul>
&lt;p>离散提示&lt;strong>T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}&lt;strong>会被Embedding为&lt;/strong>{e(D&lt;sub>0&lt;/sub>)&amp;hellip;e(D&lt;sub>i&lt;/sub>), e(x&lt;sub>0&lt;/sub>), &amp;hellip;, e(x&lt;sub>n&lt;/sub>), &amp;hellip;, e(D&lt;sub>k&lt;/sub>)}&lt;/strong>，其中e ∈
R&lt;sup>|V|×d&lt;/sup>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501151933103.png" alt="image-20240501151933103">&lt;/p>
&lt;p>&lt;strong>P-Tuning的实现怎么表达呢&lt;/strong>？如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[P&lt;sub>i&lt;/sub>]&lt;/strong>：表示第i个连续提示Embedding，注意论文的表述——&lt;strong>连续的提示词嵌入(continuous prompt embedding)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}&lt;/strong>：基于[P&lt;sub>i&lt;/sub>]的含义，那么任意一个提示词都能表达为T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;li>&lt;strong>f: [P&lt;sub>i&lt;/sub>]-&amp;gt;h&lt;sub>i&lt;/sub>&lt;/strong>：一个词嵌入函数，用来将T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}转换为{h&lt;sub>0&lt;/sub>, e(x), h&lt;sub>i+1&lt;/sub>, &amp;hellip;, h&lt;sub>j&lt;/sub>, e(y), h&lt;sub>j+1&lt;/sub>, &amp;hellip;, h&lt;sub>k&lt;/sub>}。&lt;/li>
&lt;li>&lt;strong>{P&lt;sub>i&lt;/sub>}&lt;sup>k&lt;/sup>&lt;sub>i=1&lt;/sub>&lt;/strong>：表示P-Tuning的目标——反向传播，优化损失值，在预训练模型之前学习到提示词的特征。&lt;/li>
&lt;/ul>
&lt;p>不严谨地理解一下P-Tuning的玩法——就是加了个新的神经网络，不断地在学习如下提示词：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>吾饥矣&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹中空空&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹鸣如鼓&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;/ul>
&lt;p>它会发现&lt;strong>吾饥矣、吾腹中空空、吾腹鸣如鼓&lt;/strong>的特征，都是在说&lt;strong>肚子饿了&lt;/strong>，于是在调用大语言模型之前，它就把自然语言形态的离散提示词都转变为：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>我饿了&lt;/strong>，你就要说：xxx。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153355161.png" alt="image-20240501153355161">&lt;/p>
&lt;p>最后，我们来完整地对比一下&lt;strong>自动化搜索离散提示&lt;/strong>法和&lt;strong>P-Tuning&lt;/strong>的差别：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自动搜索离散提示&lt;/strong>用的是&lt;strong>Prompt Generator&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>是用&lt;strong>Prompt Encoder&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>将数学意义上的&lt;strong>离散提示词&lt;/strong>转换为了&lt;strong>连续可微提示词&lt;/strong>，帮助AI更好地提特征。&lt;/li>
&lt;li>其实两种思路本质都一样，都是很巧妙的想法。&lt;/li>
&lt;li>论文还提到Prompt Encoder的实现采用了&lt;strong>LSTM、MLPs、identity mapping function(恒等映射函数)&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153925852.png" alt="image-20240501153925852">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1knowledge-probing">(1)实验结果1：Knowledge Probing&lt;/h2>
&lt;p>在知识探索(Knowledge Probing)型任务上，实验可以评估出AI获得现实世界知识量。&lt;/p>
&lt;p>LAMA数据集创建了三元组形式的完型填空，来实施知识探索评估。&lt;/p>
&lt;p>从实验结果上看，P-Tuning显著提高了知识探测的效果。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LAMA-34k数据集&lt;/strong>：从43.3%提高到50.6%&lt;/li>
&lt;li>&lt;strong>LAMA-29k数据集&lt;/strong>：从45.2%提高到64.2%&lt;/li>
&lt;li>&lt;strong>相较于离散提示搜索方法&lt;/strong>：P-Tuning优于离散提示搜索方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161239613.png" alt="image-20240501161239613">&lt;/p>
&lt;h2 id="2实验结果2fully-supervised-learning">(2)实验结果2：Fully-supervised Learning&lt;/h2>
&lt;p>实验采用了SuperGLUE基准测试，测试了7个自然语言理解任务(NLU)。包括：&lt;/p>
&lt;ul>
&lt;li>问答、MultiRC、文本蕴含、RTE、共指消解、因果推理、词义消歧。&lt;/li>
&lt;/ul>
&lt;p>实验使用了四个版本的预训练模型：&lt;/p>
&lt;ul>
&lt;li>GPT2-Base&lt;/li>
&lt;li>GPT2-medium&lt;/li>
&lt;li>BERT-Base&lt;/li>
&lt;li>BERT-Large&lt;/li>
&lt;/ul>
&lt;p>实验证明&lt;strong>P-Tuning可以提高 BERT和GPT上的全监督学习性能&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在 BERT-Base上，P-Tuning在5/7任务上实现了最佳性能。&lt;/li>
&lt;li>在 BERT-Large上，P-Tuning在4/7任务上超越了其他方法。&lt;/li>
&lt;li>在 GPT2-Base和 GPT2-Medium上，P-Tuning 在所有任务上始终是最佳性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161948829.png" alt="image-20240501161948829">&lt;/p>
&lt;h2 id="3实验结果3few-shot-learning">(3)实验结果3：Few-Shot Learning&lt;/h2>
&lt;p>实验使用了少样本SuperGLUE基准测试，就是FewGLUE数据集。&lt;/p>
&lt;p>实验证明了P-Tuning有一定的提升：&lt;/p>
&lt;ul>
&lt;li>ALBERT上，比PET平均高出1个点、比Prompt Tuning高出13个点。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501162327022.png" alt="image-20240501162327022">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>离散提示的问题&lt;/strong>：数学上离散，不便于AI提特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning的核心思想&lt;/strong>：将离散提示词转换为连续可微提示词，微调的目标是用LSTM这类网络学习提示词特征。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2103.10385&lt;/p></description></item><item><title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</link><pubDate>Wed, 03 Apr 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</guid><description>&lt;p>紧接着Stanford的&lt;code>Prefix Tuning&lt;/code>论文，Google迅速发表了&lt;code>Prompt Tuning&lt;/code>技术论文。Google声称该技术比&lt;code>Prefix Tuning&lt;/code>更易上手且成本更低，因此该技术随后也成为了微调技术中的一个重要分支。&lt;/p>
&lt;p>本文解读论文**《The Power of Scale for Parameter-Efficient Prompt Tuning》**，与大家共同感受&lt;code>Prompt Tuning&lt;/code>技术的奇妙之处。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403122759085.png" alt="image-20240403122759085">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Prompt Tuning&lt;/strong>与&lt;strong>Prefix Tuning&lt;/strong>一样，都是以任务为中心的思路解决问题。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>以任务为中心&lt;/strong>：它们都在试图解决&lt;strong>FFT&lt;/strong>针对不同的下游任务都需产生一个新的微调后大模型而导致的成本效率问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prompt Tuning&lt;/code>，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。统一不同下游任务的训练数据格式，并将这些不同下游任务的训练数据汇总成一个乱序的数据集，微调预训练模型，最终获得一个能处理不同下游任务的大模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在小参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>略差于FFT性能。&lt;/li>
&lt;li>在中参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>快速接近于FFT性能。&lt;/li>
&lt;li>在大参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>与FFT性能持平。&lt;/li>
&lt;li>因此，&lt;strong>Prompt Tuning&lt;/strong>在大参数规模的模型上，更具成本效率优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403092613314.png" alt="image-20240403092613314">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：论文中提到的&lt;strong>Prompt Design&lt;/strong>可以理解为大家耳熟能详的&lt;strong>提示词及提示词工程&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>经过工程实践，大家都知道提示词工程有一定的效果，但效果远不及FFT。为什么呢？论文中总结了提示词工程的两个短板：&lt;strong>the discrete space of words(离散空间的单词)&lt;strong>和&lt;/strong>requires human involvement(人类的投入)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>the discrete space of words(离散空间的单词)&lt;/strong>：提示词工程中的提示词是数学意义上的&lt;strong>离散&lt;/strong>，大语言模型不能很好地学习其隐含的特征。隐含特征不是提示词内容本身的显式特征，而是诸如提示词的句式、问法、潜台词等隐式特征。业界针对提取离散空间词汇也提出了一些算法(如：&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)，但效果也非常限。&lt;/li>
&lt;li>&lt;strong>requires human involvement(人类的投入)&lt;/strong>：提示词工程依赖有经验的提示词工程师，针对不同下游任务设计提示词，工作量巨大，并且对大模型推理能力的提升又极其有限。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：论文中提到的&lt;strong>Model Tuning&lt;/strong>和&lt;strong>Model Tuning(Multi Task)&lt;/strong>，可以理解为&lt;strong>FFT&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Model Tuing(Multi Task)&lt;/strong>：&lt;strong>Model Tuning(Multi Task)&lt;strong>是针对每个下游任务都微调出一个大模型，而&lt;/strong>Model Tuning&lt;/strong>是将N个下游任务都微调到一个大模型中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>统一下游任务的数据格式&lt;/strong>：论文中提到&lt;code>an additional k tunable tokens per downstream task to be prepended to the input text&lt;/code>，就是为了达成统一下游任务的数据格式。如：['[CLS]&amp;rsquo;, &amp;lsquo;中&amp;rsquo;, &amp;lsquo;国&amp;rsquo;, &amp;lsquo;的&amp;rsquo;, &amp;lsquo;春&amp;rsquo;, &amp;lsquo;节&amp;rsquo;, &amp;lsquo;是&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;。&amp;rsquo;, &amp;lsquo;[SEP]']。&lt;/li>
&lt;li>&lt;strong>合并下游任务的数据集合&lt;/strong>：当我们统一了下游任务的数据格式，就可以将这些下游任务数据集合混合在一起。&lt;/li>
&lt;li>&lt;strong>LLM具备学习数据集合隐式特征的能力&lt;/strong>：论文假设LLM是具备学习上述数据格式隐式特征的能力，并通过实验验证了这个假设。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的本质&lt;/strong>：该技术的本质是LLM的核心能力之一就是&lt;strong>提特征&lt;/strong>。如果特征很明显，LLM就可以低成本提取。如果特征很隐晦，LLM无法低成本提取、甚至无法提取，&lt;strong>Prompt Tuning&lt;/strong>就是改变数据集，将隐式特征转为显式特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;li>因此，在大规模模型上，&lt;strong>Promp Tuning&lt;/strong>具备巨大的成本优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101305842.png" alt="image-20240403101305842">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1下游任务数据格式归一化的理论基础">(1)下游任务数据格式归一化的理论基础&lt;/h2>
&lt;p>论文的实验对象是T5，因为T5有一个有趣的观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Following the “text-to-text” approach of T5 (Raffelet al., 2020), we cast all tasks as text generation&lt;/strong>：所有的下游任务都可以等效于文本生成。这个观点就可以支撑&lt;strong>Prompt Tuning&lt;/strong>将所有下游任务的训练数据格式统一起来。&lt;/li>
&lt;li>如：翻译下游任务，可以将训练数据构造为：&amp;ldquo;translate English to German: hello world!&amp;rdquo;&lt;/li>
&lt;li>如：摘要下游任务，可以将训练数据构造为：&amp;ldquo;summarize: xxxxxxxxxxxxxxxxxxx&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403112458776.png" alt="image-20240403112458776">&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>：在不同下游任务的训练数据可归一化的前提下，大语言模型可被建模为&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>，X是用户输入的Tokens，Y是在X发生的概率下模型的输出。&lt;/li>
&lt;li>&lt;strong>Prompt Design&lt;/strong>的短板：提示词工程需要人类不断尝试寻找出合适提示词，这种不断尝试方法可能是人工寻找的，也可能采用了非可微的搜索方法(如：前文提到的&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>Pr&lt;sub>θ;θ&lt;sub>P&lt;/sub>&lt;/sub> (Y|[P; X])&lt;/strong>：这个公式表达了Prompt Tuning的核心思想——在训练数据中植入特殊Token，大模型除了学习训练数据中的显式特征外，还能学习Prompt形式训练数据的隐式特征。对于Prompt隐式特征的学习最终影响的不是预训练模型的参数θ，而是在修正θ&lt;sub>P&lt;/sub>。&lt;/li>
&lt;li>&lt;strong>[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>] ∈ R&lt;sup>(p+n)×e&lt;/sup>&lt;/strong>：Prompt中的普通标记被大模型嵌入后得到&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)，Prompt中的特殊标记被大模型嵌入后得到&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)。[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>]则表示输入给大模型后续神经网络层的高维向量。训练的影响并不会修正&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数，只会修正&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115838249.png" alt="image-20240403115838249">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115900991.png" alt="image-20240403115900991">&lt;/p>
&lt;p>因此，实验的关注点如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的初始值&lt;/strong>：和Prefix Tuning一样，都需要关注软提示的初始值，以提升训练速度和效果。&lt;/li>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的长度&lt;/strong>：和Prefix Tuning一样，也需要关注软提示的长度，以降低训练成本。
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的参数成本=E*P&lt;/strong>：E是普通标记的向量维数，P是特殊标记的长度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3如何消减特殊标记的影响">(3)如何消减特殊标记的影响&lt;/h2>
&lt;p>由于增加了特殊标记，大模型学习的内容就不再是原汁原味的&amp;quot;人类自然语言&amp;quot;了。这样就可能导致大模型无法用自然语言作答——这就好像你在训练大模型鸟语但又期待它能说人话、你在用中文教英语最后学会的是Chinglish。&lt;/p>
&lt;p>论文中提出了**Span Corruption(跨度损失)**的概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Span Corruption(跨度损失)&lt;/strong>：比如，训练数据&lt;code>Thank you [X] me to your party [Y] week&lt;/code>，&lt;input checked="" disabled="" type="checkbox"> 、[Y]就是特殊标记，将这种特殊标记植入自然语言的目的是&amp;quot;问题模式等隐式特征的显性化&amp;rdquo;，但弊端就是让大语言模型学会了非人类的自然语言。&lt;/li>
&lt;li>&lt;strong>论文提出了三种解决方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>Span Corruption法&lt;/strong>：啥都不做，任由大语言模型输出特殊标记，忽略这种影响。&lt;/li>
&lt;li>&lt;strong>Span Corruption+Sentinel法&lt;/strong>：在大语言模型中增加Sentinel，一定程度地降低这种影响。&lt;/li>
&lt;li>&lt;strong>LM Adaptation法&lt;/strong>：采用Raffel提出的一个小模型，纠正大语言模型输出特殊标记的倾向，最终输出纯粹的自然语言。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115956245.png" alt="image-20240403115956245">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文阐述了详细的实验过程、实验数据，最终的实验结果如前文所述：&lt;/p>
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120117025.png" alt="image-20240403120117025">&lt;/p>
&lt;ul>
&lt;li>当模型规模逐渐变大，&lt;strong>Promp Tuning&lt;/strong>涉及的参数相较于&lt;strong>Prefix Tuning&lt;/strong>更少，但微调效果持平，因此&lt;strong>Prompt Tuning具备巨大的成本优势&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120213717.png" alt="image-20240403120213717">&lt;/p>
&lt;h2 id="42重要发现">4.2.重要发现&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;p>论文从可解释性方面发现了&lt;strong>语义聚合现象&lt;/strong>，进一步证明了Prompt形式的数据更有利于大语言模型学习其隐式特征：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>语义聚合现象&lt;/strong>：观测被大语言模型嵌入后的特殊标记和普通标记，可以发现出现了物以类聚的现象：
&lt;ul>
&lt;li>如：Technology / technology / Technologies / technological / technologies相关的训练数据，向量相似度发生了语义聚合。&lt;/li>
&lt;li>语义聚合的出现，说明了大语言模型学习到了Prompt形式的训练数据中的隐式特征，因此可以举一反三地处理为见过的下游任务相关输入。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文还证明了&lt;strong>Prompt Ensembling(提示集成能力)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>基于Prompt Tuning的技术思想，可以做到数据格式统一、不同下游任务的训练数据混合训练，进而达到&amp;rdquo;&lt;strong>一个大模型支持多种不同下游任务&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>这种思想可以在超大规模的模型上极大地降低训练成本、使用成本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;p>在工程实践方面，论文也给出了&lt;strong>Prompt长度、Prompt初始值&lt;/strong>的相关推荐：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt长度的影响&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在中小参数规模的模型上，Prompt长度越长，提示效果越好，但过犹不及(实验长度的临界值是150)——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;li>在大参数规模的模型上，Prompt长度反而没有什么影响了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt的初始值选择的影响&lt;/strong>：随机初始化Prompt的效果远差于用下游任务相关联提示词做初始值的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121146629.png" alt="image-20240403121146629">&lt;/p>
&lt;p>最后，论文还通过消融实验，补充了消减&lt;strong>Span Corruption&lt;/strong>的建议：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LM Adaptation&lt;/strong>：在中小规模模型上，采用LM Adaptation，对大语言模型的纠正效果更好。LM Adaptation增加步数会达到更好的纠正效果。&lt;/li>
&lt;li>在大规模模型上，Span Corruption的影响也可以忽略不计了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121223051.png" alt="image-20240403121223051">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：通过归一化不同下游任务的训练数据，并将隐式特征显性化，帮助大语言模型学习。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prompt形式的训练数据有助于LLM学习隐式特征。&lt;/li>
&lt;li>采用Prompt Tuning可用一套模型搞定多个下游任务。&lt;/li>
&lt;li>对于大规模参数的模型，Prompt长度和初始化影响很小。&lt;/li>
&lt;li>对于中小规模参数的模型，Prompt长度和初始值可参考Prefix Tuning的实践。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2104.08691.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</link><pubDate>Fri, 29 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</guid><description>&lt;p>&lt;code>Prefix Tuning&lt;/code>是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。&lt;/p>
&lt;p>本文通过解读论文**《Prefix-Tuning: Optimizing Continuous Prompts for Generation》**，与小伙伴们一起学习理解&lt;code>Prefix Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329153821295.png" alt="image-20240329153821295">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：**FFT(全参数微调)**针对不同的下游任务都需要产生一个新的微调后大模型，存在成本效率等诸多工程问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prefix Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。针对不同下游任务创建不同的&lt;strong>&lt;code>Prefix(前缀向量模块)&lt;/code>&lt;/strong>，这样不同下游任务只需要在一套预训练大模型上加载不同&lt;strong>Prefix小模型&lt;/strong>即可。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在GPT-2的&lt;strong>&lt;code>Table-To-Text(表格生成文本)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占GPT-2参数的0.1%，即可达到GPT-2同等水平。&lt;/li>
&lt;li>在BART的&lt;strong>&lt;code>Sumarization(摘要)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占BART参数的0.1%，即可达到BART同等水平。&lt;/li>
&lt;li>在上述两种实验中，额外还观察到一定的泛化涌现能力，&lt;code>Prefix Tuning&lt;/code>可外推到训练期间未见过的任务主题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329111747247.png" alt="image-20240329111747247">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>是向预训练模型中增加新的小模型，仅微调小模型参数以达到高效微调的目的。实验效果证明仅需微调2%~4%的参数即可达到全参数微调的效果。&lt;/p>
&lt;ul>
&lt;li>Adapter Tuning的详细解读可参见本技术专栏这篇文章《【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：GPT-3的一个重要贡献就是&lt;strong>Context Learning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>，GPT-3是一套统一的大语言模型，用户不需要针对下游任务单独微调，直接通过提示词和上下文，影响GPT-3输出的答案。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：&lt;strong>Prefix Tuning&lt;/strong>借鉴了&lt;strong>Adapter Tuning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>的思想：&lt;/p>
&lt;ul>
&lt;li>Prefix Tuning也额外&lt;strong>增加了N个小模型&lt;/strong>，这些小模型外挂于同一套预训练模型上，&lt;strong>不同小模型解决不同的下游任务&lt;/strong>。&lt;/li>
&lt;li>Prefix Tuning增加的这些小模型的作用类似&lt;strong>Prompt(提示词)&lt;/strong>，它们会在用户输入的文本前额外增加针对不同下游任务的&lt;strong>提示词前缀Prefix&lt;/strong>。这些前缀不是自然语言，而是Transformer架构中向量形式的Token，这种Token叫做&lt;strong>虚拟Token&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GPT-2的Table-To-Text&lt;/strong>和&lt;strong>BART的Sumarization&lt;/strong>的测试效果：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPT-2的Table-To-Text&lt;/strong>：在完整数据集上训练时，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>在表格到文本的下游任务的性能相当。&lt;/li>
&lt;li>&lt;strong>BART的Sumarization&lt;/strong>：在摘要方面，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>性能略有下降。&lt;/li>
&lt;li>&lt;strong>Low Data Settting&lt;/strong>：在数据量少的数据集上，&lt;strong>Prefix Tuning&lt;/strong>能够克服数据集样本不足、提炼数据特征困难的问题，表现出了泛化涌现能力。在上述两项任务中性能表现优于FFT。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329113337615.png" alt="image-20240329113337615">&lt;/p>
&lt;h1 id="3prefix-tuning">3.Prefix Tuning&lt;/h1>
&lt;h2 id="31problem-statement问题陈述">3.1.Problem Statement(问题陈述)&lt;/h2>
&lt;p>Prefix Tuning采用了严谨的数学描述来阐述待解决的问题，这也是算法工程中的算法建模环节，我们尽量通俗地理解这个问题模型，也感受一下算法工程师的思维模式。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>x和y&lt;/strong>：x是大语言模型的输入，y是大语言模型的输出。&lt;/p>
&lt;ul>
&lt;li>如下图右侧上方所示，在摘要型下游任务中，人类输入的原始文本就是x，大语言模型输出的总结结果就是y。&lt;/li>
&lt;li>如下图右侧下方所示，在表格转文本的下游任务中，人类输入的结构化的表格字符串是x，大语言模型输出的表格描述就是y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>自回归语言模型的问题子域&lt;/strong>(如下图左上所示)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>pφ(y | x)&lt;/strong>：根据Transformer这种自回归模型的网络结构，其本质可抽象为&lt;strong>pφ(y | x)&lt;/strong>，φ为大语言模型的参数，p就是在**&amp;ldquo;人类输入字符串x的条件下，大语言模型输出y的概率分布&amp;rdquo;**。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;/strong>：z被定义为x和y的序列，其中X&lt;sub>idx&lt;/sub>表示了x的索引，Y&lt;sub>idx&lt;/sub>表示y的索引。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>&lt;/strong>：h&lt;sub>i&lt;/sub>表示&lt;strong>时间步i的激活(activation)&lt;/strong>，h&lt;sub>i&lt;/sub>又是由第i个时间步中的n层激活组成的序列(即h&lt;sub>i&lt;/sub>= [h&lt;sub>i&lt;/sub>&lt;sup>(1)&lt;/sup>;···;h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>])，其中h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>表示在Transformer架构中的第i个时间步的第n层的激活。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：此数学公式表示——根据z&lt;sub>i&lt;/sub>，以及h&lt;sub>1&lt;/sub>~h&lt;sub>i-1&lt;/sub>，计算当前的h&lt;sub>i&lt;/sub>。展开解释一下就是，Transformer模型会根据第i个时间步的x、y，以及第1个时间步~第i-1个时间步计算的各时间步计算的各层激活向量，计算当前时间步下Transformer各层的激活。&lt;/li>
&lt;li>&lt;strong>p&lt;sub>φ&lt;/sub>(z&lt;sub>i+1&lt;/sub> | h&lt;sub>≤i&lt;/sub>) = softmax(W&lt;sub>φ&lt;/sub>h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>)&lt;/strong>：最后一层的h&lt;sub>i&lt;/sub>是Transformer训练后获得的概率分布，用来根据当前Token预测下一个Token。其中，W&lt;sub>φ&lt;/sub>是一个用于将h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>映射到词汇表上logits的预训练矩阵。&lt;/li>
&lt;li>上述数学描述中，包含很多AI相关术语(如：自回归模型、时间步i的激活(activation)、各层激活向量、词汇表上logits等)，可参见本技术专栏**《NLP底层原理》篇**的系列文章。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Encoder-Decoder模型的问题子域&lt;/strong>(如下图左下所示)：&lt;/p>
&lt;ul>
&lt;li>与自回归语言模型的问题子域大部分数学描述相同。&lt;/li>
&lt;li>不同点在于基于Encoder-Decoder架构的结构特点，x和y被拆分开了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的微调求解目标&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>max&lt;sub>φ&lt;/sub> log p&lt;sub>φ&lt;/sub>(y | x) = Σ&lt;sub>i∈Y&lt;sub>idx&lt;/sub>&lt;/sub> log p&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub> | h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：这个看着复杂的公式，就是在表达微调的最终目标就是在各个时间步的各时间步激活的条件下x和y序列的概率分布求和。说人话就是，&lt;strong>微调后的模型能够根据x预测最大概率应该输出y&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329125952458.png" alt="image-20240329125952458">&lt;/p>
&lt;h2 id="32intuition直觉">3.2.Intuition(直觉)&lt;/h2>
&lt;p>论文在正式阐述&lt;strong>Prefix Tuning&lt;/strong>的原理之前，描述了研究员的灵感来源：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Based on intuition from prompting&lt;/strong>：提示词被证明是可以影响大语言模型输出，提示词的思想本质是——大语言模型像一个什么都知道的老人，人类的输入需要使用一定的提示技巧唤醒老人的记忆，从而帮助他输出正确的答案。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;strong>和&lt;/strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：再看3.1问题陈述章节的这两个数学公式，提示词有两个作用：
&lt;ul>
&lt;li>&lt;strong>作用1&lt;/strong>：提示词作为人类输入x的一部分，它起到了&lt;strong>影响大语言模型关注x的哪些Token&lt;/strong>的作用。&lt;/li>
&lt;li>&lt;strong>作用2&lt;/strong>：在作用1的驱动下，影响了大语言模型各层的激活计算结果h&lt;sub>i&lt;/sub>，进而&lt;strong>影响了在x条件下应该接什么y的概率&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>but fail for most pretrained LMs&lt;/strong>：虽然提示词从理论上可以影响大语言模型的输出，但论文进一步阐述了提示词的局限性——做过提示词工程的小伙伴应该知道，仅仅通过自然语言形式的提示词，对大语言模型输出的效果提升很有限。&lt;/li>
&lt;li>&lt;strong>continuous word embeddings&lt;/strong>：论文根据上述头脑实验，准备新增1个Prefix模型，在人类输入x词嵌入为向量后，在这个词嵌入向量前增加1个&lt;strong>Prefix Token(前缀向量)&lt;/strong>(这种向量不同于离散向量，不会引发计算的困难)。为了保证Prefix Token有足够的提示性，论文在Transformer的所有层都增加了这种前缀向量。&lt;/li>
&lt;/ul>
&lt;h2 id="33methodparametrization-of-psubθsub原理">3.3.Method/Parametrization of P&lt;sub>θ&lt;/sub>(原理)&lt;/h2>
&lt;p>论文至此，正式阐述了&lt;strong>Prefix Tuning&lt;/strong>的实现方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>z=[x;y]到z=[prefix;x;y]&lt;/strong>：给自回归模型和Encoder-Decoder模型的z向量增加了前缀&lt;strong>PREFIX&lt;/strong>向量。其中P&lt;sub>idx&lt;/sub>表示了前缀的索引。(如：)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329142053466.png" alt="image-20240329142053466">&lt;/p>
&lt;ul>
&lt;li>基于前缀向量，h&lt;sub>i&lt;/sub>的数学公式表示为各层的激活包含前缀向量的计算和大语言模型预测的概率分布。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329143101780.png" alt="image-20240329143101780">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>|P&lt;sub>idx&lt;/sub>| × dim(h&lt;sub>i&lt;/sub>)&lt;/strong>：P&lt;sub>θ&lt;/sub>是Prefix Tuning新增的小模型的参数，参数的维数必然是有限的，因为它的维数等于前缀向量个数乘以h&lt;sub>i&lt;/sub>的维度。这也解释了为什么Prefix Tuning新增的小模型参数规模仅占预训练模型的0.1%。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P&lt;sub>θ&lt;/sub>[i, :] = MLP&lt;sub>θ&lt;/sub>(P ′&lt;sub>θ&lt;/sub>[i, :])&lt;/strong>：论文还发现直接微调P&lt;sub>θ&lt;/sub>会导致训练效果不好，于是采用了重参数化方法，引入了前馈神经网络MLP&lt;sub>θ&lt;/sub>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文的实验结论：&lt;strong>Prefix Tuning&lt;/strong>在Table-To-Text下游任务表现良好、在外推涌现方面表现良好，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Table-To-Text实验&lt;/strong>：用GPT2-Medium和GPT2-Large对比FFT和Prefix Tuning，Prefix Tuning都达到了SOTA水平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150136166.png" alt="image-20240329150136166">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Low-Data-Setting&lt;/strong>实验：在少数据测试中，Prefix Tuning的性能表现和训练稳定度优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150354433.png" alt="image-20240329150354433">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Summarization实验&lt;/strong>：使用XSUM数据集，Prefix Tuning性能略低于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150524132.png" alt="image-20240329150524132">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Extrapolation实验&lt;/strong>：外推涌现实验中，使用XSUM数据集，Prefix Tuning效果优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150541755.png" alt="image-20240329150541755">&lt;/p>
&lt;h2 id="42intrinsic-evaluation重要发现">4.2.Intrinsic Evaluation(重要发现)&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix长度的影响&lt;/strong>：
&lt;ul>
&lt;li>不同下游任务需要增加不同长度的前缀向量。&lt;/li>
&lt;li>Prefix长度越长，提示效果越好，但过犹不及——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151553754.png" alt="image-20240329151553754">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>在哪些层做Prefix的影响&lt;/strong>：仅在词嵌入层做Prefix的效果远差于在Transformer各层做Prefx的效果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151830368.png" alt="image-20240329151830368">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix和Infix的影响&lt;/strong>：用前缀法的性能效果优于中缀法。
&lt;ul>
&lt;li>研究员猜测，前缀法可能影响x和y，中缀法可能只影响y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152257513.png" alt="image-20240329152257513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix的初始值选择的影响&lt;/strong>：随机初始化Prefix向量的效果远差于与下游任务相关联的Prefix向量初始化的效果。
&lt;ul>
&lt;li>这种现象可能由于用与下游任务不相关的提示词向量，会导致更长时间的前缀神经网络的收敛(甚至不收敛)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152826581.png" alt="image-20240329152826581">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：增加一个新的具备提示能力的前缀向量小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prefix长度不宜过长或过短，需根据下游任务实验获得。&lt;/li>
&lt;li>对Transformer做全层的Prefix效果更好。&lt;/li>
&lt;li>Prefix会影响x和y，效果优于Infix。&lt;/li>
&lt;li>Prefix的初始值需选择与下游任务相关的提示向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2101.00190.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</link><pubDate>Fri, 22 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</guid><description>&lt;p>&lt;code>Adapter Tuning&lt;/code>是LLM微调技术中一个重要的技术分支，于2019年由Google的Neil Houlsby等研究员提出。&lt;/p>
&lt;p>&lt;code>Adapter Tuning&lt;/code>方法证明了：微调&lt;strong>少量参数&lt;/strong>即可获得与&lt;strong>全参数微调&lt;/strong>接近的大模型性能。&lt;/p>
&lt;p>本文解读Neil Houlsby的论文**《Parameter-Efficient Transfer Learning for NLP》**，与小伙伴们一起学习理解&lt;code>Adapter Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322130214258.png" alt="image-20240322130214258">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：基于迁移学习思想，需要针对特定的下游任务，对预训练模型进行&lt;strong>全参数微调&lt;/strong>。但针对每个下游任务都要做一次全参数微调，&lt;strong>成本高效率低&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Adapter Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>进行迁移学习的方法。&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>仅需要微调少量参数，就可以支持不同的下游任务。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GLUE基准测试&lt;/strong>中，用&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>方法仅需在预训练模型基础上&lt;strong>增加并微调3.6%的参数&lt;/strong>，即可达到BERT Transformer模型全参数微调的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>通过上述摘要的内容，我们可以想象一下，在2019年绝大多数人还在采用全参数微调这种高成本方案时，&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>仅需微调3.6%的少量参数，会产生多大的&lt;strong>生产效率差异&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322103351046.png" alt="image-20240322103351046">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>背景技术1&lt;/strong>：&lt;strong>基于特征的迁移和微调&lt;/strong>(&lt;code>feature-based transfer and fine-tuning&lt;/code>)是迁移学习思想中的重要工程方法，它也是BERT模型的重要理论基础。论文中：&lt;code>Fine-tuning involves copying the weights from a pre-trained network and tuning them on the downstream task&lt;/code>，表达了BERT模型的训练范式——复用1个BERT的预训练模型的参数(&lt;strong>基于特征的迁移&lt;/strong>)，再针对不同下游任务进行微调(&lt;strong>基于特征的微调&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>背景技术2&lt;/strong>：历史上，已证明针对预训练模型的网络结构中的高层(&lt;code>top layer&lt;/code>)进行&lt;strong>基于特征的微调&lt;/strong>，相较于&lt;strong>基于特征的全参数微调&lt;/strong>，更具有性价比。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：对比&lt;strong>Adapter Tuning&lt;/strong>、&lt;strong>Top Layer Fine Tuing&lt;/strong>、&lt;strong>Full Fine Tuning&lt;/strong>在多任务微调场景下的效果：
&lt;ul>
&lt;li>&lt;strong>在小参数模型上的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可达到&lt;strong>Full Fine Tuning&lt;/strong>的效果，&lt;strong>Top Layer Fine Tuing&lt;/strong>达不到。&lt;/li>
&lt;li>&lt;strong>在稳定性方面的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>的训练效果很稳定，&lt;strong>Top Layer Fine Tuing&lt;/strong>在小参数模型上波动大、只有在大参数模型上才能保证稳定的训练效果。&lt;/li>
&lt;li>&lt;strong>在性价比方面的对比&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可以微调少量参数，即可达到&lt;strong>Full Fine Tuning&lt;/strong>的训练效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322115129249.png" alt="image-20240322115129249">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>基于特征的迁移和微调&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, f(w))&lt;/code>，微调的过程是不断学习修改参数&lt;code>w&lt;/code>和&lt;code>v&lt;/code>，这样就导致预训练模型的参数&lt;code>w&lt;/code>被修改，进而导致极高的训练成本。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, w)&lt;/code>，微调的过程是不断学习修改参数&lt;code>v&lt;/code>，直接复用预训练模型的参数&lt;code>w&lt;/code>而不是修改它，又因为参数&lt;code>v&lt;/code>的数量级远小于参数&lt;code>w&lt;/code>，因此训练成本极低。另外，针对新的&lt;code>下游任务n&lt;/code>只需要增加新的Adapter，训练对应的参数&lt;code>vn&lt;/code>。&lt;/li>
&lt;li>&lt;code>g(v, w)&lt;/code>的具体代码实现等效于，在原有预训练模型的网络结构中，插入一些&lt;strong>Adapter层&lt;/strong>，预训练模型参数&lt;code>w&lt;/code>作为Adapter层的入参，&lt;strong>训练的目标是学习并修改Adapter层的参数&lt;code>v&lt;/code>&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322120647325.png" alt="image-20240322120647325">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>容易与Adapter Tuning混淆的其它训练方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>多任务学习&lt;/strong>：multi-task learning，也会在预训练模型的网络结构中增加新层，最终也是修改了新层的参数。但多任务学习的训练，是将所有下游任务作为训练新层参数的输入。&lt;/li>
&lt;li>&lt;strong>持续学习&lt;/strong>：contiuanl learning，是将N个下游任务组成任务流后逐一学习，这样就要求训练&lt;code>任务m&lt;/code>时，预训练模型的网络结构能够记住之前已经训练过的&lt;code>任务1~任务m-1&lt;/code>得到的参数。这将对预训练模型的记忆能力产生巨大的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322122001309.png" alt="image-20240322122001309">&lt;/p>
&lt;h1 id="3adapter-tuning原理">3.Adapter Tuning(原理)&lt;/h1>
&lt;p>Adatper Tuning具体是如何实现的呢？论文中详细解释了Adapter层的网络结构，以及如何在原始的预训练模型上插入这些Adapter层：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的插入位置&lt;/strong>：在Transformer的多头注意力+前馈网络层之后，2x前馈网络层之后，分别插入了&lt;strong>Adapter层&lt;/strong>。另外，在每个Adapter层之后还插入了一个Layer Norm层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124748365.png" alt="image-20240322124748365">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的内部结构&lt;/strong>：Adapter层包含3层
&lt;ul>
&lt;li>&lt;strong>前馈网络的向量降维层&lt;/strong>：用于将前一层预训练模型输出的高维向量，降维为低维向量。&lt;/li>
&lt;li>&lt;strong>非线性处理层&lt;/strong>：对下游任务微调时，学习参数&lt;code>v&lt;/code>。&lt;/li>
&lt;li>&lt;strong>前馈网络的向量升维层&lt;/strong>：用于将Adapter层输出的低维向量，升维为高维向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Adapter层的参数数量计算公式&lt;/strong>：&lt;code>count(v)=2md+d+m&lt;/code>
&lt;ul>
&lt;li>&lt;strong>d&lt;/strong>：前一层预训练模型输出的高维向量的维数。&lt;/li>
&lt;li>&lt;strong>m&lt;/strong>：Adapter层降维后的低维向量维数。&lt;/li>
&lt;li>&lt;strong>实践经验&lt;/strong>：当m远小于d时，Adapter层的参数量会很小。论文给出的经验数据是可以通过控制m的数值，将Adapter层的参数量控制为预训练大模型参数量的0.5%~8%。这样，可以精准控制微调成本。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124814661.png" alt="image-20240322124814661">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;p>论文至此就一个实验结论：&lt;strong>Adapter Tuning&lt;/strong>就是香，具体如下：&lt;/p>
&lt;ul>
&lt;li>在GLUE基准测试和其他17个公共文本分类任务上，适配器调优效果，优于全参数微调。&lt;/li>
&lt;li>适配器调优在参数数量大幅减少的情况下，仍能保持与全参数微调相近的性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125615723.png" alt="image-20240322125615723">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125621882.png" alt="image-20240322125621882">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的价值&lt;/strong>：追求微调少量参数，仍能达到全参数微调效果。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：增加一个新的小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的具体实现&lt;/strong>：改变预训练模型的网络结构，通过高维向量到低维向量的转换，训练不同下游任务的Adapter层。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/1902.00751.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记42-LLM微调技术概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 15 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</guid><description>&lt;p>笔者在使用大语言模型开展具体业务领域的任务时，遇到了如下问题：&lt;/p>
&lt;ul>
&lt;li>LLM的预训练模型不具备垂域知识，无法很好回答垂域的问题；&lt;/li>
&lt;li>希望用各种提示词技巧让模型理解专业问题，但效果有限，而且提高了使用门槛和模型推理成本；&lt;/li>
&lt;li>微调词嵌入模型成本低、速度快，但容易出现过拟合或泛化不足，也是治标不治本。&lt;/li>
&lt;/ul>
&lt;p>经过不断尝试总结，最终落脚点还是回到了大模型自身 &amp;ndash; 大模型的微调才是关键。&lt;/p>
&lt;p>什么是大模型微调？微调如何做？本篇开始，我们将为大家一步步揭开大模型微调的神秘面纱。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240314173850356.png" alt="image-20240314173850356">&lt;/p>
&lt;h1 id="1大语言模型的技术金字塔">1.大语言模型的技术金字塔&lt;/h1>
&lt;p>大语言模型相关技术可分为四层，其中：&lt;/p>
&lt;ul>
&lt;li>预训练的难度和成本最高，通常只有巨型公司(如谷歌、微软、OpenAI)才能承担其成本。&lt;/li>
&lt;li>提示词工程难度和成本最低，普通人就可以掌握。&lt;/li>
&lt;li>大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315141452986.png" alt="image-20240315141452986">&lt;/p>
&lt;p>以下是各层技术的详细阐述：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示词工程&lt;/strong>
&lt;ul>
&lt;li>技术原理：通过设计合适的提示或上下文来引导LLM生成期望的输出，侧重于用提示来激活预训练模型的能力，如总结摘要、翻译转换等。适用于文本处理、机器翻译等场景。&lt;/li>
&lt;li>技术特征：技术门槛低，终端用户即可掌握。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Agents&lt;/strong>：
&lt;ul>
&lt;li>技术原理：让LLM来决策一系列的动作，这些动作可以是让LLM分步解决问题，也可以是调用工具查询外部信息。这些动作形成一个工作流，最终完成任务目标。适用于相对复杂的用户交互应用，如智能客服。&lt;/li>
&lt;li>技术特征：有技术门槛，需要专业的LLM应用开发人员，并且了解大模型的基础原理，熟悉其领域的业务逻辑和流程。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>大模型微调&lt;/strong>：
&lt;ul>
&lt;li>技术原理：在预训练模型的基础上，针对特定任务的参数做调整。通过少量的数据训练即可提升模型在特定任务上的能力，同时保留模型原已学到的知识。适用于语义理解、垂域的各类应用。&lt;/li>
&lt;li>技术特征：技术门槛相对高，需要垂域有自己的LLM应用研发团队，具有数据处理、模型训练的能力和经验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>预训练技术&lt;/strong>：
&lt;ul>
&lt;li>技术原理：比如训练出一个&amp;quot;GPT3.5&amp;rdquo;，或者&amp;quot;GLM3.0&amp;rdquo;。&lt;/li>
&lt;li>技术特征：百G甚至千G的GPU资源需求，大量的大模型研究人员、数据科学家投入，超出了大部分公司的能力。一旦功成，效果显著。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>由于大模型微调的实用性，本专栏后续会展开介绍大模型微调技术。&lt;/p>
&lt;h1 id="2大模型微调的可行性及理论基础">2.大模型微调的可行性及理论基础&lt;/h1>
&lt;h2 id="1in-context-learning">(1)In-Context Learning&lt;/h2>
&lt;p>OpenAI在预训练过程中，发现LLM能够挖掘训练数据中的潜在特征和通用范式，进而习得训练数据之外的新能力。研究者把这一发现称之为In-Context Learning(基于上下文的学习)。&lt;/p>
&lt;p>基于In-Context Learning思想，衍生出了两种训练微调技术：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Prompt-tuning&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Instruction-tuning&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In-Context Learning的本质就是举一反三，基于已知的知识和训练数据可以挑战从没做过工作(下游任务)。预训练模型所蕴含的知识、预训练模型所使用的数据是广义上的&lt;code>context&lt;/code>。正因为In-Context Learning的有效性，所以对预训练模型进行微调是业界公认可行的技术路线。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315152229685.png" alt="image-20240315152229685">&lt;/p>
&lt;h2 id="2transfer-learning">(2)Transfer Learning&lt;/h2>
&lt;p>Transfer Learning(迁移学习)是人工智能发展过程中的一个重要思想，它的目标是将已经训练好的模型所包含的知识、推理能力迁移到未经训练的新模型上。比如对模型进行压缩、复用开源模型已经具备的能力来新模型，都是基于迁移学习思想。&lt;/p>
&lt;p>迁移学习包含很多具体的工程方法：&lt;/p>
&lt;ul>
&lt;li>Conservative Training&lt;/li>
&lt;li>Multi-task Learning&lt;/li>
&lt;li>Progressive Neural Network&lt;/li>
&lt;li>Domain-adversarial training&lt;/li>
&lt;li>Zero Shot Learning&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>无论有多少种工程方法，其核心思想是：&lt;strong>已训练的数据和任务与未训练的数据和任务存在因果、关联等逻辑关系&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315151715205.png" alt="image-20240315151715205">&lt;/p>
&lt;h2 id="3bert的优秀实践">(3)Bert的优秀实践&lt;/h2>
&lt;p>Bert充分发挥了Fine-tuning的技术优势和特点，在已经训练好的Bert模型基础上，加入少量的task-specific parameters。&lt;/p>
&lt;ul>
&lt;li>如分类任务，只需要在Bert模型上加一层softmax网络，然后对softmax网络进行微调。&lt;/li>
&lt;li>再如情感分析任务，取第一个token的输出表示，喂给一个softmax层得到分类结果输出。&lt;/li>
&lt;/ul>
&lt;p>微调Bert之所以成功，其本质原因是由于In-context Learning和Transfer Learning的有效性。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315153411365.png" alt="image-20240315153411365">&lt;/p>
&lt;h1 id="3大模型微调的技术全景图">3.大模型微调的技术全景图&lt;/h1>
&lt;h2 id="1大模型微调技术全景">(1)大模型微调技术全景&lt;/h2>
&lt;p>大模型微调技术从大的流派上可分为两类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Full Fine-Tuning&lt;/strong>：简称FFT，全量微调，即全参数微调。&lt;/li>
&lt;li>&lt;strong>Parameter-Efficient Fine-Tuning&lt;/strong>: 简称PEFT，高效微调，即通过某些技术手段选择部分参数微调。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315160640710.png" alt="image-20240315160640710">&lt;/p>
&lt;p>与FFT相比，PEFT是当前业界主流的技术路线，其原因主要是FFT存在如下缺陷：&lt;/p>
&lt;ul>
&lt;li>训练成本过高&lt;/li>
&lt;li>灾难性遗忘&lt;/li>
&lt;/ul>
&lt;h2 id="2peft技术分支">(2)PEFT技术分支&lt;/h2>
&lt;p>PEFT从微调目标方面可分为两类：&lt;/p>
&lt;ul>
&lt;li>Supervised Fine-Tuning: 简称SFT，属于有监督的微调。&lt;/li>
&lt;li>Reinforce Learning Human Feedback: 简称RLHF，属于利用人类反馈的强化学习。&lt;/li>
&lt;/ul>
&lt;p>其中SFT又具有诸多工程实践，因此产生了很多技术分支：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Additive&lt;/strong>: 增量派，在原有模型上增加额外小模型和少量参数。&lt;/li>
&lt;li>&lt;strong>Selective&lt;/strong>: 选择派，从原有模型的海量参数中，选择与下游任务相关的少量参数。&lt;/li>
&lt;li>&lt;strong>Reparametrization-based&lt;/strong>: 数学派，基于重参数化方法，将原有模型参数低秩化，获得小参数矩阵。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315161446294.png" alt="image-20240315161446294">&lt;/p>
&lt;p>在上述流派中，每个流派都有自己的代表方法，目前在业界广泛使用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sparse Adapter&lt;/strong>: 稀疏适配器。新增小模型，并从原有大模型中选择一部分参数，对这两部分参数进行微调。&lt;/li>
&lt;li>&lt;strong>Prefix-Tuning&lt;/strong>: 模块化轻量微调。增加prefix模块(Prefix模块会在用户输入前增加虚拟token)，训练prefix模块的参数。&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>: Low-Rank Adaption，低秩适配微调。它是目前业界在大语言模型、大视觉模型、多模态模型微调中的热门技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315162656941.png" alt="image-20240315162656941">&lt;/p>
&lt;p>上述技术分支的关系错综复杂，我们可以抽象的概括一下它们的技术思想：&lt;/p>
&lt;ul>
&lt;li>基于数据的技术分支：其核心思想是对进入预训练模型的训练语料进行前置处理。
&lt;ul>
&lt;li>如soft prompts中的Prompt-Tuning、Prefix-Tuning、P-Tuning，都是属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>基于模型的技术分支：其核心思想是在预训练模型基础上增加额外的模型分层，仅针对增量模型的参数和预训练模型的少量参数进行微调和变换。
&lt;ul>
&lt;li>如Adapter模型属于此技术分支。&lt;/li>
&lt;li>如LoRA、QLoRA、AdaLoRA也属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文从宏观上介绍了大模型微调技术的全景图，重点梳理了PEFT微调技术的众多技术分支，旨在帮助大家了解大模型微调技术的全貌。本专栏后续文章会展开阐述PEFT的各类微调技术。&lt;/p>
&lt;ul>
&lt;li>大语言模型的技术金字塔
&lt;ul>
&lt;li>大语言模型相关技术可分为四层：提示词工程、Agents、大模型微调、预训练技术。&lt;/li>
&lt;li>预训练的难度和成本最高；提示词工程难度和成本最低；大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的可行性及理论基础
&lt;ul>
&lt;li>In-Context Learning&lt;/li>
&lt;li>Transfer Learning&lt;/li>
&lt;li>Bert的优秀实践&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的技术全景图
&lt;ul>
&lt;li>Full Fine-Tuning&lt;/li>
&lt;li>Parameter-Efficient Fine-Tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>