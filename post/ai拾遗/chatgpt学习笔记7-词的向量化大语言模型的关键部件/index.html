<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件 - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。 目前炙手"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件"><meta property="og:description" content="人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。 目前炙手"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/"><meta property="article:published_time" content="2023-08-17T19:00:59+08:00"><meta property="article:modified_time" content="2023-08-17T19:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件"><meta itemprop=description content="人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。 目前炙手"><meta itemprop=datePublished content="2023-08-17T19:00:59+08:00"><meta itemprop=dateModified content="2023-08-17T19:00:59+08:00"><meta itemprop=wordCount content="4785"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件"><meta name=twitter:description content="人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。 目前炙手"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件</h1><div class=post-meta><time datetime=2023-08-17 class=post-time>2023-08-17</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 4785 字</span>
<span class=more-meta>预计阅读 10 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1什么是表示学习>1.什么是表示学习？</a></li><li><a href=#2什么是向量什么是嵌入>2.什么是向量？什么是嵌入？</a><ul><li><a href=#1向量>(1)向量</a></li><li><a href=#2嵌入>(2)嵌入</a></li></ul></li><li><a href=#3什么是词向量什么是词嵌入>3.什么是词向量？什么是词嵌入？</a></li><li><a href=#4如何实现词嵌入>4.如何实现词嵌入？</a><ul><li><a href=#1one-hot编码>(1)One-Hot编码</a></li><li><a href=#2distributed表示>(2)Distributed表示</a></li><li><a href=#3word2vecdistributed表示的一种实现>(3)Word2Vec，Distributed表示的一种实现</a></li></ul></li><li><a href=#5代码实例手撸word2vec>5.代码实例：手撸Word2Vec</a><ul><li><a href=#step1构建语料库>STEP1.构建语料库</a></li><li><a href=#step2生成skip-gram数据>STEP2.生成Skip-Gram数据</a></li><li><a href=#step3对skip-gram数据进行one-hot编码>STEP3.对Skip-Gram数据进行One-Hot编码</a></li><li><a href=#step4定义skip-gram模型>STEP4.定义Skip-Gram模型</a></li><li><a href=#step5训练skip-gram模型>STEP5.训练Skip-Gram模型</a></li><li><a href=#step6显示词向量>STEP6.显示词向量</a></li></ul></li><li><a href=#6再看表示学习嵌入>6.再看表示学习、嵌入</a></li></ul></nav></div></div><div class=post-content><p>人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。</p><p>目前炙手可热的Transformer既是如此，LSTM、Word2Vec等是它的基石，共同构筑了现在的大语言模型的关键部件和理论基础。</p><p>之前发了一个朋友圈，将对大语言模型影响深远的论文，梳理了三条脉络：</p><ul><li><strong>LSTM</strong>：在1997年那会儿解决了AI的记忆问题。</li><li><strong>Word2Vec</strong>：在2013~2014年解决了将词转换为向量，将"文字游戏"转换为了高维向量空间中的"数学游戏&rdquo;。</li><li><strong>Transformer</strong>：2014年出现了注意力机制雏形，2017年那篇著名的《Attention is All you Need》引出的Transformer，随后是OpenAI2018年的GPT-1、Google2019的BERT。</li></ul><p>本文重点阐述第2条技术线：<strong>Word2Vec</strong>涉及的技术——词的向量化，这也是大语言模型的关键部件之一。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230817105511.jpg alt=微信图片_20230817105511></p><h1 id=1什么是表示学习>1.什么是表示学习？</h1><p>我们先看一下表示学习的定义：</p><ul><li><strong>表示学习</strong>：Representation Learning，通过学习算法<strong>自动</strong>地从原始数据中<strong>学习到一种数据表达形式</strong>。<ul><li>表示学习的目标是将输入的数据转换到具有良好表现能力的特征空间中。</li><li>特征空间中的数据有<strong>可分性</strong>、<strong>可解释性</strong>、<strong>可推理性</strong>。</li></ul></li></ul><p>我们来解读一下上述这段话：</p><ul><li><strong>数据的可表示性，是连接主义的哲学基础</strong>：<ul><li>我们知道，人类彼此的沟通介质是：文字、图像、视频、语音等。这些介质的本质是数据。</li><li>我们还知道，从数据中寻找规律，是连接主义的哲学基础。</li><li>我们还知道，可以被人类大脑理解的文字、图像、视频等数据，人工智能能理解吗？不能。</li><li>人工智能无法理解这些数据，人工智能就无法从数据中寻找规律——因此，<strong>数据的可表示性，是连接主义的哲学基础</strong>。</li></ul></li><li><strong>向量化是数据可表示的一种实现</strong>：<ul><li>向量化，可以让人工智能理解人类才能理解的数据。</li><li>向量化，只是数据可表示的<strong>一种</strong>实现，当然可以有其它的实现方式。<ul><li>如：笔者前一篇《【chatGPT】学习笔记6-手撸一个上古GPT》中实现的N-Gram算法，并未对人类语言向量化，也让人工智能具备理解人类语言以及语言概率的能力。</li></ul></li><li>向量化，依然是目前数据可表示的若干种实现中最好的一种。<ul><li>基于N-Gram的人工智能，能力很弱。它的智能停留在人类语言的<strong>表面</strong>，它并不理解<strong>语义</strong>。</li><li>比如：&ldquo;我去！&ldquo;这个句子，基于N-Gram的人工智能无法识别，这句话到底想表达"我艹&rdquo;，还是想表达"我要去某个地方&rdquo;。</li></ul></li></ul></li></ul><p>上述对表示学习的解读还是挺抽象。没关系，后文有形象的例子，各位小伙伴可以先看完后文，再回头看这一段，加深对表示学习的理论理解。</p><h1 id=2什么是向量什么是嵌入>2.什么是向量？什么是嵌入？</h1><h2 id=1向量>(1)向量</h2><p>在NLP领域，无论语言模型的大小，都必须将词先表达为向量，词向量就是语言模型的输入。</p><p>在CV领域，无论视觉模型的大小，也必须将图像先表达为向量。</p><p>将文字、图像、视频、音频等数据向量化，本质是<font color=red>**将"人类可理解的数据问题"转换为"机器可理解的机器学习问题&rdquo;**</font>。</p><p>我们来看一个向量化的例子：</p><ul><li>人类小孩儿第一次看到苹果，人类小孩儿是怎么记住这种东西就是苹果呢？</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143249956.png alt=image-20230817143249956></p><ul><li>人类小孩儿会从不同维度描述苹果的特征(如：纹理、颜色、形状、大小等)</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143358437.png alt=image-20230817143358437></p><ul><li>假设人类小孩大脑的工作机制，是将不同维度的特征用数字表达并存储，这些维度的特征值就是<strong>向量</strong>了。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143456600.png alt=image-20230817143456600></p><ul><li>当人类小孩看到新的苹果，大脑也会将新苹果的特征提取为向量。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143619055.png alt=image-20230817143619055></p><ul><li>大苹果、小苹果，从颜色、大小、形状、纹理等等维度是很相似的，显然，把这些苹果抽象成多个向量，这些向量在向量空间中的距离肯定是很近的。</li><li>这就是信息向量化，词的向量化就是信息向量化的一种。</li></ul><p>上述例子，来自于一段腾讯云介绍向量数据库的视频，各位小伙伴可以通过视频，看到更多形象化理解向量的例子：</p><iframe src="//player.bilibili.com/player.html?aid=489914023&bvid=BV13N41167Q9&cid=1237875667&page=1" scrolling=no border=0 frameborder=no framespacing=0 allowfullscreen></iframe><h2 id=2嵌入>(2)嵌入</h2><p>先看一下嵌入的学术概念：</p><ul><li>嵌入：Embedding，表示学习的一种形式，用于<strong>将高维数据映射到低维空间</strong>。嵌入包括：<ul><li>词嵌入</li><li>图像嵌入</li><li>……</li></ul></li></ul><p>这里以图像嵌入的一种经典算法t-SNE(t-Distributed Stochastic Neighbor Embedding)为例：</p><ul><li>图像嵌入的第一阶段是将图像转换为高维向量。</li><li>图像嵌入的第二阶段是将高维向量映射到低维向量，但是要保证高维向量中邻近的点，在低维空间中也有相同的距离关系。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/t-sne_optimise.gif alt=t-sne_optimise></p><p><strong>嵌入</strong>本身是一个很抽象的术语，但我们可以简单地认为<strong>嵌入就是向量化</strong>的过程。</p><p>对于嵌入，最关键的信息只有一个：<strong>高维</strong>映射到<strong>低维</strong>。</p><p>什么是高维向量？什么是低维向量？为什么要从高维映射到低维？</p><p>这些问题的有趣之处，和人类的学习过程非常相似：</p><ul><li><p><strong>有一日未死之身，则有一日未闻之道</strong>：</p><ul><li>人类开始学习一门新知识，越学越觉得这个知识领域博大精深。这是人类学习的第一阶段——"<strong>把书读厚</strong>"。</li><li>在嵌入过程中，也就是把信息向量化过程中，显然是把一个信息用更高维度的向量去描述，信息越准确。</li><li>比如：小时候练习看图说话，老师会鼓励小朋友把图上看到的东西从更多角度去描绘出来，这些角度就是向量的维度。</li><li>再比如：一个哲学思想，不同流派的哲学家会从不同角度、不同立场去阐述、论证和思辨，这些角度、立场也等效于向量的维度。</li></ul></li><li><p><strong>读书之道，愈进愈简，百卷如一页</strong>：</p><ul><li>人类学习新知识到了一个阶段，会出现<strong>顿悟</strong>——发现、归纳各个知识点的内在联系。这是人类学习的第二阶段——"<strong>把书读薄</strong>"。</li><li>比如：杨过的重剑无锋，大巧不工。比如：老子的大道至简，有言无言。</li><li>在嵌入过程中，也有类似的过程，虽然单个高维向量有丰富的表达，但是多个高维向量存在某些内在联系，把它们降低为低维向量，不仅抓住了信息的本质，而且更加精炼、大道至简。</li></ul></li></ul><p>这就是<strong>嵌入</strong>的<strong>核心思想</strong>——将<strong>高维向量降低成低维向量</strong>。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817221709020.png alt=image-20230817221709020></p><h1 id=3什么是词向量什么是词嵌入>3.什么是词向量？什么是词嵌入？</h1><p>先来看词向量、词嵌入的学术概念：</p><ul><li><strong>词向量的定义</strong>：将词语转换成对应数值向量的表达形式，便于计算机读取和运算。</li><li><strong>词向量的数学表达</strong>：将字典D中的任意词w，设定固定长度的实值向量V(w)。其中：V(w)就是词向量，m表示词向量长度。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817142112513.png alt=image-20230817142112513></p><ul><li><strong>词嵌入</strong>：Word Embedding，将词映射到向量空间，形成词向量的过程和方法，就是词嵌入。</li></ul><p>再来看一个经典的例子：</p><ul><li>对<strong>男人、女人、国王、皇后</strong>开展词嵌入，它们变成了四个向量，存在与三维的向量空间中。</li><li>假设词嵌入的具体代码实现没有问题，那么男人和国王的向量距离应该很近，女人和皇后的向量距离应该很近。</li><li>这样，对人类的文字开展词嵌入后，有两个关键的输出：<ul><li><strong>词以向量的形式存在</strong>。</li><li><strong>有关联的词对应的向量的距离也会很小</strong>(如：对两个词向量求余弦相似度)。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817222631960.png alt=image-20230817222631960></p><h1 id=4如何实现词嵌入>4.如何实现词嵌入？</h1><p>词嵌入，或者说词的向量化过程分为两个阶段：</p><ul><li>文本变为One-Hot编码。</li><li>One-Hot编码变为低维词向量。</li></ul><h2 id=1one-hot编码>(1)One-Hot编码</h2><p><strong>One-Hot编码</strong>：One-Hot Representation，也叫独热编码。</p><p>比如：对于男、女两个词，One-Hot编码可以表示为：</p><ul><li>男：[1, 0]，女：[0, 1]</li></ul><p>再比如：对于初一、初二、初三三个词，可以表示为：</p><ul><li>初一：[1, 0, 0]，初二：[0, 1, 0]，初三：[0, 0, 1]</li></ul><p>如果说One-Hot编码算得上广义的向量，那么它明显的缺点是：</p><ul><li><strong>过于稀疏</strong>：在各个维度上，只有1个维度是1，其它维度都是0。</li></ul><h2 id=2distributed表示>(2)Distributed表示</h2><p><strong>Distributed表示</strong>：Distributed Representation，分布式表示，它是表示学习中的一种。</p><ul><li>Word2Vec、GloVe、fastText等都是其中一种具体实现。</li></ul><p>分布式表示的核心思想是：</p><ul><li>通过训练，将词典里每个单词转换为固定长度的低维向量。</li><li>这些向量之间可以通过余弦相似度之类的数学工具，表示向量之间的距离。语义越接近的向量之间，距离越近。</li></ul><p>Distributed表示才算得上真正的向量，它明显的优点是：</p><ul><li>从<strong>过于稀疏</strong>的One-Hot编码，变为了<strong>密集向量</strong>。</li></ul><h2 id=3word2vecdistributed表示的一种实现>(3)Word2Vec，Distributed表示的一种实现</h2><p>Word2Vec是Distributed表示的一种具体实现：</p><ul><li>Word2Vec将词汇表中的每个词表示成固定长度的向量。</li><li>Word2Vec的核心思想本质是——<strong>近朱者赤，近墨者黑</strong>：<ul><li>一个词可以根据它的的周边词，推测出自己的语义。</li><li>一个词也可以通过自己的语义，推测出它的周边词。</li></ul></li></ul><p>Word2Vec的实现算法有两种：</p><ul><li><strong>CBOW</strong>：Continuous Bag of Words，连续词袋，输入是周围词，输出是中心词。</li><li><strong>Skip-Gram</strong>：跳字模型，输入是中心词，输出是周围词。</li></ul><p>Word2Vec基于神经网络实现：</p><ul><li>但，无论是CBOW，还是Skip-Gram，都属于浅层神经网络。</li><li>一说浅，显然就不那么高级。浅层神经网络的效果可以参考Word2Vec的原始论文：https://arxiv.org/pdf/1301.3781.pdf</li></ul><h1 id=5代码实例手撸word2vec>5.代码实例：手撸Word2Vec</h1><h2 id=step1构建语料库>STEP1.构建语料库</h2><ul><li>首先，以四个句子作为训练语料</li><li>然后，形成<strong>词+索引的Map</strong>和<strong>索引+词的Map</strong></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233124145.png alt=image-20230817233124145></p><h2 id=step2生成skip-gram数据>STEP2.生成Skip-Gram数据</h2><ul><li>根据句子中的各个词，将相邻的词形成词对。</li><li>如：(小美，是)、(是，小美)，(美女，小美)，(小美，美女)等</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233354716.png alt=image-20230817233354716></p><h2 id=step3对skip-gram数据进行one-hot编码>STEP3.对Skip-Gram数据进行One-Hot编码</h2><ul><li>将STEP2的各个词对进行One-Hot编码</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233557328.png alt=image-20230817233557328></p><h2 id=step4定义skip-gram模型>STEP4.定义Skip-Gram模型</h2><ul><li>我们复现出论文描述的神经网络结构<ul><li>一个线性层学习出词向量</li><li>一个输出层验证学习到词向量作为中心词，是否能够预测出周围词。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233757848.png alt=image-20230817233757848></p><p>看一下具体的代码实现：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234116518.png alt=image-20230817234116518></p><h2 id=step5训练skip-gram模型>STEP5.训练Skip-Gram模型</h2><ul><li>训练3000次，每一次都会从输出层触发反向传播，更新线性层的参数，最终确定本次学习到的词向量。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234302231.png alt=image-20230817234302231></p><ul><li>这是损失函数曲线，经过3000次的训练，损失逐渐降低。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234532709.png alt=image-20230817234532709></p><h2 id=step6显示词向量>STEP6.显示词向量</h2><ul><li>将词向量打印出来，并绘制在向量空间中</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234606438.png alt=image-20230817234606438></p><ul><li>绘制出来的向量空间，结果如下：<ul><li>小美和大漂亮的词向量分别为： [-0.45549557 0.4471412 ]，[-0.47936678 0.40784693]</li><li>小帅和大壮的词向量分别为：[0.0154954 1.6643344]，[0.01839364 1.6814741 ]</li><li>小美和大漂亮两个词关系密切，甚至重合，说明这两个向量有关联性。</li><li>小帅和大壮两个词也关系密切，甚至重合，说明这两个向量有关联性。</li><li>但，美女、帅哥两个词向量学习的不好。小美、大漂亮更接近于帅哥，出现了<strong>不一定斩男但是一定斩女</strong>的现象。。。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234631498.png alt=image-20230817234631498></p><h1 id=6再看表示学习嵌入>6.再看表示学习、嵌入</h1><p>笔者在第5章以Word2Vec的Skip-Gram的代码实现，给大家阐述了词嵌入的实现过程和训练过程。</p><p>回头再来看1~4章出现的各种概念和理论，可以归纳词嵌入的几个关键知识点：</p><ul><li><strong>自然语言问题转换为数学问题</strong>：将文字向量化的本质是将信息转换为向量，所以文字的语义关系巧妙地转换为了向量距离的数学问题。</li><li><strong>自动化的学习过程</strong>：获得文字的向量是一个自动化的过程，通过神经网络这种工具自动学习得到词向量。</li><li><strong>词向量的降维技术</strong>：对于词嵌入，学界有很多种降维技术，Word2Vec的CBOW和Skip-Gram都是实现了降维的具体算法。</li></ul><p>再看第1章的表示学习，我们可以重新理解一下表示学习的三个特性：</p><ul><li><strong>可分性</strong>：不相关的向量距离会很远，不相关的向量之间应该会有明显的分界线。这就体现了可分性——不同类别之间的向量样本应该有明显的边界或区别。</li><li><strong>可解释性</strong>：为什么提到唐伯虎，人类会立刻联想到秋香？向量空间中相关的向量距离就会很近，这就体现了可解释性。</li><li><strong>可推理性</strong>：给出小美，我们可以联想到大漂亮、帅哥，进而可能就会联想到小帅、大壮，说不定小美和小帅之间就一段故事，这就体现了可推理性。</li></ul><p>写到这里，我们可以看到：</p><ul><li>以Word2Vec为代表的词向量化技术极大地推进了语言模型的进步</li><li>词嵌入也成为了如今大语言模型GPT、LLama、Bert的核心组件之一。</li></ul><p>在了解了词嵌入相关技术之后，我们下一步就来看大语言模型的另一个核心组件的起源：神经概率语言模型。</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-08-17</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/><span class="next-text nav-default">【chatGPT】学习笔记6-手撸一个上古GPT</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2023
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>