<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记6-手撸一个上古GPT - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？ 今天我们来手撸一个上古GPT，理解一下其"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记6-手撸一个上古GPT"><meta property="og:description" content="大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？ 今天我们来手撸一个上古GPT，理解一下其"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/"><meta property="article:published_time" content="2023-07-26T19:00:59+08:00"><meta property="article:modified_time" content="2023-07-26T19:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记6-手撸一个上古GPT"><meta itemprop=description content="大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？ 今天我们来手撸一个上古GPT，理解一下其"><meta itemprop=datePublished content="2023-07-26T19:00:59+08:00"><meta itemprop=dateModified content="2023-07-26T19:00:59+08:00"><meta itemprop=wordCount content="2164"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记6-手撸一个上古GPT"><meta name=twitter:description content="大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？ 今天我们来手撸一个上古GPT，理解一下其"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记6-手撸一个上古GPT</h1><div class=post-meta><time datetime=2023-07-26 class=post-time>2023-07-26</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2164 字</span>
<span class=more-meta>预计阅读 5 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1语言模型简介>1.语言模型简介</a><ul><li><a href=#11什么是语言模型>1.1.什么是语言模型</a></li><li><a href=#12语言模型分类>1.2.语言模型分类</a><ul><li><a href=#121基于规则的语言模型>1.2.1.基于规则的语言模型</a></li><li><a href=#122基于概率的语言模型>1.2.2.基于概率的语言模型</a></li><li><a href=#123基于深度学习的语言模型>1.2.3.基于深度学习的语言模型</a></li></ul></li><li><a href=#13语言模型的关键里程碑>1.3.语言模型的关键里程碑</a></li></ul></li><li><a href=#2上古语言模型n-gram>2.上古语言模型N-Gram</a><ul><li><a href=#21五步实现n-gram>2.1.五步实现N-Gram</a></li><li><a href=#22制作数据集>2.2.制作数据集</a></li><li><a href=#23分词>2.3.分词</a></li><li><a href=#24划分n-gram计算词频>2.4.划分N-Gram，计算词频</a></li><li><a href=#25进一步计算概率>2.5.进一步计算概率</a></li><li><a href=#26使用n-gram模型生成文本>2.6.使用N-Gram模型，生成文本</a></li></ul></li><li><a href=#3总结>3.总结</a></li></ul></nav></div></div><div class=post-content><p>大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？</p><p>今天我们来手撸一个上古GPT，理解一下其中的基本原理。</p><h1 id=1语言模型简介>1.语言模型简介</h1><h2 id=11什么是语言模型>1.1.什么是语言模型</h2><p><strong>语言模型是一个用来<font color=red>估计文本概率分布的数学模型</font></strong>。通俗的说，你给它一个词，它能告诉你这个词之后通常大概率会接什么词。</p><p>写这篇文章的时候，笔者拉了个人类做测试——我说一个词，他凭直觉补全他这句话：</p><ul><li>我说：孙悟。他答：孙悟空</li><li>我说：哪吒。他答：哪吒三太子</li><li>我说：白龙马。他答：白龙马蹄朝西</li><li>我说：猪八戒。他答：猪八戒背媳妇</li></ul><p>你看，这就是人脑中隐含了一个语言模型，直觉并不是直觉，而是一种文本概率分布。</p><p><strong>GPT家族</strong>(如：chatGPT、GPT-4)、<strong>BERT家族</strong>(如：MT-DNN、ERNIE)，都是语言模型中的一种。</p><p>除了GPT、BERT这些新一代的语言模型，古早的语言模型还有<strong>N-Gram</strong>、<strong>RNN</strong>、<strong>LSTM</strong>、<strong>GRU</strong>等。</p><p>但是，无论是新同志，还是老同志，它们作为语言模型的初心不会变——<strong>估计文本概率分布</strong>。</p><h2 id=12语言模型分类>1.2.语言模型分类</h2><h3 id=121基于规则的语言模型>1.2.1.基于规则的语言模型</h3><p>1970年，出现基于规则的语言模型。它通过语法树，将人类语言语法的规则描述出来。比如下面描述的最简单的语法树：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803090559437.png alt=image-20230803090559437></p><p>很显然，人类语言的规则复杂且会变化，这种<font color=red><strong>基于规则的语言模型过于死板</strong></font>，不太可能有很好的效果。</p><h3 id=122基于概率的语言模型>1.2.2.基于概率的语言模型</h3><p>1990年，出现基于数据驱动的统计概率语言模型。</p><p>这里不得不敬仰一下语音识别和自然语言处理专家——贾里尼克(Frederek Jelinek)。这位大神学术上极其严谨和务实，在IBM期间极度厌恶夸夸其谈的语言学家，曾经抛出了那句：&ldquo;我每开除一名语言学家,我的语音识别系统错误率就降低一个百分点"的名言。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803091349097.png alt=image-20230803091349097></p><p>著名的贾里尼克假设(<font color=red><strong>一个句子是否合理，就看看它的可能性大小如何，至于可能性就用概率来衡量</strong></font>)奠定了基于概率的语言模型。</p><p>举个例子：小美同学听到一句英文<code>The apple and 「pear」 salad is delicious.</code>，由于<code>pear</code>和<code>pair</code>发音很类似，那么到底是在这个句子中是<code>pear</code>还是<code>pair</code>呢？按照贾里尼克假设，<code>pear</code>显然可能性更多，或者说概率更大。</p><p>我们进一步将这里再展开一点：假设Y是一个有意义的句子，由一连串特定顺序排列的词X1、X2、X3&mldr;Xn组成。如何求Y在自然语言中出现的可能性呢？</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803093047437.png alt=image-20230803093047437></p><p>首先，根据条件概率进行转换：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803095827392.png alt=image-20230803095827392></p><p>然后，根据马尔科夫假设进一步简化：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png alt=image-20230803103655078></p><p>最终，就可以得到一个简化的语言模型<code>Bi-Gram</code>：<font color=red><strong>每个词称为1个<code>Gram</code>，当前词由前一个词决定</strong></font>。</p><p>这就是笔者在《【ChatGPT】ChatGPT学习笔记2-不是什么?是什么?有何方向?》中提到的GPT基础原理：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230719094340618.png alt=image-20230719094340618></p><h3 id=123基于深度学习的语言模型>1.2.3.基于深度学习的语言模型</h3><p>基于深度学习的语言模型的侧重点是学习工具，即强调使用神经网络进行学习。而基于概率的语言模型的侧重点是数据，即强调从数据中寻找规律。</p><p>因此，基于深度学习的语言模型没有突破<strong>基于概率</strong>的核心思想。</p><h2 id=13语言模型的关键里程碑>1.3.语言模型的关键里程碑</h2><p>从模型本身的发展看：</p><ul><li><strong>N-Gram</strong>：基于前N−1个词，预测序列中的下一个词</li><li><strong>Neural Probabilistic Language Model</strong>：神经概率语言模型。</li><li><strong>Pre-trained Language Model</strong>：通过更大的语料库、更深的神经网络进行预训练和微调。</li></ul><p>从词向量表示的发展看：</p><ul><li><strong>Bag-of-words</strong>：BOW，词袋模型。首次提出把词用向量表示，把文本表示为单词的集合，仅看词频(不考虑单词的顺序)。</li><li><strong>Distributed Representation</strong>：以分布式激活的形式表示词，也是词用向量表示。</li><li><strong>Word2vec</strong>：先把单词的含义学习好、用向量表示好，我们再继续用它进行后续训练。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803113828737.png alt=image-20230803113828737></p><h1 id=2上古语言模型n-gram>2.上古语言模型N-Gram</h1><h2 id=21五步实现n-gram>2.1.五步实现N-Gram</h2><ul><li>制作数据集</li><li>分词</li><li>计算每个N-Gram的词频</li><li>计算每个N-Gram的概率</li><li>根据输入词，生成连续文本</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180415107.png alt=image-20230803180415107></p><h2 id=22制作数据集>2.2.制作数据集</h2><ul><li>我们录入了三首唐诗作为数据集。实战中会录入更多的数据。</li><li>看一下代码：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803172748930.png alt=image-20230803172748930></p><ul><li>看一下结果：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174612589.png alt=image-20230803174612589></p><h2 id=23分词>2.3.分词</h2><ul><li><p>N-Gram模型会将数据集中每一条语句，拆分成N个词。每个词就是一个Gram。</p></li><li><p>分词本身不是N-Gram的重点，因此本文没有使用JieBa等分词三方件，直接按照单字来拆分。</p></li><li><p>分词时有个细节是子词，为了解决错别字等问题。</p></li><li><p>看一下代码：</p></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803173712704.png alt=image-20230803173712704></p><ul><li>看一下结果：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174634047.png alt=image-20230803174634047></p><h2 id=24划分n-gram计算词频>2.4.划分N-Gram，计算词频</h2><ul><li>我们会根据N的具体数值，去计算N个字之后紧跟的字出现的词频。</li><li>看一下代码：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174346725.png alt=image-20230803174346725></p><ul><li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174709174.png alt=image-20230803174709174></p><ul><li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174751374.png alt=image-20230803174751374></p><h2 id=25进一步计算概率>2.5.进一步计算概率</h2><ul><li>我们会进一步计算N个字之后紧跟的字出现的概率。</li><li>看一下代码：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175136237.png alt=image-20230803175136237></p><ul><li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175520160.png alt=image-20230803175520160></p><ul><li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175541199.png alt=image-20230803175541199></p><ul><li>走到这里，我们会有一个重要发现：<font color=red><strong>当N越大，后面紧跟的字的概率越趋于确定</strong>！</font></li></ul><h2 id=26使用n-gram模型生成文本>2.6.使用N-Gram模型，生成文本</h2><ul><li>当我们有了N-Gram模型，就可以根据输入词，预测生成完整的文本了。</li><li>看一下代码：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180013173.png alt=image-20230803180013173></p><ul><li>看一下结果：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180226046.png alt=image-20230803180226046></p><h1 id=3总结>3.总结</h1><ul><li>语言模型本质是一个文本概率统计模型</li><li>语言模型分为：基于规则的语言模型、基于概率的语言模型、基于深度学习的语言模型</li><li>N-Gram是最古老的语言模型，通过分词，统计出每N-1个词后紧跟的词的概率，进而形成了文本概率统计模型。</li><li>N-Gram的理论基础是：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png alt=image-20230803103655078></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-07-26</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/><span class="next-text nav-default">【chatGPT】学习笔记5-四次发展&三个世界</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2023
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>