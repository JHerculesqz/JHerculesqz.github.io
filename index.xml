<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 11 May 2024 10:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记47-LLM微调技术之P-Tuning V2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</link><pubDate>Sat, 11 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</guid><description>&lt;p>上篇专栏我们讲到，&lt;code>P-Tuning V1&lt;/code>通过在预训练模型的输入层加入可训练的连续提示，有效提升了训练效果。但其在复杂NLU任务和小参数模型上表现并不理想。&lt;/p>
&lt;p>&lt;code>P-Tuning V2&lt;/code>是对&lt;code>P-Tuning V1&lt;/code>的改进，使其能在不同规模的模型和各种NLU任务中都能与全量微调相媲美。&lt;/p>
&lt;p>本文解读论文**《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》**，探究&lt;code>Prompt Tuning V2&lt;/code>技术的原理。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112022603.png" alt="image-20240511112022603">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>的最大问题是不具备普适性(&lt;strong>a lack of universality&lt;/strong>)。
&lt;ul>
&lt;li>&lt;strong>不同规模模型的微调效果不稳定&lt;/strong>：Lack of universality across scales。模型规模超过10B时，&lt;strong>P-Tuning v1&lt;/strong>和&lt;strong>Fine Tuning&lt;/strong>水平相当。模型规模在0.1B到1B时，&lt;strong>P-Tuning v1&lt;/strong>的效果远不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>不同下游任务的微调效果不稳定&lt;/strong>：Lack of universality across tasks。实验证明，针对某些下游任务进行&lt;strong>P-Tuning v1&lt;/strong>，效果远差于&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>解决方案&lt;/strong>：论文提出&lt;strong>P-Tuning v2&lt;/strong>技术，采用&lt;strong>Deep Prompt Tuning&lt;/strong>方法，同时针对NLU任务做了一定适配和优化。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：实验证明&lt;strong>P-Tuning v2&lt;/strong>，在不同规模的模型上、在不同下游任务上都可获得较高的稳定性。是一种对&lt;strong>P-Tuning v1&lt;/strong>更好的替代方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511112132956.png" alt="image-20240511112132956">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>P-Tuning v1&lt;/strong>在不同规模的模型下、不同下游任务中，微调效果不稳定。&lt;/p>
&lt;ul>
&lt;li>如：当模型大小不大，特别是少于10B参数时，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114454639.png" alt="image-20240511114454639">&lt;/li>
&lt;li>如：抽取式问答(extractive question answering)，&lt;strong>P-Tuning v1&lt;/strong>的表现不如&lt;strong>Fine Tuning&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511114429201.png" alt="image-20240511114429201">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>P-Tuning v2采用&lt;strong>Deep P-Tuning&lt;/strong>的优化方式，用于探索垂域知识。&lt;/li>
&lt;li>Deep表现于在预训练模型的每一层注意力层增加了一个小模型，作用于每一层的输入。而P-Tuning v1仅在第一层增加了一个LSTM小模型。&lt;/li>
&lt;li>这种方法的本质是：不同规模的模型对于&lt;strong>P-Tuning v1&lt;/strong>在第一层增加的前缀向量的特征提取能力不同。越大的模型特征提取越强，后续各层都能感知注意到这个前缀向量的特征。反之，小模型特征提取能力弱，后续各层无法感知注意到前缀向量的特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511115216844.png" alt="image-20240511115216844">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>对300M到10B参数的模型上实验，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;li>以抽取式问答和命名实体识别为代表的下游任务上，&lt;strong>P-Tuning v2&lt;/strong>具备很稳定的微调效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>&lt;strong>P-Tuning v1&lt;/strong>提出了将&lt;strong>数学意义上的离散提示词转换为连续可微的提示词&lt;/strong>，但存在的问题还有2个：&lt;/p>
&lt;ul>
&lt;li>不同规模的模型微调效果不稳定。&lt;/li>
&lt;li>不同下游任务的模型微调效果不稳定。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>离散到连续&lt;/strong>是&lt;strong>Soft Prompt&lt;/strong>技术分支的&lt;strong>重要思想、重要里程碑&lt;/strong>，但&lt;strong>P-Tuning v1&lt;/strong>已经做到连续可微了，还有什么改进空间呢？&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>为了寻找突破口，我们还是进行数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>V、M、e&lt;/strong>：V表示模型M的词汇表，e表示模型M的词嵌入层。&lt;/li>
&lt;li>&lt;strong>离散到连续的转换&lt;/strong>：假定离散提示词为序列**[h&lt;sub>0&lt;/sub>, &amp;hellip;, h&lt;sub>i&lt;/sub>]**，经过**P-Tuning v1**的**Prompt Encoder**模块转换为向量序列**[e(x), e(h&lt;sub>0&lt;/sub>), &amp;hellip;, e(h&lt;sub>i&lt;/sub>)]**。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511144119291.png" alt="image-20240511144119291">&lt;/li>
&lt;/ul>
&lt;p>通过问题建模，我们可以看到&lt;strong>数学意义上&lt;/strong>的&lt;strong>离散提示&lt;/strong>已经表示为&lt;strong>连续提示&lt;/strong>。那么不同规模的模型、不同下游任务的微调效果不稳定，很可能源于&lt;strong>P-Tuning v1&lt;/strong>在输入层添加的前缀向量没有起到有效作用，从逻辑上，我们可以有如下猜测：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型规模对前缀向量的影响&lt;/strong>：不同规模的模型对前缀向量的特征提取能力是不同的，小模型特征提取不足，导致后续预训练模型的各层无法感知注意到前缀向量。&lt;/li>
&lt;li>&lt;strong>下游任务类型对前缀向量的影响&lt;/strong>：从离散提示词看，不同下游任务的提示词是不同的。同理，不同下游任务的连续提示词应该也是不同的。&lt;/li>
&lt;/ul>
&lt;p>因此：&lt;/p>
&lt;ul>
&lt;li>从数学上，&lt;strong>P-Tuning v1&lt;/strong>的连续可微前缀向量没有太多改进空间。&lt;/li>
&lt;li>从模型结构上，
&lt;ul>
&lt;li>&lt;strong>可以在Transformer的各层添加前缀向量&lt;/strong>，以抵消小模型对前缀向量的特征提取不足的局限。&lt;/li>
&lt;li>&lt;strong>可以改变前缀向量的长度&lt;/strong>，以实现不同下游任务有不同的前缀向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>为了更好地阐述论文的改进思路，我们列出相关源码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>DebertaPrefixModelForQuestionAnswering&lt;/strong>类，是针对QA下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511150051142.png" alt="image-20240511150051142">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RobertaPrefixForSequenceClassification&lt;/strong>类，是针对序列分类下游任务的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>支持通过超参数，&lt;strong>设置不同下游任务的前缀向量长度&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151145088.png" alt="image-20240511151145088">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>覆写Deberta的注意力层，支持在Deberta各注意力层都增加了前缀向量：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151537379.png" alt="image-20240511151537379">&lt;/p>
&lt;p>最后，在train方法中，将上述对模型结构的改进串联起来：&lt;/p>
&lt;ul>
&lt;li>创建前缀向量编码器对象，根据本下游任务指定的提示长度，生成前缀向量。&lt;/li>
&lt;li>前向传播时，将前缀向量传入本下游任务对应的各注意力层，实现不同层都能提取到前缀向量特征。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511151852070.png" alt="image-20240511151852070">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1across-scales">(1)实验结果1：Across Scales&lt;/h2>
&lt;p>针对不同规模的模型，&lt;strong>P-Tuning v2&lt;/strong>的微调效果比较稳定。&lt;/p>
&lt;ul>
&lt;li>在四种&lt;strong>参数小于10B的模型&lt;/strong>上，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511152631012.png" alt="image-20240511152631012">&lt;/p>
&lt;h2 id="2实验结果2across-tasks">(2)实验结果2：Across Tasks&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>CoNLL03、OntoNotes5.0、CoNLL04、SQuAD1.1dev、SQuAD2.0dev、CoNLL12、CoNLL05 WSJ、CoNLL05 Brown&lt;/strong>八种下游任务中，&lt;strong>P-Tuning v1&lt;/strong>微调效果远低于&lt;strong>P-Tuning v2&lt;/strong>微调效果，&lt;strong>P-Tuning v2&lt;/strong>微调效果与&lt;strong>Fine Tuning&lt;/strong>相当。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153327159.png" alt="image-20240511153327159">&lt;/p>
&lt;h2 id="3其它重要发现实现prompt-encoder的神经网络结构的选择技巧">(3)其它重要发现：实现Prompt Encoder的神经网络结构的选择技巧&lt;/h2>
&lt;ul>
&lt;li>在&lt;strong>P-Tuning v1&lt;/strong>中，采用&lt;strong>LSTM+MLP&lt;/strong>或&lt;strong>MLP&lt;/strong>，其中MLP采用2层线性层、ReLU作为激活函数。&lt;/li>
&lt;li>在&lt;strong>P-Tuning v2&lt;/strong>中，通过超参数针对不同下游任务选择不同神经网络，其中MLP的一种实现可以采用2层线性层、tanh作为激活函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV2/image-20240511153852283.png" alt="image-20240511153852283">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning v1&lt;/strong>的局限性：不同的下游任务、不同规模的模型，微调结果不稳定。&lt;/li>
&lt;li>&lt;strong>P-Tuning v2的核心思想&lt;/strong>：修改模型结构，在各层注意力层增加前缀处理器网络以抵消小模型对前缀向量特征提取不足的局限，支持不同下游任务选择不同前缀提示长度、选择不同前缀编码器神经网络结构。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2110.07602&lt;/p></description></item><item><title>【chatGPT】学习笔记46-LLM微调技术之P-Tuning V1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</link><pubDate>Wed, 01 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</guid><description>&lt;p>前面给大家分享了&lt;code>Soft Prompt&lt;/code>技术分支下的&lt;code>Prefix-Tuning&lt;/code>和&lt;code>Prompt Tuning&lt;/code>，在这个技术分支下，还有一项需要重点了解的微调技术——&lt;code>P-Tuning&lt;/code>。&lt;/p>
&lt;p>&lt;code>P-Tuning&lt;/code>是清华大学和MIT于2021年联合发布的一项微调技术，在NLU(自然语言理解)任务上有重大突破。&lt;/p>
&lt;p>本文解读论文**《GPT Understands, Too》**，我们一起来学习一下&lt;code>P-Tuning&lt;/code>技术的原理。&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Discrete Prompts&lt;/strong>(离散提示词)会导致大模型性能不稳定。&lt;/p>
&lt;ul>
&lt;li>比如：修改提示词中的一个单词，都可能导致大模型的性能大幅下降。&lt;/li>
&lt;li>本质：根据自然语言形式的提示词进行预测，对于大模型本身&lt;strong>从数学上是不可微的&lt;/strong>(这就是数学意义上的&lt;strong>离散性&lt;/strong>)——不可微就意味着AI无法高效、稳定地&lt;strong>提特征&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>P-Tuning&lt;/code>技术，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。将离散提示词向量化为可训练的连续提示词(&lt;strong>trainable continuous prompt embeddings&lt;/strong>)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P-Tuning&lt;/strong>通过连续提示词向量，降低了不同离散提示之间的差距，进而提升了模型的稳定性。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>在LAMA、SuperGLUE等NLU任务上，显著提高了模型性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426094803830.png" alt="image-20240426094803830">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>问题&lt;/strong>：离散提示会导致大模型的稳定性问题。
&lt;ul>
&lt;li>&lt;strong>以手动离散提示为例&lt;/strong>：提示中改变一个单词可能会导致显著的性能下降，存在很大的不稳定性。&lt;/li>
&lt;li>&lt;strong>一些优化尝试&lt;/strong>：
&lt;ul>
&lt;li>调整语言模型本身，不稳定性问题有所缓解，但不同提示之间的性能差异仍然很大(特别是在少样本场景下)。&lt;/li>
&lt;li>&lt;strong>自动提示法(automatic prompting)&lt;/strong>：试图为给定任务搜索更好的提示，但这些方法并没有改变离散提示的不稳定本质。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426114420388.png" alt="image-20240426114420388">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>prompt encoder&lt;/strong>：论文提到通过&lt;strong>prompt encoder(提示词编码器)&lt;/strong>，将输入的离散提示Token，和连续提示Embedding连接起来后，输入给大语言模型。其中，&lt;strong>prompt encoder&lt;/strong>可采用LSTM或MLP来实现。&lt;/li>
&lt;li>&lt;strong>backpropagation to optimize&amp;hellip;&lt;/strong>：可以通过反向传播，优化连续提示词，进而将离散提示转变为可微的连续提示。&lt;/li>
&lt;li>&lt;strong>P-Tuning的本质&lt;/strong>：该技术的本质打破离散提示的限制——离散则不便于&lt;strong>提特征&lt;/strong>，连续可微则可学习——因此P-Tuning抵消了离散提示中微小变化对稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在LAMA基准测试中，使用P-Tuning，比手动离散提示(manual discrete prompts)提升了20多分，比搜索提示(searched prompts)提升了9分。&lt;/li>
&lt;li>在SuperGLUE基准测试中，在全监督和少样本下都优于PET的最佳离散提示(the best discrete prompts)。&lt;/li>
&lt;li>实验还证明，在更广泛的任务中，P-Tuning降低了不同离散提示之间的差异，进而提升了模型的稳定性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240426113823515.png" alt="image-20240426113823515">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1问题域">(1)问题域&lt;/h2>
&lt;p>提示词是大家耳熟能详的激发LLM能力的技术手段，但是&lt;strong>从数学上具有极大的局限性&lt;/strong>——就是它是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;p>论文作者举了这样的一个例子：&lt;/p>
&lt;ul>
&lt;li>表格第三行和表格第四行的两个提示词，只是少了一个单词&lt;strong>In&lt;/strong>，AI猜出来X和Y填什么的准确度就下降了20分。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501144315842.png" alt="image-20240501144315842">&lt;/p>
&lt;p>在本论文发表前，业界还有一些自动化搜索离散提示的优化尝试：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>mining the training corpus&lt;/strong>：挖掘训练语料库。&lt;/li>
&lt;li>&lt;strong>gradient-based searching&lt;/strong>：基于梯度的搜索。&lt;/li>
&lt;li>&lt;strong>using pretrained generative model&lt;/strong>：使用预训练的生成模型。&lt;/li>
&lt;/ul>
&lt;p>这些优化方法的本质就是&lt;strong>自动生成提示词&lt;/strong>，但是用自然语言表示的提示词依然还是&lt;strong>数学意义上的离散&lt;/strong>。&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>M、V、h&lt;/strong>：M表示预训练模型， 词表大小V，隐藏层大小h。&lt;/li>
&lt;li>&lt;strong>{(x&lt;sub>i&lt;/sub>, y&lt;sub>j&lt;/sub>)}&lt;sub>i&lt;/sub>&lt;/strong>：表示在NLU任务中的数据集。x&lt;sub>0:n&lt;/sub>={x&lt;sub>0&lt;/sub>, x&lt;sub>1&lt;/sub>, &amp;hellip;, x&lt;sub>n&lt;/sub>}是一系列离散Token组成的输入，y∈Y表示标签。&lt;/li>
&lt;li>&lt;strong>f&lt;sub>M&lt;/sub>(x)=p(y|x)&lt;/strong>：表示预训练模型M的任务，就是预测分类的条件概率。&lt;/li>
&lt;li>&lt;strong>[Di]&lt;/strong>：表示离散提示的Token，每一个离散提示都可以表示为T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;/ul>
&lt;p>通俗一点说，上面这一通数学建模，就是描述了一个填字游戏：&lt;/p>
&lt;ul>
&lt;li>比如：The capital of &lt;input checked="" disabled="" type="checkbox"> is [y]。&lt;/li>
&lt;li>如果x=Britain，则希望AI输出y=London。&lt;/li>
&lt;li>如果x=中国，则希望AI输出y=北京。&lt;/li>
&lt;/ul>
&lt;p>离散提示&lt;strong>T = {[D&lt;sub>0:i&lt;/sub>, x, [D&lt;sub>(i+1):j&lt;/sub>], y, [D&lt;sub>(j+1):k&lt;/sub>]}&lt;strong>会被Embedding为&lt;/strong>{e(D&lt;sub>0&lt;/sub>)&amp;hellip;e(D&lt;sub>i&lt;/sub>), e(x&lt;sub>0&lt;/sub>), &amp;hellip;, e(x&lt;sub>n&lt;/sub>), &amp;hellip;, e(D&lt;sub>k&lt;/sub>)}&lt;/strong>，其中e ∈
R&lt;sup>|V|×d&lt;/sup>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501151933103.png" alt="image-20240501151933103">&lt;/p>
&lt;p>&lt;strong>P-Tuning的实现怎么表达呢&lt;/strong>？如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[P&lt;sub>i&lt;/sub>]&lt;/strong>：表示第i个连续提示Embedding，注意论文的表述——&lt;strong>连续的提示词嵌入(continuous prompt embedding)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}&lt;/strong>：基于[P&lt;sub>i&lt;/sub>]的含义，那么任意一个提示词都能表达为T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}。&lt;/li>
&lt;li>&lt;strong>f: [P&lt;sub>i&lt;/sub>]-&amp;gt;h&lt;sub>i&lt;/sub>&lt;/strong>：一个词嵌入函数，用来将T = {[P&lt;sub>0:i&lt;/sub>, x, [P&lt;sub>(i+1):j&lt;/sub>], y, [P&lt;sub>(j+1):k&lt;/sub>]}转换为{h&lt;sub>0&lt;/sub>, e(x), h&lt;sub>i+1&lt;/sub>, &amp;hellip;, h&lt;sub>j&lt;/sub>, e(y), h&lt;sub>j+1&lt;/sub>, &amp;hellip;, h&lt;sub>k&lt;/sub>}。&lt;/li>
&lt;li>&lt;strong>{P&lt;sub>i&lt;/sub>}&lt;sup>k&lt;/sup>&lt;sub>i=1&lt;/sub>&lt;/strong>：表示P-Tuning的目标——反向传播，优化损失值，在预训练模型之前学习到提示词的特征。&lt;/li>
&lt;/ul>
&lt;p>不严谨地理解一下P-Tuning的玩法——就是加了个新的神经网络，不断地在学习如下提示词：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>吾饥矣&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹中空空&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;li>如果有人说：&lt;strong>吾腹鸣如鼓&lt;/strong>，你就要说：我给你下面吃啊。&lt;/li>
&lt;/ul>
&lt;p>它会发现&lt;strong>吾饥矣、吾腹中空空、吾腹鸣如鼓&lt;/strong>的特征，都是在说&lt;strong>肚子饿了&lt;/strong>，于是在调用大语言模型之前，它就把自然语言形态的离散提示词都转变为：&lt;/p>
&lt;ul>
&lt;li>如果有人说：&lt;strong>我饿了&lt;/strong>，你就要说：xxx。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153355161.png" alt="image-20240501153355161">&lt;/p>
&lt;p>最后，我们来完整地对比一下&lt;strong>自动化搜索离散提示&lt;/strong>法和&lt;strong>P-Tuning&lt;/strong>的差别：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自动搜索离散提示&lt;/strong>用的是&lt;strong>Prompt Generator&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>是用&lt;strong>Prompt Encoder&lt;/strong>找特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning&lt;/strong>将数学意义上的&lt;strong>离散提示词&lt;/strong>转换为了&lt;strong>连续可微提示词&lt;/strong>，帮助AI更好地提特征。&lt;/li>
&lt;li>其实两种思路本质都一样，都是很巧妙的想法。&lt;/li>
&lt;li>论文还提到Prompt Encoder的实现采用了&lt;strong>LSTM、MLPs、identity mapping function(恒等映射函数)&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501153925852.png" alt="image-20240501153925852">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="1实验结果1knowledge-probing">(1)实验结果1：Knowledge Probing&lt;/h2>
&lt;p>在知识探索(Knowledge Probing)型任务上，实验可以评估出AI获得现实世界知识量。&lt;/p>
&lt;p>LAMA数据集创建了三元组形式的完型填空，来实施知识探索评估。&lt;/p>
&lt;p>从实验结果上看，P-Tuning显著提高了知识探测的效果。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LAMA-34k数据集&lt;/strong>：从43.3%提高到50.6%&lt;/li>
&lt;li>&lt;strong>LAMA-29k数据集&lt;/strong>：从45.2%提高到64.2%&lt;/li>
&lt;li>&lt;strong>相较于离散提示搜索方法&lt;/strong>：P-Tuning优于离散提示搜索方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161239613.png" alt="image-20240501161239613">&lt;/p>
&lt;h2 id="2实验结果2fully-supervised-learning">(2)实验结果2：Fully-supervised Learning&lt;/h2>
&lt;p>实验采用了SuperGLUE基准测试，测试了7个自然语言理解任务(NLU)。包括：&lt;/p>
&lt;ul>
&lt;li>问答、MultiRC、文本蕴含、RTE、共指消解、因果推理、词义消歧。&lt;/li>
&lt;/ul>
&lt;p>实验使用了四个版本的预训练模型：&lt;/p>
&lt;ul>
&lt;li>GPT2-Base&lt;/li>
&lt;li>GPT2-medium&lt;/li>
&lt;li>BERT-Base&lt;/li>
&lt;li>BERT-Large&lt;/li>
&lt;/ul>
&lt;p>实验证明&lt;strong>P-Tuning可以提高 BERT和GPT上的全监督学习性能&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在 BERT-Base上，P-Tuning在5/7任务上实现了最佳性能。&lt;/li>
&lt;li>在 BERT-Large上，P-Tuning在4/7任务上超越了其他方法。&lt;/li>
&lt;li>在 GPT2-Base和 GPT2-Medium上，P-Tuning 在所有任务上始终是最佳性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501161948829.png" alt="image-20240501161948829">&lt;/p>
&lt;h2 id="3实验结果3few-shot-learning">(3)实验结果3：Few-Shot Learning&lt;/h2>
&lt;p>实验使用了少样本SuperGLUE基准测试，就是FewGLUE数据集。&lt;/p>
&lt;p>实验证明了P-Tuning有一定的提升：&lt;/p>
&lt;ul>
&lt;li>ALBERT上，比PET平均高出1个点、比Prompt Tuning高出13个点。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPTuningV1/image-20240501162327022.png" alt="image-20240501162327022">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>离散提示的问题&lt;/strong>：数学上离散，不便于AI提特征。&lt;/li>
&lt;li>&lt;strong>P-Tuning的核心思想&lt;/strong>：将离散提示词转换为连续可微提示词，微调的目标是用LSTM这类网络学习提示词特征。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2103.10385&lt;/p></description></item><item><title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</link><pubDate>Wed, 03 Apr 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</guid><description>&lt;p>紧接着Stanford的&lt;code>Prefix Tuning&lt;/code>论文，Google迅速发表了&lt;code>Prompt Tuning&lt;/code>技术论文。Google声称该技术比&lt;code>Prefix Tuning&lt;/code>更易上手且成本更低，因此该技术随后也成为了微调技术中的一个重要分支。&lt;/p>
&lt;p>本文解读论文**《The Power of Scale for Parameter-Efficient Prompt Tuning》**，与大家共同感受&lt;code>Prompt Tuning&lt;/code>技术的奇妙之处。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403122759085.png" alt="image-20240403122759085">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：&lt;strong>Prompt Tuning&lt;/strong>与&lt;strong>Prefix Tuning&lt;/strong>一样，都是以任务为中心的思路解决问题。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>以任务为中心&lt;/strong>：它们都在试图解决&lt;strong>FFT&lt;/strong>针对不同的下游任务都需产生一个新的微调后大模型而导致的成本效率问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prompt Tuning&lt;/code>，也是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。统一不同下游任务的训练数据格式，并将这些不同下游任务的训练数据汇总成一个乱序的数据集，微调预训练模型，最终获得一个能处理不同下游任务的大模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在小参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>略差于FFT性能。&lt;/li>
&lt;li>在中参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>快速接近于FFT性能。&lt;/li>
&lt;li>在大参数规模的T5上，&lt;strong>Prompt Tuning&lt;/strong>与FFT性能持平。&lt;/li>
&lt;li>因此，&lt;strong>Prompt Tuning&lt;/strong>在大参数规模的模型上，更具成本效率优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403092613314.png" alt="image-20240403092613314">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：论文中提到的&lt;strong>Prompt Design&lt;/strong>可以理解为大家耳熟能详的&lt;strong>提示词及提示词工程&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>经过工程实践，大家都知道提示词工程有一定的效果，但效果远不及FFT。为什么呢？论文中总结了提示词工程的两个短板：&lt;strong>the discrete space of words(离散空间的单词)&lt;strong>和&lt;/strong>requires human involvement(人类的投入)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>the discrete space of words(离散空间的单词)&lt;/strong>：提示词工程中的提示词是数学意义上的&lt;strong>离散&lt;/strong>，大语言模型不能很好地学习其隐含的特征。隐含特征不是提示词内容本身的显式特征，而是诸如提示词的句式、问法、潜台词等隐式特征。业界针对提取离散空间词汇也提出了一些算法(如：&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)，但效果也非常限。&lt;/li>
&lt;li>&lt;strong>requires human involvement(人类的投入)&lt;/strong>：提示词工程依赖有经验的提示词工程师，针对不同下游任务设计提示词，工作量巨大，并且对大模型推理能力的提升又极其有限。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：论文中提到的&lt;strong>Model Tuning&lt;/strong>和&lt;strong>Model Tuning(Multi Task)&lt;/strong>，可以理解为&lt;strong>FFT&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Model Tuing(Multi Task)&lt;/strong>：&lt;strong>Model Tuning(Multi Task)&lt;strong>是针对每个下游任务都微调出一个大模型，而&lt;/strong>Model Tuning&lt;/strong>是将N个下游任务都微调到一个大模型中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>统一下游任务的数据格式&lt;/strong>：论文中提到&lt;code>an additional k tunable tokens per downstream task to be prepended to the input text&lt;/code>，就是为了达成统一下游任务的数据格式。如：['[CLS]&amp;rsquo;, &amp;lsquo;中&amp;rsquo;, &amp;lsquo;国&amp;rsquo;, &amp;lsquo;的&amp;rsquo;, &amp;lsquo;春&amp;rsquo;, &amp;lsquo;节&amp;rsquo;, &amp;lsquo;是&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;[MASK]&amp;rsquo;, &amp;lsquo;。&amp;rsquo;, &amp;lsquo;[SEP]']。&lt;/li>
&lt;li>&lt;strong>合并下游任务的数据集合&lt;/strong>：当我们统一了下游任务的数据格式，就可以将这些下游任务数据集合混合在一起。&lt;/li>
&lt;li>&lt;strong>LLM具备学习数据集合隐式特征的能力&lt;/strong>：论文假设LLM是具备学习上述数据格式隐式特征的能力，并通过实验验证了这个假设。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的本质&lt;/strong>：该技术的本质是LLM的核心能力之一就是&lt;strong>提特征&lt;/strong>。如果特征很明显，LLM就可以低成本提取。如果特征很隐晦，LLM无法低成本提取、甚至无法提取，&lt;strong>Prompt Tuning&lt;/strong>就是改变数据集，将隐式特征转为显式特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>实验效果&lt;/strong>：
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;li>因此，在大规模模型上，&lt;strong>Promp Tuning&lt;/strong>具备巨大的成本优势。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101305842.png" alt="image-20240403101305842">&lt;/p>
&lt;h1 id="3design-decisions实验设计">3.Design Decisions(实验设计)&lt;/h1>
&lt;h2 id="1下游任务数据格式归一化的理论基础">(1)下游任务数据格式归一化的理论基础&lt;/h2>
&lt;p>论文的实验对象是T5，因为T5有一个有趣的观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Following the “text-to-text” approach of T5 (Raffelet al., 2020), we cast all tasks as text generation&lt;/strong>：所有的下游任务都可以等效于文本生成。这个观点就可以支撑&lt;strong>Prompt Tuning&lt;/strong>将所有下游任务的训练数据格式统一起来。&lt;/li>
&lt;li>如：翻译下游任务，可以将训练数据构造为：&amp;ldquo;translate English to German: hello world!&amp;rdquo;&lt;/li>
&lt;li>如：摘要下游任务，可以将训练数据构造为：&amp;ldquo;summarize: xxxxxxxxxxxxxxxxxxx&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403112458776.png" alt="image-20240403112458776">&lt;/p>
&lt;h2 id="2问题建模">(2)问题建模&lt;/h2>
&lt;p>论文对问题进行了数学建模：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>：在不同下游任务的训练数据可归一化的前提下，大语言模型可被建模为&lt;strong>Pr&lt;sub>θ&lt;/sub>(Y|X)&lt;/strong>，X是用户输入的Tokens，Y是在X发生的概率下模型的输出。&lt;/li>
&lt;li>&lt;strong>Prompt Design&lt;/strong>的短板：提示词工程需要人类不断尝试寻找出合适提示词，这种不断尝试方法可能是人工寻找的，也可能采用了非可微的搜索方法(如：前文提到的&lt;strong>a search algorithm over the discrete space of words&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>Pr&lt;sub>θ;θ&lt;sub>P&lt;/sub>&lt;/sub> (Y|[P; X])&lt;/strong>：这个公式表达了Prompt Tuning的核心思想——在训练数据中植入特殊Token，大模型除了学习训练数据中的显式特征外，还能学习Prompt形式训练数据的隐式特征。对于Prompt隐式特征的学习最终影响的不是预训练模型的参数θ，而是在修正θ&lt;sub>P&lt;/sub>。&lt;/li>
&lt;li>&lt;strong>[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>] ∈ R&lt;sup>(p+n)×e&lt;/sup>&lt;/strong>：Prompt中的普通标记被大模型嵌入后得到&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)，Prompt中的特殊标记被大模型嵌入后得到&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>(e是向量空间的维度)。[P&lt;sub>e&lt;/sub>; X&lt;sub>e&lt;/sub>]则表示输入给大模型后续神经网络层的高维向量。训练的影响并不会修正&lt;strong>X&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数，只会修正&lt;strong>P&lt;sub>e&lt;/sub>&lt;/strong>关联的模型参数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115838249.png" alt="image-20240403115838249">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115900991.png" alt="image-20240403115900991">&lt;/p>
&lt;p>因此，实验的关注点如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的初始值&lt;/strong>：和Prefix Tuning一样，都需要关注软提示的初始值，以提升训练速度和效果。&lt;/li>
&lt;li>&lt;strong>P&lt;sub>e&lt;/sub>的长度&lt;/strong>：和Prefix Tuning一样，也需要关注软提示的长度，以降低训练成本。
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的参数成本=E*P&lt;/strong>：E是普通标记的向量维数，P是特殊标记的长度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3如何消减特殊标记的影响">(3)如何消减特殊标记的影响&lt;/h2>
&lt;p>由于增加了特殊标记，大模型学习的内容就不再是原汁原味的&amp;quot;人类自然语言&amp;quot;了。这样就可能导致大模型无法用自然语言作答——这就好像你在训练大模型鸟语但又期待它能说人话、你在用中文教英语最后学会的是Chinglish。&lt;/p>
&lt;p>论文中提出了**Span Corruption(跨度损失)**的概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Span Corruption(跨度损失)&lt;/strong>：比如，训练数据&lt;code>Thank you [X] me to your party [Y] week&lt;/code>，&lt;input checked="" disabled="" type="checkbox"> 、[Y]就是特殊标记，将这种特殊标记植入自然语言的目的是&amp;quot;问题模式等隐式特征的显性化&amp;rdquo;，但弊端就是让大语言模型学会了非人类的自然语言。&lt;/li>
&lt;li>&lt;strong>论文提出了三种解决方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>Span Corruption法&lt;/strong>：啥都不做，任由大语言模型输出特殊标记，忽略这种影响。&lt;/li>
&lt;li>&lt;strong>Span Corruption+Sentinel法&lt;/strong>：在大语言模型中增加Sentinel，一定程度地降低这种影响。&lt;/li>
&lt;li>&lt;strong>LM Adaptation法&lt;/strong>：采用Raffel提出的一个小模型，纠正大语言模型输出特殊标记的倾向，最终输出纯粹的自然语言。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115956245.png" alt="image-20240403115956245">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文阐述了详细的实验过程、实验数据，最终的实验结果如前文所述：&lt;/p>
&lt;ul>
&lt;li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。&lt;/li>
&lt;li>在大规模T5模型上，Promp Tuing效果与FFT持平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120117025.png" alt="image-20240403120117025">&lt;/p>
&lt;ul>
&lt;li>当模型规模逐渐变大，&lt;strong>Promp Tuning&lt;/strong>涉及的参数相较于&lt;strong>Prefix Tuning&lt;/strong>更少，但微调效果持平，因此&lt;strong>Prompt Tuning具备巨大的成本优势&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120213717.png" alt="image-20240403120213717">&lt;/p>
&lt;h2 id="42重要发现">4.2.重要发现&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;p>论文从可解释性方面发现了&lt;strong>语义聚合现象&lt;/strong>，进一步证明了Prompt形式的数据更有利于大语言模型学习其隐式特征：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>语义聚合现象&lt;/strong>：观测被大语言模型嵌入后的特殊标记和普通标记，可以发现出现了物以类聚的现象：
&lt;ul>
&lt;li>如：Technology / technology / Technologies / technological / technologies相关的训练数据，向量相似度发生了语义聚合。&lt;/li>
&lt;li>语义聚合的出现，说明了大语言模型学习到了Prompt形式的训练数据中的隐式特征，因此可以举一反三地处理为见过的下游任务相关输入。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文还证明了&lt;strong>Prompt Ensembling(提示集成能力)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>基于Prompt Tuning的技术思想，可以做到数据格式统一、不同下游任务的训练数据混合训练，进而达到&amp;rdquo;&lt;strong>一个大模型支持多种不同下游任务&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>这种思想可以在超大规模的模型上极大地降低训练成本、使用成本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png" alt="image-20240403101321581">&lt;/p>
&lt;p>在工程实践方面，论文也给出了&lt;strong>Prompt长度、Prompt初始值&lt;/strong>的相关推荐：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prompt长度的影响&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在中小参数规模的模型上，Prompt长度越长，提示效果越好，但过犹不及(实验长度的临界值是150)——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;li>在大参数规模的模型上，Prompt长度反而没有什么影响了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt的初始值选择的影响&lt;/strong>：随机初始化Prompt的效果远差于用下游任务相关联提示词做初始值的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121146629.png" alt="image-20240403121146629">&lt;/p>
&lt;p>最后，论文还通过消融实验，补充了消减&lt;strong>Span Corruption&lt;/strong>的建议：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LM Adaptation&lt;/strong>：在中小规模模型上，采用LM Adaptation，对大语言模型的纠正效果更好。LM Adaptation增加步数会达到更好的纠正效果。&lt;/li>
&lt;li>在大规模模型上，Span Corruption的影响也可以忽略不计了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121223051.png" alt="image-20240403121223051">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prompt Tuning的核心思想&lt;/strong>：通过归一化不同下游任务的训练数据，并将隐式特征显性化，帮助大语言模型学习。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prompt形式的训练数据有助于LLM学习隐式特征。&lt;/li>
&lt;li>采用Prompt Tuning可用一套模型搞定多个下游任务。&lt;/li>
&lt;li>对于大规模参数的模型，Prompt长度和初始化影响很小。&lt;/li>
&lt;li>对于中小规模参数的模型，Prompt长度和初始值可参考Prefix Tuning的实践。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2104.08691.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</link><pubDate>Fri, 29 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</guid><description>&lt;p>&lt;code>Prefix Tuning&lt;/code>是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。&lt;/p>
&lt;p>本文通过解读论文**《Prefix-Tuning: Optimizing Continuous Prompts for Generation》**，与小伙伴们一起学习理解&lt;code>Prefix Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329153821295.png" alt="image-20240329153821295">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：**FFT(全参数微调)**针对不同的下游任务都需要产生一个新的微调后大模型，存在成本效率等诸多工程问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Prefix Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Soft Prompt(软提示)&lt;/code>&lt;/strong>进行迁移学习的方法。针对不同下游任务创建不同的&lt;strong>&lt;code>Prefix(前缀向量模块)&lt;/code>&lt;/strong>，这样不同下游任务只需要在一套预训练大模型上加载不同&lt;strong>Prefix小模型&lt;/strong>即可。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在GPT-2的&lt;strong>&lt;code>Table-To-Text(表格生成文本)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占GPT-2参数的0.1%，即可达到GPT-2同等水平。&lt;/li>
&lt;li>在BART的&lt;strong>&lt;code>Sumarization(摘要)&lt;/code>&lt;/strong>下游任务中，&lt;code>Prefix&lt;/code>模型参数仅占BART参数的0.1%，即可达到BART同等水平。&lt;/li>
&lt;li>在上述两种实验中，额外还观察到一定的泛化涌现能力，&lt;code>Prefix Tuning&lt;/code>可外推到训练期间未见过的任务主题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329111747247.png" alt="image-20240329111747247">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>背景技术1&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>是向预训练模型中增加新的小模型，仅微调小模型参数以达到高效微调的目的。实验效果证明仅需微调2%~4%的参数即可达到全参数微调的效果。&lt;/p>
&lt;ul>
&lt;li>Adapter Tuning的详细解读可参见本技术专栏这篇文章《【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>背景技术2&lt;/strong>：GPT-3的一个重要贡献就是&lt;strong>Context Learning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>，GPT-3是一套统一的大语言模型，用户不需要针对下游任务单独微调，直接通过提示词和上下文，影响GPT-3输出的答案。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：&lt;strong>Prefix Tuning&lt;/strong>借鉴了&lt;strong>Adapter Tuning&lt;/strong>和&lt;strong>Prompt Engineering&lt;/strong>的思想：&lt;/p>
&lt;ul>
&lt;li>Prefix Tuning也额外&lt;strong>增加了N个小模型&lt;/strong>，这些小模型外挂于同一套预训练模型上，&lt;strong>不同小模型解决不同的下游任务&lt;/strong>。&lt;/li>
&lt;li>Prefix Tuning增加的这些小模型的作用类似&lt;strong>Prompt(提示词)&lt;/strong>，它们会在用户输入的文本前额外增加针对不同下游任务的&lt;strong>提示词前缀Prefix&lt;/strong>。这些前缀不是自然语言，而是Transformer架构中向量形式的Token，这种Token叫做&lt;strong>虚拟Token&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GPT-2的Table-To-Text&lt;/strong>和&lt;strong>BART的Sumarization&lt;/strong>的测试效果：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPT-2的Table-To-Text&lt;/strong>：在完整数据集上训练时，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>在表格到文本的下游任务的性能相当。&lt;/li>
&lt;li>&lt;strong>BART的Sumarization&lt;/strong>：在摘要方面，&lt;strong>Prefix Tuning&lt;/strong>和&lt;strong>FFT&lt;/strong>性能略有下降。&lt;/li>
&lt;li>&lt;strong>Low Data Settting&lt;/strong>：在数据量少的数据集上，&lt;strong>Prefix Tuning&lt;/strong>能够克服数据集样本不足、提炼数据特征困难的问题，表现出了泛化涌现能力。在上述两项任务中性能表现优于FFT。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329113337615.png" alt="image-20240329113337615">&lt;/p>
&lt;h1 id="3prefix-tuning">3.Prefix Tuning&lt;/h1>
&lt;h2 id="31problem-statement问题陈述">3.1.Problem Statement(问题陈述)&lt;/h2>
&lt;p>Prefix Tuning采用了严谨的数学描述来阐述待解决的问题，这也是算法工程中的算法建模环节，我们尽量通俗地理解这个问题模型，也感受一下算法工程师的思维模式。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>x和y&lt;/strong>：x是大语言模型的输入，y是大语言模型的输出。&lt;/p>
&lt;ul>
&lt;li>如下图右侧上方所示，在摘要型下游任务中，人类输入的原始文本就是x，大语言模型输出的总结结果就是y。&lt;/li>
&lt;li>如下图右侧下方所示，在表格转文本的下游任务中，人类输入的结构化的表格字符串是x，大语言模型输出的表格描述就是y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>自回归语言模型的问题子域&lt;/strong>(如下图左上所示)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>pφ(y | x)&lt;/strong>：根据Transformer这种自回归模型的网络结构，其本质可抽象为&lt;strong>pφ(y | x)&lt;/strong>，φ为大语言模型的参数，p就是在**&amp;ldquo;人类输入字符串x的条件下，大语言模型输出y的概率分布&amp;rdquo;**。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;/strong>：z被定义为x和y的序列，其中X&lt;sub>idx&lt;/sub>表示了x的索引，Y&lt;sub>idx&lt;/sub>表示y的索引。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>&lt;/strong>：h&lt;sub>i&lt;/sub>表示&lt;strong>时间步i的激活(activation)&lt;/strong>，h&lt;sub>i&lt;/sub>又是由第i个时间步中的n层激活组成的序列(即h&lt;sub>i&lt;/sub>= [h&lt;sub>i&lt;/sub>&lt;sup>(1)&lt;/sup>;···;h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>])，其中h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>表示在Transformer架构中的第i个时间步的第n层的激活。&lt;/li>
&lt;li>&lt;strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：此数学公式表示——根据z&lt;sub>i&lt;/sub>，以及h&lt;sub>1&lt;/sub>~h&lt;sub>i-1&lt;/sub>，计算当前的h&lt;sub>i&lt;/sub>。展开解释一下就是，Transformer模型会根据第i个时间步的x、y，以及第1个时间步~第i-1个时间步计算的各时间步计算的各层激活向量，计算当前时间步下Transformer各层的激活。&lt;/li>
&lt;li>&lt;strong>p&lt;sub>φ&lt;/sub>(z&lt;sub>i+1&lt;/sub> | h&lt;sub>≤i&lt;/sub>) = softmax(W&lt;sub>φ&lt;/sub>h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>)&lt;/strong>：最后一层的h&lt;sub>i&lt;/sub>是Transformer训练后获得的概率分布，用来根据当前Token预测下一个Token。其中，W&lt;sub>φ&lt;/sub>是一个用于将h&lt;sub>i&lt;/sub>&lt;sup>(n)&lt;/sup>映射到词汇表上logits的预训练矩阵。&lt;/li>
&lt;li>上述数学描述中，包含很多AI相关术语(如：自回归模型、时间步i的激活(activation)、各层激活向量、词汇表上logits等)，可参见本技术专栏**《NLP底层原理》篇**的系列文章。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Encoder-Decoder模型的问题子域&lt;/strong>(如下图左下所示)：&lt;/p>
&lt;ul>
&lt;li>与自回归语言模型的问题子域大部分数学描述相同。&lt;/li>
&lt;li>不同点在于基于Encoder-Decoder架构的结构特点，x和y被拆分开了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prefix Tuning的微调求解目标&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>max&lt;sub>φ&lt;/sub> log p&lt;sub>φ&lt;/sub>(y | x) = Σ&lt;sub>i∈Y&lt;sub>idx&lt;/sub>&lt;/sub> log p&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub> | h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：这个看着复杂的公式，就是在表达微调的最终目标就是在各个时间步的各时间步激活的条件下x和y序列的概率分布求和。说人话就是，&lt;strong>微调后的模型能够根据x预测最大概率应该输出y&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329125952458.png" alt="image-20240329125952458">&lt;/p>
&lt;h2 id="32intuition直觉">3.2.Intuition(直觉)&lt;/h2>
&lt;p>论文在正式阐述&lt;strong>Prefix Tuning&lt;/strong>的原理之前，描述了研究员的灵感来源：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Based on intuition from prompting&lt;/strong>：提示词被证明是可以影响大语言模型输出，提示词的思想本质是——大语言模型像一个什么都知道的老人，人类的输入需要使用一定的提示技巧唤醒老人的记忆，从而帮助他输出正确的答案。&lt;/li>
&lt;li>&lt;strong>z = [x; y]&lt;strong>和&lt;/strong>h&lt;sub>i&lt;/sub>=LM&lt;sub>φ&lt;/sub>(z&lt;sub>i&lt;/sub>, h&lt;sub>&amp;lt;i&lt;/sub>)&lt;/strong>：再看3.1问题陈述章节的这两个数学公式，提示词有两个作用：
&lt;ul>
&lt;li>&lt;strong>作用1&lt;/strong>：提示词作为人类输入x的一部分，它起到了&lt;strong>影响大语言模型关注x的哪些Token&lt;/strong>的作用。&lt;/li>
&lt;li>&lt;strong>作用2&lt;/strong>：在作用1的驱动下，影响了大语言模型各层的激活计算结果h&lt;sub>i&lt;/sub>，进而&lt;strong>影响了在x条件下应该接什么y的概率&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>but fail for most pretrained LMs&lt;/strong>：虽然提示词从理论上可以影响大语言模型的输出，但论文进一步阐述了提示词的局限性——做过提示词工程的小伙伴应该知道，仅仅通过自然语言形式的提示词，对大语言模型输出的效果提升很有限。&lt;/li>
&lt;li>&lt;strong>continuous word embeddings&lt;/strong>：论文根据上述头脑实验，准备新增1个Prefix模型，在人类输入x词嵌入为向量后，在这个词嵌入向量前增加1个&lt;strong>Prefix Token(前缀向量)&lt;/strong>(这种向量不同于离散向量，不会引发计算的困难)。为了保证Prefix Token有足够的提示性，论文在Transformer的所有层都增加了这种前缀向量。&lt;/li>
&lt;/ul>
&lt;h2 id="33methodparametrization-of-psubθsub原理">3.3.Method/Parametrization of P&lt;sub>θ&lt;/sub>(原理)&lt;/h2>
&lt;p>论文至此，正式阐述了&lt;strong>Prefix Tuning&lt;/strong>的实现方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>z=[x;y]到z=[prefix;x;y]&lt;/strong>：给自回归模型和Encoder-Decoder模型的z向量增加了前缀&lt;strong>PREFIX&lt;/strong>向量。其中P&lt;sub>idx&lt;/sub>表示了前缀的索引。(如：)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329142053466.png" alt="image-20240329142053466">&lt;/p>
&lt;ul>
&lt;li>基于前缀向量，h&lt;sub>i&lt;/sub>的数学公式表示为各层的激活包含前缀向量的计算和大语言模型预测的概率分布。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329143101780.png" alt="image-20240329143101780">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>|P&lt;sub>idx&lt;/sub>| × dim(h&lt;sub>i&lt;/sub>)&lt;/strong>：P&lt;sub>θ&lt;/sub>是Prefix Tuning新增的小模型的参数，参数的维数必然是有限的，因为它的维数等于前缀向量个数乘以h&lt;sub>i&lt;/sub>的维度。这也解释了为什么Prefix Tuning新增的小模型参数规模仅占预训练模型的0.1%。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>P&lt;sub>θ&lt;/sub>[i, :] = MLP&lt;sub>θ&lt;/sub>(P ′&lt;sub>θ&lt;/sub>[i, :])&lt;/strong>：论文还发现直接微调P&lt;sub>θ&lt;/sub>会导致训练效果不好，于是采用了重参数化方法，引入了前馈神经网络MLP&lt;sub>θ&lt;/sub>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;h2 id="41实验结果">4.1.实验结果&lt;/h2>
&lt;p>论文的实验结论：&lt;strong>Prefix Tuning&lt;/strong>在Table-To-Text下游任务表现良好、在外推涌现方面表现良好，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Table-To-Text实验&lt;/strong>：用GPT2-Medium和GPT2-Large对比FFT和Prefix Tuning，Prefix Tuning都达到了SOTA水平。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150136166.png" alt="image-20240329150136166">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Low-Data-Setting&lt;/strong>实验：在少数据测试中，Prefix Tuning的性能表现和训练稳定度优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150354433.png" alt="image-20240329150354433">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Summarization实验&lt;/strong>：使用XSUM数据集，Prefix Tuning性能略低于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150524132.png" alt="image-20240329150524132">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Extrapolation实验&lt;/strong>：外推涌现实验中，使用XSUM数据集，Prefix Tuning效果优于FFT。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150541755.png" alt="image-20240329150541755">&lt;/p>
&lt;h2 id="42intrinsic-evaluation重要发现">4.2.Intrinsic Evaluation(重要发现)&lt;/h2>
&lt;p>论文在前述实验结果下，有如下重要发现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix长度的影响&lt;/strong>：
&lt;ul>
&lt;li>不同下游任务需要增加不同长度的前缀向量。&lt;/li>
&lt;li>Prefix长度越长，提示效果越好，但过犹不及——超过了一定的阈值，就会出现推理性能下降。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151553754.png" alt="image-20240329151553754">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>在哪些层做Prefix的影响&lt;/strong>：仅在词嵌入层做Prefix的效果远差于在Transformer各层做Prefx的效果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151830368.png" alt="image-20240329151830368">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix和Infix的影响&lt;/strong>：用前缀法的性能效果优于中缀法。
&lt;ul>
&lt;li>研究员猜测，前缀法可能影响x和y，中缀法可能只影响y。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152257513.png" alt="image-20240329152257513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix的初始值选择的影响&lt;/strong>：随机初始化Prefix向量的效果远差于与下游任务相关联的Prefix向量初始化的效果。
&lt;ul>
&lt;li>这种现象可能由于用与下游任务不相关的提示词向量，会导致更长时间的前缀神经网络的收敛(甚至不收敛)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152826581.png" alt="image-20240329152826581">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prefix Tuning的价值&lt;/strong>：追求一套预训练模型，搞定多个下游任务。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的核心思想&lt;/strong>：增加一个新的具备提示能力的前缀向量小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Prefix Tuning的工程实践经验&lt;/strong>：
&lt;ul>
&lt;li>Prefix长度不宜过长或过短，需根据下游任务实验获得。&lt;/li>
&lt;li>对Transformer做全层的Prefix效果更好。&lt;/li>
&lt;li>Prefix会影响x和y，效果优于Infix。&lt;/li>
&lt;li>Prefix的初始值需选择与下游任务相关的提示向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/2101.00190.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</link><pubDate>Fri, 22 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</guid><description>&lt;p>&lt;code>Adapter Tuning&lt;/code>是LLM微调技术中一个重要的技术分支，于2019年由Google的Neil Houlsby等研究员提出。&lt;/p>
&lt;p>&lt;code>Adapter Tuning&lt;/code>方法证明了：微调&lt;strong>少量参数&lt;/strong>即可获得与&lt;strong>全参数微调&lt;/strong>接近的大模型性能。&lt;/p>
&lt;p>本文解读Neil Houlsby的论文**《Parameter-Efficient Transfer Learning for NLP》**，与小伙伴们一起学习理解&lt;code>Adapter Tuning&lt;/code>思想和方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322130214258.png" alt="image-20240322130214258">&lt;/p>
&lt;h1 id="1abstract摘要">1.Abstract(摘要)&lt;/h1>
&lt;p>首先我们看一下论文摘要，快速理解论文的&lt;strong>核心内容&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>问题&lt;/strong>：基于迁移学习思想，需要针对特定的下游任务，对预训练模型进行&lt;strong>全参数微调&lt;/strong>。但针对每个下游任务都要做一次全参数微调，&lt;strong>成本高效率低&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>解决方案&lt;/strong>：论文提出的&lt;code>Adapter Tuning&lt;/code>，是一种使用&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>进行迁移学习的方法。&lt;strong>&lt;code>Adapter(适配器模块)&lt;/code>&lt;/strong>仅需要微调少量参数，就可以支持不同的下游任务。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验效果&lt;/strong>：在&lt;strong>GLUE基准测试&lt;/strong>中，用&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>方法仅需在预训练模型基础上&lt;strong>增加并微调3.6%的参数&lt;/strong>，即可达到BERT Transformer模型全参数微调的效果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>通过上述摘要的内容，我们可以想象一下，在2019年绝大多数人还在采用全参数微调这种高成本方案时，&lt;strong>&lt;code>Adapter Tuning&lt;/code>&lt;/strong>仅需微调3.6%的少量参数，会产生多大的&lt;strong>生产效率差异&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322103351046.png" alt="image-20240322103351046">&lt;/p>
&lt;h1 id="2introduction介绍">2.Introduction(介绍)&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>背景技术1&lt;/strong>：&lt;strong>基于特征的迁移和微调&lt;/strong>(&lt;code>feature-based transfer and fine-tuning&lt;/code>)是迁移学习思想中的重要工程方法，它也是BERT模型的重要理论基础。论文中：&lt;code>Fine-tuning involves copying the weights from a pre-trained network and tuning them on the downstream task&lt;/code>，表达了BERT模型的训练范式——复用1个BERT的预训练模型的参数(&lt;strong>基于特征的迁移&lt;/strong>)，再针对不同下游任务进行微调(&lt;strong>基于特征的微调&lt;/strong>)。&lt;/li>
&lt;li>&lt;strong>背景技术2&lt;/strong>：历史上，已证明针对预训练模型的网络结构中的高层(&lt;code>top layer&lt;/code>)进行&lt;strong>基于特征的微调&lt;/strong>，相较于&lt;strong>基于特征的全参数微调&lt;/strong>，更具有性价比。&lt;/li>
&lt;li>&lt;strong>实验效果&lt;/strong>：对比&lt;strong>Adapter Tuning&lt;/strong>、&lt;strong>Top Layer Fine Tuing&lt;/strong>、&lt;strong>Full Fine Tuning&lt;/strong>在多任务微调场景下的效果：
&lt;ul>
&lt;li>&lt;strong>在小参数模型上的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可达到&lt;strong>Full Fine Tuning&lt;/strong>的效果，&lt;strong>Top Layer Fine Tuing&lt;/strong>达不到。&lt;/li>
&lt;li>&lt;strong>在稳定性方面的表现&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>的训练效果很稳定，&lt;strong>Top Layer Fine Tuing&lt;/strong>在小参数模型上波动大、只有在大参数模型上才能保证稳定的训练效果。&lt;/li>
&lt;li>&lt;strong>在性价比方面的对比&lt;/strong>：&lt;strong>Adapter Tuning&lt;/strong>可以微调少量参数，即可达到&lt;strong>Full Fine Tuning&lt;/strong>的训练效果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322115129249.png" alt="image-20240322115129249">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>基于特征的迁移和微调&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, f(w))&lt;/code>，微调的过程是不断学习修改参数&lt;code>w&lt;/code>和&lt;code>v&lt;/code>，这样就导致预训练模型的参数&lt;code>w&lt;/code>被修改，进而导致极高的训练成本。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning&lt;/strong>的思想是将预训练模型抽象为&lt;code>f(w)&lt;/code>，对下游任务微调抽象为&lt;code>g(v, w)&lt;/code>，微调的过程是不断学习修改参数&lt;code>v&lt;/code>，直接复用预训练模型的参数&lt;code>w&lt;/code>而不是修改它，又因为参数&lt;code>v&lt;/code>的数量级远小于参数&lt;code>w&lt;/code>，因此训练成本极低。另外，针对新的&lt;code>下游任务n&lt;/code>只需要增加新的Adapter，训练对应的参数&lt;code>vn&lt;/code>。&lt;/li>
&lt;li>&lt;code>g(v, w)&lt;/code>的具体代码实现等效于，在原有预训练模型的网络结构中，插入一些&lt;strong>Adapter层&lt;/strong>，预训练模型参数&lt;code>w&lt;/code>作为Adapter层的入参，&lt;strong>训练的目标是学习并修改Adapter层的参数&lt;code>v&lt;/code>&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322120647325.png" alt="image-20240322120647325">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>容易与Adapter Tuning混淆的其它训练方法&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>多任务学习&lt;/strong>：multi-task learning，也会在预训练模型的网络结构中增加新层，最终也是修改了新层的参数。但多任务学习的训练，是将所有下游任务作为训练新层参数的输入。&lt;/li>
&lt;li>&lt;strong>持续学习&lt;/strong>：contiuanl learning，是将N个下游任务组成任务流后逐一学习，这样就要求训练&lt;code>任务m&lt;/code>时，预训练模型的网络结构能够记住之前已经训练过的&lt;code>任务1~任务m-1&lt;/code>得到的参数。这将对预训练模型的记忆能力产生巨大的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322122001309.png" alt="image-20240322122001309">&lt;/p>
&lt;h1 id="3adapter-tuning原理">3.Adapter Tuning(原理)&lt;/h1>
&lt;p>Adatper Tuning具体是如何实现的呢？论文中详细解释了Adapter层的网络结构，以及如何在原始的预训练模型上插入这些Adapter层：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的插入位置&lt;/strong>：在Transformer的多头注意力+前馈网络层之后，2x前馈网络层之后，分别插入了&lt;strong>Adapter层&lt;/strong>。另外，在每个Adapter层之后还插入了一个Layer Norm层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124748365.png" alt="image-20240322124748365">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter层的内部结构&lt;/strong>：Adapter层包含3层
&lt;ul>
&lt;li>&lt;strong>前馈网络的向量降维层&lt;/strong>：用于将前一层预训练模型输出的高维向量，降维为低维向量。&lt;/li>
&lt;li>&lt;strong>非线性处理层&lt;/strong>：对下游任务微调时，学习参数&lt;code>v&lt;/code>。&lt;/li>
&lt;li>&lt;strong>前馈网络的向量升维层&lt;/strong>：用于将Adapter层输出的低维向量，升维为高维向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Adapter层的参数数量计算公式&lt;/strong>：&lt;code>count(v)=2md+d+m&lt;/code>
&lt;ul>
&lt;li>&lt;strong>d&lt;/strong>：前一层预训练模型输出的高维向量的维数。&lt;/li>
&lt;li>&lt;strong>m&lt;/strong>：Adapter层降维后的低维向量维数。&lt;/li>
&lt;li>&lt;strong>实践经验&lt;/strong>：当m远小于d时，Adapter层的参数量会很小。论文给出的经验数据是可以通过控制m的数值，将Adapter层的参数量控制为预训练大模型参数量的0.5%~8%。这样，可以精准控制微调成本。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322124814661.png" alt="image-20240322124814661">&lt;/p>
&lt;h1 id="4experiments实验结果">4.Experiments(实验结果)&lt;/h1>
&lt;p>论文至此就一个实验结论：&lt;strong>Adapter Tuning&lt;/strong>就是香，具体如下：&lt;/p>
&lt;ul>
&lt;li>在GLUE基准测试和其他17个公共文本分类任务上，适配器调优效果，优于全参数微调。&lt;/li>
&lt;li>适配器调优在参数数量大幅减少的情况下，仍能保持与全参数微调相近的性能。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125615723.png" alt="image-20240322125615723">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdapterTuning/image-20240322125621882.png" alt="image-20240322125621882">&lt;/p>
&lt;h1 id="5总结">5.总结&lt;/h1>
&lt;p>从上述论文解读中，我们收获了如下技术观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adapter Tuning的价值&lt;/strong>：追求微调少量参数，仍能达到全参数微调效果。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的核心思想&lt;/strong>：增加一个新的小模型，微调小模型的少量参数，冻结预训练模型的海量参数。&lt;/li>
&lt;li>&lt;strong>Adapter Tuning的具体实现&lt;/strong>：改变预训练模型的网络结构，通过高维向量到低维向量的转换，训练不同下游任务的Adapter层。&lt;/li>
&lt;/ul>
&lt;p>论文链接：https://arxiv.org/pdf/1902.00751.pdf&lt;/p></description></item><item><title>【chatGPT】学习笔记42-LLM微调技术概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 15 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</guid><description>&lt;p>笔者在使用大语言模型开展具体业务领域的任务时，遇到了如下问题：&lt;/p>
&lt;ul>
&lt;li>LLM的预训练模型不具备垂域知识，无法很好回答垂域的问题；&lt;/li>
&lt;li>希望用各种提示词技巧让模型理解专业问题，但效果有限，而且提高了使用门槛和模型推理成本；&lt;/li>
&lt;li>微调词嵌入模型成本低、速度快，但容易出现过拟合或泛化不足，也是治标不治本。&lt;/li>
&lt;/ul>
&lt;p>经过不断尝试总结，最终落脚点还是回到了大模型自身 &amp;ndash; 大模型的微调才是关键。&lt;/p>
&lt;p>什么是大模型微调？微调如何做？本篇开始，我们将为大家一步步揭开大模型微调的神秘面纱。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240314173850356.png" alt="image-20240314173850356">&lt;/p>
&lt;h1 id="1大语言模型的技术金字塔">1.大语言模型的技术金字塔&lt;/h1>
&lt;p>大语言模型相关技术可分为四层，其中：&lt;/p>
&lt;ul>
&lt;li>预训练的难度和成本最高，通常只有巨型公司(如谷歌、微软、OpenAI)才能承担其成本。&lt;/li>
&lt;li>提示词工程难度和成本最低，普通人就可以掌握。&lt;/li>
&lt;li>大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315141452986.png" alt="image-20240315141452986">&lt;/p>
&lt;p>以下是各层技术的详细阐述：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示词工程&lt;/strong>
&lt;ul>
&lt;li>技术原理：通过设计合适的提示或上下文来引导LLM生成期望的输出，侧重于用提示来激活预训练模型的能力，如总结摘要、翻译转换等。适用于文本处理、机器翻译等场景。&lt;/li>
&lt;li>技术特征：技术门槛低，终端用户即可掌握。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Agents&lt;/strong>：
&lt;ul>
&lt;li>技术原理：让LLM来决策一系列的动作，这些动作可以是让LLM分步解决问题，也可以是调用工具查询外部信息。这些动作形成一个工作流，最终完成任务目标。适用于相对复杂的用户交互应用，如智能客服。&lt;/li>
&lt;li>技术特征：有技术门槛，需要专业的LLM应用开发人员，并且了解大模型的基础原理，熟悉其领域的业务逻辑和流程。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>大模型微调&lt;/strong>：
&lt;ul>
&lt;li>技术原理：在预训练模型的基础上，针对特定任务的参数做调整。通过少量的数据训练即可提升模型在特定任务上的能力，同时保留模型原已学到的知识。适用于语义理解、垂域的各类应用。&lt;/li>
&lt;li>技术特征：技术门槛相对高，需要垂域有自己的LLM应用研发团队，具有数据处理、模型训练的能力和经验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>预训练技术&lt;/strong>：
&lt;ul>
&lt;li>技术原理：比如训练出一个&amp;quot;GPT3.5&amp;rdquo;，或者&amp;quot;GLM3.0&amp;rdquo;。&lt;/li>
&lt;li>技术特征：百G甚至千G的GPU资源需求，大量的大模型研究人员、数据科学家投入，超出了大部分公司的能力。一旦功成，效果显著。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>由于大模型微调的实用性，本专栏后续会展开介绍大模型微调技术。&lt;/p>
&lt;h1 id="2大模型微调的可行性及理论基础">2.大模型微调的可行性及理论基础&lt;/h1>
&lt;h2 id="1in-context-learning">(1)In-Context Learning&lt;/h2>
&lt;p>OpenAI在预训练过程中，发现LLM能够挖掘训练数据中的潜在特征和通用范式，进而习得训练数据之外的新能力。研究者把这一发现称之为In-Context Learning(基于上下文的学习)。&lt;/p>
&lt;p>基于In-Context Learning思想，衍生出了两种训练微调技术：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Prompt-tuning&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Instruction-tuning&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In-Context Learning的本质就是举一反三，基于已知的知识和训练数据可以挑战从没做过工作(下游任务)。预训练模型所蕴含的知识、预训练模型所使用的数据是广义上的&lt;code>context&lt;/code>。正因为In-Context Learning的有效性，所以对预训练模型进行微调是业界公认可行的技术路线。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315152229685.png" alt="image-20240315152229685">&lt;/p>
&lt;h2 id="2transfer-learning">(2)Transfer Learning&lt;/h2>
&lt;p>Transfer Learning(迁移学习)是人工智能发展过程中的一个重要思想，它的目标是将已经训练好的模型所包含的知识、推理能力迁移到未经训练的新模型上。比如对模型进行压缩、复用开源模型已经具备的能力来新模型，都是基于迁移学习思想。&lt;/p>
&lt;p>迁移学习包含很多具体的工程方法：&lt;/p>
&lt;ul>
&lt;li>Conservative Training&lt;/li>
&lt;li>Multi-task Learning&lt;/li>
&lt;li>Progressive Neural Network&lt;/li>
&lt;li>Domain-adversarial training&lt;/li>
&lt;li>Zero Shot Learning&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>无论有多少种工程方法，其核心思想是：&lt;strong>已训练的数据和任务与未训练的数据和任务存在因果、关联等逻辑关系&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315151715205.png" alt="image-20240315151715205">&lt;/p>
&lt;h2 id="3bert的优秀实践">(3)Bert的优秀实践&lt;/h2>
&lt;p>Bert充分发挥了Fine-tuning的技术优势和特点，在已经训练好的Bert模型基础上，加入少量的task-specific parameters。&lt;/p>
&lt;ul>
&lt;li>如分类任务，只需要在Bert模型上加一层softmax网络，然后对softmax网络进行微调。&lt;/li>
&lt;li>再如情感分析任务，取第一个token的输出表示，喂给一个softmax层得到分类结果输出。&lt;/li>
&lt;/ul>
&lt;p>微调Bert之所以成功，其本质原因是由于In-context Learning和Transfer Learning的有效性。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315153411365.png" alt="image-20240315153411365">&lt;/p>
&lt;h1 id="3大模型微调的技术全景图">3.大模型微调的技术全景图&lt;/h1>
&lt;h2 id="1大模型微调技术全景">(1)大模型微调技术全景&lt;/h2>
&lt;p>大模型微调技术从大的流派上可分为两类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Full Fine-Tuning&lt;/strong>：简称FFT，全量微调，即全参数微调。&lt;/li>
&lt;li>&lt;strong>Parameter-Efficient Fine-Tuning&lt;/strong>: 简称PEFT，高效微调，即通过某些技术手段选择部分参数微调。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315160640710.png" alt="image-20240315160640710">&lt;/p>
&lt;p>与FFT相比，PEFT是当前业界主流的技术路线，其原因主要是FFT存在如下缺陷：&lt;/p>
&lt;ul>
&lt;li>训练成本过高&lt;/li>
&lt;li>灾难性遗忘&lt;/li>
&lt;/ul>
&lt;h2 id="2peft技术分支">(2)PEFT技术分支&lt;/h2>
&lt;p>PEFT从微调目标方面可分为两类：&lt;/p>
&lt;ul>
&lt;li>Supervised Fine-Tuning: 简称SFT，属于有监督的微调。&lt;/li>
&lt;li>Reinforce Learning Human Feedback: 简称RLHF，属于利用人类反馈的强化学习。&lt;/li>
&lt;/ul>
&lt;p>其中SFT又具有诸多工程实践，因此产生了很多技术分支：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Additive&lt;/strong>: 增量派，在原有模型上增加额外小模型和少量参数。&lt;/li>
&lt;li>&lt;strong>Selective&lt;/strong>: 选择派，从原有模型的海量参数中，选择与下游任务相关的少量参数。&lt;/li>
&lt;li>&lt;strong>Reparametrization-based&lt;/strong>: 数学派，基于重参数化方法，将原有模型参数低秩化，获得小参数矩阵。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315161446294.png" alt="image-20240315161446294">&lt;/p>
&lt;p>在上述流派中，每个流派都有自己的代表方法，目前在业界广泛使用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sparse Adapter&lt;/strong>: 稀疏适配器。新增小模型，并从原有大模型中选择一部分参数，对这两部分参数进行微调。&lt;/li>
&lt;li>&lt;strong>Prefix-Tuning&lt;/strong>: 模块化轻量微调。增加prefix模块(Prefix模块会在用户输入前增加虚拟token)，训练prefix模块的参数。&lt;/li>
&lt;li>&lt;strong>LoRA&lt;/strong>: Low-Rank Adaption，低秩适配微调。它是目前业界在大语言模型、大视觉模型、多模态模型微调中的热门技术。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/image-20240315162656941.png" alt="image-20240315162656941">&lt;/p>
&lt;p>上述技术分支的关系错综复杂，我们可以抽象的概括一下它们的技术思想：&lt;/p>
&lt;ul>
&lt;li>基于数据的技术分支：其核心思想是对进入预训练模型的训练语料进行前置处理。
&lt;ul>
&lt;li>如soft prompts中的Prompt-Tuning、Prefix-Tuning、P-Tuning，都是属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>基于模型的技术分支：其核心思想是在预训练模型基础上增加额外的模型分层，仅针对增量模型的参数和预训练模型的少量参数进行微调和变换。
&lt;ul>
&lt;li>如Adapter模型属于此技术分支。&lt;/li>
&lt;li>如LoRA、QLoRA、AdaLoRA也属于此技术分支。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文从宏观上介绍了大模型微调技术的全景图，重点梳理了PEFT微调技术的众多技术分支，旨在帮助大家了解大模型微调技术的全貌。本专栏后续文章会展开阐述PEFT的各类微调技术。&lt;/p>
&lt;ul>
&lt;li>大语言模型的技术金字塔
&lt;ul>
&lt;li>大语言模型相关技术可分为四层：提示词工程、Agents、大模型微调、预训练技术。&lt;/li>
&lt;li>预训练的难度和成本最高；提示词工程难度和成本最低；大模型微调的难度和成本次高，但从效果来看，是中小型公司可落地的折中技术。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的可行性及理论基础
&lt;ul>
&lt;li>In-Context Learning&lt;/li>
&lt;li>Transfer Learning&lt;/li>
&lt;li>Bert的优秀实践&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>大模型微调的技术全景图
&lt;ul>
&lt;li>Full Fine-Tuning&lt;/li>
&lt;li>Parameter-Efficient Fine-Tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记41-多模态-Sora浅析</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</link><pubDate>Mon, 26 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</guid><description>&lt;p>Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。&lt;/p>
&lt;p>本篇基于OpenAI发布的技术报告对Sora的技术特点和原理进行解读。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226144755598.png" alt="image-20240226144755598">&lt;/p>
&lt;h1 id="1sora是什么">1.Sora是什么？&lt;/h1>
&lt;p>Sora是一个文生视频的AI模型，可以根据文本信息生成真实且富有想象力的视频内容。主要特点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>以自然语言为输入(提示词)，生成符合提示词描述的视频&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>可生成1分钟内容连贯的视频，视频尺寸/分辨率可调整，目前只有Sora做到&lt;/strong>。其它模型只能生成4秒以内、256x256固定尺寸的视频。&lt;/li>
&lt;li>&lt;strong>真实世界的模拟器&lt;/strong>，不仅理解物理实体(如人、猫、狗、&amp;hellip;)，还懂得物理规律(如光照、碰撞、粒子、&amp;hellip;)。&lt;/li>
&lt;/ul>
&lt;p>Sora出道即颠峰，让一众顶级文生视频模型望尘莫及。我们来直观感受下Sora生成的视频的震撼效果：&lt;/p>
&lt;p>提示词如下：&lt;/p>
&lt;blockquote>
&lt;p>Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field.&lt;/p>
&lt;/blockquote>
&lt;p>视频如下：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=1301071173&amp;bvid=BV1zu4m1c7gg&amp;cid=1452188766&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="2sora原理浅析和技术优势">2.Sora原理浅析和技术优势&lt;/h1>
&lt;p>OpenAI没有公开Sora的模型细节，本文后续分析依据OpenAI的&amp;quot;Technic Report&amp;quot;推测所得。&lt;/p>
&lt;h2 id="21sora是否采用了新的模型架构技术">2.1.Sora是否采用了新的模型架构&amp;amp;技术？&lt;/h2>
&lt;p>答案是没有。&lt;/p>
&lt;p>文生视频可能涉及的模型架构如下：&lt;/p>
&lt;ul>
&lt;li>RRN (循环神经网络)&lt;/li>
&lt;li>GAN (生成式对抗网络)&lt;/li>
&lt;li>自回归Transformer&lt;/li>
&lt;li>Diffusion (扩散模型)&lt;/li>
&lt;/ul>
&lt;p>Sora采用的是：&lt;/p>
&lt;ul>
&lt;li>自回归Transformer&lt;/li>
&lt;li>Diffusion (扩散模型)&lt;/li>
&lt;/ul>
&lt;h2 id="22sora对现有模型架构技术的创新">2.2.Sora对现有模型架构&amp;amp;技术的创新&lt;/h2>
&lt;p>Sora架构是结合了Diffusion扩散模型和Transformer架构的创新设计。&lt;/p>
&lt;h3 id="1用transformer架构学习视频特征">(1)用Transformer架构，学习视频特征&lt;/h3>
&lt;p>灵感来自于Tranformer架构在GPT中的成功应用，OpenAI把自然语言的特征表示方法引入到了视频处理中：&lt;/p>
&lt;ul>
&lt;li>通过编码器，把视频的每一帧转换为有时序的向量，若干帧形成了&lt;strong>向量矩阵&lt;/strong>，Sora称之为&lt;strong>Turning visual data into patches&lt;/strong>。(大语言模型是把文字转换成一个个token，Sora则是把视频转换成一个个patch。)&lt;/li>
&lt;li>与大语言模型中token线性序列不同，由patch组成的高维向量矩阵包含了时序、分辨率、高宽比等丰富信息。&lt;/li>
&lt;li>和大语言模型一样(文本信息压缩网络)，Sora模型本质是一个视频压缩网络&lt;strong>Video compression network&lt;/strong>，使用该视频压缩网络把高维向量矩阵压缩成单维向量序列。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240223161152675.png" alt="image-20240223161152675">&lt;/p>
&lt;h3 id="2用结合了transformer的diffusion模型学习还原视频的特征">(2)用结合了Transformer的Diffusion模型，学习还原视频的特征&lt;/h3>
&lt;h4 id="--什么是扩散模型diffusion-model">- 什么是扩散模型Diffusion Model&lt;/h4>
&lt;p>Diffusion Model的基本原理是将原始图片逐渐加入噪声(Noise)，让原本清晰的图片逐渐变成全是噪声的状态。&lt;/p>
&lt;p>Diffusion Model主要有两个过程：前向处理和反向处理。图解如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>前向处理&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>AI训练过程中的1次噪声扩散&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226131740185.png" alt="image-20240226131740185">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AI训练过程中，进行N次噪声扩散&lt;/p>
&lt;ul>
&lt;li>进行N次噪声扩散，将图片变成全是噪点的图像&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132019285.png" alt="image-20240226132019285">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>后向处理&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>AI训练过程中，进行N次噪声降噪，变成清晰图片&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226132143014.png" alt="image-20240226132143014">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>经过如上扩散处理，我们得到了&lt;strong>噪声与图像的关系。&lt;/strong>&lt;/p>
&lt;p>利用transformer模型，既可以理解噪声的特性向量，也可以理解自然语言的特征向量，那么就可以建立&lt;strong>噪声与提示词的关系&lt;/strong>。基于这种机制，模型就能够根据提示词来生成图片了。图片是视频的一帧，文生视频也可以用同样的原理来生成。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240227084715255.png" alt="image-20240227084715255">&lt;/p>
&lt;h4 id="--sora的diffusion-transformer模型">- Sora的Diffusion Transformer模型&lt;/h4>
&lt;p>在Diffusion Model的基础上，引入Transformer技术方法：&lt;/p>
&lt;ul>
&lt;li>通过编码器，将每一帧噪声转换为向量，若干帧形成了&lt;strong>噪声向量矩阵&lt;/strong>，Sora称之为&lt;strong>Noisy patches&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104456184.png" alt="image-20240226104456184">&lt;/p>
&lt;ul>
&lt;li>Sora的创新点：&lt;strong>一次性生成&lt;/strong>Noise Vector Cude，用以保证帧与帧之间的逻辑关系。从而生成时间和空间更流畅、更连贯的视频。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226104436506.png" alt="image-20240226104436506">&lt;/p>
&lt;h2 id="23sora的技术优势">2.3.Sora的技术优势&lt;/h2>
&lt;p>虽然OpenAI没有公布Sora模型的训练细节，但即使公布了，其他厂家可能也很难复制或追赶。&lt;/p>
&lt;h3 id="1sora的video-compression-network依托于强大的算力">(1)Sora的Video compression network依托于强大的算力&lt;/h3>
&lt;p>模型成功的一个重要因素是海量的训练数据，训练需要消耗大量算力。Sora采用高维向量矩阵模式的视频处理方式，意味着更大算力消耗。中信证券曾简单估算，一个6~8秒的视频（约60帧）需要约6万个Patches，如果去噪步数是20的话，相当于要生成120万个Tokens，这是相当大的计算量。那么Sora生成1分钟视频需要的算力可想而知。&lt;/p>
&lt;p>同一个提示词，算力越高，生成视频效果越好。Sora给出基础算力、4倍算力、32倍算力下的效果展示：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1uC411s7y9&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1ey421q72G&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h3 id="2gpt4加速孵化多模态大模型多模态大模型反哺llm">(2)GPT4加速孵化多模态大模型，多模态大模型反哺LLM&lt;/h3>
&lt;ul>
&lt;li>遥遥领先的自然语言理解能力：
&lt;ul>
&lt;li>文生视频模型的训练离不开大量带有文字标注的视频，OpenAI基于GPT4专门训练一个高度描述性的字幕模型，使用它来为训练数据集中的所有视频生成文本字幕。这样进行训练极大的保证了文本保真度以及视频的整体质量。&lt;/li>
&lt;li>同时，GPT优秀的文本扩展能力，可以丰富用户输入的提示词，让文本描述更丰富、更细致，从而生成丰富、细腻的视频。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226141517838.png" alt="image-20240226141517838">&lt;/p>
&lt;h1 id="3sora有哪些创意玩法">3.Sora有哪些创意玩法&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>Animating DALL·E images&lt;/strong>：图生视频
&lt;ul>
&lt;li>左边是图片和提示词，右边是生成的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1pZ421y7RF&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Extending generated videos&lt;/strong>：视频续写
&lt;ul>
&lt;li>基于原视频，向前或向后续写新的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1oK42187Kc&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Video-to-video editing&lt;/strong>：编辑视频
&lt;ul>
&lt;li>左边是原视频，右边是根据提示词修改的视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1Vu4m1w7QY&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Connecting videos&lt;/strong>：连接视频
&lt;ul>
&lt;li>通过左侧视频和右侧视频，生成中间过渡视频&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV19w4m1f7dr&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Image generation capabilities&lt;/strong>：生成高质量图片
&lt;ul>
&lt;li>生成各种尺寸的图片，分辨率最高可达 2048x2048&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-Sora%E6%B5%85%E6%9E%90/image-20240226173623135.png" alt="image-20240226173623135">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>卓越的仿真能力&lt;/strong>：生成现实世界仿真视频&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>3D consistency&lt;/strong>：空间一致性&lt;/p>
&lt;ul>
&lt;li>模拟摄像机镜头旋转和运行，画面中的人和场景元素在三维空间中也一致的移动。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV11K421t75e&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Long-range coherence and object permanence&lt;/strong>：时间一致性
&lt;ul>
&lt;li>Sora能够有效地为短期和长期依赖关系建模。例如，模型可以保存人物、动物和物体，即使它们被遮挡或离开了框架(下左视频)。同样，可以基于单个样本生成同一角色的多个镜头，并在整个视频中保持其外观(下右视频)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1Av421k7vD&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>&lt;strong>Interacting with the world&lt;/strong>：模拟真实世界的动作和结果
&lt;ul>
&lt;li>随着画笔的移动，画面中增加了花瓣；人吃汉堡，汉堡留下咬痕。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?bvid=BV1vF4m157Hm&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="4sora对视频相关领域的影响">4.Sora对视频相关领域的影响&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>内容创作与媒体行业&lt;/strong>：&lt;/p>
&lt;p>在内容创作领域，Sora将缩短创作周期，降低制作成本。&lt;/p>
&lt;p>导演、视频编辑等岗位将直面Sora的影响。&lt;/p>
&lt;p>抖音类短视频App是否会面临挑战？&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>教育和培训：&lt;/strong>
教育工作者可以利用Sora创建生动的教学材料，提高课程互动性和学习效果。
但Sora仍然面临垂域微调的难题——我们尝试生成一个排序算法的Demo，得到的视频不尽如人意。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>营销和广告&lt;/strong>
广告设计师和品牌经理可以通过Sora来快速生成视频广告内容。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>游戏开发与动画制作&lt;/strong>
游戏设计师和动画制作人员可能会使用Sora快速原型制作和动画创建。
这不仅能够加快项目开发速度，还能使得复杂场景的测试和迭代变得更为高效。
不过，这也可能引发对传统动画和建模技艺的重新评估。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>上下游基础设施：&lt;/strong>
上游厂商：AI服务器、AI芯片、通信行业、云厂商。
下游应用：大量的短/中/长视频应用和服务需求。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="5参考">5.参考&lt;/h1>
&lt;blockquote>
&lt;p>&lt;a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记40-LLM应用-如何构建RAG数据集</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</link><pubDate>Mon, 05 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</guid><description>&lt;p>2023年是基础大模型的爆发元年，专家预测2024年将是AI应用的爆发元年。&lt;/p>
&lt;p>因此，本专栏希望通过一系列文章，和大家探讨AI应用的规划、落地、实践等问题：&lt;/p>
&lt;blockquote>
&lt;p>如何在千行百业寻找AI应用的落地点？如何用AI为客户带来真实的价值？&lt;/p>
&lt;p>如何开发高质量AI应用？如何评估和控制AI应用的开发成本？&lt;/p>
&lt;/blockquote>
&lt;p>RAG是目前AI应用落地的主要技术领域，本文首先来探讨RAG相关的产品实践。&lt;/p>
&lt;h1 id="1开发ai应用前我们应该考虑什么">1.开发AI应用前，我们应该考虑什么？&lt;/h1>
&lt;h2 id="1我们真的需要ai应用吗">(1)我们真的需要AI应用吗？&lt;/h2>
&lt;p>相信大家有一种感觉：AI很强大，但问及AI解决什么具体的行业问题？又无从回答。&lt;/p>
&lt;p>这是一个产品规划问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>行业痛点&lt;/strong>：我的行业领域有什么Gap点？&lt;/li>
&lt;li>&lt;strong>业务设计&lt;/strong>：AI能解决这些Gap点吗？&lt;/li>
&lt;li>&lt;strong>价值呈现&lt;/strong>：这些问题解决后能给客户带来什么价值？&lt;/li>
&lt;/ul>
&lt;p>我们不能叶公好龙式地迷失在AI浪潮中，还是要落脚于&lt;strong>解决客户痛点、呈现产品价值&lt;/strong>。&lt;/p>
&lt;p>以&lt;strong>Khan Academy&lt;/strong>在AI赋能教育的实践为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Khan Academy是一家具备全球影响力的在线教育平台，愿景是为提供&lt;strong>普惠教育&lt;/strong>(为无法享受基础教育的学生提供覆盖学科广泛、内容专业的在线课程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>行业痛点&lt;/strong>：Khan Academy的普惠教育理想，意味着既要&lt;strong>高教学质量、千人千面&lt;/strong>，又要&lt;strong>控制教学成本&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>业务设计&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>怎么学&lt;/strong>：通过AI对不同学生的画像，自动化(成本)、专业化(高质量)、针对性(千人千面)地给出学习地图。&lt;/li>
&lt;li>&lt;strong>学什么&lt;/strong>：通过AI自主挖掘发现学习资源，自动化(成本)、针对性(千人千面)地为学生推荐学习资源。&lt;/li>
&lt;li>&lt;strong>监督引导&lt;/strong>：通过AI，自动化(成本)、有耐心(高质量)地监督与调整学生进度及学习习惯。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>价值呈现&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>自主学习、个性化学习是普惠教育的基石。&lt;/li>
&lt;li>&lt;strong>AI从教学质量和成本上使得自主学习、个性化学习成为了可能&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206095448606.png" alt="image-20240206095448606">&lt;/p>
&lt;h2 id="2解决问题的手段只有锤子吗">(2)解决问题的手段只有锤子吗？&lt;/h2>
&lt;p>AI智能问答是一把锤子(也是目前国内AI应用的主要形态)，但它只是AI能力的很小一部分。&lt;/p>
&lt;p>为了更好地选择合适的AI能力解决客户问题，我们需要理解AI能力的全集。&lt;/p>
&lt;p>AI能力可以分为&lt;strong>基础能力&lt;/strong>和&lt;strong>综合能力&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基础能力&lt;/strong>：AI的听说读写能力，这些能力可以让AI将&amp;quot;文字&amp;rdquo;、&amp;ldquo;语音&amp;rdquo;、&amp;ldquo;图像及视频&amp;quot;进行相互转换。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240203083813459.png" alt="image-20240203083813459">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>综合能力&lt;/strong>：综合能力依托于AI基础能力，进而解决客户的业务问题。
&lt;ul>
&lt;li>&lt;strong>RAG能力&lt;/strong>：AI可以通过外挂形式，进行垂域知识问答。&lt;/li>
&lt;li>&lt;strong>ReAct能力&lt;/strong>：可以发挥AI具备的一定的推理能力，分解任务，自动执行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206101931437.png" alt="image-20240206101931437">&lt;/p>
&lt;p>面向不同类型的客户问题，我们选择的AI能力不同：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>检索类问题适合采用RAG&lt;/strong>。
&lt;ul>
&lt;li>如：了解工作流程、学习专业知识等检索类客户问题，适合使用AI的RAG综合能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>任务执行类问题适合采用ReAct&lt;/strong>。
&lt;ul>
&lt;li>如：挖掘海量互联网学习资源、根据学生学习情况监督调整学习计划等任务执行类问题，适合采用ReAct综合能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="2如何构建rag数据集">2.如何构建RAG数据集？&lt;/h1>
&lt;p>在确定采用RAG技术解决客户问题后，构建高质量的RAG数据集是RAG成功的关键(否则就是&amp;quot;垃圾进，垃圾出&amp;quot;的结果)。&lt;/p>
&lt;p>我们接下来讨论如何构建高质量RAG数据集。&lt;/p>
&lt;h2 id="21问题分类">2.1.问题分类&lt;/h2>
&lt;p>在制作数据集前，要理清垂域中的&lt;strong>垂域用户问题&lt;/strong>和&lt;strong>垂域知识&lt;/strong>的&lt;strong>特点&lt;/strong>。&lt;/p>
&lt;p>垂域用户问题有如下特点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>高频问题易识别&lt;/strong>: 常见问题和重复性问题出现的频率较高，垂域专家清楚高频问题是什么。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>针对性强&amp;amp;辨识度高&lt;/strong>：通常针对特定的知识进行提问，问题描述包含垂域专业术语，不闲聊。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>兼具广度和深度&lt;/strong>：可能是问结构纲领级的问题，也可能是针对某个具体细节的问题。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>垂域知识有以下特点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>独特性&lt;/strong>：垂域知识有其独特的知识体系和术语，需要具备一定的专业背景和知识储备才能理解和应用。&lt;/li>
&lt;li>&lt;strong>结构明确&lt;/strong>：垂域知识通常有明确的结构和层次，各层知识间不存在重复和二义性。垂域知识更新很快，但知识结构变化相对小。&lt;/li>
&lt;/ul>
&lt;p>基于以上特点，可以将垂域用户向AI提问的&lt;strong>问题类型分为2类&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高频问题&lt;/strong>：适合采用FAQ问答技术，一则可使&lt;strong>答案准确可控&lt;/strong>，二则减少AI的资源消耗。&lt;/li>
&lt;li>&lt;strong>知识问答&lt;/strong>：适合采用文档问答技术，一则文档所覆盖的垂域知识深度与广度兼备，二则构建成本相对较低。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206103355976.png" alt="image-20240206103355976">&lt;/p>
&lt;h2 id="22faq问答数据集构建">2.2.FAQ问答数据集构建&lt;/h2>
&lt;h3 id="1关注构建成本">(1)关注构建成本&lt;/h3>
&lt;p>FAQ问答数据数据集构建时，可能存在一种误区：&lt;strong>问题不够加问题&lt;/strong>？&lt;/p>
&lt;ul>
&lt;li>问题不是不够，而是AI的理解能力有限，无法理解同一个问题五花八门的表达形式。&lt;/li>
&lt;li>问题不够加问题，本质就是穷举问题表达形式，但自然语言的表达形式穷举成本极高甚至不可穷举，因此会导致数据集构建成本不可承受。&lt;/li>
&lt;/ul>
&lt;p>如果上述方法行不通，则说明我们需要：用&lt;strong>有限的问答对&lt;/strong>覆盖&lt;strong>多样的问题表达形式&lt;/strong>，进而才能控制FAQ数据集的构建成本。&lt;/p>
&lt;p>如下是我们的工程实践：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从知识的覆盖度上构建问答对，而不是从问题的表达多样性上构建问答对——这样&lt;strong>问答对的数量就是有限的&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>覆盖多样的问题表达形式，可以采用关键词、扩展问等技巧。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这个过程必须投入业务专家，根据上述两点工程实践进行把关和输出，业务专家需要保证。&lt;/p>
&lt;ul>
&lt;li>每个问题要保证&lt;strong>语义唯一性&lt;/strong>。&lt;/li>
&lt;li>所有问题构成的语义集合要保证&lt;strong>业务覆盖性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h3 id="2关注关键词提取">(2)关注关键词提取&lt;/h3>
&lt;p>关于前文提到的关键词技巧，也存在一个误区：直接将问题中的主谓宾作为关键词。&lt;/p>
&lt;ul>
&lt;li>这里举个例子：&lt;code>Java&lt;/code>是&amp;quot;如何提高Java的调试与定位能力？&amp;ldquo;这个问题的关键词吗？&lt;/li>
&lt;li>显然，从自然语言角度&lt;code>Java&lt;/code>是关键词，但从业务角度&lt;code>Java&lt;/code>不是关键词。
&lt;ul>
&lt;li>因为用户对Java领域的常见问题，都会带有Java这个单词。&lt;/li>
&lt;li>如果将Java作为本问题的关键词，那么所有的垂域问题都会被AI认为与本问题有关。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们的实践经验是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>每个关键词必须具备&lt;strong>独特性&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>上述例子中，&lt;code>Java&lt;/code>没有独特性，失去了特征，而&lt;code>调试与定位能力&lt;/code>作为这个问题的关键词更为合适，因其具有独特性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>关键词集合必须具备&lt;strong>丰富性&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>从业务角度，&amp;ldquo;调试与定位能力&amp;rdquo;，也有&amp;quot;调试定位&amp;rdquo;、&amp;ldquo;调试与定位&amp;quot;这种&lt;strong>惯用语&lt;/strong>，因此关键词集合可以丰富为[&amp;ldquo;调试与定位能力&amp;rdquo;, &amp;ldquo;调试与定位&amp;rdquo;, &amp;ldquo;调试定位&amp;rdquo;]。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="23文档问答数据集构建">2.3.文档问答数据集构建&lt;/h2>
&lt;h3 id="1关注文档质量">(1)关注文档质量&lt;/h3>
&lt;p>文档问答是AI对垂域文档进行学习理解(向量化)。这个过程类似老师(AI工程师)教小孩(AI)学习一本教材(垂域文档)。&lt;/p>
&lt;p>需要充分理解AI的特点(&lt;strong>因材施教&lt;/strong>)，设计出AI更容易理解垂域文档(&lt;strong>好教材&lt;/strong>)，是构建文档问答数据集的关键技术。&lt;/p>
&lt;p>构建垂域文档，有如下实践：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>知识组织形式&lt;/strong>会影响召回率：
&lt;ul>
&lt;li>通过实测和尝试，将垂域知识构建为树状结构，比较易于当前国内LLM理解和学习。&lt;/li>
&lt;li>不要在垂域知识树的节点之间产生关联关系形成有向图，可以通过搬移树节点的形式用线性的形式表达知识节点的关系。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-LLM%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BARAG%E6%95%B0%E6%8D%AE%E9%9B%86/image-20240206113907749.png" alt="image-20240206113907749">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>每个知识块节点的&lt;strong>粒度适中&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>知识块不宜过大&lt;/strong>：将整个垂域文档设计为一个知识块，会导致问啥问题都返回这个知识块。&lt;/li>
&lt;li>&lt;strong>知识块不宜过小&lt;/strong>：将垂域文档设计为一句话一个知识块，会导致知识点太碎，知识点之间存在复杂的逻辑关联，远超出现有LLM的推理能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>每个知识块节点内容&lt;strong>避免重复&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个知识块节点内容&lt;strong>避免矛盾&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="24ai辅助构建数据集">2.4.AI辅助构建数据集&lt;/h2>
&lt;p>前述FAQ数据集构建、文档问答数据集构建的过程，都可以采用AI、自动化工具等方式辅助构建。&lt;/p>
&lt;p>我们会在后续文章中分享AI辅助构建数据集的工程方法与实践。&lt;/p>
&lt;p>但，无论采用怎样的数据集构建过程，还是要关注数据集的内容本身，是否满足上述工程实践的要求和原则。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>构建RAG数据集，需要考虑一系列实践方法，建立标注规范，确保数据的质量和有效性。具体实践经验如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>AI应用的规划与设计：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>要用产品规划方法进行业务设计，落脚于&lt;strong>解决客户痛点、呈现产品价值&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>要选择合适的AI能力解决业务问题，可用的AI能力包括&lt;strong>基础能力(听说读写)&lt;strong>和&lt;/strong>综合能力(RAG、ReAct)&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>构建RAG数据集方法：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>问题分类：&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>高频问题&lt;/strong>：适合采用FAQ问答技术，一则可使&lt;strong>答案准确可控&lt;/strong>，二则减少AI的资源消耗。&lt;/li>
&lt;li>&lt;strong>知识问答&lt;/strong>：适合采用文档问答技术，一则文档所覆盖的垂域知识深度与广度兼备，二则构建成本相对较低。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>FAQ问答数据集构建方法：
&lt;ul>
&lt;li>每个问题要保证&lt;strong>语义唯一性&lt;/strong>。&lt;/li>
&lt;li>所有问题构成的语义集合要保证&lt;strong>业务覆盖性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>文档问答数据集构建方法：
&lt;ul>
&lt;li>&lt;strong>知识组织形式&lt;/strong>会影响召回率：将垂域知识构建为树状结构，可以通过搬移树节点的形式用线性的形式表达知识节点的关系。&lt;/li>
&lt;li>每个知识块节点的&lt;strong>粒度适中&lt;/strong>：知识块不宜过大，也不宜过小。&lt;/li>
&lt;li>每个知识块节点内容&lt;strong>避免重复&lt;/strong>。&lt;/li>
&lt;li>每个知识块节点内容&lt;strong>避免矛盾&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可采用AI辅助构建数据集，但关键还是&lt;strong>要关注数据集的内容本身符合上述工程实践的要求和原则&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记39-LangChain解读-LCEL语言之领域功能(5)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD5/</link><pubDate>Thu, 01 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD5/</guid><description>&lt;p>在LLM应用程序中，不可避免的会存在大量可知或不可知的故障点，比如模型API调用异常、链组合集成的问题、自定义的组件运行出错等。如果针对这些情况准备了fallback备用方法，就可以让程序更稳健。&lt;/p>
&lt;p>本篇我们来看下在LCEL中如何实现&amp;quot;fallback&amp;rdquo;。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202010923281.png" alt="image-20240202010923281">&lt;/p>
&lt;h1 id="1-lcel的fallback方法">1. LCEL的Fallback方法&lt;/h1>
&lt;p>LCEL在runnable中定义了fallback方法&lt;code>withfallback&lt;/code>，runnable组件及其组成的链都可以直接使用这个方法。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>当调用方出现异常，则执行fallback中预置的备用方法。&lt;/li>
&lt;li>通常用于预防LLM API错误，也可用于其它runnable组件，或者工作链。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>withfallbacks&lt;/code>的第一个参数是&lt;strong>备用方法&lt;/strong>，放在方括号&lt;code>[]&lt;/code>中。&lt;/li>
&lt;li>第二个参数&lt;code>exceptions_to_handle&lt;/code>为可选，用于&lt;strong>指定要处理的错误&lt;/strong>——当发生这些错误时才执行备用方法。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> runnable对象.withfallback&lt;span class="o">(&lt;/span>
&lt;span class="o">[&lt;/span>备用方法&lt;span class="o">]&lt;/span>,
&lt;span class="nv">exceptions_to_handle&lt;/span>&lt;span class="o">=(&lt;/span>KeyboardInterrupt, ...,&lt;span class="o">)&lt;/span>
&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以智能客服场景为例，模拟LLM模型发生异常的情况，使用fallback来预防这些错误，提升用户体验。&lt;/p>
&lt;h2 id="step1-构建故障链">STEP1: 构建故障链&lt;/h2>
&lt;ul>
&lt;li>构造一个工作链，专门负责回答物流问题。通过设置不存在的模型来模拟模型故障。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003427497.png" alt="image-20240202003427497">&lt;/p>
&lt;ul>
&lt;li>运行链，返回关于模型的错误信息。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003628125.png" alt="image-20240202003628125">&lt;/p>
&lt;h2 id="step2-构建备用链">STEP2: 构建备用链&lt;/h2>
&lt;ul>
&lt;li>构建默认回复链，该链使用正常模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003914611.png" alt="image-20240202003914611">&lt;/p>
&lt;h2 id="step3-构建fallback链">STEP3: 构建Fallback链&lt;/h2>
&lt;ul>
&lt;li>使用&lt;code>with_fallbacks&lt;/code>方法，构造工作链service_chain，把默认回复链作为物流链的后备方法。&lt;/li>
&lt;li>运行具有fallback能力service_chain，顺利得到回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202003948900.png" alt="image-20240202003948900">&lt;/p>
&lt;ul>
&lt;li>除了上述基础用法，我们还可以指定fallback服务于特殊的异常类型。如下示例中，指定故障链一旦发生了KeyboardInterrupt异常，才执行备用链。&lt;/li>
&lt;li>具体如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202004341255.png" alt="image-20240202004341255">&lt;/p>
&lt;h2 id="step4-构建fallback链自定义备用方法">STEP4: 构建Fallback链：自定义备用方法&lt;/h2>
&lt;ul>
&lt;li>假设LLM模型全部失效，把默认回复链的模型也改成无效的模型。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005006014.png" alt="image-20240202005006014">&lt;/p>
&lt;ul>
&lt;li>自定义一个备用方法，来兜底保证给用户及时回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005208000.png" alt="image-20240202005208000">&lt;/p>
&lt;ul>
&lt;li>构造新的工作链new_service_chain，把兜底回复作为后备方法。&lt;/li>
&lt;li>运行链，顺利得到回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B039-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(5)/image-20240202005233635.png" alt="image-20240202005233635">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL的fallback方法&lt;code>withfallbacks&lt;/code>，可以有效解决LLM API、模型输出不佳以及其他集成问题带来的问题。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>&lt;code>withfallbacks&lt;/code>方法&lt;/strong>：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LCEL在runnable中定义了fallback方法&lt;code>withfallbacks&lt;/code>，runnable组件及其组成的链都可以直接使用这个方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>withfallbacks&lt;/code>的第一个参数是&lt;strong>备用方法&lt;/strong>，第二个参数&lt;code>exceptions_to_handle&lt;/code>为可选，用于&lt;strong>指定要处理的错误&lt;/strong>——当发生这些错误时才执行备用方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使用上述方法，构建了一个具有fallback机制的客服问答工作链。模拟LLM模型发生异常时，通过预置的后备方法及时回复用户消息。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记38-LangChain解读-LCEL语言之领域功能(4)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD4/</link><pubDate>Sun, 21 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD4/</guid><description>&lt;p>langchain基础的链包括三个组件：提示词、LLM模型、输出解析器，但实际LLM应用开发会复杂很多，可能需要更多组件。&lt;/p>
&lt;p>本篇我们介绍LCEL提供的几个常用方法，可以帮助开发者构建自定义runnable组件，进而组合成功能更强大的链。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120130624759.png" alt="">&lt;/p>
&lt;h1 id="1-lcel组件构建方法">1. LCEL组件构建方法&lt;/h1>
&lt;p>LCEL在Runnable协议中定义了如下方法，帮助开发者灵活构建runnable组件：&lt;/p>
&lt;ul>
&lt;li>RunnablePassthrough&lt;/li>
&lt;li>RunnableLambda&lt;/li>
&lt;li>RunnableBranch&lt;/li>
&lt;/ul>
&lt;h2 id="1runnablepassthrough">(1)RunnablePassthrough:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>透传用户输入。通常与一个键组成键值对，用于构造下一个组件的输入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，prompt组件需要的输入是以topic为键的字典，但实际用户输入是字符串&amp;quot;袋鼠&amp;rdquo;。&lt;/li>
&lt;li>利用&lt;code>RunnablePassthrough&lt;/code>透传用户输入，并与topic组成键值对，构造了promt所需的输入。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">prompt&lt;/span> &lt;span class="o">=&lt;/span> ChatPromptTemplate.from_template&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;请讲一个关于{topic}的笑话。&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="nv">output_parser&lt;/span> &lt;span class="o">=&lt;/span> StrOutputParser&lt;span class="o">()&lt;/span>
&lt;span class="nv">model&lt;/span> &lt;span class="o">=&lt;/span> ChatOpenAI&lt;span class="o">()&lt;/span>
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">{&lt;/span>&lt;span class="s2">&amp;#34;topic&amp;#34;&lt;/span>: RunnablePassthrough&lt;span class="o">()}&lt;/span> &lt;span class="p">|&lt;/span> prompt&lt;span class="p">|&lt;/span> model &lt;span class="p">|&lt;/span> output_parser
chain.invoke&lt;span class="o">(&lt;/span>&lt;span class="s2">&amp;#34;袋鼠&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="2runnablelambda">(2)RunnableLambda:&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>&lt;code>RunnableLambda&lt;/code>可以把自定义函数转换成Runnable对象，以便在组件中使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，自定义函数my_function，然后通过RunnableLambda(my_function)转换为runnable组件。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">def my_function&lt;span class="o">()&lt;/span>
...
&lt;span class="k">return&lt;/span> xxxx
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> RunnableLambda&lt;span class="o">(&lt;/span>my_function&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> 其它组件&lt;span class="p">|&lt;/span> ...
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="3runnablebranch">(3)RunnableBranch&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>功能简介&lt;/strong>：&lt;/p>
&lt;p>类似路由功能，可以根据条件选择要运行的分支，即当条件满足时，则执行该分支。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>使用语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>如下示例，&lt;code>RunnableBranch&lt;/code>的参数是&amp;quot;条件、分支&amp;quot;对 + 默认分支。&lt;/li>
&lt;li>当某个条件满足时，则执行对应分支；如果条件都不满足，则执行默认分支。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">branch&lt;/span> &lt;span class="o">=&lt;/span> RunnableBranch&lt;span class="o">(&lt;/span>
&lt;span class="o">(&lt;/span>条件1, 分支1&lt;span class="o">)&lt;/span>,
...,
&lt;span class="o">(&lt;/span>条件n, 分支n&lt;span class="o">)&lt;/span>,
默认分支,
&lt;span class="o">)&lt;/span>
&lt;span class="nv">chain&lt;/span> &lt;span class="o">=&lt;/span> branch &lt;span class="p">|&lt;/span> 其它组件&lt;span class="p">|&lt;/span> ...
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以智能客服场景为例，我们使用上述三个方法来组成一个相对复杂的工作链：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>利用LLM判断问题类别、感情色彩&lt;/p>
&lt;/li>
&lt;li>
&lt;p>自定义函数对LLM输出结果格式化，以便后续组件使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据问题类别，构造不同的客服分支，组建客服路由链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>组成完整工作链，根据问题类别回答问题&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="step1-构建问题分类链">STEP1: 构建问题分类链&lt;/h2>
&lt;ul>
&lt;li>构建问题分类链topic_chain，任务是判断问题属于&amp;quot;产品质量&amp;rdquo; or &amp;ldquo;物流&amp;rdquo;，情绪是&amp;quot;积极&amp;rdquo;、&amp;ldquo;消极&amp;rdquo; or &amp;ldquo;中立&amp;rdquo;。&lt;/li>
&lt;li>使用&lt;code>RunnablePassthrough&lt;/code>透传用户问题，并与&amp;quot;question&amp;quot;组成键值对，作为prompt组件的输入。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120115926742.png" alt="">&lt;/p>
&lt;h2 id="step2-自定义runnable组件格式化输出结果">STEP2: 自定义Runnable组件：格式化输出结果&lt;/h2>
&lt;ul>
&lt;li>LangChain自带的输出解析器&lt;code>StrOutputParser&lt;/code>输出的是字符串，但我们下个组件的输入要求是字典。所以自己写个函数format_func，把字符串转成字典格式。&lt;/li>
&lt;li>使用&lt;code>RunnableLambda&lt;/code>把自定义函数format_func转换成runnable，并作为组件加入到工作链service_chain中。&lt;/li>
&lt;li>使用相同的用户问题调用工作链service_chain，输出结果为字典。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120121343812.png" alt="">&lt;/p>
&lt;h2 id="step3-构建路由链由3个分支链组成物流产品和默认">STEP3: 构建路由链：由3个分支链组成(物流、产品和默认)&lt;/h2>
&lt;ul>
&lt;li>先创建3个分支链，用于回答产品质量、物流等不同类别的问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120122633665.png" alt="">&lt;/p>
&lt;ul>
&lt;li>利用&lt;code>RunnableBranch&lt;/code>构造客服路由链anwser_routing_chain：
&lt;ul>
&lt;li>当用户问题类别为产品质量时，走产品客服链prod_response&lt;/li>
&lt;li>当用户问题类别为物流时，走物流客服链logistic_response链&lt;/li>
&lt;li>当用户问题类别不是上面两类时，走默认链general_response&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120123827219.png" alt="">&lt;/p>
&lt;h2 id="step4-组成完整工作链">STEP4: 组成完整工作链&lt;/h2>
&lt;ul>
&lt;li>现在，把工作链service_chain增加客服路由组件，组成完整工作链。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120124737826.png" alt="">&lt;/p>
&lt;ul>
&lt;li>使用不同的问题来测试，不同类别的问题都得到了对应领域客服的回复。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B038-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(4)/image-20240120124807112.png" alt="">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL提供的几种常用方法，可以帮助开发者灵活构建自己的Runnable组件，以支撑开发复杂的LLM应用。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnablePassthrough: 透传用户输入。通常与一个键组成键值对，用于构造下一个组件的输入。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnableLambda: 把自定义函数转换成Runnable对象，以便在组件中使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnableBranch: 类似路由功能，可以根据条件选择要运行的分支，即当条件满足时，则执行该分支。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>使用上述方法，构建了一个客服问答工作链，自动判断用户问题类别并路由到对应领域的客服。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记37-LangChain解读-LCEL语言之领域功能(3)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD3/</link><pubDate>Mon, 08 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD3/</guid><description>&lt;p>LCEL 可以轻松的构建复杂的链，并灵活调用它们，是因为LCEL基于&lt;code>Runnable&lt;/code>协议封装了一系列组合方法和调用接口。&lt;/p>
&lt;p>之前我们介绍了LCEL的核心调用接口&lt;code>invoke/ainvoke&lt;/code>、&lt;code>stream/astream&lt;/code>、&lt;code>batch/abatch&lt;/code>，本篇开始我们来学习LCEL的组合方法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240105205805572.png" alt="">&lt;/p>
&lt;h1 id="1-lcel的组合原语">1. LCEL的组合原语&lt;/h1>
&lt;p>LCEL的主要组合原语有两种：**RunnableSequence(串行组合)**和 &lt;strong>RunnableParallel( 并行组合)&lt;/strong>。&lt;/p>
&lt;h2 id="1runnablesequence-串行组合">(1)RunnableSequence: 串行组合&lt;/h2>
&lt;p>&lt;strong>方法简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableSequence可以将多个链组成一个串行工作流，各链串行执行。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个链的输出是下个链的输入，最后一个链的输出就是该整个工作流的输出。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>组合语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableSequence的原始调用语法如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableSequence对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableSequence&lt;span class="o">(&lt;/span>&lt;span class="nv">first&lt;/span>&lt;span class="o">=&lt;/span>组件1, &lt;span class="nv">middle&lt;/span>&lt;span class="o">=[&lt;/span>组件2, 组件3, ...&lt;span class="o">]&lt;/span>, &lt;span class="nv">last&lt;/span>&lt;span class="o">=&lt;/span>组件n&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableSequence在构建串行工作流的示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">串行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableSequence&lt;span class="o">(&lt;/span>&lt;span class="nv">first&lt;/span>&lt;span class="o">=&lt;/span>chain1, &lt;span class="nv">middle&lt;/span>&lt;span class="o">=[&lt;/span>chain2, chain3, ...&lt;span class="o">]&lt;/span>, &lt;span class="nv">last&lt;/span>&lt;span class="o">=&lt;/span>chainn&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>LCEL实现了用管道符&lt;code>|&lt;/code>来做串行组合的连接符。我们之前的案例都是在用这种方式来做串行组合。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">串行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> chain1 &lt;span class="p">|&lt;/span> chain2 &lt;span class="p">|&lt;/span> ... &lt;span class="p">|&lt;/span> chainn&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="2runnableparallel-并行组合">(2)RunnableParallel: 并行组合&lt;/h2>
&lt;p>&lt;strong>方法简介&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>RunnableParallel可以把多个链组成并行工作流，各链并行执行。&lt;/li>
&lt;li>各链的输入是相同的，整个工作流的输出是用各链的输出构造的字典。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>组合语法&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RunnableParallel的调用语法如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableParallel对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">({&lt;/span>&lt;span class="s2">&amp;#34;Key1&amp;#34;&lt;/span>: 组件1, &lt;span class="s2">&amp;#34;Key2&amp;#34;&lt;/span>: 组件2, ..., &lt;span class="s2">&amp;#34;Keyn&amp;#34;&lt;/span>: 组件n&lt;span class="o">})&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>由于&lt;code>Runnable&lt;/code>支持类型转换，所以也可以这样调用：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">RunnableParallel对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">(&lt;/span>&lt;span class="nv">Key1&lt;/span>&lt;span class="o">=&lt;/span>组件1, &lt;span class="nv">Key2&lt;/span>&lt;span class="o">=&lt;/span>组件2, ..., &lt;span class="nv">Keyn&lt;/span>&lt;span class="o">=&lt;/span>组件n&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableParallel的输出是字典格式，是由原键+组件执行结果组成的键值对，示例如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">{&lt;/span>&lt;span class="s1">&amp;#39;Key1&amp;#39;&lt;/span>: 组件1的结果, &lt;span class="s1">&amp;#39;Key2&amp;#39;&lt;/span>: 组件2的结果, ..., &lt;span class="s1">&amp;#39;Keyn&amp;#39;&lt;/span>: 组件n的结果&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>
&lt;p>RunnableParallel构建工作流的示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">并行工作流对象&lt;/span> &lt;span class="o">=&lt;/span> RunnableParallel&lt;span class="o">({&lt;/span>&lt;span class="s2">&amp;#34;Key1&amp;#34;&lt;/span>: chain1, &lt;span class="s2">&amp;#34;Key2&amp;#34;&lt;/span>: chain2, ..., &lt;span class="s2">&amp;#34;Keyn&amp;#34;&lt;/span>: chainn&lt;span class="o">})&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="3runnablesequence和runnableparallel的组合使用">(3)RunnableSequence和RunnableParallel的组合使用&lt;/h2>
&lt;p>遵循LCEL &lt;code>Runnable&lt;/code>协议的组件都可以使用上述两种方式组合。同时，由于组合成的链也是一个Runnable对象，这就意味着在一个链里可以混合使用RunnableSequence和RunnableParallel两种组合方式，进而实现了串并结合的工作流。&lt;/p>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>下面通过一个实例来看下RunnableSequence和RunnableParallel的用法。&lt;/p>
&lt;p>示例场景：利用LLM做病情摘要，然后提供治疗建议，最后把结果翻译成英文和日语。&lt;/p>
&lt;p>我们用4个链实现上述功能，然后用RunnableSequence和RunnableParallel组合成完整工作流。&lt;/p>
&lt;h2 id="step1-构建chain1病情简介">STEP1: 构建chain1：病情简介&lt;/h2>
&lt;ul>
&lt;li>引入相关类库，构建第一个链chain1，任务是根据疾病标题和患者年龄生成病情简介。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195544772.png" alt="">&lt;/p>
&lt;ul>
&lt;li>先执行chain1观察下输出结果，输出内容符合预期。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195656025.png" alt="">&lt;/p>
&lt;h2 id="step2-构建chain2治疗方案">STEP2: 构建chain2：治疗方案&lt;/h2>
&lt;ul>
&lt;li>构建第二个链chain2，任务是根据病情简介生成治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108202230067.png" alt="">&lt;/p>
&lt;ul>
&lt;li>执行chain2观察输出结果，LLM给出了治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108195753143.png" alt="">&lt;/p>
&lt;h2 id="step3构建chain3chain4文本翻译">STEP3:构建Chain3、Chain4：文本翻译&lt;/h2>
&lt;ul>
&lt;li>构建Chain3、Chain4，任务是把指定的文本内容分别翻译成英文、日语&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108192549849.png" alt="">&lt;/p>
&lt;h2 id="step4-构建工作流链medical_chain组合chain1chain4">STEP4: 构建工作流链medical_chain：组合chain1~chain4&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>基于上面4个任务链，工作流设计为：首先生成病情简介，然后根据病情生成治疗方案，最后把治疗方案同时翻译成英语、日语。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>因此，工作流链medical_chain组合如下：&lt;/p>
&lt;ul>
&lt;li>chain1、chain2与后面的翻译链(chain3、chain4)是串行组合&lt;/li>
&lt;li>chain3、chain4使用RunnableParallel组合，把chain2的输出作为输入，并行执行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108200421256.png" alt="">&lt;/p>
&lt;ul>
&lt;li>执行medical_chain观察结果，得到了英语和日语的治疗方案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B037-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(3)/image-20240108200458628.png" alt="">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本篇介绍了LCEL的两种主要的组合原语：**RunnableSequence(串行组合)**和 &lt;strong>RunnableParallel( 并行组合)&lt;/strong>。&lt;/p>
&lt;p>关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>RunnableSequence&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>可以将多个链组成一个串行工作流，各链串行执行。支持用管道符&lt;code>|&lt;/code>组合。&lt;/li>
&lt;li>每个链的输出是下个链的输入，最后一个链的输出就是该整个工作流的输出。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RunnableParallel&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>可以把多个链组成并行工作流，各链并行执行。&lt;/li>
&lt;li>各链的输入是相同的，整个工作流的输出是用各链的输出构造的字典。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>用RunnableSequence和RunnableParallel构建了一个诊疗工作流。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记36-LangChain解读-LCEL语言之领域功能(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD2/</link><pubDate>Wed, 03 Jan 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD2/</guid><description>&lt;p>并发是LLM应用常见的使用场景，LCEL提供了非常简便的方式来实现并发处理。&lt;/p>
&lt;p>本篇我们就来学习LCEL这一核心特性——batch/abatch。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104090138975.png" alt="">&lt;/p>
&lt;h1 id="1-lcel并发接口-batchabatch">1. LCEL并发接口-batch/abatch&lt;/h1>
&lt;h2 id="1接口介绍">(1)接口介绍&lt;/h2>
&lt;p>batch和abatch是两个非常实用的接口，它们允许并发处理多个用户输入，从而提高程序的执行效率。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>batch&lt;/strong>: 开发LLM应用时，有可能需要同时处理多个用户输入(如：多个提示词)，batch内部通过线程池并行处理多个用户输入(每个线程都是在调用上一篇介绍的&lt;code>invoke&lt;/code>接口)，最终汇总结果。&lt;/li>
&lt;li>&lt;strong>abatch&lt;/strong>: batch的非阻塞接口。&lt;/li>
&lt;/ul>
&lt;h2 id="2接口语法">(2)接口语法&lt;/h2>
&lt;p>batch/abatch是LCEL在Runnable协议中定义的标准接口，LangChain的组件和链(Runnable对象)可以直接调用该方法。&lt;/p>
&lt;p>以链chain为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>chain.batch([&amp;ldquo;input1&amp;rdquo;, &amp;ldquo;input2&amp;rdquo;, &amp;hellip;])&lt;/p>
&lt;/li>
&lt;li>
&lt;p>await chain.abatch([&amp;ldquo;input1&amp;rdquo;, &amp;ldquo;input2&amp;rdquo;, &amp;hellip;])&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以利用LLM做试题解析为例，用batch/abatch功能来批量处理多个试题。&lt;/p>
&lt;ul>
&lt;li>构造任务链&lt;/li>
&lt;li>使用batch方法处理&lt;/li>
&lt;li>使用abatch方法处理&lt;/li>
&lt;/ul>
&lt;h2 id="step1-构造任务链">STEP1: 构造任务链&lt;/h2>
&lt;p>任务链的构造方法不再赘述，见下面代码。&lt;/p>
&lt;ul>
&lt;li>链的任务是判断题目是否正确。&lt;/li>
&lt;li>设置列表变量topics，存放3个题目。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084034633.png" alt="">&lt;/p>
&lt;h2 id="step2-使用batch方法处理">STEP2: 使用batch方法处理&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.batch(topics)&lt;/li>
&lt;li>执行效果：任务启动后，马上得到答案，没有因为问题多导致过多等待。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084838085.png" alt="">&lt;/p>
&lt;h2 id="step3-使用abatch方法处理">STEP3: 使用abatch方法处理&lt;/h2>
&lt;ul>
&lt;li>调用语法：await.chain.abatch(topics)&lt;/li>
&lt;li>执行效果：任务启动后，马上得到答案。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104084749830.png" alt="">&lt;/p>
&lt;h1 id="3batch-vs-invoke">3.Batch vs Invoke&lt;/h1>
&lt;p>看到这里，我们再来回顾一下本文的&lt;code>batch&lt;/code>和前文的&lt;code>invoke&lt;/code>的区别：&lt;/p>
&lt;ul>
&lt;li>&lt;code>batch&lt;/code>不仅实现了批量处理多个输入，而且是多线程并行调用LLM，很好的提升了处理效率。&lt;/li>
&lt;li>而&lt;code>invoke&lt;/code>更适合需要串行执行。&lt;/li>
&lt;/ul>
&lt;p>具体看前文的例子，我们发现耗时的差别如下：&lt;/p>
&lt;ul>
&lt;li>使用invoke处理单个题目，耗时1.13s&lt;/li>
&lt;li>使用batch处理3个题目，耗时1.3s&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B036-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(2)/image-20240104010153222.png" alt="">&lt;/p>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>Batch/abatch 是LCEL中用于实现并发请求的重要工具，也为开发人员带来高效、简洁的编程体验。&lt;/p>
&lt;p>本文关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>LCEL并发接口-batch/abatch&lt;/p>
&lt;ul>
&lt;li>输入为列表，内部实际是使用线程池执行器&lt;strong>并行&lt;/strong>执行invoke()。&lt;/li>
&lt;li>LangChain的Runnable对象可以直接调用batch/abatch方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>以让LLM来批改试卷为例，演示batch/abatch语法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记35-LangChain解读-LCEL语言之领域功能(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD1/</link><pubDate>Wed, 27 Dec 2023 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD1/</guid><description>&lt;p>LCEL是专门为方便开发LLM应用而设计的编程语言，它提供了一系列直观、好用的功能和语法。&lt;/p>
&lt;p>本篇我们来学习提升LLM应用体验的一个功能——stream流式输出。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227115947187.png" alt="image-20231227115947187">&lt;/p>
&lt;h1 id="1-lcel流式输出接口-stream">1. LCEL流式输出接口-stream&lt;/h1>
&lt;h2 id="1为什么要用流式输出">(1)为什么要用流式输出&lt;/h2>
&lt;p>在使用LLM应用时，用户总是期望立即得到答案，不管他的问题有多么复杂，或者让LLM生成的文本有多么长。&lt;/p>
&lt;p>&amp;ldquo;&lt;strong>流式输出&lt;/strong>&amp;ldquo;这时就派上用场了，它能够&lt;strong>使LLM在生成文本时逐步提供结果&lt;/strong>，而不是等到整个文本生成完成后再一次性返回给用户。&lt;/p>
&lt;p>流式输出的方式有以下几个优点：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>实时性&lt;/strong>：不用等待整个文本生成完毕再返回结果，减少响应时间，用户获得实时响应体验。&lt;/li>
&lt;li>&lt;strong>交互性&lt;/strong>：生成结果逐步提供，可匹配用户阅读速度，增强交互感。&lt;/li>
&lt;li>&lt;strong>节省资源和带宽&lt;/strong>：逐步返回生成的结果可以减少对网络带宽的需求。&lt;/li>
&lt;li>&lt;strong>支持长文本生成&lt;/strong>：生成长文本时无需担心超出内存限制。&lt;/li>
&lt;/ol>
&lt;p>总之，流式输出可以让用户得到更好的体验，让用户觉得你的LLM程序&amp;rdquo;&lt;strong>性能&lt;/strong>&amp;ldquo;很棒。&lt;/p>
&lt;h2 id="2流式输出接口的使用">(2)流式输出接口的使用&lt;/h2>
&lt;p>LangChain在Runnable协议中定义了流式输出方法&lt;strong>stream&lt;/strong>，以及它的异步方法&lt;strong>astream&lt;/strong>。&lt;/p>
&lt;p>LangChain的很多组件都是遵循Runnable的对象，可以直接调用stream/astream方法。以之前专栏多次构建过的链chain为例：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>chain.stream()&lt;/p>
&lt;/li>
&lt;li>
&lt;p>chain.astream()&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>以让LLM生成一首诗为例，对比下直接输出、流式输出的差异效果。&lt;/p>
&lt;p>任务链的构建不再赘述，如下代码，我们构建了名为&lt;code>chain&lt;/code>的链，它的功能是根据输入的主题生成一首诗。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227125844993.png" alt="image-20231227125844993">&lt;/p>
&lt;h2 id="1使用invoke方法直接调用">(1)使用invoke方法直接调用&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.invoke(&amp;ldquo;犀牛&amp;rdquo;)&lt;/li>
&lt;li>执行效果：任务启动后，没有立即得到响应；大概6秒后，生成结果一次性闪现。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/invoke.gif" alt="invoke">&lt;/p>
&lt;h2 id="2使用stream方法流式输出">(2)使用stream方法流式输出&lt;/h2>
&lt;ul>
&lt;li>调用语法：chain.stream(&amp;ldquo;犀牛&amp;rdquo;)
&lt;ul>
&lt;li>注意：由于stream()是迭代输出，因此用for语句来循环遍历输出结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>执行效果：任务启动后，立即得到响应，以一定的字频逐行输出结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/stream.gif" alt="stream">&lt;/p>
&lt;ul>
&lt;li>如果想用异步方式astream，则调用语法如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227145757283.png" alt="image-20231227145757283">&lt;/p>
&lt;h2 id="3-对比总结-invoke-vs-stream">(3) 对比总结 Invoke vs Stream&lt;/h2>
&lt;p>对于一个交互式的LLM应用程序，stream流式输出的响应实时性和交互体验都更好。&lt;/p>
&lt;p>在 Python 中，没有名为&amp;quot;stream&amp;rdquo; 的内置方法，但是有类似于流式操作的机制，比如生成器(yield)、迭代器(iterator)。所以，如果没有LCEL提供的stream方法，而是要我们自己来实现的话，将是一件费脑筋的事。&lt;/p>
&lt;p>如下是不使用LCEL实现流式输出的示例代码，可以跟上面简单的一行方法调用对比下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B035-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%8A%9F%E8%83%BD(1)/image-20231227124359421.png" alt="image-20231227124359421">&lt;/p>
&lt;p>LangChain的大牛们用python实现了与Java &amp;ldquo;Stream API&amp;quot;类似的流式处理方法，极大的方便了开发人员。&lt;/p>
&lt;p>某种意义上来说，LangChain不仅仅是定义了一套Runnable协议，也是对Python语法的重构，构建了自己的语法——LCEL。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain LCEL语法中提升性能的一种方法——stream流式输出。关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>流式输出接口&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;流式输出&amp;quot;使LLM在生成文本时逐步提供结果，提升LLM应用程序的性能，让用户得到更好的体验。&lt;/li>
&lt;li>LangChain的Runnable对象可以直接调用stream/astream方法。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>对比invoke直接输出和stream流式输出的效果差异。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记34-Show一下我们的语音克隆技术</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B034-show%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%AD%E9%9F%B3%E5%85%8B%E9%9A%86%E6%8A%80%E6%9C%AF/</link><pubDate>Sun, 24 Dec 2023 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B034-show%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%AD%E9%9F%B3%E5%85%8B%E9%9A%86%E6%8A%80%E6%9C%AF/</guid><description>&lt;p>小刚刚同学是小伙伴中的TTS专家，他训练的语音克隆模型已经初见雏形，口音、语速、情绪都还不错。&lt;/p>
&lt;p>小刚刚的AI模型克隆了他自己的声音之后为我们念出了如下文章，听到人工智能的声音，激动且开心，为小刚刚的AI儿子点赞！&lt;/p>
&lt;ul>
&lt;li>&lt;strong>这是AI的念稿的声音&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.bilibili.com/audio/au4209312?type=1">B站链接&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>如下为AI念的稿件&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>接下来，我给大家介绍一下TTS原理：&lt;/p>
&lt;h2 id="1-引言">1. 引言&lt;/h2>
&lt;p>文字到语音（TTS，Text-to-Speech）技术是将人类语言文本转换为人类语音输出的技术。随着人工智能、自然语言处理等技术的快速发展，TTS技术在智能语音助手、虚拟主播、教育、娱乐等领域得到了广泛应用。本文将介绍TTS技术的原理及其发展历程，并探讨其在未来的发展趋势。&lt;/p>
&lt;h2 id="2-tts原理">2. TTS原理&lt;/h2>
&lt;h3 id="21-语音合成">2.1 语音合成&lt;/h3>
&lt;p>语音合成是将文本转换为语音的过程，主要包括以下几个步骤：&lt;/p>
&lt;ol>
&lt;li>音素到状态的转换：将输入的音素序列转换为声道状态序列。&lt;/li>
&lt;li>声道状态到声码元的转换：将声道状态序列转换为声码元序列。&lt;/li>
&lt;li>声码元到语音的转换：将声码元序列转换为语音信号。&lt;/li>
&lt;/ol>
&lt;h3 id="22-语音合成模型">2.2 语音合成模型&lt;/h3>
&lt;p>目前主流的语音合成模型主要包括以下几种：&lt;/p>
&lt;ol>
&lt;li>参数模型：将语音合成看作是一个参数估计问题，通过训练模型来获得参数值。&lt;/li>
&lt;li>统计模型：基于统计学原理，通过概率模型来生成语音。&lt;/li>
&lt;li>深度学习模型：利用深度神经网络模型进行语音合成。&lt;/li>
&lt;/ol>
&lt;h3 id="23-声学模型">2.3 声学模型&lt;/h3>
&lt;p>声学模型是TTS技术中的关键部分，其主要任务是模拟人类听觉系统，通过声学模型可以计算出每个音素的声学特征，并将其用于语音合成。目前主流的声学模型包括线性预测编码（LPC）、高斯混合模型（GMM）等。&lt;/p>
&lt;h2 id="3-tts发展历程">3. TTS发展历程&lt;/h2>
&lt;p>TTS技术的发展历程可以分为以下几个阶段：&lt;/p>
&lt;ol>
&lt;li>基于规则的方法：早期的TTS技术采用基于规则的方法，通过手动设计规则来生成语音。&lt;/li>
&lt;li>基于模板的方法：基于模板的方法通过预先定义的语音模板来生成语音，效率较低。&lt;/li>
&lt;li>基于统计的方法：基于统计的方法采用概率模型来生成语音，效果较好，但需要大量的训练数据。&lt;/li>
&lt;li>基于深度学习的方法：基于深度学习的方法利用神经网络模型进行语音合成，效果最好，但需要大量的训练数据和计算资源。&lt;/li>
&lt;/ol>
&lt;h2 id="4-tts未来发展趋势">4. TTS未来发展趋势&lt;/h2>
&lt;p>随着人工智能、自然语言处理等技术的不断发展，TTS技术在未来将会呈现出以下发展趋势：&lt;/p>
&lt;ol>
&lt;li>更高的语音质量：通过改进声学模型和语音合成算法，提高语音质量。&lt;/li>
&lt;li>更自然的发音：通过改进语音合成算法，使生成的语音更加自然。&lt;/li>
&lt;li>更丰富的语言支持：通过扩大语言模型和语音合成模型的训练数据集，支持更多的语言。&lt;/li>
&lt;li>更广泛的应用：通过改进TTS技术，使其在更多的领域得到应用，如智能客服、智能家居等。&lt;/li>
&lt;/ol>
&lt;h2 id="5-结论">5. 结论&lt;/h2>
&lt;p>TTS技术是将文本转换为语音的技术，其原理主要包括语音合成、语音合成模型、声学模型等。随着人工智能、自然语言处理等技术的不断发展，TTS技术在未来将会呈现出更高的语音质量、更自然的发音、更丰富的语言支持和更广泛的应用等特点。&lt;/p></description></item><item><title>【chatGPT】学习笔记33-LangChain解读-LCEL语言之基础语法(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%952/</link><pubDate>Sat, 23 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%952/</guid><description>&lt;p>LCEL让我们可以用&lt;code>|&lt;/code>管道符把不同的组件或链组合起来，这都得益于这些组件都实现了Runnable接口。&lt;/p>
&lt;p>本篇我们就带着Runnable的概念继续学习下LCEL的基础用法。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231222104906413.png" alt="image-20231222104906413">&lt;/p>
&lt;h1 id="1-lcel核心接口runnable">1. LCEL核心接口Runnable&lt;/h1>
&lt;h2 id="1runnable接口">(1)Runnable接口&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Langchain定义了“Runnable”协议，协议中实现了一系列方法，比如对管道运算符&lt;code>|&lt;/code>的支持、invoke方法等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这里介绍下Runnable定义的一组标准调用接口（其中后三个是前面三个异步方式）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#stream">&lt;code>stream&lt;/code>&lt;/a>：流式输出&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#invoke">&lt;code>invoke&lt;/code>&lt;/a>：基于单一输入调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#batch">&lt;code>batch&lt;/code>&lt;/a>：基于列表输入调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-stream">&lt;code>astream&lt;/code>&lt;/a>：异步流式输出&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-invoke">&lt;code>ainvoke&lt;/code>&lt;/a>：基于单一输入异步调用链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://python.langchain.com/docs/expression_language/interface#async-batch">&lt;code>abatch&lt;/code>&lt;/a>：基于列表输入异步调用链&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>LangChain的很多组件(对象)都应用了Runnable协议，可以把他们称为Runnable对象，这些对象都实现了上述接口，因此这些对象组成的链也都支持上述调用方法。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2常用runnable对象的输入输出类型">(2)常用Runnable对象的输入输出类型&lt;/h2>
&lt;p>LCEL中使用&lt;code>|&lt;/code>将前一个Runnable对象的输出传递给下一个Runnable对象作为输入，因此需要保证管道两端的型号一致。&lt;/p>
&lt;p>LangChain常用组件的输入、输出数据类型如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>组件&lt;/strong>&lt;/th>
&lt;th>&lt;strong>输入类型&lt;/strong>&lt;/th>
&lt;th>&lt;strong>输出类型&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Prompt (提示词)&lt;/td>
&lt;td>字典&lt;/td>
&lt;td>PromptValue&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatModel (聊天模型)&lt;/td>
&lt;td>单个字符串、聊天消息列表或 PromptValue&lt;/td>
&lt;td>ChatMessage&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLM (非聊天模型)&lt;/td>
&lt;td>单个字符串、聊天消息列表或 PromptValue&lt;/td>
&lt;td>String&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OutputParser (输出解析器)&lt;/td>
&lt;td>LLM 或 ChatModel 的输出&lt;/td>
&lt;td>取决于解析器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Retriever (检索器 )&lt;/td>
&lt;td>单个字符串&lt;/td>
&lt;td>文档清单&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tool (工具)&lt;/td>
&lt;td>单个字符串或字典，具体取决于工具&lt;/td>
&lt;td>取决于工具&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="3-runnablepassthrough在管道中的作用">(3) RunnablePassthrough在管道中的作用&lt;/h2>
&lt;p>在使用LCEL构建链时，原始用户输入可能不仅要传给第一个组件，还要传给后续组件，这时可以用RunnablePassthrough。RunnablePassthrough可以透传用户输入。&lt;/p>
&lt;h1 id="2快速上手">2.快速上手&lt;/h1>
&lt;p>下面我们通过一个包含4个组件的RAG检索链来再次体验LCEL的便捷。&lt;/p>
&lt;p>示例场景是基于给定的文本回答用户问题。&lt;/p>
&lt;h2 id="step1环境准备">STEP1.环境准备&lt;/h2>
&lt;ul>
&lt;li>注意：由于示例的是检索本地文本，所以安装了向量数据库及其所需的库。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223004634709.png" alt="image-20231223004634709">&lt;/p>
&lt;h2 id="step2构建检索器组件">STEP2.构建检索器组件&lt;/h2>
&lt;ul>
&lt;li>为方便演示，本地文本简单的放一句话：”大鱼吃小鱼，小鱼吃虾米“。&lt;/li>
&lt;li>用langchain的检索器&lt;code>as_retriever&lt;/code>，它可以根据输入查询向量库并返回相关文本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223005234058.png" alt="image-20231223005234058">&lt;/p>
&lt;h2 id="step3构建prompt提示词组件">STEP3.构建Prompt提示词组件&lt;/h2>
&lt;ul>
&lt;li>提示词设计的是根据给定的文字回答问题。&lt;/li>
&lt;li>提示词中共2个变量：&lt;code>question&lt;/code>用来接收用户问题，&lt;code>context&lt;/code>来接收检索器的查询结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223010525145.png" alt="image-20231223010525145">&lt;/p>
&lt;h2 id="step4构建llm组件">STEP4.构建llm组件&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011307090.png" alt="image-20231223011307090">&lt;/p>
&lt;h2 id="step5组合成链">STEP5.组合成链&lt;/h2>
&lt;ul>
&lt;li>把检索器、提示词、模型、输出解析器四个组件串起来，命名为检索链&lt;code>retrieval_chain&lt;/code>。&lt;/li>
&lt;li>这一步注意两点：
&lt;ul>
&lt;li>用户输入的问题，不止组件1的检索器要用，组件2也要用它来构建提示词，因此组件1使用RunnablePassthrough方法把原始输入透传给下一步。&lt;/li>
&lt;li>由于组件2 prompt的输入要求是字典类型，所以组件1把检索器和用户问题写成字典格式，并用组件2的变量作为键。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011350292.png" alt="image-20231223011350292">&lt;/p>
&lt;h2 id="step6运行链">STEP6.运行链&lt;/h2>
&lt;ul>
&lt;li>传入用户问题，得到期望结果。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B033-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(2)/image-20231223011640703.png" alt="image-20231223011640703">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain LCEL及其Runnable基础语法，关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Runnable接口&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Runnable是Langchain表达式语言的核心接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Runnable提供了一组标准调用接口：stream、invoke、batch及对应的异步方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Langchain很多组件都遵循Runnable协议，因此可以方便的组合、调用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>常用Runnable对象的输入输出类型要求，用管道&lt;code>|&lt;/code>方式组合时需注意管道两端的类型一致。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RunnablePassthrough在管道中的作用。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>构建了包含四个组件的检索链。&lt;/li>
&lt;li>通过实操关注了管道前后数据类型的匹配、RunnablePassthrough的作用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记32-LangChain解读-LCEL语言之基础语法(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%951/</link><pubDate>Wed, 13 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-langchain%E8%A7%A3%E8%AF%BB-lcel%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%951/</guid><description>&lt;p>在《【chatGPT】学习笔记29-LangChain解读1-快速入门》中，我们学习了LangChain的入门文档，了解到LangChain的模块化组件和链让开发LLM应用变得简单。&lt;/p>
&lt;p>同时，LangChain还推出了自己的语法——LCEL(LangChain表达式语言)，让复杂的组合链变得更简单。&lt;/p>
&lt;p>本篇我们跟着官方文档来学习LCEL的概念及如何使用。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231208175706824.png" alt="image-20231208175706824">&lt;/p>
&lt;h1 id="1-lcel概览">1. LCEL概览&lt;/h1>
&lt;h2 id="1什么是lcel">(1)什么是LCEL&lt;/h2>
&lt;p>LCEL是LangChain表达式语言(LangChain Expression Language)的缩写，是LangChain官方推出的一种新的语法，它提供了一种声明式的方法(而不是编写普通代码)来组合链，简化了构建复杂LLM应用的过程。&lt;/p>
&lt;h2 id="2为什么要用lcel">(2)为什么要用LCEL&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>声明式编程&lt;/strong>：由于LLM强大的理解和生成能力，LLM应用开发侧重业务逻辑的编排，LCEL提供的声明式编程方法让这类操作更高效。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>统一接口&lt;/strong>：LCEL中实现了“Runnable”协议，每个LCEL 对象都应用了“Runnable”接口。&lt;/p>
&lt;ul>
&lt;li>该接口定义了一组通用的调用方法（&lt;code>invoke&lt;/code>、&lt;code>batch&lt;/code>、&lt;code>stream&lt;/code>、&lt;code>ainvoke&lt;/code>&amp;hellip;），因此采用LCEL构建的任何链都将自动支持流、同步、异步和批处理等能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>组合原语&lt;/strong>：LCEL 提供了许多原语，可以轻松组合链、并行化组件、添加回退、动态配置链内部等。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为了更好地理解 LCEL，我们将分期介绍这些能力和实际应用效果。本篇我们先从看看如何用LCEL的声明式方法来组合业务组件和链。&lt;/p>
&lt;h1 id="2快速上手lcel">2.快速上手LCEL&lt;/h1>
&lt;p>以让LLM生成文本为例，来看看如何使用LCEL来完成任务。&lt;/p>
&lt;blockquote>
&lt;p>环境准备请参考《【chatGPT】学习笔记29-LangChain解读1-快速入门》。&lt;/p>
&lt;/blockquote>
&lt;h2 id="step1构造三组件">STEP1.构造三组件&lt;/h2>
&lt;p>我们在LangChain快速入门中学习过，一个常见的链包括三个组件：&lt;strong>提示词模板、模型、输出解析器&lt;/strong>。&lt;/p>
&lt;p>使用LangChain的方法构建如下——任务是给某个人或物写一句表扬的话：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210112756607.png" alt="image-20231210112756607">&lt;/p>
&lt;h2 id="step2构建自定义链">STEP2.构建自定义链&lt;/h2>
&lt;p>使用LCEL语法，用上面的3个组件组合成链，代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210113216116.png" alt="image-20231210113216116">&lt;/p>
&lt;p>你会发现，LCEL用&lt;code>|&lt;/code>把不同组件链接在一起，该符号类似于unix的管道运算符，将一个组件的输出作为下一个组件的输入。&lt;/p>
&lt;h2 id="step3调用链">STEP3.调用链&lt;/h2>
&lt;p>LCEL这种表达式方法是有效呢，我们调用一下看看。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B032-LangChain%E8%A7%A3%E8%AF%BB-LCEL%E8%AF%AD%E8%A8%80%E4%B9%8B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95(1)/image-20231210113642196.png" alt="image-20231210113642196">&lt;/p>
&lt;p>让链工作的方法也很简单，使用&lt;code>Runnable&lt;/code>接口的&lt;code>invoke&lt;/code>方法，传入主题字符串，任务就完成了。&lt;/p>
&lt;p>从上面的示例，我们可以看到LCEL的运作流程如下：&lt;/p>
&lt;ul>
&lt;li>传入用户输入，为提示词模板中的变量赋值。本例中是山姆奥特曼，则格式为{&amp;ldquo;topic&amp;rdquo;: &amp;ldquo;山姆奥特曼&amp;rdquo;}。&lt;/li>
&lt;li>&lt;code>prompt&lt;/code>获取用户输入，并使用&amp;quot;topic&amp;quot;构建提示词。&lt;/li>
&lt;li>&lt;code>model&lt;/code>组件拿到生成的提示词，并传递给LLM模型进行处理。模型生成的输出是一个&lt;code>ChatMessage&lt;/code>对象。&lt;/li>
&lt;li>最后，&lt;code>output_parser&lt;/code>组件接收&lt;code>ChatMessage&lt;/code>并将其转换为 Python 字符串，该字符串通过 invoke 方法返回。&lt;/li>
&lt;/ul>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain官方文档的“LangChain Expression Language (LCEL)”的部分章节，关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>LCEL概览&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>LCEL是LangChain官方推出的一种声明式编程语法，让构建复杂LLM应用变得更简单。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LCEL的主要价值点：&lt;/p>
&lt;ul>
&lt;li>声明式编程方法，便于组件和链的编排。&lt;/li>
&lt;li>统一接口，每个LCEL 对象都应用了“Runnable”接口。&lt;/li>
&lt;li>组合原语。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>构造链的组件。&lt;/li>
&lt;li>LCEL使用&lt;code>|&lt;/code>把不同的组件组合成链。&lt;/li>
&lt;li>使用invoke方法调用链，完成任务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>参考文档：翻译的LangChain官方文档&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记31-提示词解读6-实战案例之扩展</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/</link><pubDate>Sun, 03 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/</guid><description>&lt;p>LLM可以帮忙写文案、写剧本、写论文？相信很多小伙伴当初都是被LLM的这个爆裂功能路转粉的。&lt;/p>
&lt;p>这种文本生成的能力——通常也被叫做AIGC(人工智能生成内容)，就是LLM的扩展能力。&lt;/p>
&lt;p>本篇我们跟着吴恩达老师的课程，学习如何激发LLM这个最受欢迎的能力——扩展(&lt;code>Expanding&lt;/code>)。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95//image-20231130080342036.png" alt="image-20231130080342036">&lt;/p>
&lt;h1 id="1激发扩展能力的提示词">1.激发扩展能力的提示词&lt;/h1>
&lt;h2 id="1什么是llm的扩展能力">(1)什么是LLM的扩展能力&lt;/h2>
&lt;p>LLM的扩展能力是指&lt;strong>基于一小段文字或指令或主题，让LLM生成连贯、有逻辑的长文本&lt;/strong>。&lt;/p>
&lt;p>LLM的扩展能力使得LLM可以生成更长、更丰富的文本，可以用在很多创作类的工作：&lt;/p>
&lt;ul>
&lt;li>写营销文案&lt;/li>
&lt;li>写工作报告&lt;/li>
&lt;li>创意策划(Brainstorming)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h2 id="2如何激发llm的扩展能力">(2)如何激发LLM的扩展能力&lt;/h2>
&lt;p>我们可以这样构建提示词：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>明确的指令词&lt;/strong>，让LLM知道是写文章还是出点子，如&amp;quot;请&lt;strong>撰写&lt;/strong>&amp;hellip;&amp;quot;，&amp;ldquo;请&lt;strong>设计&lt;/strong>&amp;hellip;&amp;quot;。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>明确的内容要求&lt;/strong>，如&amp;quot;以xxx&lt;strong>为主题&lt;/strong>&amp;quot;，&amp;ldquo;涵盖xxx&lt;strong>内容要点&lt;/strong>&amp;quot;，&amp;ldquo;针对xxx&lt;strong>举个例子&lt;/strong>&amp;quot;，&amp;ldquo;用新潮有趣的&lt;strong>文字风格&lt;/strong>&amp;quot;。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>提供背景信息&lt;/strong>，让LLM更好的模拟语境。如&amp;quot;你是个童话大王&amp;rdquo;，&amp;ldquo;这是给总裁的汇报材料&amp;rdquo;。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2实战案例">2.实战案例&lt;/h1>
&lt;p>笔者最近在研究Java编程语言的面试题，要针对一些难度大的面试题编写案例解析。&lt;/p>
&lt;p>以下面这道面试题为例，用LLM的扩展能力来帮忙：&lt;/p>
&lt;ul>
&lt;li>制定提纲&lt;/li>
&lt;li>生成文章内容&lt;/li>
&lt;li>优化部分章节&lt;/li>
&lt;/ul>
&lt;p>面试题如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">以下有关垃圾收集器&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">说法正确的有&lt;/span>&lt;span class="err">：（&lt;/span> &lt;span class="err">）&lt;/span>
&lt;span class="n">A&lt;/span>&lt;span class="o">.&lt;/span> &lt;span class="n">ParNew收集器支持多线程垃圾收集&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">所以不会回收停顿&lt;/span>&lt;span class="err">。&lt;/span>&lt;span class="n">当老年代选择CMS收集器后&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">新生代智能选择Serial或ParNew收集器&lt;/span>&lt;span class="err">。&lt;/span>
&lt;span class="n">B&lt;/span>&lt;span class="o">.&lt;/span> &lt;span class="n">Parallel&lt;/span> &lt;span class="n">Scavenge收集器是一个新生代收集器&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">其目标是达到一个可控的吞吐量&lt;/span>&lt;span class="err">。&lt;/span>&lt;span class="n">MaxGCPauseMillis参数值设置越小&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">系统垃圾手机速度越快&lt;/span>&lt;span class="err">。&lt;/span>
&lt;span class="n">C&lt;/span>&lt;span class="o">.&lt;/span> &lt;span class="n">CMS收集器出生标记和重新标记阶段均需要停顿&lt;/span>&lt;span class="err">。&lt;/span>&lt;span class="n">CMS收集器若出现Concurrent&lt;/span> &lt;span class="n">Mode&lt;/span> &lt;span class="n">Failure&lt;/span>&lt;span class="err">，&lt;/span>&lt;span class="n">虚拟机就会启动Serial&lt;/span> &lt;span class="n">Old收集器进行垃圾回收&lt;/span>&lt;span class="err">。&lt;/span>
&lt;span class="n">D&lt;/span>&lt;span class="o">.&lt;/span> &lt;span class="n">G1收集器可用于新生代和老年代的垃圾回收&lt;/span>&lt;span class="err">。&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="step1制定提纲">STEP1.制定提纲&lt;/h2>
&lt;p>画虎先画骨。先跟LLM头脑风暴一下，把案例提纲定下来。&lt;/p>
&lt;p>让LLM生成题目解析案例的提纲。&lt;/p>
&lt;ul>
&lt;li>我的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231203161432017.png" alt="image-20231203161432017">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231203160613178.png" alt="image-20231203160613178">&lt;/p>
&lt;p>LLM给出了一个相对全面的目录结构，结合LLM带来的灵感，最终目录确定如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1.题目描述&lt;/strong>：描述Java面试题原文&lt;/li>
&lt;li>&lt;strong>2.题目解析&lt;/strong>：对每个选项进行分析，说明是否是正确答案&lt;/li>
&lt;li>&lt;strong>3.知识点解读&lt;/strong>：列出该Java试题涉及的知识点，并做解读&lt;/li>
&lt;li>&lt;strong>4.知识点总结&lt;/strong>：对知识点做总结，说明用途和错误影响&lt;/li>
&lt;li>&lt;strong>5.推荐学习资料&lt;/strong>：该知识点相关的学习资料&lt;/li>
&lt;/ul>
&lt;h2 id="step2生成文章内容">STEP2.生成文章内容&lt;/h2>
&lt;p>接着让LLM根据目录提纲生成案例主体内容。&lt;/p>
&lt;ul>
&lt;li>我的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231203161448434.png" alt="image-20231203161448434">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231201200439022.png" alt="image-20231201200439022">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231201232902361.png" alt="image-20231201232902361">&lt;/p>
&lt;p>LLM根据提纲很好的生成了案例内容，对试题四个选项的解析、正确答案的识别也很到位。&lt;/p>
&lt;h2 id="step3优化部分章节">STEP3.优化部分章节&lt;/h2>
&lt;p>知识点解读是重点章节，增加一些示例可以帮助读者更好的理解。&lt;/p>
&lt;p>所以，让LLM帮忙优化知识点解读章节的内容，增加代码示例：&lt;/p>
&lt;ul>
&lt;li>我的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231201233451643.png" alt="image-20231201233451643">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B031-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB6-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%89%A9%E5%B1%95/image-20231201233557314.png" alt="image-20231201233557314">&lt;/p>
&lt;p>LLM不仅精通自然语言，同时也是个编程语言专家，所以生成代码示例的任务也很轻松的完成了。&lt;/p>
&lt;h2 id="step4编写正式文稿">STEP4.编写正式文稿&lt;/h2>
&lt;p>LLM已经帮忙完成了文稿内容的生成和优化，现在该笔者我出马了。&lt;/p>
&lt;p>笔者的意见是——稿件内容审核通过，可以用于发布 😄&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文介绍了激发LLM扩展能力的提示词技巧，让LLM成为设计、写作等工作中的得力助手。&lt;/p>
&lt;p>扩展提示词的构建方法：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>明确的指令词&lt;/strong>，让LLM知道是写文章还是出点子，如&amp;quot;请&lt;strong>撰写&lt;/strong>&amp;hellip;&amp;quot;，&amp;ldquo;请&lt;strong>设计&lt;/strong>&amp;hellip;&amp;quot;。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>明确的内容要求&lt;/strong>，如&amp;quot;以xxx&lt;strong>为主题&lt;/strong>&amp;quot;，&amp;ldquo;涵盖xxx&lt;strong>内容要点&lt;/strong>&amp;quot;，&amp;ldquo;针对xxx&lt;strong>举个例子&lt;/strong>&amp;quot;，&amp;ldquo;用新潮有趣的&lt;strong>文字风格&lt;/strong>&amp;quot;。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>提供背景信息&lt;/strong>，让LLM更好的模拟语境。如&amp;quot;你是个童话大王&amp;rdquo;，&amp;ldquo;这是给总裁的汇报材料&amp;rdquo;。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>至此，我们已经完成了吴恩达老师提示词课程的学习，包括：&lt;/p>
&lt;ul>
&lt;li>提示词的基本原则&lt;/li>
&lt;li>提示词的四个组成要素，及通用使用技巧&lt;/li>
&lt;li>提示词实战，解锁LLM四大能力：总结、推理、转换、扩展&lt;/li>
&lt;/ul>
&lt;p>LLM是个&amp;quot;通才&amp;rdquo;，而且还在快速成长，提示词是我们与这个&amp;quot;通才&amp;quot;对话的主要接口。小伙伴们也都行动起来吧，用好提示词，轻松驾驭LLM。&lt;/p>
&lt;h1 id="4资料汇总">4.资料汇总&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>本技术专栏&lt;/strong>：
&lt;ul>
&lt;li>《【chatGPT】学习笔记24-提示词解读1-提示词基本概念》&lt;/li>
&lt;li>《【chatGPT】学习笔记25-提示词解读2-通用技巧》&lt;/li>
&lt;li>《【chatGPT】学习笔记27-提示词解读3-实战案例之摘要总结》&lt;/li>
&lt;li>《【chatGPT】学习笔记28-提示词解读4-实战案例之推理》&lt;/li>
&lt;li>《【chatGPT】学习笔记30-提示词解读5-实战案例之转换》&lt;/li>
&lt;li>《【chatGPT】学习笔记31-提示词解读6-实战案例之扩展》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>吴恩达提示词课程：
&lt;ul>
&lt;li>原文：https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction&lt;/li>
&lt;li>翻译：https://jherculesqz.gitbook.io/chatgpt-prompt-engineering-for-developers-1/&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记30-提示词解读5-实战案例之转换</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/</link><pubDate>Sat, 02 Dec 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/</guid><description>&lt;p>今天我们跟着吴恩达老师的课程，学习LLM另一个强大的能力——转换(&lt;code>Transforming&lt;/code>)。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231128162542742.png" alt="image-20231128162542742">&lt;/p>
&lt;h1 id="1激发转换能力的提示词">1.激发转换能力的提示词&lt;/h1>
&lt;h2 id="1什么是llm的转换能力">(1)什么是LLM的转换能力&lt;/h2>
&lt;p>LLM的转换能力是指&lt;strong>将文本从一种形式或格式转换为另一种形式或格式的能力&lt;/strong>。&lt;/p>
&lt;p>转换能力使得LLM成为一个强大的文本处理工具，可以在如下场景应用：&lt;/p>
&lt;ul>
&lt;li>文本翻译&lt;/li>
&lt;li>修正内容错误&lt;/li>
&lt;li>改变内容风格&lt;/li>
&lt;li>转换文本格式&lt;/li>
&lt;/ul>
&lt;p>注：目前很火的&lt;strong>LLM4SE领域&lt;/strong>(如：代码转换、代码修正、代码检视等)，&lt;strong>本质都是在激发LLM的转换能力&lt;/strong>。&lt;/p>
&lt;h2 id="2如何激发llm的转换能力">(2)如何激发LLM的转换能力&lt;/h2>
&lt;p>我们可以这样构建提示词：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>根据任务类型，使用清晰明确的指令词，如：&lt;strong>转换&lt;/strong>xxx、&lt;strong>翻译&lt;/strong>xxx、校正xxx。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>提供足够的上下文信息，让LLM知道要处理的内容。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="2实战案例">2.实战案例&lt;/h1>
&lt;p>假设我是某跨境电商的售前客服，要用LLM的转换能力来支撑处理用户问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>翻译&lt;/strong>国外用户反馈的问题。&lt;/li>
&lt;li>写问题回复，让LLM&lt;strong>校正&lt;/strong>内容错误。&lt;/li>
&lt;li>把回复内容&lt;strong>转换&lt;/strong>成商务邮件风格。&lt;/li>
&lt;li>将问题及回复&lt;strong>转换&lt;/strong>成JSON和表格，便于后续建单留档。&lt;/li>
&lt;/ul>
&lt;h2 id="step1翻译用户问题">STEP1.翻译用户问题&lt;/h2>
&lt;p>黑色星期五，电商客服收到一条问题如下：&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>“Avez-vous encore des stocks de smartphones ? Comment puis-je participer à vos promotions ?”&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>这是哪国语言，什么意思？😵 (客服小姐姐一脸黑线)&lt;/p>
&lt;p>程序员小哥哥英雄救美，让LLM帮忙&lt;strong>翻译&lt;/strong>下。&lt;/p>
&lt;ul>
&lt;li>程序员小哥的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129171159198.png" alt="image-20231129171159198">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129171243074.png" alt="image-20231129171243074">&lt;/p>
&lt;p>LLM识别出了文字的语种，并按指令进行了机器翻译。&lt;/p>
&lt;h2 id="step2回复客户邮件">STEP2.回复客户邮件&lt;/h2>
&lt;p>生意上门，客服小姐姐非常开心，但迅速陷入了沉思：如何给法语客户回复专业的售前邮件呢？😵&lt;/p>
&lt;p>通常，回复售前邮件需要有如下流程：&lt;/p>
&lt;ul>
&lt;li>编写回复信息并校正&lt;/li>
&lt;li>设计回复信息风格&lt;/li>
&lt;li>记录JSON和表格，归档到IT系统&lt;/li>
&lt;/ul>
&lt;p>此时，程序员小哥哥再次出手，英雄救美。&lt;/p>
&lt;h3 id="step21编写并校正回复信息">STEP2.1.编写并校正回复信息&lt;/h3>
&lt;p>客服小姐姐用中文写了回复信息：&lt;em>“我们手机的库存非长充足。优惠活动是买两部送一步。尽管放心下单吧。”&lt;/em>&lt;/p>
&lt;p>程序员小哥哥让LLM帮忙&lt;strong>校对&lt;/strong>下是否有文字错误。&lt;/p>
&lt;ul>
&lt;li>程序员小哥的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129173833270.png" alt="image-20231129173833270">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129174109866.png" alt="image-20231129174109866">&lt;/p>
&lt;p>LLM识别出了错别字，并修正了回复信息。&lt;/p>
&lt;h3 id="step22改变内容风格">STEP2.2.改变内容风格&lt;/h3>
&lt;p>商务对话要正式点儿，让LLM把初始信息&lt;strong>转换&lt;/strong>成商务&lt;strong>风格&lt;/strong>的回复，然后翻译成法语。&lt;/p>
&lt;ul>
&lt;li>程序员小哥的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129181528670.png" alt="image-20231129181528670">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129181557507.png" alt="image-20231129181557507">&lt;/p>
&lt;p>LLM很好的完成了任务，让客服小姐姐不用在翻译、措辞上花太多时间。&lt;/p>
&lt;h3 id="step23将问题及回复转换成json和表格">STEP2.3.将问题及回复转换成JSON和表格&lt;/h3>
&lt;p>客户问题很多，需要客服小姐姐将问题及回复，整理成JSON和表格形式，提交到IT系统归档。&lt;/p>
&lt;p>程序员小哥哥，利用LLM的&lt;strong>文本格式转换功能&lt;/strong>快速处理这些文本。&lt;/p>
&lt;p>如：转换成程序容易处理的JSON格式：&lt;/p>
&lt;ul>
&lt;li>程序员小哥的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129183618410.png" alt="image-20231129183618410">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129183643134.png" alt="image-20231129183643134">&lt;/p>
&lt;p>还可以把JSON转成直观易读的表格：&lt;/p>
&lt;ul>
&lt;li>程序员小哥的提示词如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129183705048.png" alt="image-20231129183705048">&lt;/p>
&lt;ul>
&lt;li>LLM的回答如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B030-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB5-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%BD%AC%E6%8D%A2/image-20231129183727101.png" alt="image-20231129183727101">&lt;/p>
&lt;p>最后，翻译、校正、文本转换、格式转换，只需用LLM一个工具就可以搞定。&lt;/p>
&lt;p>最后，程序员小哥哥和客服小姐姐开启了一段美好的爱情故事。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文介绍了一个LLM另一个实用的提示词技巧，可以充分利用LLM的文本转换能力，帮助我们处理繁琐的文档工作。&lt;/p>
&lt;p>转换提示词的构建方法：&lt;/p>
&lt;ul>
&lt;li>根据任务类型，使用清晰明确的指令词，如&lt;strong>转换&lt;/strong>xxx、&lt;strong>翻译&lt;/strong>xxx、校正xxx。&lt;/li>
&lt;li>提供足够的上下文信息，让LLM知道要处理的内容。&lt;/li>
&lt;/ul>
&lt;p>此外，这个故事还告诉我们：作为程序员小哥哥，要努力学习AI，高效使用AI，才能抱得美人归。&lt;/p></description></item><item><title>【chatGPT】学习笔记29-LangChain解读1-快速入门</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-langchain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</link><pubDate>Thu, 23 Nov 2023 14:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-langchain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</guid><description>&lt;p>虽然利用提示词工程，我们已经能较好地使用LLM，但即使开发一个很简单的LLM应用，依然需要编写大量复杂代码(调用LLM只是最简单的一步)。&lt;/p>
&lt;p>&lt;strong>LangChain&lt;/strong>的目标就是让开发LLM应用变的简单，但LangChain更新极快，导致我们的学习成本较高。&lt;/p>
&lt;p>因此，我们准备做两件事，帮助大家提升学习LangChain的效率：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>翻译LangChain官方文档&lt;/strong>
&lt;ul>
&lt;li>翻译链接：https://jherculesqz.gitbook.io/langchain-guan-fang-wen-dang&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>解读LangChain官方文档&lt;/strong>
&lt;ul>
&lt;li>在本技术专栏中，将详细地逐一解读LangChain官方文档中的各个重要特性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>本篇我们就来解读LangChain官方文档的《&lt;strong>Quickstart&lt;/strong>》章节(&lt;a href="https://python.langchain.com/docs/get_started/quickstart">https://python.langchain.com/docs/get_started/quickstart&lt;/a>)，帮助大家快速上手LangChain。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-LangChain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/5d7KfRriC6zji11ZFnwLotdqcHQ.svg" alt="LangChain">&lt;/p>
&lt;h1 id="1langchain概览">1.LangChain概览&lt;/h1>
&lt;h2 id="1什么是langchain">(1)什么是LangChain&lt;/h2>
&lt;p>&lt;strong>LangChain 是一个开源框架，支持由LLM驱动的应用程序的开发&lt;/strong>。它使应用程序能够：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>具有上下文感知能力&lt;/strong>：连接大语言模型和上下文的数据源 (如：提示词、few-shot、聊天历史记录等)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>推理&lt;/strong>：依靠语言模型进行推理 (如：根据给出的上下文回答问题，或者决定下一步动作）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>LangChain 的价值点主要有两个:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模块化组件&lt;/strong>：专注于组合和模块化，提供了基于LLM的各种组件。这些组件可以单独使用，也可以组合使用。&lt;/li>
&lt;li>&lt;strong>直接可用的链&lt;/strong>：将各种组件组合成可完成特定任务的&amp;quot;链&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;h2 id="2langchain整体架构">(2)LangChain整体架构&lt;/h2>
&lt;p>LangChain为以下模块提供标准的、可扩展的接口和外部集成，从简单到复杂排序如下:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Model I/O&lt;/strong>：与大语言模型的接口。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>检索 (Retrieval)&lt;/strong>：与应用程序特定数据的接口。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>链 (Chains)&lt;/strong>：构建调用序列。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>代理 (Agents)&lt;/strong>：让模型根据高级指令选择使用哪些工具。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Memory&lt;/strong>：在链运行期间保持应用程序状态。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Callbacks&lt;/strong>：记录并传送链的中间步骤。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="3如何构建llm应用程序">(3)如何构建LLM应用程序&lt;/h2>
&lt;p>LangChain提供了许多用来构建LLM应用程序的模块，其中最常用且最重要的模块是&lt;strong>LLMChain&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>LLMChain&lt;/strong>包含三个主要组件:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LLM&lt;/strong>：LLM是你要构建的应用程序的核心推理引擎。GPT、GLM等大模型LangChain都已适配支持。&lt;/li>
&lt;li>&lt;strong>提示模板&lt;/strong>：它为LLM提供指令，从而控制LLM的输出。所以大家要好好学习提示词工程。&lt;/li>
&lt;li>&lt;strong>输出解析器&lt;/strong>：把LLM的原始响应转化为程序更易处理的格式，方便后续使用。&lt;/li>
&lt;/ul>
&lt;p>接下来我们就基于LLMChain来体验LangChain的便捷吧。&lt;/p>
&lt;h1 id="2快速上手langchain">2.快速上手LangChain&lt;/h1>
&lt;p>下面，我们演示如何用LangChain来做文本总结：&lt;/p>
&lt;ul>
&lt;li>环境准备&lt;/li>
&lt;li>构造LLM&lt;/li>
&lt;li>构造提示词模板&lt;/li>
&lt;li>创建Chain，执行指定任务&lt;/li>
&lt;/ul>
&lt;h2 id="step1环境准备">STEP1.环境准备&lt;/h2>
&lt;ul>
&lt;li>安装LangChain。因为LangChain版本更新很快，安装时优先用&amp;rdquo;-U&amp;quot;升级模式安装。&lt;/li>
&lt;li>安装openai(需申请OpenAPI的Token)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-LangChain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20231123190055109.png" alt="image-20231123190055109">&lt;/p>
&lt;h2 id="step2构造llm">STEP2.构造LLM&lt;/h2>
&lt;ul>
&lt;li>使用langchain 中的 OpenAI 函数来初始化一个大语言模型&lt;code>llm&lt;/code>。
&lt;ul>
&lt;li>本例用的是&amp;quot;text-davinci-003&amp;rdquo;，当然你可以根据需求用gpt或其它模型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-LangChain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20231123174527077.png" alt="image-20231123174527077">&lt;/p>
&lt;h2 id="step3构造提示词模板">STEP3.构造提示词模板&lt;/h2>
&lt;blockquote>
&lt;p>应用程序不会把用户的输入直接传给LLM，通常的做法是把用户输入传给提示词模板。&lt;/p>
&lt;p>提示词模板的好处是：&lt;/p>
&lt;ul>
&lt;li>格式化的提示词结构，包括指令、上下文、输入、输出要求等，为给LLM提供更详细的语境。&lt;/li>
&lt;li>支持设置变量，这可以让好用的提示词最大化被复用。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ul>
&lt;li>使用langchain的提示词模板函数初始化一个提示词模板&lt;code>prompt_template&lt;/code>
&lt;ul>
&lt;li>提示词指令是总结文本内容&lt;/li>
&lt;li>要处理的文本内容设成变量，本次任务是处理一段新闻，后面也可以随意处理其它文本内容&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-LangChain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20231123202743359.png" alt="image-20231123202743359">&lt;/p>
&lt;h2 id="step4创建chain">STEP4.创建Chain&lt;/h2>
&lt;p>现在，我们将以上组件组合成一个链，就可以执行任务了。&lt;/p>
&lt;ul>
&lt;li>使用LLMChain构建我们自己的链&lt;code>chain&lt;/code>，传入上面的两个组件&lt;code>llm&lt;/code>、&lt;code>prompt_template&lt;/code>。&lt;/li>
&lt;li>运行&lt;code>chain&lt;/code>，参数只需传入变量text(要处理的文本内容)，可以看到如下运行过程：
&lt;ul>
&lt;li>链开始，&lt;/li>
&lt;li>读取text变量，格式化提示词，传给LLM&lt;/li>
&lt;li>得到结果，链结束&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B029-LangChain%E8%A7%A3%E8%AF%BB1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20231123203230173.png" alt="image-20231123203230173">&lt;/p>
&lt;p>从上面的示例可以看到，只需一个命令就实现了提示词构建和LLM调用，我们利用Langchain很好的完成了任务。&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文解读了LangChain官方文档的《QuickStart》章节，并给出了基于LangChain构建LLM应用的实例代码。QuickStart章节关键要点如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>LangChain简介&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>LangChain 是一个开源框架，支持由LLM驱动的应用程序的开发。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LangChain 的主要价值点：提供了各种模块化组件和好用的链&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LLMChain是LangChain最常见且最重要的一个链，它包含3个主要组件：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LLM&lt;/strong>：应用程序的核心推理引擎。&lt;/li>
&lt;li>&lt;strong>提示模板&lt;/strong>：它为LLM提供指令，从而控制LLM的输出。&lt;/li>
&lt;li>&lt;strong>输出解析器&lt;/strong>：把LLM的原始响应转化为程序更易处理的格式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实例演示&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>基于LangChain构建具备总结功能的LLM应用
&lt;ul>
&lt;li>环境准备&lt;/li>
&lt;li>构造LLM&lt;/li>
&lt;li>构造提示词模板&lt;/li>
&lt;li>创建Chain，执行指定任务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>