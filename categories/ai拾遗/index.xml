<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI拾遗 on 妙木山</title><link>https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/</link><description>Recent content in AI拾遗 on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 10 Jan 2025 11:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/index.xml" rel="self" type="application/rss+xml"/><item><title>【chatGPT】学习笔记59-AgentScope解读-提示工程</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B059-agentscope%E8%A7%A3%E8%AF%BB-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/</link><pubDate>Fri, 10 Jan 2025 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B059-agentscope%E8%A7%A3%E8%AF%BB-%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/</guid><description>提示工程(Prompt Engineering)是与大语言模型交互的一项关键技术，作为以大语言模型为大脑的智能体，提示工程同样扮演着重要角色。</description></item><item><title>【chatGPT】学习笔记58-AgentScope解读-流式输出</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B058-agentscope%E8%A7%A3%E8%AF%BB-%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/</link><pubDate>Fri, 27 Dec 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B058-agentscope%E8%A7%A3%E8%AF%BB-%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/</guid><description>流式输出是大语言模型的重要功能，可以提高模型输出效率、提升用户交互体验。 本文为小伙伴们介绍AgentScope中流式输出的用法。 1.Agen</description></item><item><title>【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/</link><pubDate>Thu, 14 Nov 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/</guid><description>在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精</description></item><item><title>【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/</link><pubDate>Fri, 18 Oct 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/</guid><description>笔者在写模型量化技术相关文章时，正逢2024诺贝尔颁奖，Hiton、Lecun这些大神萦绕耳边。那我们就借Lecun在1990年发表的《Op</description></item><item><title>【chatGPT】学习笔记56-AgentScope解读-模型</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-agentscope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 18 Oct 2024 11:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B056-agentscope%E8%A7%A3%E8%AF%BB-%E6%A8%A1%E5%9E%8B/</guid><description>模型是智能体的&amp;quot;大脑&amp;rdquo;，帮助智能体实现复杂的自然语言交互和精准的推理决策。 本文为小伙伴介绍AgentScope中模型的</description></item><item><title>【chatGPT】学习笔记54-AgentScope解读-概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-agentscope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 18 Oct 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B054-agentscope%E8%A7%A3%E8%AF%BB-%E6%A6%82%E8%A7%88/</guid><description>人类一直试图构建一个能够自主完成任务的代理或实体，即智能体 (AI Agents 或 Agents)。大型语言模型（LLM）的引入为智能体的构建提供了新的核心组件</description></item><item><title>【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/</link><pubDate>Fri, 20 Sep 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/</guid><description>我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后</description></item><item><title>【chatGPT】学习笔记52-麦肯锡《生成式AI与美国的未来工作》报告解读</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/</link><pubDate>Mon, 15 Jul 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/</guid><description>近期萝卜快跑成为了热点话题，人工智能对就业产生多大的影响成为社会关注的焦点。 我们试图通过解读麦肯锡《生成式AI与美国的未来工作》研究报告，为</description></item><item><title>【chatGPT】学习笔记51-LLM微调技术之AdaLoRA</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/</link><pubDate>Wed, 26 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/</guid><description>今天我们通过解读论文《ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING》来学习一下AdaLoRA。 1.Abstract(摘要) 首先我们看一下</description></item><item><title>【chatGPT】学习笔记50-LLM微调技术之LoRA</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/</link><pubDate>Mon, 10 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/</guid><description>前面的专栏我们介绍了Adapt Tuning、Soft Prompt Tuning等微调技术，本文让我们跟随着论文《LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS》，来</description></item><item><title>【chatGPT】学习笔记49-AiDD 2024_AI人才培养分论坛参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Wed, 05 Jun 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>在AIGC时代，AI对高等教育及人才培养有怎样的变革？如何培养适应时代发展需求的计算机人才？ 1.AIGC时代下的人才需求趋势 (1)时代背景 生</description></item><item><title>【chatGPT】学习笔记48-AiDD 2024参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Fri, 24 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B048-aidd-2024%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>本文记录笔者参加AiDD 2024(AI+研发数字峰会)相关的议题，方便各位小伙伴快速了解最新AI理论研究和行业应用情况。 1.AiDD 2024概览 (1)</description></item><item><title>【chatGPT】学习笔记47-LLM微调技术之P-Tuning V2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</link><pubDate>Sat, 11 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B047-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv2/</guid><description>上篇专栏我们讲到，P-Tuning V1通过在预训练模型的输入层加入可训练的连续提示，有效提升了训练效果。但其在复杂NLU任务和小参数模型上表</description></item><item><title>【chatGPT】学习笔记46-LLM微调技术之P-Tuning V1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</link><pubDate>Wed, 01 May 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/</guid><description>前面给大家分享了Soft Prompt技术分支下的Prefix-Tuning和Prompt Tuning，在这个技术分支下，还有一项需要重点了解</description></item><item><title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</link><pubDate>Wed, 03 Apr 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/</guid><description>紧接着Stanford的Prefix Tuning论文，Google迅速发表了Prompt Tuning技术论文。Google声称该技术比Pre</description></item><item><title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</link><pubDate>Fri, 29 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/</guid><description>Prefix Tuning是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。 本文通过解读论文**《Prefix-Tuning:</description></item><item><title>【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</link><pubDate>Fri, 22 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/</guid><description>Adapter Tuning是LLM微调技术中一个重要的技术分支，于2019年由Google的Neil Houlsby等研究员提出。 Adapter Tuning方法证明了</description></item><item><title>【chatGPT】学习笔记42-LLM微调技术概览</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</link><pubDate>Fri, 15 Mar 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B042-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/</guid><description>笔者在使用大语言模型开展具体业务领域的任务时，遇到了如下问题： LLM的预训练模型不具备垂域知识，无法很好回答垂域的问题； 希望用各种提示词技巧</description></item><item><title>【chatGPT】学习笔记41-多模态-Sora浅析</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</link><pubDate>Mon, 26 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B041-%E5%A4%9A%E6%A8%A1%E6%80%81-sora%E6%B5%85%E6%9E%90/</guid><description>Sora自2024年2月16日发布以来，持续霸屏、热度不断。从OpenAI官网上的演示视频看，效果也是相当震撼。 本篇基于OpenAI发布的技</description></item><item><title>【chatGPT】学习笔记40-LLM应用-如何构建RAG数据集</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</link><pubDate>Mon, 05 Feb 2024 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B040-llm%E5%BA%94%E7%94%A8-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BArag%E6%95%B0%E6%8D%AE%E9%9B%86/</guid><description>2023年是基础大模型的爆发元年，专家预测2024年将是AI应用的爆发元年。 因此，本专栏希望通过一系列文章，和大家探讨AI应用的规划、落地、</description></item></channel></rss>