<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记50-LLM微调技术之LoRA - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="前面的专栏我们介绍了Adapt Tuning、Soft Prompt Tuning等微调技术，本文让我们跟随着论文《LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS》，来"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记50-LLM微调技术之LoRA"><meta property="og:description" content="前面的专栏我们介绍了Adapt Tuning、Soft Prompt Tuning等微调技术，本文让我们跟随着论文《LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS》，来"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/"><meta property="article:published_time" content="2024-06-10T10:00:59+08:00"><meta property="article:modified_time" content="2024-06-10T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记50-LLM微调技术之LoRA"><meta itemprop=description content="前面的专栏我们介绍了Adapt Tuning、Soft Prompt Tuning等微调技术，本文让我们跟随着论文《LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS》，来"><meta itemprop=datePublished content="2024-06-10T10:00:59+08:00"><meta itemprop=dateModified content="2024-06-10T10:00:59+08:00"><meta itemprop=wordCount content="1684"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记50-LLM微调技术之LoRA"><meta name=twitter:description content="前面的专栏我们介绍了Adapt Tuning、Soft Prompt Tuning等微调技术，本文让我们跟随着论文《LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS》，来"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记50-LLM微调技术之LoRA</h1><div class=post-meta><time datetime=2024-06-10 class=post-time>2024-06-10</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 1684 字</span>
<span class=more-meta>预计阅读 4 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1abstract摘要>1.Abstract(摘要)</a></li><li><a href=#2introduction介绍>2.Introduction(介绍)</a></li><li><a href=#3design-decisions实验设计>3.Design Decisions(实验设计)</a><ul><li><a href=#1问题域>(1)问题域</a></li><li><a href=#2问题建模>(2)问题建模</a></li></ul></li><li><a href=#4experiments实验结果>4.Experiments(实验结果)</a><ul><li><a href=#1实验结果>(1)实验结果</a></li><li><a href=#2关键发现>(2)关键发现</a></li></ul></li><li><a href=#5总结>5.总结</a></li></ul></nav></div></div><div class=post-content><p>前面的专栏我们介绍了<strong>Adapt Tuning</strong>、<strong>Soft Prompt Tuning</strong>等微调技术，本文让我们跟随着论文《<strong>LORA: LOW-RANK ADAPTATION OF LARGE LAN-</strong>
<strong>GUAGE MODELS</strong>》，来看一下<strong>LoRA微调技术</strong>。</p><h1 id=1abstract摘要>1.Abstract(摘要)</h1><p>首先我们看一下论文摘要，快速理解论文的<strong>核心内容</strong>：</p><ul><li><strong>问题</strong>：<strong>LoRA</strong>也是解决全参数微调参数量过大、成本过高的问题。</li><li><strong>解决方案</strong>：论文提出了<strong>LoRA</strong>微调技术，冻结预训练模型权重，在Transformer架构的每一层注入可训练的低秩矩阵。</li><li><strong>实验效果</strong>：LoRA大幅减少了训练参数量，与使用Adam微调的GPT-3 175B相比，LoRA将训练参数数量减少了10000倍，GPU需求减少3倍。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240610131206816.png alt=image-20240610131206816></p><h1 id=2introduction介绍>2.Introduction(介绍)</h1><ul><li><p><strong>问题</strong>：全参数微调成本过高，现有高效微调技术扩展了模型深度导致了推理延迟or减少了模型可用序列长度。</p><ul><li>全参微调的主要缺点是新模型包含的参数量与原始模型一样多，进而导致成本巨大。如：GPT-3拥有1750亿参数，全参微调成本巨大。</li><li>Houlsby、Rebuffi等专家提出的高效微调技术，增加了模型深度，也会导致推理延迟，也可能导致减少了模型可用序列长度。</li></ul></li><li><p><strong>LoRA的核心思想</strong>：</p><ul><li>可以共享一个预训练模型，在其基础上构建许多用于不同任务的小LoRA模块。</li><li>LoRA模块用两个低秩矩阵A和B，替换原有的参数。利用低秩矩阵的数学特性，减少参数量。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240610132355705.png alt=image-20240610132355705></p><ul><li><strong>LoRA的优势：</strong><ul><li>LoRA的这种设计使训练更加高效，将GPU计算量降低了3倍。</li><li>LoRA模块对Transformer的改变是线性的，因此不会由于加深网络结构导致推理延迟。</li><li>LoRA的设计和现有高效微调技术是正交的，可以和它们结合使用。</li></ul></li></ul><h1 id=3design-decisions实验设计>3.Design Decisions(实验设计)</h1><h2 id=1问题域>(1)问题域</h2><ul><li><p><strong>对全量微调建模</strong>：在全量微调下，预训练模型参数的初始值为Φ<sub>0</sub>，随着梯度更新为Φ<sub>0</sub>+∆Φ，则找到∆Φ的任务可定义为：</p></li><li><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611040221623.png alt=image-20240611040221623></p></li><li><p><strong>对全量微调缺点建模</strong>：∆Φ的维度等于|Φ<sub>0</sub>|，当模型很大时，梯度更新Φ<sub>0</sub>将导致巨大的计算成本。</p></li></ul><h2 id=2问题建模>(2)问题建模</h2><p>LoRA是这样寻找突破口的：</p><ul><li><strong>LoRA建模</strong>：∆Φ=∆Φ(Θ)，|Θ|维度&#171;|Φ<sub>0</sub>|，则找到∆Φ的任务可转换为对Θ的优化：</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611041007236.png alt=image-20240611041007236></li></ul><p>如何做到∆Φ=∆Φ(Θ)呢？其本质就是<strong>降维</strong>：</p><ul><li><strong>低秩矩阵</strong>：低秩矩阵的概念本文不会展开描述，小伙伴们可以复习一下线性代数。但低秩矩阵的本质是2个低阶矩阵的运算可以<strong>约等于</strong>1个高阶矩阵。</li><li><strong>低秩矩阵与深度学习的结合</strong>：假设预训练模型的参数矩阵为W<sub>0</sub>，则梯度更新可认为W<sub>0</sub>+∆W。如果将∆W分解为2个低秩矩阵B和A，即W<sub>0</sub>+∆W=W<sub>0</sub>+BA<sub>x</sub>。</li><li><strong>低秩矩阵与Transformer的结合</strong>：在Transformer架构中，W<sub>q</sub>、W<sub>k</sub>、W<sub>v</sub>，W<sub>o</sub>分别表示Query、Key、Value、输出投影矩阵。W<sub>q</sub>、W<sub>k</sub>、W<sub>v</sub>是单一维度为dmodel*dmodel的矩阵。W<sub>q</sub>+∆W、W<sub>k</sub>+∆W、W<sub>v</sub>+∆W中的∆W分解为低秩矩阵B和A，则也将问题转换为∆W小矩阵问题。</li></ul><h1 id=4experiments实验结果>4.Experiments(实验结果)</h1><h2 id=1实验结果>(1)实验结果</h2><ul><li><p>在GLUE基准测试中使用RoBERTa(基础和大型)和DeBERTa 1.5B获得了与全参数微调相当或更优的结果，同时只训练和存储了一小部分参数。</p></li><li><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611072021335.png alt=image-20240611072021335></p></li><li><p>在GPT-2上，LoRA与全参数微调和其他高效调整方法和前缀调整相比具有优势。</p></li><li><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611072244925.png alt=image-20240611072244925></p></li></ul><h2 id=2关键发现>(2)关键发现</h2><ul><li><p>LoRA与Transformer结合，<strong>应该如何划分低秩矩阵</strong>？</p><ul><li>实验证明：r=8或r=4时，适用于GPT-3(175B模型)的96层注意力层。</li><li>实验证明：不要仅仅只刷新∆W<sub>q</sub>或∆W<sub>v</sub>，同时刷新它们会产生更好的性能。</li></ul></li><li><p>∆W和W强相关，∆W增强了W学习到的一些特征。</p></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BLoRA/image-20240611092100515.png alt=image-20240611092100515></p><h1 id=5总结>5.总结</h1><p>从上述论文解读中，我们收获了如下技术观点：</p><ul><li><strong>LoRA的核心思想</strong>：LoRA模块用两个低秩矩阵A和B，替换原有的参数。利用低秩矩阵的数学特性，减少参数量。。</li><li><strong>LoRA的优势</strong>：LoRA的这种设计使训练更加高效，且不会由于加深网络结构导致推理延迟。</li><li><strong>LoRA实验的关键发现</strong>：<ul><li>LoRA与Transformer结合，要考虑如何划分低秩矩阵。</li><li>∆W和W强相关，∆W增强了W学习到的一些特征。</li></ul></li></ul><p>论文链接：https://arxiv.org/abs/2106.09685</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-06-10</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B049-aidd-2024_ai%E4%BA%BA%E6%89%8D%E5%9F%B9%E5%85%BB%E5%88%86%E8%AE%BA%E5%9D%9B%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/><span class="next-text nav-default">【chatGPT】学习笔记49-AiDD 2024_AI人才培养分论坛参会纪要</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>