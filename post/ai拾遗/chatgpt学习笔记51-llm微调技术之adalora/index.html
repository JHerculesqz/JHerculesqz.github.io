<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记51-LLM微调技术之AdaLoRA - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="今天我们通过解读论文《ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING》来学习一下AdaLoRA。 1.Abstract(摘要) 首先我们看一下"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记51-LLM微调技术之AdaLoRA"><meta property="og:description" content="今天我们通过解读论文《ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING》来学习一下AdaLoRA。 1.Abstract(摘要) 首先我们看一下"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badalora/"><meta property="article:published_time" content="2024-06-26T10:00:59+08:00"><meta property="article:modified_time" content="2024-06-26T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记51-LLM微调技术之AdaLoRA"><meta itemprop=description content="今天我们通过解读论文《ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING》来学习一下AdaLoRA。 1.Abstract(摘要) 首先我们看一下"><meta itemprop=datePublished content="2024-06-26T10:00:59+08:00"><meta itemprop=dateModified content="2024-06-26T10:00:59+08:00"><meta itemprop=wordCount content="2558"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记51-LLM微调技术之AdaLoRA"><meta name=twitter:description content="今天我们通过解读论文《ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING》来学习一下AdaLoRA。 1.Abstract(摘要) 首先我们看一下"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记51-LLM微调技术之AdaLoRA</h1><div class=post-meta><time datetime=2024-06-26 class=post-time>2024-06-26</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2558 字</span>
<span class=more-meta>预计阅读 6 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1abstract摘要>1.Abstract(摘要)</a></li><li><a href=#2introduction介绍>2.Introduction(介绍)</a></li><li><a href=#3design-decisions实验设计>3.Design Decisions(实验设计)</a><ul><li><a href=#1问题域>(1)问题域</a></li><li><a href=#2问题建模>(2)问题建模</a></li></ul></li><li><a href=#4experiments实验结果>4.Experiments(实验结果)</a><ul><li><a href=#1实验结果>(1)实验结果</a></li><li><a href=#2关键发现>(2)关键发现</a></li></ul></li><li><a href=#5总结>5.总结</a></li></ul></nav></div></div><div class=post-content><p>今天我们通过解读论文《<strong>ADALORA: ADAPTIVE BUDGET ALLOCATION FOR</strong>
<strong>PARAMETER-EFFICIENT FINE-TUNING</strong>》来学习一下AdaLoRA。</p><h1 id=1abstract摘要>1.Abstract(摘要)</h1><p>首先我们看一下论文摘要，快速理解论文的<strong>核心内容</strong>：</p><ul><li><strong>问题</strong>：<strong>AdaLoRA</strong>也是解决全参数微调参数量过大、成本过高的问题。</li><li><strong>解决方案</strong>：论文提出了<strong>AdaLoRA</strong>微调技术，是对<strong>LoRA</strong>的一种改进，<strong>LoRA</strong>没有考虑不同权重参数的重要性不同。AdaLoRA以奇异值分解的形式参数化增量更新，有效地修剪不重要更新的奇异值，避免了密集的精确SVD计算。</li><li><strong>实验效果</strong>：在自然语言处理、问答和自然语言生成等多个预训练模型上的实验表明，AdaLoRA在低预算设置下，微调效果有显著的改进。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621144023004.png alt=image-20240621144023004></p><h1 id=2introduction介绍>2.Introduction(介绍)</h1><ul><li><strong>问题</strong>：LoRA有哪些局限性呢？论文给出了一个测试。<ul><li><strong>左图</strong>：针对不同模块进行LoRA微调，效果是不一样的。针对前馈网络FFN进行LoRA微调的效果要远优于自注意力模块的LoRA微调。</li><li><strong>右图</strong>：针对不同层进行LoRA微调，效果也是不一样的。10~12层的微调效果要远优于1~3层的微调效果。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621164711429.png alt=image-20240621164711429></p><ul><li><p><strong>AdaLoRA的核心思想</strong>：</p><ul><li>通过上述实验可以看到：在理想状态下，<strong>微调关键模块/关键层的权重矩阵是最有效的</strong>，而微调不太重要的模块/层的权重矩阵不仅毫无意义，甚至有负面的影响。</li><li>AdaLoRA针对关键矩阵设置为高秩，对于不重要的矩阵修剪为低秩。设置为高秩的增量矩阵可以捕获更细粒度的特征。</li><li>AdaLoRA如何实现上述这种<strong>对秩的调整</strong>呢？答案是数学工具<strong>SVD(矩阵奇异值分解)</strong>，SVD是将高维矩阵拆分为三个矩阵，即<strong>∆=PΛQ</strong>。P和Q分别表示∆的左/右奇异向量，Λ表示∆的对角矩阵。SVD在数据科学、机器学习中应用广泛，常用于降维、数据压缩、噪声过滤、文本数据的潜在语义结构提取等。</li><li>AdaLoRA通过对某层、某模块的重要性评分，动态调整<strong>∆=PΛQ</strong>的秩。AdaLoRA构造了N个三元组，每个三元组G<sub>i</sub>=(P, Λ, Q)，AdaLoRA设计了一种重要性度量方法，重要性得分高的三元组被赋予高秩。</li></ul></li><li><p><strong>AdaLoRA的实验效果：</strong></p><ul><li>针对DeBERTaV3-base进行自然语言理解(GLUE)、问题(SQuADv1)进行了评估，AdaLoRA有了更好的性能表现。</li><li>针对BART-large进行自然语言生成(XSum)进行了评估，AdaLoRA始终优于基线的性能。</li></ul></li></ul><h1 id=3design-decisions实验设计>3.Design Decisions(实验设计)</h1><h2 id=1问题域>(1)问题域</h2><ul><li><p><strong>对Transformer的建模</strong>：Transformer模型有L个堆叠的块组成，每个块包含2个子模块——MHA(多头注意力)、FFN(全连接前馈网络)。</p></li><li><p><strong>MHA函数</strong>：给定输入序列X∈R<sup>n×d</sup>，MHA执行了h个多头注意力函数表示为</p><ul><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621184305094.png alt=image-20240621184305094></li></ul></li><li><p><strong>FFN函数</strong>：前馈网络由两个线性变换组成，中间有一个ReLU激活函数。</p><ul><li><img src=/AI拾遗/【chatGPT】学习笔记51-LLM微调技术之AdaLoRA/image-20240621185258000.png alt=image-20240621185258000 style=zoom:80%></li></ul></li><li><p><strong>对LoRA建模</strong>：LoRA的本质是用两个小矩阵替代一个大矩阵。</p><ul><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240621185936493.png alt=image-20240621185936493></li></ul></li><li><p><strong>LoRA模型的问题</strong>：</p><ul><li>LoRA的低秩分解方法选择了相同的秩r</li><li>不同层的矩阵参数都采用相同的AB分解方法吗？</li></ul></li></ul><h2 id=2问题建模>(2)问题建模</h2><ul><li><strong>SVD</strong>：SVD是对高维矩阵进行降维的行之有效的数学工具。<ul><li><strong>SVD的目标</strong>：将1个高维矩阵，转换为3个低维矩阵，并且可以通过调整秩r来控制对角矩阵Λ(公式3)。</li><li><strong>Λ的数学性质</strong>：对角矩阵Λ的特征值逐步递减，不严谨地说特征值越高，这个维度的特征越重要。</li><li><strong>Λ的近似</strong>：当我们用秩r舍弃对角矩阵Λ较小的特征值，就实现了降维的同时又不丢失有效的信息。</li><li><strong>PQ的数学性质</strong>：在SVD中，PQ要满足正交性，因此论文中引入了对P和Q的正则损失约束(公式4)。</li><li>SVD的详细原理和数学推导本文不赘述，可以参考机器学习的PCA方法。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626162821750.png alt=image-20240626162821750></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626170654390.png alt=image-20240626170654390></p><ul><li><p><strong>重要性评估</strong>：某一层的某个参数矩阵对于整个Transformer到底是不是重要的？</p><ul><li><strong>G<sub>k,i</sub></strong>： 假设G这个三元组表示训练过程中对P、Λ、Q的更新。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626170842830.png alt=image-20240626170842830></li><li><strong>C(P, E, Q)</strong>：假设C表示对P、Λ、Q的更新代价。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171118350.png alt=image-20240626171118350></li><li><strong>R(P<sub>k</sub>, Q<sub>k</sub>)</strong>：假设R表示P和Q满足公式4R(P, Q)正交性的代价。</li><li><strong>L(P, E, Q)</strong>：则，L表示训练目标是要保证C和R都尽量小，才能得到满足训练效果的最优训练成本。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171425594.png alt=image-20240626171425594></li><li><strong>Λ<sup>(t)</sup><sub>k</sub></strong>：则，Λ<sup>(t)</sup><sub>k</sub>表示采用梯度更新的计算过程(公式5)。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171551548.png alt=image-20240626171551548></li><li>但是我们知道对每轮迭代、对每一层的高维参数矩阵进行SVD，存在巨大的计算成本，因此论文终于从逻辑上引出了重要性评估：</li><li><strong>SVD剪枝</strong>：假设S是一个重要性评估的函数，那么Λ<sup>(t)</sup><sub>k</sub>和S<sup>(t)</sup><sub>k</sub>共同决定了Λ<sup>(t+1)</sup><sub>k</sub>是否还有必要进行更新(公式6)。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626171655605.png alt=image-20240626171655605></li><li><strong>S<sup>t</sup><sub>k</sub></strong>：重要性评估函数考虑P、Λ、Q三个矩阵重要性的影响(公式7)，其中**s(·)**表示具体的重要性计算方法。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172223409.png alt=image-20240626172223409></li></ul></li><li><p><strong>s(·)的三种计算方法</strong>：</p><ul><li><strong>不考虑P、Q的影响</strong>：对角矩阵Λ中特征值越大越重要，这是传统的SVD判定重要性的方法，这种方法忽略了P和Q对各层参数矩阵的重要性影响。</li><li><strong>考虑P、Q影响，但不考虑重要性函数的平滑</strong>：(公式8)。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172632400.png alt=image-20240626172632400></li><li>考虑P、Q影响，但考虑重要性函数的平滑：什么情况会导致重要性不平滑呢？通俗地说，模型对于某一批次的数据已经训练地很好了，那么可能会导致认为当前的参数矩阵对整个模型并不重要。3个公式相当于增加了二级平滑的函数，保证了考虑P和Q的同时，也能相对客观地评估它们的重要性(公式9、公式10、公式11)。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172937743.png alt=image-20240626172937743></li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626172945391.png alt=image-20240626172945391></li></ul></li></ul><h1 id=4experiments实验结果>4.Experiments(实验结果)</h1><h2 id=1实验结果>(1)实验结果</h2><p>从整体的实验结果来看，AdaLoRA的微调效果全面优于LoRA。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174406600.png alt=image-20240626174406600></p><h2 id=2关键发现>(2)关键发现</h2><ul><li><p><strong>增加计算预算</strong>，AdaLoRA的训练效果会显著优于LoRA。</p><ul><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626173949733.png alt=image-20240626173949733></li></ul></li><li><p><strong>SVD降维、保证PQ矩阵的正交性</strong>，对AdaLoRA的训练效果都有影响。</p><ul><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174221946.png alt=image-20240626174221946></li></ul></li><li><p>Transformer中，<strong>FNN的参数矩阵重要性要大于自注意力层的参数矩阵</strong>、<strong>顶层的参数矩阵重要性大于低层的参数矩阵重要性</strong>。</p><ul><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B051-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BAdaLoRA/image-20240626174532657.png alt=image-20240626174532657></li></ul></li></ul><h1 id=5总结>5.总结</h1><p>从上述论文解读中，我们收获了如下技术观点：</p><ul><li><strong>AdaLoRA的核心思想</strong>：采用SVD、正则损失、二次平滑的重要性评估，精准地控制每一层的参数矩阵降维。</li><li><strong>AdaLoRA的关键发现</strong>：<ul><li><strong>SVD降维、保证PQ矩阵的正交性</strong>，对AdaLoRA的训练效果都有影响。</li><li>Transformer中，<strong>FNN的参数矩阵重要性要大于自注意力层的参数矩阵</strong>、<strong>顶层的参数矩阵重要性大于低层的参数矩阵重要性</strong>。</li></ul></li></ul><p><strong>阅读AdaLoRA最大的感触就是数学的奇妙，AdaLoRA是一个主要依赖数学公式推导解决算法问题的典型代表</strong>。</p><p>论文链接：https://arxiv.org/pdf/2303.10512</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-06-26</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记52-麦肯锡《生成式AI与美国的未来工作》报告解读</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B050-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Blora/><span class="next-text nav-default">【chatGPT】学习笔记50-LLM微调技术之LoRA</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>