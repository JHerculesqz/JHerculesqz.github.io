<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="紧接着Stanford的Prefix Tuning论文，Google迅速发表了Prompt Tuning技术论文。Google声称该技术比Pre"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning"><meta property="og:description" content="紧接着Stanford的Prefix Tuning论文，Google迅速发表了Prompt Tuning技术论文。Google声称该技术比Pre"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/"><meta property="article:published_time" content="2024-04-03T10:00:59+08:00"><meta property="article:modified_time" content="2024-04-03T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning"><meta itemprop=description content="紧接着Stanford的Prefix Tuning论文，Google迅速发表了Prompt Tuning技术论文。Google声称该技术比Pre"><meta itemprop=datePublished content="2024-04-03T10:00:59+08:00"><meta itemprop=dateModified content="2024-04-03T10:00:59+08:00"><meta itemprop=wordCount content="4126"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning"><meta name=twitter:description content="紧接着Stanford的Prefix Tuning论文，Google迅速发表了Prompt Tuning技术论文。Google声称该技术比Pre"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</h1><div class=post-meta><time datetime=2024-04-03 class=post-time>2024-04-03</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 4126 字</span>
<span class=more-meta>预计阅读 9 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1abstract摘要>1.Abstract(摘要)</a></li><li><a href=#2introduction介绍>2.Introduction(介绍)</a></li><li><a href=#3design-decisions实验设计>3.Design Decisions(实验设计)</a><ul><li><a href=#1下游任务数据格式归一化的理论基础>(1)下游任务数据格式归一化的理论基础</a></li><li><a href=#2问题建模>(2)问题建模</a></li><li><a href=#3如何消减特殊标记的影响>(3)如何消减特殊标记的影响</a></li></ul></li><li><a href=#4experiments实验结果>4.Experiments(实验结果)</a><ul><li><a href=#41实验结果>4.1.实验结果</a></li><li><a href=#42重要发现>4.2.重要发现</a></li></ul></li><li><a href=#5总结>5.总结</a></li></ul></nav></div></div><div class=post-content><p>紧接着Stanford的<code>Prefix Tuning</code>论文，Google迅速发表了<code>Prompt Tuning</code>技术论文。Google声称该技术比<code>Prefix Tuning</code>更易上手且成本更低，因此该技术随后也成为了微调技术中的一个重要分支。</p><p>本文解读论文**《The Power of Scale for Parameter-Efficient Prompt Tuning》**，与大家共同感受<code>Prompt Tuning</code>技术的奇妙之处。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403122759085.png alt=image-20240403122759085></p><h1 id=1abstract摘要>1.Abstract(摘要)</h1><p>首先我们看一下论文摘要，快速理解论文的<strong>核心内容</strong>：</p><ul><li><p><strong>问题</strong>：<strong>Prompt Tuning</strong>与<strong>Prefix Tuning</strong>一样，都是以任务为中心的思路解决问题。</p><ul><li><strong>以任务为中心</strong>：它们都在试图解决<strong>FFT</strong>针对不同的下游任务都需产生一个新的微调后大模型而导致的成本效率问题。</li></ul></li><li><p><strong>解决方案</strong>：论文提出的<code>Prompt Tuning</code>，也是一种使用<strong><code>Soft Prompt(软提示)</code></strong>进行迁移学习的方法。统一不同下游任务的训练数据格式，并将这些不同下游任务的训练数据汇总成一个乱序的数据集，微调预训练模型，最终获得一个能处理不同下游任务的大模型。</p></li><li><p><strong>实验效果</strong>：</p><ul><li>在小参数规模的T5上，<strong>Prompt Tuning</strong>略差于FFT性能。</li><li>在中参数规模的T5上，<strong>Prompt Tuning</strong>快速接近于FFT性能。</li><li>在大参数规模的T5上，<strong>Prompt Tuning</strong>与FFT性能持平。</li><li>因此，<strong>Prompt Tuning</strong>在大参数规模的模型上，更具成本效率优势。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403092613314.png alt=image-20240403092613314></p><h1 id=2introduction介绍>2.Introduction(介绍)</h1><ul><li><p><strong>背景技术1</strong>：论文中提到的<strong>Prompt Design</strong>可以理解为大家耳熟能详的<strong>提示词及提示词工程</strong>。</p><ul><li>经过工程实践，大家都知道提示词工程有一定的效果，但效果远不及FFT。为什么呢？论文中总结了提示词工程的两个短板：<strong>the discrete space of words(离散空间的单词)<strong>和</strong>requires human involvement(人类的投入)</strong>。</li><li><strong>the discrete space of words(离散空间的单词)</strong>：提示词工程中的提示词是数学意义上的<strong>离散</strong>，大语言模型不能很好地学习其隐含的特征。隐含特征不是提示词内容本身的显式特征，而是诸如提示词的句式、问法、潜台词等隐式特征。业界针对提取离散空间词汇也提出了一些算法(如：<strong>a search algorithm over the discrete space of words</strong>)，但效果也非常限。</li><li><strong>requires human involvement(人类的投入)</strong>：提示词工程依赖有经验的提示词工程师，针对不同下游任务设计提示词，工作量巨大，并且对大模型推理能力的提升又极其有限。</li></ul></li><li><p><strong>背景技术2</strong>：论文中提到的<strong>Model Tuning</strong>和<strong>Model Tuning(Multi Task)</strong>，可以理解为<strong>FFT</strong>。</p><ul><li><strong>Model Tuing(Multi Task)</strong>：<strong>Model Tuning(Multi Task)<strong>是针对每个下游任务都微调出一个大模型，而</strong>Model Tuning</strong>是将N个下游任务都微调到一个大模型中。</li></ul></li><li><p><strong>Prompt Tuning的核心思想</strong>：</p><ul><li><strong>统一下游任务的数据格式</strong>：论文中提到<code>an additional k tunable tokens per downstream task to be prepended to the input text</code>，就是为了达成统一下游任务的数据格式。如：['[CLS]&rsquo;, &lsquo;中&rsquo;, &lsquo;国&rsquo;, &lsquo;的&rsquo;, &lsquo;春&rsquo;, &lsquo;节&rsquo;, &lsquo;是&rsquo;, &lsquo;[MASK]&rsquo;, &lsquo;[MASK]&rsquo;, &lsquo;。&rsquo;, &lsquo;[SEP]']。</li><li><strong>合并下游任务的数据集合</strong>：当我们统一了下游任务的数据格式，就可以将这些下游任务数据集合混合在一起。</li><li><strong>LLM具备学习数据集合隐式特征的能力</strong>：论文假设LLM是具备学习上述数据格式隐式特征的能力，并通过实验验证了这个假设。</li><li><strong>Prompt Tuning的本质</strong>：该技术的本质是LLM的核心能力之一就是<strong>提特征</strong>。如果特征很明显，LLM就可以低成本提取。如果特征很隐晦，LLM无法低成本提取、甚至无法提取，<strong>Prompt Tuning</strong>就是改变数据集，将隐式特征转为显式特征。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png alt=image-20240403101321581></p><ul><li><strong>实验效果</strong>：<ul><li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。</li><li>在大规模T5模型上，Promp Tuing效果与FFT持平。</li><li>因此，在大规模模型上，<strong>Promp Tuning</strong>具备巨大的成本优势。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101305842.png alt=image-20240403101305842></p><h1 id=3design-decisions实验设计>3.Design Decisions(实验设计)</h1><h2 id=1下游任务数据格式归一化的理论基础>(1)下游任务数据格式归一化的理论基础</h2><p>论文的实验对象是T5，因为T5有一个有趣的观点：</p><ul><li><strong>Following the “text-to-text” approach of T5 (Raffelet al., 2020), we cast all tasks as text generation</strong>：所有的下游任务都可以等效于文本生成。这个观点就可以支撑<strong>Prompt Tuning</strong>将所有下游任务的训练数据格式统一起来。</li><li>如：翻译下游任务，可以将训练数据构造为：&ldquo;translate English to German: hello world!&rdquo;</li><li>如：摘要下游任务，可以将训练数据构造为：&ldquo;summarize: xxxxxxxxxxxxxxxxxxx&rdquo;</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403112458776.png alt=image-20240403112458776></p><h2 id=2问题建模>(2)问题建模</h2><p>论文对问题进行了数学建模：</p><ul><li><strong>Pr<sub>θ</sub>(Y|X)</strong>：在不同下游任务的训练数据可归一化的前提下，大语言模型可被建模为<strong>Pr<sub>θ</sub>(Y|X)</strong>，X是用户输入的Tokens，Y是在X发生的概率下模型的输出。</li><li><strong>Prompt Design</strong>的短板：提示词工程需要人类不断尝试寻找出合适提示词，这种不断尝试方法可能是人工寻找的，也可能采用了非可微的搜索方法(如：前文提到的<strong>a search algorithm over the discrete space of words</strong>)。</li><li><strong>Pr<sub>θ;θ<sub>P</sub></sub> (Y|[P; X])</strong>：这个公式表达了Prompt Tuning的核心思想——在训练数据中植入特殊Token，大模型除了学习训练数据中的显式特征外，还能学习Prompt形式训练数据的隐式特征。对于Prompt隐式特征的学习最终影响的不是预训练模型的参数θ，而是在修正θ<sub>P</sub>。</li><li><strong>[P<sub>e</sub>; X<sub>e</sub>] ∈ R<sup>(p+n)×e</sup></strong>：Prompt中的普通标记被大模型嵌入后得到<strong>X<sub>e</sub></strong>(e是向量空间的维度)，Prompt中的特殊标记被大模型嵌入后得到<strong>P<sub>e</sub></strong>(e是向量空间的维度)。[P<sub>e</sub>; X<sub>e</sub>]则表示输入给大模型后续神经网络层的高维向量。训练的影响并不会修正<strong>X<sub>e</sub></strong>关联的模型参数，只会修正<strong>P<sub>e</sub></strong>关联的模型参数。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115838249.png alt=image-20240403115838249></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115900991.png alt=image-20240403115900991></p><p>因此，实验的关注点如下：</p><ul><li><strong>P<sub>e</sub>的初始值</strong>：和Prefix Tuning一样，都需要关注软提示的初始值，以提升训练速度和效果。</li><li><strong>P<sub>e</sub>的长度</strong>：和Prefix Tuning一样，也需要关注软提示的长度，以降低训练成本。<ul><li><strong>Prompt Tuning的参数成本=E*P</strong>：E是普通标记的向量维数，P是特殊标记的长度。</li></ul></li></ul><h2 id=3如何消减特殊标记的影响>(3)如何消减特殊标记的影响</h2><p>由于增加了特殊标记，大模型学习的内容就不再是原汁原味的"人类自然语言"了。这样就可能导致大模型无法用自然语言作答——这就好像你在训练大模型鸟语但又期待它能说人话、你在用中文教英语最后学会的是Chinglish。</p><p>论文中提出了**Span Corruption(跨度损失)**的概念：</p><ul><li><strong>Span Corruption(跨度损失)</strong>：比如，训练数据<code>Thank you [X] me to your party [Y] week</code>，<input checked disabled type=checkbox> 、[Y]就是特殊标记，将这种特殊标记植入自然语言的目的是"问题模式等隐式特征的显性化&rdquo;，但弊端就是让大语言模型学会了非人类的自然语言。</li><li><strong>论文提出了三种解决方法</strong>：<ul><li><strong>Span Corruption法</strong>：啥都不做，任由大语言模型输出特殊标记，忽略这种影响。</li><li><strong>Span Corruption+Sentinel法</strong>：在大语言模型中增加Sentinel，一定程度地降低这种影响。</li><li><strong>LM Adaptation法</strong>：采用Raffel提出的一个小模型，纠正大语言模型输出特殊标记的倾向，最终输出纯粹的自然语言。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403115956245.png alt=image-20240403115956245></p><h1 id=4experiments实验结果>4.Experiments(实验结果)</h1><h2 id=41实验结果>4.1.实验结果</h2><p>论文阐述了详细的实验过程、实验数据，最终的实验结果如前文所述：</p><ul><li>在小规模T5模型上，提示词工程效果最差、FFT效果最好、Promp Tuning效果中等。</li><li>在大规模T5模型上，Promp Tuing效果与FFT持平。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120117025.png alt=image-20240403120117025></p><ul><li>当模型规模逐渐变大，<strong>Promp Tuning</strong>涉及的参数相较于<strong>Prefix Tuning</strong>更少，但微调效果持平，因此<strong>Prompt Tuning具备巨大的成本优势</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403120213717.png alt=image-20240403120213717></p><h2 id=42重要发现>4.2.重要发现</h2><p>论文在前述实验结果下，有如下重要发现：</p><p>论文从可解释性方面发现了<strong>语义聚合现象</strong>，进一步证明了Prompt形式的数据更有利于大语言模型学习其隐式特征：</p><ul><li><strong>语义聚合现象</strong>：观测被大语言模型嵌入后的特殊标记和普通标记，可以发现出现了物以类聚的现象：<ul><li>如：Technology / technology / Technologies / technological / technologies相关的训练数据，向量相似度发生了语义聚合。</li><li>语义聚合的出现，说明了大语言模型学习到了Prompt形式的训练数据中的隐式特征，因此可以举一反三地处理为见过的下游任务相关输入。</li></ul></li></ul><p>论文还证明了<strong>Prompt Ensembling(提示集成能力)</strong>：</p><ul><li>基于Prompt Tuning的技术思想，可以做到数据格式统一、不同下游任务的训练数据混合训练，进而达到&rdquo;<strong>一个大模型支持多种不同下游任务</strong>"。</li><li>这种思想可以在超大规模的模型上极大地降低训练成本、使用成本。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403101321581.png alt=image-20240403101321581></p><p>在工程实践方面，论文也给出了<strong>Prompt长度、Prompt初始值</strong>的相关推荐：</p><ul><li><p><strong>Prompt长度的影响</strong>：</p><ul><li>在中小参数规模的模型上，Prompt长度越长，提示效果越好，但过犹不及(实验长度的临界值是150)——超过了一定的阈值，就会出现推理性能下降。</li><li>在大参数规模的模型上，Prompt长度反而没有什么影响了。</li></ul></li><li><p><strong>Prompt的初始值选择的影响</strong>：随机初始化Prompt的效果远差于用下游任务相关联提示词做初始值的效果。</p></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121146629.png alt=image-20240403121146629></p><p>最后，论文还通过消融实验，补充了消减<strong>Span Corruption</strong>的建议：</p><ul><li><strong>LM Adaptation</strong>：在中小规模模型上，采用LM Adaptation，对大语言模型的纠正效果更好。LM Adaptation增加步数会达到更好的纠正效果。</li><li>在大规模模型上，Span Corruption的影响也可以忽略不计了。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPromptTuning/image-20240403121223051.png alt=image-20240403121223051></p><h1 id=5总结>5.总结</h1><p>从上述论文解读中，我们收获了如下技术观点：</p><ul><li><strong>Prompt Tuning的价值</strong>：追求一套预训练模型，搞定多个下游任务。</li><li><strong>Prompt Tuning的核心思想</strong>：通过归一化不同下游任务的训练数据，并将隐式特征显性化，帮助大语言模型学习。</li><li><strong>Prefix Tuning的工程实践经验</strong>：<ul><li>Prompt形式的训练数据有助于LLM学习隐式特征。</li><li>采用Prompt Tuning可用一套模型搞定多个下游任务。</li><li>对于大规模参数的模型，Prompt长度和初始化影响很小。</li><li>对于中小规模参数的模型，Prompt长度和初始值可参考Prefix Tuning的实践。</li></ul></li></ul><p>论文链接：https://arxiv.org/pdf/2104.08691.pdf</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-04-03</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B046-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bptuningv1/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记46-LLM微调技术之P-Tuning V1</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/><span class="next-text nav-default">【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>