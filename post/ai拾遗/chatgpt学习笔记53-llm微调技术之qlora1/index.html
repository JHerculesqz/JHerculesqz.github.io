<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记53-LLM微调技术之QLoRA(1) - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)"><meta property="og:description" content="我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora1/"><meta property="article:published_time" content="2024-09-20T10:00:59+08:00"><meta property="article:modified_time" content="2024-09-20T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)"><meta itemprop=description content="我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后"><meta itemprop=datePublished content="2024-09-20T10:00:59+08:00"><meta itemprop=dateModified content="2024-09-20T10:00:59+08:00"><meta itemprop=wordCount content="3061"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)"><meta name=twitter:description content="我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记53-LLM微调技术之QLoRA(1)</h1><div class=post-meta><time datetime=2024-09-20 class=post-time>2024-09-20</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 3061 字</span>
<span class=more-meta>预计阅读 7 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1模型量化技术概览>1.模型量化技术概览</a><ul><li><a href=#11模型参数与显存占用计算方法>1.1.模型参数与显存占用计算方法</a></li><li><a href=#12模型量化解决的问题及关键要素>1.2.模型量化解决的问题及关键要素</a></li></ul></li><li><a href=#2量化三步曲>2.量化三步曲</a><ul><li><a href=#21向量量化>2.1.向量量化</a><ul><li><a href=#1int4>(1)INT4</a></li><li><a href=#2fp4>(2)FP4</a></li><li><a href=#3nf4>(3)NF4</a></li></ul></li><li><a href=#22算子量化>2.2.算子量化</a></li><li><a href=#23网络量化>2.3.网络量化</a></li></ul></li><li><a href=#3总结>3.总结</a></li></ul></nav></div></div><div class=post-content><p>我们下一步是解读QLoRA这种微调技术。与LoRA、AdaLoRA不同，理解QLoRA还需要对模型量化技术有一定的了解。因此，本专栏将通过后续3~4篇文章来详细理解一下QLoRA。今天我先简单地介绍一下模型量化技术。</p><h1 id=1模型量化技术概览>1.模型量化技术概览</h1><h2 id=11模型参数与显存占用计算方法>1.1.模型参数与显存占用计算方法</h2><p>在模型训练、模型推理的过程中，需要考虑GPU的显存是否满足要求，那么模型参数与GPU显存具体怎么估算呢？</p><ul><li><strong>总参数量</strong>：已Qwen2-7B-Instruct为例，7B就是参数量为7Billion，即70亿参数。</li><li><strong>每个参数的显存占用</strong>：假设Qwen2-7B-Instruct的dtype选择为Float16，那么每个参数占2个字节，即16bit。</li><li><strong>总显存量</strong>：<font color=red>总参数量 = 每个参数的显存占用 * 总参数量</font>，因此：<ul><li>QWen2-7B-Instruct总显存占用量 = 2字节 * 70亿 = 140亿字节 = 14 * 10<sup>9</sup>字节</li></ul></li><li><strong>总显存量换算</strong>：<font color=red>1GB = 2<sup>30</sup>B ≈ 10<sup>9</sup>字节</font>，因此：<ul><li>Qwen2-7B-Instruct总显存占用量 = 14GB</li></ul></li></ul><p>我们将Qwen2-7B-Instruct以Float16加载到GPU中，观察一下GPU的显存占用大致也是14GB。</p><p>根据上述原理，我们可以提出一个问题：<strong><font color=red>如果降低每个参数的显存占用，是不是就可以降低GPU的总显存量？</font></strong></p><h2 id=12模型量化解决的问题及关键要素>1.2.模型量化解决的问题及关键要素</h2><p><strong>模型量化(Model Quantization)</strong>，就是聚焦于用更少位数的信息表示法，降低模型每个参数的显存占用量，进而降低模型的总显存占用量。</p><ul><li>如：模型权重以Float32存储(即每个参数的显存占用量为32bit)，假设总显存占用量M。</li><li>如果将每个参数量化为Float16(即每个参数的显存占用量为16bit)，则只需要<strong>二分之一</strong>的总显存占用量M。</li><li>如果将每个参数量化为Int8(即每个参数的显存占用量为8bit)，则只需要<strong>四分之一</strong>的总显存占用量M。</li><li>如果将每个参数量话为NF4(即每个菜单树的显存占用量为4bit)，则只需要<strong>八分之一</strong>的总显存占用量M。</li></ul><p>因此，我们可以很自然地想到模型量化技术的<strong>关键要素之一</strong>——<strong>量化精度</strong>：</p><ul><li>用更少位数表示更高位数的数字，精度不会丢失吗？丢失的精度会给模型训练和模型推理带来多少误差？</li><li>简单地将模型的参数向量的每个元素认为是一个用<strong>32bit表达的小数转为用4bit表达</strong>，直觉上就会带来极大地误差。</li></ul><p>另外，还有其它2个关键要素：</p><ul><li><strong>模型架构</strong></li><li><strong>硬件设备</strong>：GPU、NPU、TPU等</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723154227521.png alt=image-20240723154227521></p><h1 id=2量化三步曲>2.量化三步曲</h1><p>量化过程包括：<font color=red><strong>向量量化、算子量化、模型量化</strong></font></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723160736130.png alt=image-20240723160736130></p><h2 id=21向量量化>2.1.向量量化</h2><p>向量量化是所有量化的基础，量化过程包括<strong>量化函数</strong>和<strong>反量化函数</strong>。</p><ul><li>假设<strong>量化函数</strong>为：<strong>X</strong>表示原始向量，<strong>INT8</strong>表示<strong>量化后向量</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162534164.png alt=image-20240723162534164></p><ul><li>假设<strong>反量化函数</strong>为：<strong>INT8</strong>表示<strong>量化后向量</strong>，<strong>QX</strong>表示通过<strong>量化后向量</strong>，还原的<strong>还原向量</strong>。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162633015.png alt=image-20240723162633015></p><ul><li>假设<strong>误差函数</strong>为：<strong>原始向量X</strong>和<strong>还原向量QX</strong>可能存在误差，误差当然越小越好，衡量这个误差的函数就是误差函数。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723162954824.png alt=image-20240723162954824></p><h3 id=1int4>(1)INT4</h3><p>以<strong>INT4的向量量化</strong>过程为例：</p><ul><li>假设<strong>向量系数</strong>为：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165432046.png alt=image-20240723165432046></p><ul><li>假设<strong>原始向量</strong>为：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165524411.png alt=image-20240723165524411></p><ul><li>对<strong>原始向量</strong>进行计算：<ul><li>求每个分量的绝对值的最大值，结果为1.52。</li><li>将每个分量除以1.52，得到<strong>过程向量</strong>[0.18, 0.38, -0.49, 1.00]。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165559900.png alt=image-20240723165559900></p><ul><li>对<strong>过程向量</strong>进行计算：<ul><li>在量化系数中，找到最近的值，得到<strong>最近值向量</strong>。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723165922698.png alt=image-20240723165922698></p><ul><li>对<strong>最近值向量</strong>进行计算，得到最终的<strong>量化后向量</strong>：<ul><li>将最近值向量的每个分量用量化系数的索引进行替换，得到最终的<strong>量化后向量</strong>[9, 10, 4, 15]。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170126385.png alt=image-20240723170126385></p><p>再来看看<strong>INT4的向量反量化</strong>过程：</p><ul><li>将<strong>量化后向量</strong>，根据索引还原为<strong>过程向量</strong>：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170439886.png alt=image-20240723170439886></p><ul><li>将<strong>过程向量</strong>乘以稀疏1.52，得到<strong>还原向量</strong>：<ul><li>还原向量为[0.304, 0.5016, -0.7144, 1.52]。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170605292.png alt=image-20240723170605292></p><ul><li>我们可以看到，<strong>原始向量</strong>和<strong>还原向量</strong>存在一定的<strong>误差</strong>：</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240723170731983.png alt=image-20240723170731983></p><p>从数据分布上看，INT4量化方法更适合如下数据分布特征(中段数据过于一致，此量化方法有局限性)：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240724095119629.png alt=image-20240724095119629></p><h3 id=2fp4>(2)FP4</h3><p>FP4的计算过程不赘述，可参考：</p><blockquote><p><a href=https://huggingface.co/blog/4bit-transformers-bitsandbytes#fp4-precision-in-a-few-words>https://huggingface.co/blog/4bit-transformers-bitsandbytes#fp4-precision-in-a-few-words</a></p></blockquote><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920110352774.png alt=image-20240920110352774></p><h3 id=3nf4>(3)NF4</h3><p>NF4 Quant是QLoRA提出的一种新的数据类型(4-bit NormalFloat)，量化过程中保留了零点，使用2<sup>k</sup>位表示k位数据类型。</p><p>NF4通过估计<strong>两个范围的分位数q<sup>i</sup></strong>，创建了一个非对称的数据类型。第一个范围表示负数部分[-1, 0]的2<sup>k-1</sup>，第二个范围表示正数部分[0, 1]的2<sup>k-1</sup>+1.</p><p>这种量化方法在每个量化bin中都具备相等的期望值数量，这种以零为中心的正态分布数据在信息论中是最优的。</p><p>上述数学方法到底解决什么问题呢？预训练的权重通常具有以零为中心的正态分布(标准差为σ)，缩放σ可以使得权重分布适应NF的范围，量化时的数据类型和神经网络权重的分位数最终都会被归一化到[-1, 1]的范围内。计算分位数的公式：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920112507633.png alt=image-20240920112507633></p><p>更详细的代码参考：</p><blockquote><p><a href=https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/csrc/kernels.cu>https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/csrc/kernels.cu</a></p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-cpp data-lang=cpp><span class=n>__device__</span> <span class=k>static</span> <span class=kt>float</span> <span class=n>nf4_data</span><span class=p>[</span><span class=mi>16</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6961928009986877</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5250730514526367</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.39491748809814453</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.28444138169288635</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.18477343022823334</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.09105003625154495</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.07958029955625534</span><span class=p>,</span> <span class=mf>0.16093020141124725</span><span class=p>,</span> <span class=mf>0.24611230194568634</span><span class=p>,</span> <span class=mf>0.33791524171829224</span><span class=p>,</span> <span class=mf>0.44070982933044434</span><span class=p>,</span> <span class=mf>0.5626170039176941</span><span class=p>,</span> <span class=mf>0.7229568362236023</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>};</span>
</code></pre></td></tr></table></div></div><h2 id=22算子量化>2.2.算子量化</h2><p>在数学领域，一个函数空间到函数空间上的映射O：X→Y，称为算子。广义上，对任何函数进行某一项操作都是一个算子：</p><ul><li>如：微分算子</li><li>如：不定积分算子</li></ul><p>在深度学习中，各种网络结构，都是由若干的计算单元组成，这些计算单元就是算子(Operator)。</p><ul><li>如：卷积层(Convolution Layer)是一个算子</li><li>如：全连接层(Fully-connected Layer， FC layer)中的权值求和过程，也是一个算子。</li></ul><p>如下图所示，Conv1、Pool1、Conv2都是网络中的算子，Conv1和Conv2都是做卷积运算的算子(Convolution)。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920133118786.png alt=image-20240920133118786></p><blockquote><p>参考：https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/devguide/opdevg/tbeaicpudevg/atlasopdev_10_0006.html</p></blockquote><p>算子量化如何开展呢？我们可以看到算子的前端有输入，算子的后端有输出。因此，算子量化就是将输入向量进行量化，对输出向量进行量化，而不修改算子的逻辑。但算子量化也有一些特殊情况：有的算子不能量化输入，有的算子不能量化输出。</p><h2 id=23网络量化>2.3.网络量化</h2><ul><li><p><strong>关闭冗余量化信息</strong>：</p><ul><li><p>在量化神经网络过程中，需要有量化控制信息向量TQC。</p></li><li><p>在神经网络中，算子Conv连接到算子Reshape上，那么就会出现两个一模一样的TQC。</p></li><li><p>这种量化控制信息就是多余的，我们就需要消除掉Reshape侧的量化控制信息。</p></li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920134759671.png alt=image-20240920134759671></p><ul><li><strong>图融合</strong><ul><li>本质是将N个标准的小算子进行融合，如：对Conv后的激活函数进行算子融合。</li><li>算子融合后得到了一个新的非标的大算子，进而减少了量化控制信息。</li><li>如下图：为了实现FlashAttention，将图中的MatMul算子(Cube）、Scale算子(Vector)、Mask算子(Vector)、SoftMax算子(Vector)融合为一个大的算子Flash Attention。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B053-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(1)/image-20240920135602648.png alt=image-20240920135602648></p><blockquote><p>参考：https://www.hiascend.com/doc_center/source/zh/canncommercial/80RC2/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0041.html</p></blockquote><h1 id=3总结>3.总结</h1><p>为了解读QLoRA的原理，我们简要地介绍了模型量化技术的概念和基本原理：</p><ul><li>模型量化的第一层是向量量化，即针对向量量化，有量化函数和反量化函数。</li><li>模型量化的第二层是算子量化，即针对算子的输入向量和输出向量进行量化。</li><li>模型量化的第三层是网络量化，即针对网络去除冗余的量化控制信息、将小算子融合为大算子进行整体量化处理。</li></ul><p>有了本文的模型量化基础知识，我们就可以进一步阅读QLoRA的论文了。</p><p>论文链接：https://arxiv.org/pdf/2303.10512</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-09-20</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B052-%E9%BA%A6%E8%82%AF%E9%94%A1%E7%94%9F%E6%88%90%E5%BC%8Fai%E4%B8%8E%E7%BE%8E%E5%9B%BD%E5%B7%A5%E4%BD%9C%E6%9C%AA%E6%9D%A5%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB/><span class="next-text nav-default">【chatGPT】学习笔记52-麦肯锡《生成式AI与美国的未来工作》报告解读</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>