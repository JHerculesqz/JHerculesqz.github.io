<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 05 Sep 2023 19:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</link><pubDate>Tue, 05 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</guid><description>&lt;p>今天我们再来写一篇关于大语言模型的实战应用——如何开发一个垂直领域的知识问答系统？&lt;/p>
&lt;h1 id="1原理">1.原理&lt;/h1>
&lt;h2 id="1基于传统技术的实现方案">(1)基于传统技术的实现方案&lt;/h2>
&lt;p>在没有大语言模型之前，有如下传统技术实现知识问答系统：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>关键词匹配&lt;/strong>：在系统内预设一些关键词，系统根据用户提出的问题进行关键词匹配，从中提取出匹配的答案。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：仅适用于简单、明确的问题，但复杂问题、多义词等，效果就不好。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>规则匹配&lt;/strong>：在系统内预设一些规则模板，系统根据用户提出的问题结构进行匹配，这些规则模板包括：语法规则、语义规则、业务领域规则。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家编写规则模板，对领域知识的抽象和表达能力有一定要求。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>统计方法&lt;/strong>：系统基于统计学模型构建，如：条件随机场(CRF，Conditional Random Field)，进行问题分类、命名实体识别等。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：强依赖数据，数据采集、数据清洗、数据标注，都要消耗人类巨大的工作量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>知识图谱&lt;/strong>：系统基于领域相关的知识图谱或实体构建，可以通过图谱中的实体、关系和属性实现问题解析和答案生成。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家对业务领域的知识进行建模、抽象，好不容易构建好，知识刷新了。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2基于大语言模型的实现方案">(2)基于大语言模型的实现方案&lt;/h2>
&lt;p>和老李师傅聊大语言模型，他说：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>大语言模型，是从&lt;strong>word&lt;/strong>到&lt;strong>world&lt;/strong>的过程——AI学习一堆word的关系，然后就具备了概括、抽象、推理的能力去描述世界。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>人类程序员，是从&lt;strong>world&lt;/strong>到&lt;strong>word&lt;/strong>的过程——人类学习这个世界，然后用代码描述世界(代码就是一堆word的组合)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>最后，他老人家感叹道：&amp;ldquo;N年后，别人看我们，就像我们看伏尔加河上的纤夫。纤夫光着腚，我们光着头。&amp;rdquo;&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230905163751495.png" alt="image-20230905163751495">&lt;/p>
&lt;p>的确如此，相较于前述传统技术，基于大语言模型实现知识问答系统，有很多天然优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学习效率高&lt;/strong>：大语言模型学习速度极快。后文实践章节中，有这么一个例子：我找公路军团的同学要了1000多份交通专业的文档和书籍，大语言模型&lt;strong>1分钟学习10个文档，2小时学完1000份&lt;/strong>，而&lt;strong>每小时只消耗了1.39元&lt;/strong>。这位交通领域的专家无比惊讶地告诉我——这1000多个文档，他也只详细看完了其中200个。&lt;/li>
&lt;li>&lt;strong>无需人工干预&lt;/strong>：传统技术需要大量的人工干预，特别强依赖人类专家那些&lt;strong>只可意会、不可言传&lt;/strong>的经验。而大语言模型可以自动学习和更新知识，无需人工干预。这意味着问答应用可以及时获取最新的知识，随着时间的推移变得更加智能和准确。&lt;/li>
&lt;li>&lt;strong>多轮对话&lt;/strong>：大语言模型还可以处理复杂的问题和多轮对话。它能够理解问题的语义和上下文，并根据用户的追问进行适当的回答。这使得问答应用更加交互式和人性化，提供更好的用户体验。&lt;/li>
&lt;li>……好处太多，省略千言万语……&lt;/li>
&lt;/ul>
&lt;h2 id="3大语言模型选型">(3)大语言模型选型&lt;/h2>
&lt;p>大语言模型的选型需要根据LLM App的应用场景，并且也不是只选1个，而是选择N个，形成大语言模型矩阵。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906090930061.png" alt="image-20230906090930061">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>世界知识&lt;/strong>：从世界知识看，目前主流的LLM如下：
&lt;ul>
&lt;li>&lt;strong>GPT系&lt;/strong>：当红炸子鸡的它，提供的接口包括&lt;code>Model&lt;/code>、&lt;code>Completions&lt;/code>、&lt;code>Chat&lt;/code>、&lt;code>Edits&lt;/code>、&lt;code>Images&lt;/code>、&lt;code>Embeddings&lt;/code>、&lt;code>Audio&lt;/code>、&lt;code>Files&lt;/code>、&lt;code>Fine-tunes&lt;/code>、&lt;code>Others&lt;/code>等10大类，完整地覆盖了&lt;strong>训练&lt;/strong>、&lt;strong>推理&lt;/strong>场景。针对&lt;strong>微调场景&lt;/strong>，覆盖了词嵌入、微调，在&lt;strong>推理&lt;/strong>场景，覆盖了预测、聊天、修正、多模态。&lt;strong>知识文档不涉密且不差钱&lt;/strong>的公司，可直接使用。笔者为了测试GPT4的API，&lt;font color=green>&lt;strong>半小时就花光了40大洋&lt;/strong>&lt;/font>，求赞助！&lt;/li>
&lt;li>&lt;strong>LLama系&lt;/strong>：卷王Meta的开源大戏，LLama一出，瞬间抢走了chatGPT的焦点。随后各顶流大学就开始了基于LLama的训练微调，推出了以Vicuna、Alpaca为代表的一系列模型。LLama2发布后更是好评无数，没几天又推出了最会写代码的CodeLLama。。。大语言模型界的卷王实至名归！&lt;/li>
&lt;li>&lt;strong>Claude系&lt;/strong>：谷歌前员工的大神们拉旗单干的产品，也是GPT系的强劲对手。&lt;/li>
&lt;li>&lt;strong>GLM系&lt;/strong>：清华大学出品，公司化商业运作。chatGLM的中文能力非常不错，GLM-130B成为2022年亚洲唯一入选全球30个主流大模型全方位测评报告的候选对象。国货之光，本文的知识问答系统就是演示的它。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPT系&lt;/td>
&lt;td>GPT3.5Turbo、GPT4&lt;/td>
&lt;td>OpenAI&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>LLama、LLama2、CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Claude系&lt;/td>
&lt;td>Claude-instant、Claude-2-100k&lt;/td>
&lt;td>Anthropic&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>chatGLM2、GLM-130B、VisualGLM-6B&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>领域知识&lt;/strong>：以编程辅助领域为例，目前表现不错的LLM如下：&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BigCode系&lt;/td>
&lt;td>StarCoder、OctoPack、SantaCoder&lt;/td>
&lt;td>Hugging Face&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>CodeGeeX2&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>综上分析：&lt;/p>
&lt;ul>
&lt;li>对世界知识的表现体现的是大语言模型的&lt;strong>通才&lt;/strong>，对领域知识的表现体现的是大语言模型的&lt;strong>专才&lt;/strong>。&lt;/li>
&lt;li>在没有一个大语言模型即是通才又是专才的限制下，垂直领域知识问答系统需要的是&lt;strong>大语言模型矩阵&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="4大语言模型之外的关键技术">(4)大语言模型之外的关键技术&lt;/h2>
&lt;p>是不是选择好了大语言模型，LLM APP就信手拈来呢？很不幸，不是！&lt;/p>
&lt;p>在最近的一次行业交流中，有两个议题值得思考：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量变引起质变&lt;/strong>：百度智能云/技术委员会主席王耀，针对大语言模型的训练推理场景，阐述了通用云计算、分布式云、智能计算的计算范式的变化。其中，最心酸的一个故事是：以前硬件上某个万分之一的故障是可以接受的，现在不可以了，因为大语言模型训练的计算量级无比巨大！&lt;/li>
&lt;li>&lt;strong>淘金时代的卖铲人&lt;/strong>：LangChain CEO Harrision Chase分享的议题是《Why LangChain，What I Saw When Building AI Application with LLM》，在各大厂商逐鹿中原的时候，LangChain忽然火了，按照这个趋势，它应该会成为LLM APP的开发框架了。&lt;/li>
&lt;/ul>
&lt;p>因为，&lt;strong>训练&lt;/strong>大语言模型的计算量剧增，对原有的云计算有新的诉求。&lt;/p>
&lt;p>因为，&lt;strong>粘合&lt;/strong>大语言模型需要有很多工作，需要有LLM APP的开发框架。&lt;/p>
&lt;p>通过LangChain的特性介绍和架构，我们可以知道大语言模型之外，我们还需要做如下事情：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>具体可以看笔者这篇文章：《【chatGPT】学习笔记10-LangChain之ModelIO，对LLM的抽象1》&lt;/p>
&lt;h2 id="5架构示例langchain-chatchat">(5)架构示例：LangChain-ChatChat&lt;/h2>
&lt;p>垂直领域问答系统的开源项目已经有很多了，以LangChain-ChatChat为例，可以看到这类LLM APP的软件架构：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095324183.png" alt="image-20230906095324183">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>训练环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unstructured Loader&lt;/strong>：负责将知识文档加载并解析。&lt;/li>
&lt;li>&lt;strong>Text Splitter&lt;/strong>：将文档按照标点符号，拆分断句。&lt;/li>
&lt;li>&lt;strong>Text Chunks&lt;/strong>：将断句分词分组，分词分组间存在一定的上下文关联。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095608282.png" alt="image-20230906095608282">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Embedding&lt;/strong>：词嵌入，并将词向量存储于向量数据库中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>应用环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Query Embedding&lt;/strong>：将用户提问转换了词向量。&lt;/li>
&lt;li>&lt;strong>Vector Similarity&lt;/strong>：在向量数据库中，用问题词向量，去搜索相关的知识文本向量。&lt;/li>
&lt;li>&lt;strong>Related Text Chunks&lt;/strong>：根据相关的知识文本向量，反查出对应的知识文本分词分组。这里有个细节，还会将文本分词分组的前后分词分组一并返回(因为这些文字可能有上下文关联关系)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100110319.png" alt="image-20230906100110319">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Template&lt;/strong>：提示词模板，这个模块是知识问答系统的关键！最简单的提示词模板就是：
&lt;ul>
&lt;li>&lt;strong>已知&lt;/strong>：[Related Text Chunks获得的知识文本分词分组]，&lt;strong>请问&lt;/strong>[用户的问题]如何答复？&lt;/li>
&lt;li>这，不就是开卷考试吗？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100510639.png" alt="image-20230906100510639">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>从上述架构可以看出来，曾经很复杂的知识问答系统，大部分核心难点，都被大语言模型搞定了。&lt;/p>
&lt;p>接下来，我们来动手实践一下，构建一个我们自己的知识问答系统。&lt;/p>
&lt;h1 id="2实践">2.实践&lt;/h1>
&lt;h2 id="21推理环境搭建">2.1.推理环境搭建&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.基于MiniCoda，创建虚拟环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101650577.png" alt="image-20230906101650577">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.激活虚拟环境，安装运行时环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101852342.png" alt="image-20230906101852342">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP3.下载依赖。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102057137.png" alt="image-20230906102057137">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP5.初始化向量数据库。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102553686.png" alt="image-20230906102553686">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP6.配置model_config.py、server_config.py、llm_api.py。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102733935.png" alt="image-20230906102733935">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102716203.png" alt="image-20230906102716203">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102703074.png" alt="image-20230906102703074">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102652268.png" alt="image-20230906102652268">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP7.启动应用&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103336869.png" alt="image-20230906103336869">&lt;/p>
&lt;h2 id="22模型部署">2.2.模型部署&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.下载大语言模型和词嵌入模型&lt;/strong>——chatGLM2-6B、m3e-base&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102213647.png" alt="image-20230906102213647">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102246753.png" alt="image-20230906102246753">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.部署大语言模型和词嵌入模型。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103747404.png" alt="image-20230906103747404">&lt;/p>
&lt;h2 id="23模型微调">2.3.模型微调&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.通过WebUI，上传知识文档。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103254463.png" alt="image-20230906103254463">&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>&lt;strong>STEP2.编写提示词模板&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104642146.png" alt="image-20230906104642146">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104709680.png" alt="image-20230906104709680">&lt;/p>
&lt;ul>
&lt;li>经过2小时的学习，LLM修炼完了1000多份智慧交通的资料，这学习效率杠杠滴！
&lt;ul>
&lt;li>PS：感谢公路军团的专家，提供珍藏多年的资料，改天请您吃饭！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906105032126.png" alt="image-20230906105032126">&lt;/p>
&lt;h2 id="24模型测试">2.4.模型测试&lt;/h2>
&lt;p>直接上视频，咨询了一下交通行业的这位专家，回答挺靠谱：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=830744059&amp;bvid=BV1C34y1N7QG&amp;cid=1258811184&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文阐述了基于大语言模型的知识问答系统的实现原理及原型实践过程，原型距离可商用还有很多细节需打磨，但我们可以得到如下观点：&lt;/p>
&lt;ul>
&lt;li>从实现原理看：
&lt;ul>
&lt;li>&lt;strong>技术关键点1&lt;/strong>：要&lt;font color=red>&lt;strong>选择合适的多种大语言模型，形成大语言模型的矩阵&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>技术关键点2&lt;/strong>：除大语言模型选型，还需&lt;strong>借助LangChain处理好诸多环节&lt;/strong>(文档结构化、分段分词、词嵌入、上下文、向量搜索、提示词等)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>从实践看：
&lt;ul>
&lt;li>&lt;strong>数据&lt;/strong>：数据收集、数据清洗、词嵌入等，极为重要。&lt;/li>
&lt;li>&lt;strong>大语言模型&lt;/strong>：需跟踪各大厂商大语言模型的最新能力，快速替换到应用软件中，&lt;font color=red>&lt;strong>保持大语言模型的先进性&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>提示词&lt;/strong>：提示词模板的设计，决定了能否&lt;font color=red>&lt;strong>充分发挥大语言模型的潜力&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>工具链&lt;/strong>：熟练运用LangChain、向量数据库等工具，有助于设计并实现适应业务场景的问答业务流。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>今天就写到这里，后续专栏会继续展示基于大语言模型的LLM应用，欢迎共同探索(上述环境已做成镜像，需要的同学可后台联系作者)。&lt;/p></description></item><item><title>【chatGPT】学习笔记10-LangChain之Model IO，对LLM的抽象1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</link><pubDate>Mon, 28 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</guid><description>&lt;p>最近的专栏都是在拆解大语言模型的内部实现及论文，我们再来写点儿偏工程实践的内容——LangChain。&lt;/p>
&lt;h1 id="1langchain简介">1.LangChain简介&lt;/h1>
&lt;h2 id="1如何实现基于ai的app">(1)如何实现基于AI的App？&lt;/h2>
&lt;p>如果我们想开发一个基于大语言模型的AI知识库，怎么做呢？这个App的架构如下图：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>准备环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP1.数据管理&lt;/strong>：需要将垂直领域的知识进行词嵌入，放到向量数据库中。&lt;/li>
&lt;li>&lt;strong>STEP2.提示词管理&lt;/strong>：需要构造好提示词模板。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>微调&amp;amp;应用环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP3.知识查询&lt;/strong>：将用户输入的自然语言问题向量化，寻找与输入问题相关的知识向量。&lt;/li>
&lt;li>&lt;strong>STEP4.提示词查询&lt;/strong>：找到和输入问题有关的提示词模板。&lt;/li>
&lt;li>&lt;strong>STEP5.调用大语言模型&lt;/strong>：将&lt;code>知识向量&lt;/code>+&lt;code>提示词模板&lt;/code>传递给大语言模型(大语言模型可能在本地，也可能在云端，大语言模型可能是1个也可能是多个)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901152726410.png" alt="image-20230901152726410">&lt;/p>
&lt;p>从这个架构可以看出，存在以下问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文档解析多样&lt;/strong>：垂直领域的知识文档，存在不同的文件格式，如何解析？&lt;/li>
&lt;li>&lt;strong>提示词需要复用&lt;/strong>：无论面向多少种不同厂商的大语言模型，提示词如何最大化复用？&lt;/li>
&lt;li>&lt;strong>模型多样&lt;/strong>：需要面向不同厂商大语言模型的不同调用接口，如何适配？&lt;/li>
&lt;/ul>
&lt;h2 id="2rag">(2)RAG&lt;/h2>
&lt;p>LangChain，为私有知识库App这类应用，定义了一个新的领域——&lt;strong>RAG&lt;/strong>(Retrieval Augmented Generation，生成式检索增强)。&lt;/p>
&lt;p>在RAG领域，通常有5类需要考虑的因素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154308580.png" alt="image-20230901154308580">&lt;/p>
&lt;h2 id="3langchain的体系结构">(3)LangChain的体系结构&lt;/h2>
&lt;p>LangChain为了解决RAG领域的五类问题，提供了6大模块，如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154845846.png" alt="image-20230901154845846">&lt;/p>
&lt;p>我们在接下来的内容中，逐一解读各个模块的特性以及关键源码。&lt;/p>
&lt;h1 id="2核心模块1model-io">2.核心模块1：Model I/O&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>LangChain的第一个核心模块，就是&lt;code>Model I/O&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O最重要的能力就是封装了各大厂商大语言模型的不同接口。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O包括三个子特性：&lt;/p>
&lt;ul>
&lt;li>Prompts&lt;/li>
&lt;li>Models&lt;/li>
&lt;li>Output Parsers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/model_io-1f23a36233d7731e93576d6885da2750.jpg" alt="model_io_diagram">&lt;/p>
&lt;h2 id="21models模型">2.1.Models(模型)&lt;/h2>
&lt;ul>
&lt;li>在Models特性中，LangChain抽象了两类模型：
&lt;ul>
&lt;li>语言模型：LLMs，LangChain在这里封装了各厂商大语言模型的接口。&lt;/li>
&lt;li>聊天模型：ChatModels，对语言模型的高层封装，提供输入一组聊天消息对象、输出聊天结果对象的模式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="211llm语言模型">2.1.1.LLM(语言模型)&lt;/h3>
&lt;h4 id="1langchain源码解读">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过3个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLM&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>与LLM相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>LLMReusult&lt;/code>，&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManagerForLLMRun&lt;/code>，&lt;code>AsyncCallbackManagerForLLMRun&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManager&lt;/code>, &lt;code>AsyncCallbackManager&lt;/code>&lt;/li>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901160851034.png" alt="image-20230901160851034">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>class &lt;strong>BaseLanguageModel&lt;/strong>：语言模型的基类，所有语言模型封装子类，都继承自本类。&lt;/p>
&lt;ul>
&lt;li>def &lt;strong>generate_prompt&lt;/strong>：这是一个抽象函数，该函数由各个语言模型封装子类自行实现，输入提示词序列，输出是语言模型返回的结果。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901161907636.png" alt="image-20230901161907636">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>BaseLLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>，实现了各语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162431013.png" alt="image-20230901162431013">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLLM&lt;/strong>，进一步封装个语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162650458.png" alt="image-20230901162650458">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>LangChain实现的各类语言模型封装的子类：&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163511284.png" alt="image-20230901163511284">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="2例子">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT的达芬奇003模型&lt;/strong>。其中，class &lt;strong>OpenAI&lt;/strong>就是对GPT的模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：给我一个Java的ArrayList的代码示例。&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：是一段ArrayList代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163837808.png" alt="image-20230901163837808">&lt;/p>
&lt;h3 id="212chatmodel聊天模型">2.1.2.ChatModel(聊天模型)&lt;/h3>
&lt;h4 id="1langchain源码解读-1">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过2个关键类实现聊天模型：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseChatModel&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>, &lt;code>HumanMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901164637931.png" alt="image-20230901164637931">&lt;/p>
&lt;h4 id="2例子-1">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT3.5 Turbo模型&lt;/strong>。其中，class &lt;strong>ChatOpenAI&lt;/strong>就是对GPT的聊天模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：一组聊天消息对象&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：聊天消息的回答对象。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165247536.png" alt="image-20230901165247536">&lt;/p>
&lt;h2 id="22prompts提示词">2.2.Prompts(提示词)&lt;/h2>
&lt;h3 id="1langchain源码解读-2">(1)LangChain源码解读&lt;/h3>
&lt;ul>
&lt;li>LangChain通过2个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BasePromptTemplate&lt;/strong>：基础提示词的基类。&lt;/li>
&lt;li>class &lt;strong>BaseMessagePromptTemplate&lt;/strong>：聊天模型提示词的基类。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165527920.png" alt="image-20230901165527920">&lt;/p>
&lt;h3 id="2例子-基础提示词">(2)例子-基础提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_template&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>lang&lt;/code>变量，通过1个提示词模板，实现了生成&lt;code>java&lt;/code>、&lt;code>python&lt;/code>、&lt;code>C++&lt;/code>的冒泡排序代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170042600.png" alt="image-20230901170042600">&lt;/p>
&lt;h3 id="2例子-chatmodel提示词">(2)例子-ChatModel提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_messages&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>user_input&lt;/code>变量，通过1个提示词模板，实现了生成对Java的ArrayList的知识点摘要。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了聊天模型的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170414376.png" alt="image-20230901170414376">&lt;/p>
&lt;h3 id="3例子-fewshot提示词">(3)例子-FewShot提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>FewShotPromptTemplate&lt;/code>构造了FewShot提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>input&lt;/code>变量，通过1个提示词模板，实现了一系列FewShot语料。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了FewShot的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170805004.png" alt="image-20230901170805004">&lt;/p>
&lt;h2 id="23output-parsers输出解析器">2.3.Output Parsers(输出解析器)&lt;/h2>
&lt;ul>
&lt;li>输出解析器是一个很有趣的特性，大语言模型返回的答案是千奇百怪的，如何解析呢？&lt;/li>
&lt;li>这里以&lt;code>List Parser&lt;/code>为例：
&lt;ul>
&lt;li>LangChain提供了class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>，这个类会在提示词中限定大语言模型的返回。&lt;/li>
&lt;li>比如：输入的问题是&lt;code>请问《圣斗士星矢》中，有哪几个主角？&lt;/code>。&lt;/li>
&lt;li>class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>会构造提示词：Your response should be a list of comma separated values, eg: &lt;code>foo, bar, baz&lt;/code>。&lt;/li>
&lt;li>大语言模型回答问题的文本就有了固定格式：&lt;code>星矢、紫龙、冰河、瞬、一辉&lt;/code>，这样就可以解析成list了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901171701655.png" alt="image-20230901171701655">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文介绍了：&lt;/p>
&lt;ul>
&lt;li>LangChain的功能，为大家构建了对LangChain的宏观认识。&lt;/li>
&lt;li>LangChain的Model I/O核心模块：
&lt;ul>
&lt;li>该模块通过Models，抽象了各厂商的大语言模型，我们可以使用统一接口去调用不同的大语言模型。&lt;/li>
&lt;li>该模块通过Prompts，提供了多种提示词模板的构建。&lt;/li>
&lt;li>该模块通过Output Parsers，提供了对大语言模型输出结果的结构化解析。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</link><pubDate>Mon, 21 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</guid><description>&lt;h1 id="1seq2seqtransformer的雏形">1.Seq2Seq，Transformer的雏形&lt;/h1>
&lt;h2 id="11为什么会出现seq2seq">1.1.为什么会出现Seq2Seq？&lt;/h2>
&lt;p>在神经概率语言模型NPLM出现后的很长一段时间，都是在这种网络架构下进行优化。但，依然面临很多难题(主要是循环神经网络RNNs的局限)：&lt;/p>
&lt;ul>
&lt;li>如：输入序列长度增加时性能下降&lt;/li>
&lt;li>如：顺讯处理导致计算效率低&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>在2014年，Seq2Seq的提出，给人类一个不错的启发。&lt;/p>
&lt;p>随后在自注意力机制的加持下，Transformer就诞生了。&lt;/p>
&lt;h2 id="12seq2seq架构概览">1.2.Seq2Seq架构概览&lt;/h2>
&lt;p>大神&lt;code>Ilya Sutskever&lt;/code>在2014年，以第一作者的身份，发表了论文《Sequence to Sequence Learning with Neural Networks》。&lt;/p>
&lt;blockquote>
&lt;p>论文地址：https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162206474.png" alt="image-20230831162206474">&lt;/p>
&lt;p>顺便说一嘴，大神&lt;code>Ilya Sutskever&lt;/code>就是这位大哥，OpenAI联合创始人和首席科学家，各大自媒体都在播放他老人家的演讲视频：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162529973.png" alt="image-20230831162529973">&lt;/p>
&lt;p>这篇论文的核心就是这张图，阐述了Seq2Seq的&lt;strong>编码器-解码器架构&lt;/strong>：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/p>
&lt;p>初次理解这张图，&lt;strong>需要费点儿脑细胞&lt;/strong>，我们接下来详细拆解。&lt;/p>
&lt;h2 id="13seq2seq架构详解">1.3.Seq2Seq架构详解&lt;/h2>
&lt;h3 id="1整体模型架构">(1)&amp;ldquo;整体模型&amp;quot;架构&lt;/h3>
&lt;p>Seq2Seq会将变长的&lt;strong>输入序列&lt;/strong>，转换为变长的&lt;strong>输出序列&lt;/strong>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170025140.png" alt="image-20230831170025140">&lt;/p>
&lt;p>这里举一个例子——将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;rdquo;：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：Seq2Seq输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：Seq2Seq输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：Seq2Seq输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170404160.png" alt="image-20230831170404160">&lt;/p>
&lt;h3 id="2编码器-解码器架构">(2)&amp;ldquo;编码器-解码器&amp;quot;架构&lt;/h3>
&lt;p>进一步拆解整体模型架构，Seq2Seq由编码器+解码器组成：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171003578.png" alt="image-20230831171003578">&lt;/p>
&lt;p>这里还是以将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;quot;例子来解析：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：编码器将学习到上下文向量，传递给解码器&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：解码器输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：解码器输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t8时刻&lt;/strong>：解码器输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171625206.png" alt="image-20230831171625206">&lt;/p>
&lt;h3 id="3编码器解码器微观逻辑">(3)&amp;ldquo;编码器、解码器&amp;quot;微观逻辑&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>编码器、解码器的具体实现&lt;/strong>：在论文中，它们都是采用RNN实现。如下图所示：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的输出&lt;/strong>：1号隐藏层状态向量 + 1号词输出向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输入&lt;/strong>：1号隐藏层状态向量 + 2号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输出&lt;/strong>：2号隐藏层状态向量 + 2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831222457684.png" alt="image-20230831222457684">&lt;/li>
&lt;li>&lt;strong>解码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的输出&lt;/strong>：1号词输出向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输入&lt;/strong>：0号隐藏层状态向量 + 1号词输出向量 + 2号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输出&lt;/strong>：2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225007834.png" alt="image-20230831225007834">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>这还是不太直观，我们再把编码器、解码器&lt;strong>按时序进一步展开&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器，编码器输出&lt;code>1号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字 + &lt;code>1号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>2号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字 + &lt;code>2号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>3号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;Who&amp;quot;字，输入给解码器，解码器输出&lt;code>Who&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;are&amp;quot;字 + 解码器输出&lt;code>who&lt;/code>，输入给解码器，解码器输出&lt;code>are&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;u&amp;quot;字 + 解码器输出&lt;code>are&lt;/code>，输入给解码器，解码器输出&lt;code>u&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225749199.png" alt="image-20230831225749199">&lt;/p>
&lt;ul>
&lt;li>这里有一个细节，什么是&lt;strong>Teach Forcing&lt;/strong>？
&lt;ul>
&lt;li>我们可以想象，如果1号解码器的预测错了，那么2号、3号解码器都会错，进而导致学习效率非常低。&lt;/li>
&lt;li>Teach Forcing好像&lt;strong>开卷考试&lt;/strong>，如果1号解码器预测错了，Teach Forcing会纠正预测结果，进而加速学习效率。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3优点">(3)优点&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>变长序列&lt;/strong>：由于编码器、解码器采用RNN实现，所以输入序列可以是变长、输出序列也可以是变长。而CNN、DNN都不支持变长序列。&lt;/li>
&lt;li>&lt;strong>信息压缩&lt;/strong>：隐藏层状态，或者叫上下文向量，本质上将输入序列进行了信息压缩，转变为含有上下文语义的向量。&lt;/li>
&lt;/ul>
&lt;h3 id="4劣势">(4)劣势&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>长序列信息损失&lt;/strong>：由于上下文向量为定长，所以当输入序列过长时编码器会出现信息损失。&lt;font color=red>&lt;strong>这就是注意力机制的发力点，先埋个伏笔&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>效率&lt;/strong>：采用RNN(如：LSTM、GRU)实现编码器、解码器，RNN会面临梯度消失、梯度爆炸等问题。&lt;/li>
&lt;/ul>
&lt;h1 id="2代码">2.代码&lt;/h1>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231557775.png" alt="image-20230831231557775">&lt;/p>
&lt;h2 id="step12结构化训练数据">STEP1.2.结构化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231753952.png" alt="image-20230831231753952">&lt;/p>
&lt;h2 id="step21定义编码器和解码器类">STEP2.1.定义编码器和解码器类&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231855888.png" alt="image-20230831231855888">&lt;/p>
&lt;h2 id="step22定义seq2seq模型">STEP2.2.定义Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231925444.png" alt="image-20230831231925444">&lt;/p>
&lt;h2 id="step3训练seq2seq模型">STEP3.训练Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232035429.png" alt="image-20230831232035429">&lt;/p>
&lt;h2 id="step4测试seq2seq模型">STEP4.测试Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232116723.png" alt="image-20230831232116723">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;ul>
&lt;li>本文解析了Seq2Seq的内部实现，理解下图的关键是——&lt;strong>按照时序&lt;/strong>，分析清楚每个编码器or解码器&lt;strong>有几个输入、几个输出&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/li>
&lt;li>理解了Seq2Seq，我们接下来就可以逐步实现完整的Transformer了，且听下回分解。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</link><pubDate>Sat, 19 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</guid><description>&lt;h1 id="1问题词嵌入的局限性">1.问题：词嵌入的局限性&lt;/h1>
&lt;p>在《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》中，我们了解了词嵌入。&lt;/p>
&lt;p>获取词向量后，在向量空间中可以获得每个词的向量表示，也可以通过向量了解词和词之间的语义关联性。&lt;/p>
&lt;p>似乎，仅通过词嵌入就能解决很多NLP问题了，但词嵌入存在如下局限性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>：Word2Vec，以及后续的GloVe，在训练完成后，学习到的词向量是不会再被更新的。&lt;/li>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>决定了：
&lt;ul>
&lt;li>&lt;strong>词嵌入无法应对多义词&lt;/strong>：1个词只有1个向量，无法表示多义词。&lt;/li>
&lt;li>&lt;strong>词嵌入无法应对未知词&lt;/strong>：没见过的词，显然Word2Vec不可能在向量空间中无中生有它对应的向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>于是，神经概率语言模型NPLM横空出世。&lt;/p>
&lt;h1 id="2对策神经概率语言模型nplm">2.对策：神经概率语言模型NPLM&lt;/h1>
&lt;h2 id="1什么是神经概率语言模型">(1)什么是神经概率语言模型？&lt;/h2>
&lt;p>科学家和大神们，很早就有了引入具有强大表示力和学习力的神经网络的想法。大致思想是：&lt;/p>
&lt;ul>
&lt;li>输入海量语料，神经网络学习词语在不同上下文中的概率分布。&lt;/li>
&lt;li>词最终还是会被向量化，只是这些词向量的学习过程成为神经网络的一部分，词向量表达的人类语言规律(词语、语义等)最终被记录到神经网络的参数中。&lt;/li>
&lt;/ul>
&lt;p>接下来，深度学习三巨头之一Bengio，在2003年推出了神作《A Neural Probabilistic Language Model》：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NPLM&lt;/strong>：Neural Probabilistic Language Model，神经概率语言模型。NPLM包含输入层、隐藏层、输出层。&lt;/li>
&lt;li>&lt;strong>输入层&lt;/strong>：将单词映射到连续的词向量空间。&lt;/li>
&lt;li>&lt;strong>隐藏层&lt;/strong>：通过非线性激活函数学习单词间的复杂关系。&lt;/li>
&lt;li>&lt;strong>输出层&lt;/strong>：通过Softmax层产生下一个单词的概率分布。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>论文链接：https://dl.acm.org/doi/pdf/10.5555/944919.944966&lt;/p>
&lt;/blockquote>
&lt;p>下图是Bengio论文中的阐述的神经概率语言模型的架构，后续NLP方向上的各种技术都是围绕这个架构进行各层的优化！&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230818065825957.png" alt="image-20230818065825957">&lt;/p>
&lt;p>对上图进行细化，笔者标出了输入层、隐藏层、输出层：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825094252544.png" alt="image-20230825094252544">&lt;/p>
&lt;h2 id="2输入层">(2)输入层&lt;/h2>
&lt;ul>
&lt;li>输入层的&lt;strong>职责&lt;/strong>：将输入的词，开展词嵌入，学习到词向量后存储在输入层。因此，输入层也可以叫嵌入层。&lt;/li>
&lt;li>输入层的&lt;strong>输入&lt;/strong>：词本身&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095132476.png" alt="image-20230825095132476">&lt;/li>
&lt;li>输入层的&lt;strong>输出&lt;/strong>：词向量&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095205472.png" alt="image-20230825095205472">&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095238666.png" alt="image-20230825095238666">&lt;/p>
&lt;h2 id="3隐藏层">(3)隐藏层&lt;/h2>
&lt;ul>
&lt;li>隐藏层的&lt;strong>职责&lt;/strong>：学习词与词之间的关系&lt;/li>
&lt;li>隐藏层采用了&lt;strong>非线性激活函数&lt;/strong>：采用非线性激活函数的本质是让神经网络的**&amp;ldquo;脑回路&amp;rdquo;**复杂起来(如：使用激活函数可以将线性层提升为非线性层)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095635192.png" alt="image-20230825095635192">&lt;/p>
&lt;h2 id="4输出层">(4)输出层&lt;/h2>
&lt;ul>
&lt;li>输出层的&lt;strong>职责&lt;/strong>：通过Softmax层，输出下个单词的概率分布&lt;/li>
&lt;li>&lt;strong>Softmax&lt;/strong>：归一化，Softmax层的输入是各个词得到的分数，输出是将这些分数归一化到0~1的值域内。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825102215771.png" alt="image-20230825102215771">&lt;/p>
&lt;h2 id="4隐藏层的优化">(4)隐藏层的优化&lt;/h2>
&lt;ul>
&lt;li>神经网络似乎天生不擅长长序列问题，所以后续很多NLP发展的技术，都是在&lt;strong>优化NPLM在长序列上的表现&lt;/strong>。
&lt;ul>
&lt;li>浅层网络无法捕捉文本中复杂的信息规律。&lt;/li>
&lt;li>普通的深层神经网络也不能很好处理长距离的依赖关系。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>NPLM的巧妙之处在于：&lt;strong>隐藏层可以使用任意的神经网络去替换&lt;/strong>。&lt;/li>
&lt;li>RNN、LSTM横空出世，在NLPM中长期霸榜：
&lt;ul>
&lt;li>RNN，循环神经网络，这种特殊的神经网络结构，可以将网络的输出作为网络的输入，使得神经网络能够处理数据的同时保留了&lt;strong>前世的记忆&lt;/strong>。&lt;/li>
&lt;li>LSTM，是RNN的经典代表作，很长一段时间作为NLP问题的SOTA(state of the art)模型
&lt;ul>
&lt;li>注意：被评为SOTA，而不是benchmark，或者baseline，是一种极高的荣誉。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3代码nplm">3.代码：NPLM&lt;/h1>
&lt;p>接下来，进入代码环节。&lt;/p>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络">STEP2.构建NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093351356.png" alt="image-20230826093351356">&lt;/p>
&lt;h2 id="step3训练nplm网络">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093407105.png" alt="image-20230826093407105">&lt;/p>
&lt;h2 id="step4测试nplm">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093420906.png" alt="image-20230826093420906">&lt;/p>
&lt;h1 id="4代码nplm优化">4.代码：NPLM优化&lt;/h1>
&lt;p>再使用RNN、LSTM优化NPLM架构。&lt;/p>
&lt;h2 id="step11构建数据集-1">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据-1">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络-1">STEP2.构建NPLM网络&lt;/h2>
&lt;ul>
&lt;li>核心就是在此替换隐藏层和输出层&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093621577.png" alt="image-20230826093621577">&lt;/p>
&lt;h2 id="step3训练nplm网络-1">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093643273.png" alt="image-20230826093643273">&lt;/p>
&lt;h2 id="step4测试nplm-1">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093701798.png" alt="image-20230826093701798">&lt;/p>
&lt;h1 id="5小结">5.小结&lt;/h1>
&lt;p>最后，我们来做一些对比和小结：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从目标看&lt;/strong>：NPLM是解决词汇出现概率的问题，Word2Vec是解决如何将词转换为向量的问题。&lt;/li>
&lt;li>&lt;strong>从实现看&lt;/strong>：NPLM和Word2Vec都是基于神经网络的模型，但Word2Vec没有激活函数，属于利用了浅层神经网络。&lt;/li>
&lt;li>&lt;strong>从词向量看&lt;/strong>：NPLM中的词向量是神经网络的一部分，基于NPLM的目标，人类对它的训练是不会停止的，训练一次，词向量就会变化一次。而基于Word2Vec的目标，人类只会对它训练一次，训练好，词向量就固化不变了。&lt;/li>
&lt;/ul>
&lt;p>NPLM可以算作如今大语言模型的祖师爷了，在学界存在极高的地位和价值，后续很多年的RNN、LSTM都只能算作对NPLM的架构优化。&lt;/p>
&lt;p>但，NPLM也存在历史局限性，于是才有了后来的大模型的关键部件Transformer，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</link><pubDate>Thu, 17 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</guid><description>&lt;p>人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。&lt;/p>
&lt;p>目前炙手可热的Transformer既是如此，LSTM、Word2Vec等是它的基石，共同构筑了现在的大语言模型的关键部件和理论基础。&lt;/p>
&lt;p>之前发了一个朋友圈，将对大语言模型影响深远的论文，梳理了三条脉络：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LSTM&lt;/strong>：在1997年那会儿解决了AI的记忆问题。&lt;/li>
&lt;li>&lt;strong>Word2Vec&lt;/strong>：在2013~2014年解决了将词转换为向量，将&amp;quot;文字游戏&amp;quot;转换为了高维向量空间中的&amp;quot;数学游戏&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>Transformer&lt;/strong>：2014年出现了注意力机制雏形，2017年那篇著名的《Attention is All you Need》引出的Transformer，随后是OpenAI2018年的GPT-1、Google2019的BERT。&lt;/li>
&lt;/ul>
&lt;p>本文重点阐述第2条技术线：&lt;strong>Word2Vec&lt;/strong>涉及的技术——词的向量化，这也是大语言模型的关键部件之一。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230817105511.jpg" alt="微信图片_20230817105511">&lt;/p>
&lt;h1 id="1什么是表示学习">1.什么是表示学习？&lt;/h1>
&lt;p>我们先看一下表示学习的定义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>表示学习&lt;/strong>：Representation Learning，通过学习算法&lt;strong>自动&lt;/strong>地从原始数据中&lt;strong>学习到一种数据表达形式&lt;/strong>。
&lt;ul>
&lt;li>表示学习的目标是将输入的数据转换到具有良好表现能力的特征空间中。&lt;/li>
&lt;li>特征空间中的数据有&lt;strong>可分性&lt;/strong>、&lt;strong>可解释性&lt;/strong>、&lt;strong>可推理性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们来解读一下上述这段话：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>：
&lt;ul>
&lt;li>我们知道，人类彼此的沟通介质是：文字、图像、视频、语音等。这些介质的本质是数据。&lt;/li>
&lt;li>我们还知道，从数据中寻找规律，是连接主义的哲学基础。&lt;/li>
&lt;li>我们还知道，可以被人类大脑理解的文字、图像、视频等数据，人工智能能理解吗？不能。&lt;/li>
&lt;li>人工智能无法理解这些数据，人工智能就无法从数据中寻找规律——因此，&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>向量化是数据可表示的一种实现&lt;/strong>：
&lt;ul>
&lt;li>向量化，可以让人工智能理解人类才能理解的数据。&lt;/li>
&lt;li>向量化，只是数据可表示的&lt;strong>一种&lt;/strong>实现，当然可以有其它的实现方式。
&lt;ul>
&lt;li>如：笔者前一篇《【chatGPT】学习笔记6-手撸一个上古GPT》中实现的N-Gram算法，并未对人类语言向量化，也让人工智能具备理解人类语言以及语言概率的能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>向量化，依然是目前数据可表示的若干种实现中最好的一种。
&lt;ul>
&lt;li>基于N-Gram的人工智能，能力很弱。它的智能停留在人类语言的&lt;strong>表面&lt;/strong>，它并不理解&lt;strong>语义&lt;/strong>。&lt;/li>
&lt;li>比如：&amp;ldquo;我去！&amp;ldquo;这个句子，基于N-Gram的人工智能无法识别，这句话到底想表达&amp;quot;我艹&amp;rdquo;，还是想表达&amp;quot;我要去某个地方&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>上述对表示学习的解读还是挺抽象。没关系，后文有形象的例子，各位小伙伴可以先看完后文，再回头看这一段，加深对表示学习的理论理解。&lt;/p>
&lt;h1 id="2什么是向量什么是嵌入">2.什么是向量？什么是嵌入？&lt;/h1>
&lt;h2 id="1向量">(1)向量&lt;/h2>
&lt;p>在NLP领域，无论语言模型的大小，都必须将词先表达为向量，词向量就是语言模型的输入。&lt;/p>
&lt;p>在CV领域，无论视觉模型的大小，也必须将图像先表达为向量。&lt;/p>
&lt;p>将文字、图像、视频、音频等数据向量化，本质是&lt;font color=red>**将&amp;quot;人类可理解的数据问题&amp;quot;转换为&amp;quot;机器可理解的机器学习问题&amp;rdquo;**&lt;/font>。&lt;/p>
&lt;p>我们来看一个向量化的例子：&lt;/p>
&lt;ul>
&lt;li>人类小孩儿第一次看到苹果，人类小孩儿是怎么记住这种东西就是苹果呢？&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143249956.png" alt="image-20230817143249956">&lt;/p>
&lt;ul>
&lt;li>人类小孩儿会从不同维度描述苹果的特征(如：纹理、颜色、形状、大小等)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143358437.png" alt="image-20230817143358437">&lt;/p>
&lt;ul>
&lt;li>假设人类小孩大脑的工作机制，是将不同维度的特征用数字表达并存储，这些维度的特征值就是&lt;strong>向量&lt;/strong>了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143456600.png" alt="image-20230817143456600">&lt;/p>
&lt;ul>
&lt;li>当人类小孩看到新的苹果，大脑也会将新苹果的特征提取为向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143619055.png" alt="image-20230817143619055">&lt;/p>
&lt;ul>
&lt;li>大苹果、小苹果，从颜色、大小、形状、纹理等等维度是很相似的，显然，把这些苹果抽象成多个向量，这些向量在向量空间中的距离肯定是很近的。&lt;/li>
&lt;li>这就是信息向量化，词的向量化就是信息向量化的一种。&lt;/li>
&lt;/ul>
&lt;p>上述例子，来自于一段腾讯云介绍向量数据库的视频，各位小伙伴可以通过视频，看到更多形象化理解向量的例子：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=489914023&amp;bvid=BV13N41167Q9&amp;cid=1237875667&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="2嵌入">(2)嵌入&lt;/h2>
&lt;p>先看一下嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>嵌入：Embedding，表示学习的一种形式，用于&lt;strong>将高维数据映射到低维空间&lt;/strong>。嵌入包括：
&lt;ul>
&lt;li>词嵌入&lt;/li>
&lt;li>图像嵌入&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这里以图像嵌入的一种经典算法t-SNE(t-Distributed Stochastic Neighbor Embedding)为例：&lt;/p>
&lt;ul>
&lt;li>图像嵌入的第一阶段是将图像转换为高维向量。&lt;/li>
&lt;li>图像嵌入的第二阶段是将高维向量映射到低维向量，但是要保证高维向量中邻近的点，在低维空间中也有相同的距离关系。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/t-sne_optimise.gif" alt="t-sne_optimise">&lt;/p>
&lt;p>&lt;strong>嵌入&lt;/strong>本身是一个很抽象的术语，但我们可以简单地认为&lt;strong>嵌入就是向量化&lt;/strong>的过程。&lt;/p>
&lt;p>对于嵌入，最关键的信息只有一个：&lt;strong>高维&lt;/strong>映射到&lt;strong>低维&lt;/strong>。&lt;/p>
&lt;p>什么是高维向量？什么是低维向量？为什么要从高维映射到低维？&lt;/p>
&lt;p>这些问题的有趣之处，和人类的学习过程非常相似：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>有一日未死之身，则有一日未闻之道&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类开始学习一门新知识，越学越觉得这个知识领域博大精深。这是人类学习的第一阶段——&amp;quot;&lt;strong>把书读厚&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>在嵌入过程中，也就是把信息向量化过程中，显然是把一个信息用更高维度的向量去描述，信息越准确。&lt;/li>
&lt;li>比如：小时候练习看图说话，老师会鼓励小朋友把图上看到的东西从更多角度去描绘出来，这些角度就是向量的维度。&lt;/li>
&lt;li>再比如：一个哲学思想，不同流派的哲学家会从不同角度、不同立场去阐述、论证和思辨，这些角度、立场也等效于向量的维度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>读书之道，愈进愈简，百卷如一页&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类学习新知识到了一个阶段，会出现&lt;strong>顿悟&lt;/strong>——发现、归纳各个知识点的内在联系。这是人类学习的第二阶段——&amp;quot;&lt;strong>把书读薄&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>比如：杨过的重剑无锋，大巧不工。比如：老子的大道至简，有言无言。&lt;/li>
&lt;li>在嵌入过程中，也有类似的过程，虽然单个高维向量有丰富的表达，但是多个高维向量存在某些内在联系，把它们降低为低维向量，不仅抓住了信息的本质，而且更加精炼、大道至简。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这就是&lt;strong>嵌入&lt;/strong>的&lt;strong>核心思想&lt;/strong>——将&lt;strong>高维向量降低成低维向量&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817221709020.png" alt="image-20230817221709020">&lt;/p>
&lt;h1 id="3什么是词向量什么是词嵌入">3.什么是词向量？什么是词嵌入？&lt;/h1>
&lt;p>先来看词向量、词嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的定义&lt;/strong>：将词语转换成对应数值向量的表达形式，便于计算机读取和运算。&lt;/li>
&lt;li>&lt;strong>词向量的数学表达&lt;/strong>：将字典D中的任意词w，设定固定长度的实值向量V(w)。其中：V(w)就是词向量，m表示词向量长度。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817142112513.png" alt="image-20230817142112513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词嵌入&lt;/strong>：Word Embedding，将词映射到向量空间，形成词向量的过程和方法，就是词嵌入。&lt;/li>
&lt;/ul>
&lt;p>再来看一个经典的例子：&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>男人、女人、国王、皇后&lt;/strong>开展词嵌入，它们变成了四个向量，存在与三维的向量空间中。&lt;/li>
&lt;li>假设词嵌入的具体代码实现没有问题，那么男人和国王的向量距离应该很近，女人和皇后的向量距离应该很近。&lt;/li>
&lt;li>这样，对人类的文字开展词嵌入后，有两个关键的输出：
&lt;ul>
&lt;li>&lt;strong>词以向量的形式存在&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>有关联的词对应的向量的距离也会很小&lt;/strong>(如：对两个词向量求余弦相似度)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817222631960.png" alt="image-20230817222631960">&lt;/p>
&lt;h1 id="4如何实现词嵌入">4.如何实现词嵌入？&lt;/h1>
&lt;p>词嵌入，或者说词的向量化过程分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>文本变为One-Hot编码。&lt;/li>
&lt;li>One-Hot编码变为低维词向量。&lt;/li>
&lt;/ul>
&lt;h2 id="1one-hot编码">(1)One-Hot编码&lt;/h2>
&lt;p>&lt;strong>One-Hot编码&lt;/strong>：One-Hot Representation，也叫独热编码。&lt;/p>
&lt;p>比如：对于男、女两个词，One-Hot编码可以表示为：&lt;/p>
&lt;ul>
&lt;li>男：[1, 0]，女：[0, 1]&lt;/li>
&lt;/ul>
&lt;p>再比如：对于初一、初二、初三三个词，可以表示为：&lt;/p>
&lt;ul>
&lt;li>初一：[1, 0, 0]，初二：[0, 1, 0]，初三：[0, 0, 1]&lt;/li>
&lt;/ul>
&lt;p>如果说One-Hot编码算得上广义的向量，那么它明显的缺点是：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>过于稀疏&lt;/strong>：在各个维度上，只有1个维度是1，其它维度都是0。&lt;/li>
&lt;/ul>
&lt;h2 id="2distributed表示">(2)Distributed表示&lt;/h2>
&lt;p>&lt;strong>Distributed表示&lt;/strong>：Distributed Representation，分布式表示，它是表示学习中的一种。&lt;/p>
&lt;ul>
&lt;li>Word2Vec、GloVe、fastText等都是其中一种具体实现。&lt;/li>
&lt;/ul>
&lt;p>分布式表示的核心思想是：&lt;/p>
&lt;ul>
&lt;li>通过训练，将词典里每个单词转换为固定长度的低维向量。&lt;/li>
&lt;li>这些向量之间可以通过余弦相似度之类的数学工具，表示向量之间的距离。语义越接近的向量之间，距离越近。&lt;/li>
&lt;/ul>
&lt;p>Distributed表示才算得上真正的向量，它明显的优点是：&lt;/p>
&lt;ul>
&lt;li>从&lt;strong>过于稀疏&lt;/strong>的One-Hot编码，变为了&lt;strong>密集向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="3word2vecdistributed表示的一种实现">(3)Word2Vec，Distributed表示的一种实现&lt;/h2>
&lt;p>Word2Vec是Distributed表示的一种具体实现：&lt;/p>
&lt;ul>
&lt;li>Word2Vec将词汇表中的每个词表示成固定长度的向量。&lt;/li>
&lt;li>Word2Vec的核心思想本质是——&lt;strong>近朱者赤，近墨者黑&lt;/strong>：
&lt;ul>
&lt;li>一个词可以根据它的的周边词，推测出自己的语义。&lt;/li>
&lt;li>一个词也可以通过自己的语义，推测出它的周边词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Word2Vec的实现算法有两种：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CBOW&lt;/strong>：Continuous Bag of Words，连续词袋，输入是周围词，输出是中心词。&lt;/li>
&lt;li>&lt;strong>Skip-Gram&lt;/strong>：跳字模型，输入是中心词，输出是周围词。&lt;/li>
&lt;/ul>
&lt;p>Word2Vec基于神经网络实现：&lt;/p>
&lt;ul>
&lt;li>但，无论是CBOW，还是Skip-Gram，都属于浅层神经网络。&lt;/li>
&lt;li>一说浅，显然就不那么高级。浅层神经网络的效果可以参考Word2Vec的原始论文：https://arxiv.org/pdf/1301.3781.pdf&lt;/li>
&lt;/ul>
&lt;h1 id="5代码实例手撸word2vec">5.代码实例：手撸Word2Vec&lt;/h1>
&lt;h2 id="step1构建语料库">STEP1.构建语料库&lt;/h2>
&lt;ul>
&lt;li>首先，以四个句子作为训练语料&lt;/li>
&lt;li>然后，形成&lt;strong>词+索引的Map&lt;/strong>和&lt;strong>索引+词的Map&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233124145.png" alt="image-20230817233124145">&lt;/p>
&lt;h2 id="step2生成skip-gram数据">STEP2.生成Skip-Gram数据&lt;/h2>
&lt;ul>
&lt;li>根据句子中的各个词，将相邻的词形成词对。&lt;/li>
&lt;li>如：(小美，是)、(是，小美)，(美女，小美)，(小美，美女)等&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233354716.png" alt="image-20230817233354716">&lt;/p>
&lt;h2 id="step3对skip-gram数据进行one-hot编码">STEP3.对Skip-Gram数据进行One-Hot编码&lt;/h2>
&lt;ul>
&lt;li>将STEP2的各个词对进行One-Hot编码&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233557328.png" alt="image-20230817233557328">&lt;/p>
&lt;h2 id="step4定义skip-gram模型">STEP4.定义Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>我们复现出论文描述的神经网络结构
&lt;ul>
&lt;li>一个线性层学习出词向量&lt;/li>
&lt;li>一个输出层验证学习到词向量作为中心词，是否能够预测出周围词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233757848.png" alt="image-20230817233757848">&lt;/p>
&lt;p>看一下具体的代码实现：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234116518.png" alt="image-20230817234116518">&lt;/p>
&lt;h2 id="step5训练skip-gram模型">STEP5.训练Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>训练3000次，每一次都会从输出层触发反向传播，更新线性层的参数，最终确定本次学习到的词向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234302231.png" alt="image-20230817234302231">&lt;/p>
&lt;ul>
&lt;li>这是损失函数曲线，经过3000次的训练，损失逐渐降低。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234532709.png" alt="image-20230817234532709">&lt;/p>
&lt;h2 id="step6显示词向量">STEP6.显示词向量&lt;/h2>
&lt;ul>
&lt;li>将词向量打印出来，并绘制在向量空间中&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234606438.png" alt="image-20230817234606438">&lt;/p>
&lt;ul>
&lt;li>绘制出来的向量空间，结果如下：
&lt;ul>
&lt;li>小美和大漂亮的词向量分别为： [-0.45549557 0.4471412 ]，[-0.47936678 0.40784693]&lt;/li>
&lt;li>小帅和大壮的词向量分别为：[0.0154954 1.6643344]，[0.01839364 1.6814741 ]&lt;/li>
&lt;li>小美和大漂亮两个词关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>小帅和大壮两个词也关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>但，美女、帅哥两个词向量学习的不好。小美、大漂亮更接近于帅哥，出现了&lt;strong>不一定斩男但是一定斩女&lt;/strong>的现象。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234631498.png" alt="image-20230817234631498">&lt;/p>
&lt;h1 id="6再看表示学习嵌入">6.再看表示学习、嵌入&lt;/h1>
&lt;p>笔者在第5章以Word2Vec的Skip-Gram的代码实现，给大家阐述了词嵌入的实现过程和训练过程。&lt;/p>
&lt;p>回头再来看1~4章出现的各种概念和理论，可以归纳词嵌入的几个关键知识点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自然语言问题转换为数学问题&lt;/strong>：将文字向量化的本质是将信息转换为向量，所以文字的语义关系巧妙地转换为了向量距离的数学问题。&lt;/li>
&lt;li>&lt;strong>自动化的学习过程&lt;/strong>：获得文字的向量是一个自动化的过程，通过神经网络这种工具自动学习得到词向量。&lt;/li>
&lt;li>&lt;strong>词向量的降维技术&lt;/strong>：对于词嵌入，学界有很多种降维技术，Word2Vec的CBOW和Skip-Gram都是实现了降维的具体算法。&lt;/li>
&lt;/ul>
&lt;p>再看第1章的表示学习，我们可以重新理解一下表示学习的三个特性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>可分性&lt;/strong>：不相关的向量距离会很远，不相关的向量之间应该会有明显的分界线。这就体现了可分性——不同类别之间的向量样本应该有明显的边界或区别。&lt;/li>
&lt;li>&lt;strong>可解释性&lt;/strong>：为什么提到唐伯虎，人类会立刻联想到秋香？向量空间中相关的向量距离就会很近，这就体现了可解释性。&lt;/li>
&lt;li>&lt;strong>可推理性&lt;/strong>：给出小美，我们可以联想到大漂亮、帅哥，进而可能就会联想到小帅、大壮，说不定小美和小帅之间就一段故事，这就体现了可推理性。&lt;/li>
&lt;/ul>
&lt;p>写到这里，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>以Word2Vec为代表的词向量化技术极大地推进了语言模型的进步&lt;/li>
&lt;li>词嵌入也成为了如今大语言模型GPT、LLama、Bert的核心组件之一。&lt;/li>
&lt;/ul>
&lt;p>在了解了词嵌入相关技术之后，我们下一步就来看大语言模型的另一个核心组件的起源：神经概率语言模型。&lt;/p></description></item><item><title>【chatGPT】学习笔记6-手撸一个上古GPT</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/</link><pubDate>Wed, 26 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/</guid><description>&lt;p>大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？&lt;/p>
&lt;p>今天我们来手撸一个上古GPT，理解一下其中的基本原理。&lt;/p>
&lt;h1 id="1语言模型简介">1.语言模型简介&lt;/h1>
&lt;h2 id="11什么是语言模型">1.1.什么是语言模型&lt;/h2>
&lt;p>&lt;strong>语言模型是一个用来&lt;font color=red>估计文本概率分布的数学模型&lt;/font>&lt;/strong>。通俗的说，你给它一个词，它能告诉你这个词之后通常大概率会接什么词。&lt;/p>
&lt;p>写这篇文章的时候，笔者拉了个人类做测试——我说一个词，他凭直觉补全他这句话：&lt;/p>
&lt;ul>
&lt;li>我说：孙悟。他答：孙悟空&lt;/li>
&lt;li>我说：哪吒。他答：哪吒三太子&lt;/li>
&lt;li>我说：白龙马。他答：白龙马蹄朝西&lt;/li>
&lt;li>我说：猪八戒。他答：猪八戒背媳妇&lt;/li>
&lt;/ul>
&lt;p>你看，这就是人脑中隐含了一个语言模型，直觉并不是直觉，而是一种文本概率分布。&lt;/p>
&lt;p>&lt;strong>GPT家族&lt;/strong>(如：chatGPT、GPT-4)、&lt;strong>BERT家族&lt;/strong>(如：MT-DNN、ERNIE)，都是语言模型中的一种。&lt;/p>
&lt;p>除了GPT、BERT这些新一代的语言模型，古早的语言模型还有&lt;strong>N-Gram&lt;/strong>、&lt;strong>RNN&lt;/strong>、&lt;strong>LSTM&lt;/strong>、&lt;strong>GRU&lt;/strong>等。&lt;/p>
&lt;p>但是，无论是新同志，还是老同志，它们作为语言模型的初心不会变——&lt;strong>估计文本概率分布&lt;/strong>。&lt;/p>
&lt;h2 id="12语言模型分类">1.2.语言模型分类&lt;/h2>
&lt;h3 id="121基于规则的语言模型">1.2.1.基于规则的语言模型&lt;/h3>
&lt;p>1970年，出现基于规则的语言模型。它通过语法树，将人类语言语法的规则描述出来。比如下面描述的最简单的语法树：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803090559437.png" alt="image-20230803090559437">&lt;/p>
&lt;p>很显然，人类语言的规则复杂且会变化，这种&lt;font color=red>&lt;strong>基于规则的语言模型过于死板&lt;/strong>&lt;/font>，不太可能有很好的效果。&lt;/p>
&lt;h3 id="122基于概率的语言模型">1.2.2.基于概率的语言模型&lt;/h3>
&lt;p>1990年，出现基于数据驱动的统计概率语言模型。&lt;/p>
&lt;p>这里不得不敬仰一下语音识别和自然语言处理专家——贾里尼克(Frederek Jelinek)。这位大神学术上极其严谨和务实，在IBM期间极度厌恶夸夸其谈的语言学家，曾经抛出了那句：&amp;ldquo;我每开除一名语言学家,我的语音识别系统错误率就降低一个百分点&amp;quot;的名言。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803091349097.png" alt="image-20230803091349097">&lt;/p>
&lt;p>著名的贾里尼克假设(&lt;font color=red>&lt;strong>一个句子是否合理，就看看它的可能性大小如何，至于可能性就用概率来衡量&lt;/strong>&lt;/font>)奠定了基于概率的语言模型。&lt;/p>
&lt;p>举个例子：小美同学听到一句英文&lt;code>The apple and 「pear」 salad is delicious.&lt;/code>，由于&lt;code>pear&lt;/code>和&lt;code>pair&lt;/code>发音很类似，那么到底是在这个句子中是&lt;code>pear&lt;/code>还是&lt;code>pair&lt;/code>呢？按照贾里尼克假设，&lt;code>pear&lt;/code>显然可能性更多，或者说概率更大。&lt;/p>
&lt;p>我们进一步将这里再展开一点：假设Y是一个有意义的句子，由一连串特定顺序排列的词X1、X2、X3&amp;hellip;Xn组成。如何求Y在自然语言中出现的可能性呢？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803093047437.png" alt="image-20230803093047437">&lt;/p>
&lt;p>首先，根据条件概率进行转换：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803095827392.png" alt="image-20230803095827392">&lt;/p>
&lt;p>然后，根据马尔科夫假设进一步简化：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png" alt="image-20230803103655078">&lt;/p>
&lt;p>最终，就可以得到一个简化的语言模型&lt;code>Bi-Gram&lt;/code>：&lt;font color=red>&lt;strong>每个词称为1个&lt;code>Gram&lt;/code>，当前词由前一个词决定&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>这就是笔者在《【ChatGPT】ChatGPT学习笔记2-不是什么?是什么?有何方向?》中提到的GPT基础原理：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230719094340618.png" alt="image-20230719094340618">&lt;/p>
&lt;h3 id="123基于深度学习的语言模型">1.2.3.基于深度学习的语言模型&lt;/h3>
&lt;p>基于深度学习的语言模型的侧重点是学习工具，即强调使用神经网络进行学习。而基于概率的语言模型的侧重点是数据，即强调从数据中寻找规律。&lt;/p>
&lt;p>因此，基于深度学习的语言模型没有突破&lt;strong>基于概率&lt;/strong>的核心思想。&lt;/p>
&lt;h2 id="13语言模型的关键里程碑">1.3.语言模型的关键里程碑&lt;/h2>
&lt;p>从模型本身的发展看：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>N-Gram&lt;/strong>：基于前N−1个词，预测序列中的下一个词&lt;/li>
&lt;li>&lt;strong>Neural Probabilistic Language Model&lt;/strong>：神经概率语言模型。&lt;/li>
&lt;li>&lt;strong>Pre-trained Language Model&lt;/strong>：通过更大的语料库、更深的神经网络进行预训练和微调。&lt;/li>
&lt;/ul>
&lt;p>从词向量表示的发展看：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Bag-of-words&lt;/strong>：BOW，词袋模型。首次提出把词用向量表示，把文本表示为单词的集合，仅看词频(不考虑单词的顺序)。&lt;/li>
&lt;li>&lt;strong>Distributed Representation&lt;/strong>：以分布式激活的形式表示词，也是词用向量表示。&lt;/li>
&lt;li>&lt;strong>Word2vec&lt;/strong>：先把单词的含义学习好、用向量表示好，我们再继续用它进行后续训练。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803113828737.png" alt="image-20230803113828737">&lt;/p>
&lt;h1 id="2上古语言模型n-gram">2.上古语言模型N-Gram&lt;/h1>
&lt;h2 id="21五步实现n-gram">2.1.五步实现N-Gram&lt;/h2>
&lt;ul>
&lt;li>制作数据集&lt;/li>
&lt;li>分词&lt;/li>
&lt;li>计算每个N-Gram的词频&lt;/li>
&lt;li>计算每个N-Gram的概率&lt;/li>
&lt;li>根据输入词，生成连续文本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180415107.png" alt="image-20230803180415107">&lt;/p>
&lt;h2 id="22制作数据集">2.2.制作数据集&lt;/h2>
&lt;ul>
&lt;li>我们录入了三首唐诗作为数据集。实战中会录入更多的数据。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803172748930.png" alt="image-20230803172748930">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174612589.png" alt="image-20230803174612589">&lt;/p>
&lt;h2 id="23分词">2.3.分词&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>N-Gram模型会将数据集中每一条语句，拆分成N个词。每个词就是一个Gram。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分词本身不是N-Gram的重点，因此本文没有使用JieBa等分词三方件，直接按照单字来拆分。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分词时有个细节是子词，为了解决错别字等问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>看一下代码：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803173712704.png" alt="image-20230803173712704">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174634047.png" alt="image-20230803174634047">&lt;/p>
&lt;h2 id="24划分n-gram计算词频">2.4.划分N-Gram，计算词频&lt;/h2>
&lt;ul>
&lt;li>我们会根据N的具体数值，去计算N个字之后紧跟的字出现的词频。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174346725.png" alt="image-20230803174346725">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174709174.png" alt="image-20230803174709174">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174751374.png" alt="image-20230803174751374">&lt;/p>
&lt;h2 id="25进一步计算概率">2.5.进一步计算概率&lt;/h2>
&lt;ul>
&lt;li>我们会进一步计算N个字之后紧跟的字出现的概率。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175136237.png" alt="image-20230803175136237">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175520160.png" alt="image-20230803175520160">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175541199.png" alt="image-20230803175541199">&lt;/p>
&lt;ul>
&lt;li>走到这里，我们会有一个重要发现：&lt;font color=red>&lt;strong>当N越大，后面紧跟的字的概率越趋于确定&lt;/strong>！&lt;/font>&lt;/li>
&lt;/ul>
&lt;h2 id="26使用n-gram模型生成文本">2.6.使用N-Gram模型，生成文本&lt;/h2>
&lt;ul>
&lt;li>当我们有了N-Gram模型，就可以根据输入词，预测生成完整的文本了。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180013173.png" alt="image-20230803180013173">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180226046.png" alt="image-20230803180226046">&lt;/p>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;ul>
&lt;li>语言模型本质是一个文本概率统计模型&lt;/li>
&lt;li>语言模型分为：基于规则的语言模型、基于概率的语言模型、基于深度学习的语言模型&lt;/li>
&lt;li>N-Gram是最古老的语言模型，通过分词，统计出每N-1个词后紧跟的词的概率，进而形成了文本概率统计模型。&lt;/li>
&lt;li>N-Gram的理论基础是：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png" alt="image-20230803103655078">&lt;/p></description></item><item><title>【chatGPT】学习笔记5-四次发展&amp;三个世界</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/</link><pubDate>Tue, 25 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/</guid><description>&lt;p>红杉资本在2022年7月，发布了大语言模型洞察报告。一年后的今天回看这篇洞察报告，洞察地很准。&lt;/p>
&lt;p>我们详细解读一下这份报告，从中提炼一些观点和知识，以提升我们对生成式AI的宏观洞见。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230726094029267.png" alt="image-20230726094029267">&lt;/p>
&lt;h1 id="1ai的四次大发展">1.AI的四次大发展&lt;/h1>
&lt;p>AI历史上经历了四次大发展：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>早期人工智能时代&lt;/strong>：AI的第一次大发展，1950年早期人工智能出现。&lt;/li>
&lt;li>&lt;strong>机器学习时代&lt;/strong>：AI的第二次大发展，1980年，机器学习逐步兴起，并蓬勃发展。&lt;/li>
&lt;li>&lt;strong>深度学习时代&lt;/strong>：AI的第三次大发展，2010年，深度学习的突破，驱动了AI的进一步发展。&lt;/li>
&lt;li>&lt;strong>大语言模型时代&lt;/strong>：AI的第四次大发展，2020年，大语言模型在&lt;strong>生成式学习&lt;/strong>路线上大放异彩。&lt;/li>
&lt;/ul>
&lt;p>报告洞察的时间段就是从后深度学习时代，到大语言模型时代，阐述了大语言模型时代的四次浪潮。&lt;/p>
&lt;p>另外，我们还可以发现一个有趣的现象：&lt;font color="red">&lt;strong>AI历次大发展的时间间隔逐步变小&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230726102141799.png" alt="image-20230726102141799">&lt;/p>
&lt;h1 id="2生成式ai的四次浪潮">2.生成式AI的四次浪潮&lt;/h1>
&lt;blockquote>
&lt;p>Sure enough, as the models get bigger and bigger, they begin to deliver human-level, and then superhuman results.&lt;/p>
&lt;p>果然，随着模型变得越来越大，它们开始提供人类的水平，然后是超人的结果。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第一个核心观点是：生成式AI越来越强大，在某些领域开始超越人类的水平。&lt;/p>
&lt;h2 id="21生成式ai与分析式ai的边界">2.1.生成式AI与分析式AI的边界&lt;/h2>
&lt;p>报告首先划定了&lt;strong>生成式AI&lt;/strong>与&lt;strong>分析式AI&lt;/strong>的边界：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>分析式AI&lt;/strong>：&lt;strong>Analytical AI&lt;/strong>，从早期人工智能时代到深度学习时代，人工智能更多地应用领域覆盖的是&lt;strong>人类的分析能力&lt;/strong>。如：识别车牌、识别垃圾邮件、用户画像与智能推荐等等。人工智能在这个时代，主要是在&lt;strong>分析已有的数据&lt;/strong>，进行&lt;strong>回归&lt;/strong>与&lt;strong>分类&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>生成式AI&lt;/strong>：&lt;strong>Generative AI&lt;/strong>，人工智能的应用领域开始覆盖&lt;strong>人类的创造能力&lt;/strong>。如：GPT写诗、Midjourney绘画等。以前人工智能在这些方面无法与人类抗衡，但现在人工智能已经开始可以创造有意义并具备美感的作品了。在这个时代，人工智能&lt;strong>不仅仅是分析已有的数据&lt;/strong>，而是在&lt;strong>生成新的内容&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="22agi第一波浪潮">2.2.AGI第一波浪潮&lt;/h2>
&lt;p>在前深度学习时代(2015年之前)，&lt;strong>小模型&lt;/strong>是主流模型，是某个领域的&lt;strong>专才&lt;/strong>，这些一个个的小模型都擅长做特定领域的&lt;strong>人类分析工作&lt;/strong>。&lt;/p>
&lt;p>但，对于生成内容的任务，小模型普遍都不擅长。&lt;/p>
&lt;h2 id="23agi第二波浪潮">2.3.AGI第二波浪潮&lt;/h2>
&lt;p>2019年，谷歌发表的《Attention is All You Need》，向我们描述了一种用于NLP的新型神经网络架构Transformer，随后各大厂在这条赛道上展开了激烈竞争。&lt;/p>
&lt;p>这个时期的大语言模型由于极其庞大、强依赖GPU等原因，大部分都没有公开可商用版本，或者都是内测版本，更没有可以落地的应用。&lt;/p>
&lt;p>但，随着大语言模型越来越大，也出现了很多神奇的现象(如：涌现能力)，在某些方面开始超越人类水平。&lt;/p>
&lt;p>在《Compute Trends Across Three Eras of Machine Learning》论文中，详细展示了人工智能在手写识别、语音识别、图像识别、阅读、语言理解的领域中，超越人类基准水平的测试结果。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727090714788.png" alt="image-20230727090714788">&lt;/p>
&lt;h2 id="24agi第三波浪潮">2.4.AGI第三波浪潮&lt;/h2>
&lt;p>在2022年之后，大语言模型开始进入&lt;strong>更好、更快、更便宜&lt;/strong>的阶段。计算成本开始下降、新的技术出现(如：diffusion models、CoT、ToT等)，都降低了训练和推理的成本。最大的进展不是大语言模型本身的进步，而是开源，很多学术界机构能够基于基础大语言模型去进一步训练和研究。&lt;/p>
&lt;p>笔者认为特别是训练技术方面，成本下降很快，成本的下降带来了门槛的下降。几个月前我去搭建自己的本地GPT环境花了很长时间和费用，但上周LLama2发布后，搭建自己的本地大模型就非常便宜和容易。&lt;/p>
&lt;p>此时，随之而来的应用开始出现(比如：chatGPT)。很多国人是这个时候才听说了大语言模型的概念，甚至很多人的知识结构还停留在机器学习时代。&lt;/p>
&lt;h2 id="24agi第四波浪潮">2.4.AGI第四波浪潮&lt;/h2>
&lt;p>这是红杉资本对眼前和未来的预期：杀手级应用涌现。随着大语言模型继续变得更好、更快、更便宜，会出现越来越多的免费、开源的模型。应用层也会出现大爆发。红杉资本在本文中多次强调：&amp;quot;&lt;strong>我们预计杀手级应用也将出现在生成式AI领域中，大家争相发力，前景让人期待&lt;/strong>&amp;quot;。&lt;/p>
&lt;h1 id="3生成式ai的市场格局">3.生成式AI的市场格局&lt;/h1>
&lt;blockquote>
&lt;p>The best Generative AI companies can generate a sustainable competitive advantage by executing relentlessly on the flywheel between user engagement/data and model performance.&lt;/p>
&lt;p>最好的生成式人工智能公司可以通过在用户参与度、数据、模型能力之间不断地迭代，进而产生可持续的竞争优势。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第二个核心观点是：根据生成式AI的市场格局，在文本、代码、图像、语音、视频、3D等领域，需要应用、模型、数据的相互配合、共同成熟。&lt;/p>
&lt;h2 id="31市场格局">3.1.市场格局&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>文本方向&lt;/strong>：模型以GPT-3为代表，应用领域的机会点会出现在行销、销售、服务、写作、笔记等垂直领域。&lt;/li>
&lt;li>&lt;strong>代码方向&lt;/strong>：模型以GPT-3为代表，应用领域的机会点会出现在代码生成、文档生成、自然语言转代码、Web/App快速构建。&lt;/li>
&lt;li>&lt;strong>图像方向&lt;/strong>：模型以DallE、Stable Diffusion为代表，应用领域机会点在设计领域。&lt;/li>
&lt;li>&lt;strong>语音/视频/3D方向&lt;/strong>：目前这些方向不明。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/genai-landscape-8.webp" alt="genai-landscape-8">&lt;/p>
&lt;h2 id="32市场机会窗">3.2.市场机会窗&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>文本方向&lt;/strong>：关窗时间大约在2025年。&lt;/li>
&lt;li>&lt;strong>代码方向&lt;/strong>：关窗时间大约在2025年。&lt;/li>
&lt;li>&lt;strong>图像方向&lt;/strong>：关窗时间大约在2030年。&lt;/li>
&lt;/ul>
&lt;p>&lt;font color="red" size=bold>**看到这个时间窗，是不是有一种时不我待的危机感？**&lt;/font>&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/genai-timeline-7.webp" alt="genai-timeline-7">&lt;/p>
&lt;h1 id="4对杀手级应用的预测">4.对杀手级应用的预测&lt;/h1>
&lt;blockquote>
&lt;p>The best Generative AI companies can generate a sustainable competitive advantage by executing relentlessly on the flywheel between user engagement/data and model performance.&lt;/p>
&lt;p>最好的生成式人工智能公司可以通过在用户参与度/数据和模型性能之间不断地执行飞轮来产生可持续的竞争优势。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第三个核心观点是：强调数据和模型的良性循环是杀手级应用的成功关键。&lt;/p>
&lt;h2 id="41杀手级应用的形态">4.1.杀手级应用的形态&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>强大底座&lt;/strong>：杀手级应用一定依托一个强大的大模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>插件化&lt;/strong>：这些杀手级应用会以插件的形式寄生于现有的软件中，比如：VSCode中集成Code Copilot、PhotoShop中集成某种自动生成图片的插件。&lt;/p>
&lt;ul>
&lt;li>PS：笔者刚开始不太认同这一点，难道就不能是一个新的应用软件吗？&lt;strong>看到4.2才理解红杉资本的思路&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>交互式体验&lt;/strong>：目前的GPT都是&lt;strong>一次交互&lt;/strong>——比如：用户问一次，AI就生成一张图片。未来应该会出现&lt;strong>多次交互&lt;/strong>——用户问一次，AI生成一个图片。用户再要求AI根据这个图片继续修正，直到做出用户满意的图片。&lt;/p>
&lt;ul>
&lt;li>PS：这一点很有道理。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="42如何成为杀手级应用">4.2.如何成为杀手级应用？&lt;/h2>
&lt;p>红杉资本强调**用户参与度/数据(user engagement/data)&lt;strong>与&lt;/strong>模型能力(model performance)**要形成良性循环：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>良性循环&lt;/strong>：①获得极高的用户参与度→②获得更多用户数据以训练出更好的模型(如：提示词工程、Fine-tuning，将用户行为作为标记的训练数据等)→③优秀的模型吸引更多的用户并提升参与度。&lt;/li>
&lt;li>&lt;strong>聚焦垂直领域&lt;/strong>：杀手级应用不必做的大而全，而是聚焦某个垂直领域。&lt;/li>
&lt;li>&lt;strong>植入客户工作流程&lt;/strong>：第一步先将AI以插件的形式整合到用户的生产流程中，进而实现用户增长和产品分发。第二步再做出新的软件系统替代用户的老软件系统。
&lt;ul>
&lt;li>PS：红杉资本给大伙儿指明了赚钱方法了，还不快去。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="5你我只知道图灵测试它却知道三个世界">5.你我只知道图灵测试，它却知道三个世界&lt;/h1>
&lt;blockquote>
&lt;p>Generative AI is still very early. The platform layer is just getting good, and the application space has barely gotten going.&lt;/p>
&lt;p>生成式人工智能仍处于早期阶段。平台层刚刚变得越来越好，而应用程序空间几乎没有发展起来。&lt;/p>
&lt;/blockquote>
&lt;p>这是红杉资本的最后一个观点，报告中的这一部分不太有更多干货了。笔者阅读这一部分的时候，思路情不自禁地跳跃到那个一直没有想通的&lt;strong>哲学问题&lt;/strong>：&lt;font color=red>**人类智能到底是什么？通用人工智能到底是什么？是什么驱动了人类智能和通用人工智能的发展？**&lt;/font>&lt;/p>
&lt;p>其实很多人只听说过图灵测试，并且误解了图灵测试。图灵测试是一种思想测试，这个测试的理论基础是什么？如果理论基础是错的，通过了图灵测试又能证明什么？笔者在《【chatGPT】学习笔记1-机器还需要多久才能像人一样思考》文中，试图用&lt;strong>计算理论&lt;/strong>来解释人类智能、通用人工智能的关系与差异，但&lt;strong>计算理论&lt;/strong>作为图灵测试的理论基础也不太充分。&lt;/p>
&lt;p>最近阅读&lt;strong>中文房间(Chinese Room)思想实验&lt;/strong>时，找到了&lt;strong>Karl Raimund Popper&lt;/strong>的&lt;strong>三个世界理论&lt;/strong>，这个理论似乎可以支撑图灵测试：&lt;/p>
&lt;ul>
&lt;li>世界1：物质的世界(客观世界)，如：物质、能量、有机物质or无机物质&lt;/li>
&lt;li>世界2：心理的世界(主观世界)，如：感觉、意识、心里状态的过程&lt;/li>
&lt;li>世界3：思想的世界，或者叫知识的世界。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727113454114.png" alt="image-20230727113454114">&lt;/p>
&lt;p>那么，三个世界理论的内在逻辑是什么呢？&lt;/p>
&lt;ul>
&lt;li>宇宙的发展是从世界1，到世界2，到世界3的连续过程。&lt;/li>
&lt;li>世界1和世界2存在直接相互作用，世界2和世界3存在直接相互作用，世界1和世界3的相互作用需要通过世界2为中介。&lt;/li>
&lt;li>再通俗一点，人的意识(世界2)可以通过知识(世界3)创造新的物质(世界1)。&lt;/li>
&lt;li>再通俗一点，现有知识(世界3)不足以解决客观问题(世界1)，会抛出&lt;strong>问题&lt;/strong>，驱动人的意识(世界2)发现、总结出新的知识(世界3)，同时解决问题(世界1)。&lt;/li>
&lt;li>再通俗一点，人工智能可以看作一个知识大合集，人工智能存在于(世界3)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727114108607.png" alt="image-20230727114108607">&lt;/p>
&lt;p>最后，提出三个世界的哲学价值是什么？&lt;/p>
&lt;ul>
&lt;li>正因为知识(世界3)能够&lt;strong>抛出问题&lt;/strong>，直接影响世界2，间接影响世界3，所以世界3的知识是可以自我发展的。&lt;/li>
&lt;li>比如：是人类发现了&lt;strong>数列&lt;/strong>这种知识，于是世界3就增加了&lt;strong>数列&lt;/strong>这个新知识，因为世界3中有了数列，世界3还会增加新知识&lt;strong>奇偶数&lt;/strong>。奇偶数并不是人类意识(世界2)的新发现，而是由于&lt;strong>人类在世界3创造了数列这种新知识&lt;/strong>而产生的&lt;strong>结果&lt;/strong>。&lt;/li>
&lt;li>通俗一点说，世界3知识的起点可能是人类，但世界3知识的发展、终点不是人类可以决定的。&lt;/li>
&lt;li>这就是三个世界的哲学价值！这就是哲学意义上的&lt;font color=red>&lt;strong>人工智能科学发展观&lt;/strong>&lt;/font>！是不是可以自洽地解释通用人工智能是什么、通用人工智能会如何发展？&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727115506988.png" alt="image-20230727115506988">&lt;/p>
&lt;p>写到这里，有两点感慨：&lt;/p>
&lt;ul>
&lt;li>很多时候，&lt;strong>人类没有&lt;/strong>自己的&lt;strong>独立思想和观点&lt;/strong>：可能没有(比如：他什么都不说)、可能只是重复(比如：他常常将图灵测试挂在嘴边，却不理解其中深意)、可能混乱(比如：他把图灵测试这种结果当做本质的起因)。&lt;/li>
&lt;li>&lt;strong>AI是独立思想和观点本身&lt;/strong>：AI存在于世界3，AI是知识的合集，那么可不可以说AI就是知识、思想、观点本身呢？&lt;/li>
&lt;/ul>
&lt;p>所以，&lt;strong>你我只知道图灵测试，它却知道三个世界&lt;/strong>。&lt;/p>
&lt;p>所以，&lt;strong>我们应该拥抱它，还是恐惧它，还是服从它。。。&lt;/strong>？&lt;/p>
&lt;h1 id="6参考">6.参考&lt;/h1>
&lt;blockquote>
&lt;p>红杉资本报告原文：https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记4-机器学习基本原理(下)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8B/</link><pubDate>Sat, 22 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8B/</guid><description>&lt;p>上篇为大家建立了宏观的机器学习概念，下篇我们通过一个真实的机器学习任务来理解一下机器学习的微观逻辑。&lt;/p>
&lt;h1 id="1机器学习任务介绍">1.机器学习任务介绍&lt;/h1>
&lt;p>前段时间，同事上班路上骑车摔泥坑里骨折了，我就萌生了让人工智能自动识别路面泥坑的想法。&lt;/p>
&lt;p>毕竟，同事要互相关怀、相亲相爱。另外，我们也可以通过这个机器学习的任务理解一下机器学习的基本原理。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722092656554.png" alt="image-20230722092656554">&lt;/p>
&lt;h1 id="2机器学习中的常用术语一览">2.机器学习中的常用术语一览&lt;/h1>
&lt;p>开始机器学习的训练任务之前，我们快速浏览一下机器学习领域中的常用术语。假设AI识别路面泥坑是一个函数：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722093341203.png" alt="image-20230722093341203">&lt;/p>
&lt;h2 id="21模型是什么">2.1.模型是什么?&lt;/h2>
&lt;p>函数f的输入是路面的视频，输出是路面上是否有泥坑。输入称为&lt;strong>自变量&lt;/strong>，在机器学习中也称为&lt;strong>特征&lt;/strong>。输出称为&lt;strong>因变量&lt;/strong>，在机器学习中也称为&lt;strong>标签&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722093803701.png" alt="image-20230722093803701">&lt;/p>
&lt;p>这个函数f称为&lt;strong>模型&lt;/strong>，复杂的模型可能有数量庞大的自变量。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722094251783.png" alt="image-20230722094251783">&lt;/p>
&lt;h2 id="22怎样才算找到了函数f">2.2.怎样才算找到了函数f？&lt;/h2>
&lt;p>怎么才算找到了函数f呢？假设这个函数是个简单的函数：&lt;code>y=ax+b&lt;/code>，那么找到a和b，就算找到了这个函数f。a、b也被称为&lt;strong>模型的参数&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722095702788.png" alt="image-20230722095702788">&lt;/p>
&lt;p>除了模型的参数，还有一种参数叫做&lt;strong>超参数(hyperparameter)&lt;/strong>，这些模型的外部参数，属于训练和调试的过程参数。&lt;/p>
&lt;h2 id="23如何找到函数f">2.3.如何找到函数f？&lt;/h2>
&lt;p>那么，我们又如何找到函数f呢？机器学习的&lt;strong>核心思想&lt;/strong>就是&amp;rdquo;&lt;strong>给机器一堆数据，让机器自己从数据中提取特征、寻找规律&lt;/strong>&amp;quot;。这个过程，也称为&lt;strong>炼丹&lt;/strong>。&lt;/p>
&lt;p>这些数据称为&lt;strong>数据集&lt;/strong>，这些数据通常被分为三波，一波用于训练、一波用于验证、一波用于最终的测试。&lt;/p>
&lt;p>用于训练的称为&lt;strong>训练集&lt;/strong>，用于验证的称为&lt;strong>验证集&lt;/strong>，用于测试的称为&lt;strong>测试集&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722100415709.png" alt="image-20230722100415709">&lt;/p>
&lt;h2 id="24找函数f的流派">2.4.找函数f的流派&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>从数据的角度看，人类对全部训练数据做了标注称为&lt;strong>自监督学习&lt;/strong>，对部分训练数据做了标注称为&lt;strong>半监督学习&lt;/strong>、对训练数据没有任何标注称为&lt;strong>无监督学习&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从数学工具的角度看，使用神经网络开展学习称为&lt;strong>机器学习&lt;/strong>，将神经网络做深称为&lt;strong>深度学习&lt;/strong>(不严谨，通俗点儿就这么理解吧)，因此深度学习是机器学习中的一种。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>除了机器学习流派，&lt;strong>强化学习&lt;/strong>(目前也很热门的流派)、&lt;strong>生成式学习&lt;/strong>、&lt;strong>迁移学习&lt;/strong>等等。这些流派的边界没有太清晰，彼此会有交集和互相引用。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这里想到两个有趣的小点：&lt;/p>
&lt;ul>
&lt;li>神经网络很早就发明了，但后续却要换个机器学习、深度学习的名字，可能是联结主义之前被符号主义学派打的太惨了，需要换个响亮的名号吧。&lt;/li>
&lt;li>GPT火了以后，有很多名词术语，听到有人把&amp;quot;用提示词的引导行为&amp;quot;称为&amp;quot;zero shot、few shot&amp;rdquo;，乱抛名词、概念搞混，根因就是没理解前述这些机器学习的基本概念。&lt;/li>
&lt;/ul>
&lt;h1 id="3从1个例子看机器学习过程">3.从1个例子看机器学习过程&lt;/h1>
&lt;h2 id="31数据的收集和预处理">3.1.数据的收集和预处理&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>我们要识别路面泥坑，首先要获得路面视频数据。这个过程称为&lt;strong>原始数据收集&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我们需要通过一些数据标注的工具，在视频上标注出路面泥坑，这个过程称为&lt;strong>数据标注&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据标注完后，需要将&lt;strong>数据向量化&lt;/strong>：data vectorization，就是把原始数据格式化，使机器可以读取(如：将原始图片转换为机器可以读取的矩阵)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>处理坏数据和缺失值：数据不是全都用的，要利用数据处理工具把&amp;rdquo;&lt;strong>捣乱数据&lt;/strong>&amp;ldquo;处理掉(冗余数据、离群数据、错误数据)，把&amp;rdquo;&lt;strong>缺失值&lt;/strong>&amp;ldquo;补上。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>数据预处理有很多技术和技巧：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>特征缩放&lt;/strong>：feature scaling，包括&lt;strong>数据标准化&lt;/strong>(standardization)和&lt;strong>数据规范化&lt;/strong>(normalization)等：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据标准化&lt;/strong>：对数据特征分布的转换，目标是使数据特征符合正态分布(均值为0，标准差为1)。如果数据特征不符合正态分布，会影响机器学习效率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据规范化&lt;/strong>：它是数据标准化的一种变体，将数据特征压缩到给定的最小值和最大值之间(通常为0~1)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>特征提取&lt;/strong>：feature extraction，通过子特征的选择，减少冗余特征，使初始测量数据更简洁，同时保留最有用的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>特征工程&lt;/strong>：使用数据的领域知识，来创建使得机器学习算法起作用的特征过程。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>通俗地理解：&lt;/p>
&lt;ul>
&lt;li>数据预处理的本质：就是&lt;strong>准备好机器看得懂、看得清的教材&lt;/strong>，方便机器进行学习。&lt;/li>
&lt;/ul>
&lt;p>我们再来看看实战中我们具体怎么做的：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>原始数据收集&lt;/strong>：通过行车记录仪，我收集了上班路上的路面情况。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723000930967.png" alt="image-20230723000930967">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据标注&lt;/strong>：通过标注工具，对视频中的泥坑进行标注。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001158467.png" alt="image-20230723001158467">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据集划分&lt;/strong>：我们通常会将标注好的数据划分为训练集、验证集、测试集。划分有一些方法论，如：K-fold validation。我将数据集划分编写成了一段脚本，执行后就自动划分好了训练集、验证集、测试集。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001237851.png" alt="image-20230723001237851">&lt;/p>
&lt;blockquote>
&lt;p>实战中，特征工程、数据向量化等，大多都被封装到了相应的AI训练框架中，本文不赘述。&lt;/p>
&lt;/blockquote>
&lt;h2 id="32设定范围">3.2.设定范围&lt;/h2>
&lt;p>设定范围指的是设定选择模型的范围，这要根据训练任务选择合适的数学模型。常见的模型如下：&lt;/p>
&lt;ul>
&lt;li>线性模型：线性回归、逻辑回归&lt;/li>
&lt;li>非线性模型：支持向量机、k最邻近分类&lt;/li>
&lt;li>基于树和集成的模型：决策树、随机森林、梯度提升树等&lt;/li>
&lt;li>神经网络：人工神经网络、卷积神经网络、长短期记忆网络等&lt;/li>
&lt;/ul>
&lt;p>模型选择是最为关键的环节，实战中，训练资料越少，模型就要圈定在很小很严格的范围内。&lt;/p>
&lt;p>否则，会产生一种现象：选择的这种模型在训练中表现很好，但换一波数据就不行了，说白了就是这个模型把数据死记硬背下来了。&lt;/p>
&lt;h2 id="33设定标准">3.3.设定标准&lt;/h2>
&lt;p>在确定了模型之后，就需要设定损失函数，损失函数用来表示机器&lt;strong>学到位&lt;/strong>了。&lt;/p>
&lt;p>以监督学习为例：&lt;/p>
&lt;ul>
&lt;li>人类老师标定了猪用103表示(如右图)&lt;/li>
&lt;li>人类老师标定了蓝精灵用17表示(如右图)&lt;/li>
&lt;li>当机器学习到了一个函数f1时，
&lt;ul>
&lt;li>输入一个猪头，函数f1输出100，那么距离正确答案103的差值为3。&lt;/li>
&lt;li>输入一个蓝精灵，函数f1输出18，那么距离正确答案17的差值为1。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>因此，函数f1整体距离正确答案为4，这就表示函数f1的损失函数Loss(f1)=4。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722221402457.png" alt="image-20230722221402457">&lt;/p>
&lt;h2 id="34达成目标">3.4.达成目标&lt;/h2>
&lt;p>当机器开始学习后，可能学到了N个不同的函数，那么就要从里面选择一个最优的函数，这个过程叫&lt;strong>最佳化(Optimization)&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722222441299.png" alt="image-20230722222441299">&lt;/p>
&lt;p>整个过程，可以归纳为如下公式：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722223058532.png" alt="image-20230722223058532">&lt;/p>
&lt;h2 id="35炼丹师超参数调优">3.5.炼丹师：超参数调优&lt;/h2>
&lt;p>从&amp;quot;设定范围&amp;quot;到&amp;quot;达成目标&amp;rdquo;，整个过程不是一帆风顺的，在训练过程中、在超参数调优过程中，都会出现&lt;strong>过拟合&lt;/strong>(过拟合就是当前这批数据表现不错，换一批数据就不灵了)。因此需要反复迭代。人工智能工程师此时就很像一个古代的炼丹师，不断地通过调参，对模型进行优化和泛化。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>优化&lt;/strong>：optimazation，就是让模型更好地拟合现有的数据。&lt;/li>
&lt;li>&lt;strong>泛化&lt;/strong>：generalization，就是将当前的模型泛化到其他数据集上。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723000619543-16900419818941.png" alt="image-20230723000619543">&lt;/p>
&lt;p>实战中需要考虑算法工程化，比如本训练任务，我们只需要简单的命令行，就能把训练过程跑起来。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001453414.png" alt="image-20230723001453414">&lt;/p>
&lt;h2 id="36模型测试">3.6.模型测试&lt;/h2>
&lt;p>当模型训练好后，就要用测试数据集进行测试了，我们可以通过：&lt;/p>
&lt;ul>
&lt;li>各类训练指标进行判断。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001817880.png" alt="image-20230723001817880">&lt;/p>
&lt;ul>
&lt;li>当然，也可以用直观的方法观看(PS：本次训练任务有点出乎意料，除了能识别泥坑，路面的裂缝也被识别出来了。)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002107388.png" alt="image-20230723002107388">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002143084.png" alt="image-20230723002143084">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002208208.png" alt="image-20230723002208208">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002237309.png" alt="image-20230723002237309">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002257695.png" alt="image-20230723002257695">&lt;/p>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记3-机器学习基本原理(上)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8A/</link><pubDate>Tue, 18 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8A/</guid><description>&lt;p>现在的热点是大语言模型，为什么我们还要了解机器学习？因为从机器学习到深度学习，再到如今的大语言模型，环环相扣。天底下没有新鲜事，并不是突然出现了一个划时代的技术。从历史、从论文都能按图索骥地找到大语言模型的源头——机器学习。&lt;/p>
&lt;h1 id="1机器学习的目标是什么">1.机器学习的目标是什么?&lt;/h1>
&lt;p>机器学习的目标就是从数据中发现规律。&lt;/p>
&lt;p>这个过程本质是一种统计建模的过程，是统计学方法的一种具体应用。&lt;/p>
&lt;p>下图引用自李宏毅老师的课件：&lt;/p>
&lt;ul>
&lt;li>ChatGPT找到了回答问题的函数f，输入&amp;quot;什么是机器学习？&amp;quot;，函数f就能生成答案。&lt;/li>
&lt;li>Midjourney找到了生成图片的函数f，输入&amp;quot;一只可爱的猫&amp;rdquo;，函数f就能生成小猫的图片。&lt;/li>
&lt;li>AlphaGo找到了下棋的函数f，输入&amp;quot;对手的一步棋&amp;rdquo;，函数f就能生成下一步应该怎么走。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720150809229.png" alt="image-20230720150809229">&lt;/p>
&lt;h1 id="2机器学习和人类学习的相似之处">2.机器学习和人类学习的相似之处&lt;/h1>
&lt;p>人类学习和机器学习有很多相似之处，本质上：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>人类学习&lt;/strong>：从经验中总结规律——俗称经受社会的毒打。&lt;/li>
&lt;li>&lt;strong>机器学习&lt;/strong>：从数据中总结模型——俗称经受数据的洗礼。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720152149924.png" alt="image-20230720152149924">&lt;/p>
&lt;h1 id="3机器学习打破了早期ai范式">3.机器学习打破了早期AI范式&lt;/h1>
&lt;p>人工智能的发展大致经历了四代，第一代AI以符号主义为主流，第二代至今以联结主义为主流(机器学习、深度学习就属于这个流派)。&lt;/p>
&lt;p>后来还有三个法国神人，绘制了人工智能发展历史上的两个流派的竞争过程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720153137135.png" alt="image-20230720153137135">&lt;/p>
&lt;p>早期的人工智能到底是怎样的呢？大致如下：&lt;/p>
&lt;ul>
&lt;li>STEP1.人类某个领域的专家总结该领域的业务规则。&lt;/li>
&lt;li>STEP2.人类程序员将业务规则以代码的形式实现，形成专家系统。&lt;/li>
&lt;li>STEP3.最终用户输入问题后，专家系统匹配规则，进行答复。&lt;/li>
&lt;/ul>
&lt;p>早期的人工智能的本质就是&lt;strong>人类总结规则、设定规则&lt;/strong>。这种范式的最大问题就是人类总结的规则不足，甚至人类本身总结不出来过于复杂的规则，导致早期的人工智能无法推广应用。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720154059662.png" alt="image-20230720154059662">&lt;/p>
&lt;p>而出自于联结主义的机器学习推崇的就是&lt;strong>不用人类总结规则，给我数据我来总结规律&lt;/strong>。&lt;/p>
&lt;p>有一种言论说：神经网络的产生、海量数据的出现、算力的提升造就了机器学习的风靡。&lt;/p>
&lt;p>这种言论应该不是机器学习风靡的根因，真正的根因应该是&lt;strong>机器学习改变了早期AI的范式&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记2-不是什么?是什么?有何方向?</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/</link><pubDate>Tue, 18 Jul 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/</guid><description>&lt;p>最近有小伙伴提意见，希望我的专栏能用小短篇的形式，将大模型相关知识通俗阐述一下。&lt;/p>
&lt;p>特此请ChatGPT协助我一起将近几个月阅读的资料以笔记体记录下来，与大家分享。&lt;/p>
&lt;h1 id="1chatgpt不是什么">1.ChatGPT不是什么?&lt;/h1>
&lt;h2 id="11古早的聊天机器人">1.1.古早的聊天机器人&lt;/h2>
&lt;p>一些古早的聊天机器人、客服机器人，有采用如下方式进行实现：&lt;/p>
&lt;ul>
&lt;li>利用分词组件对用户输入的问题进行分词，获得问题中的关键词。&lt;/li>
&lt;li>将关键词，在数据库、在互联网的上中进行搜索。
&lt;ul>
&lt;li>其中，数据库中预置了很多问题和答案。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将得到的答案过滤清洗、随机返回其中1个答案。&lt;/li>
&lt;/ul>
&lt;h2 id="12现代的聊天机器人">1.2.现代的聊天机器人&lt;/h2>
&lt;p>&lt;strong>ChatGPT及其更早的NLP领域技术&lt;/strong>，早已&lt;strong>不是从数据库、互联网&lt;/strong>生成答案的。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230718234119962.png" alt="image-20230718234119962">&lt;/p>
&lt;p>ChatGPT官方也明确表达了&amp;quot;ChatGPT is not connected to the internet&amp;rdquo;。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719065005501.png" alt="image-20230719065005501">&lt;/p>
&lt;h1 id="2chatgpt是什么">2.ChatGPT是什么？&lt;/h1>
&lt;h2 id="21大模型的生成原理chatgpt版">2.1.大模型的生成原理(ChatGPT版)&lt;/h2>
&lt;ul>
&lt;li>请ChatGPT解释一下大模型的生成原理，如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719092626552.png" alt="image-20230719092626552">&lt;/p>
&lt;h2 id="22大模型的生成原理通俗版">2.2.大模型的生成原理(通俗版)&lt;/h2>
&lt;p>我们再来通俗理解一下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>：问一个问题&amp;quot;什么是ChatGPT？&amp;quot;，可以将ChatGPT看成1个函数F，这个函数会输出即将回答的答案中第一个字(词)的概率。比如：根据问题，答案第一个字是&amp;quot;c&amp;quot;的概率是最大，最终就认为答案的第1个字是&amp;quot;c&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719093253167.png" alt="image-20230719093253167">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2&lt;/strong>：将STEP1的输出追加到问题上，问题就变成了&amp;quot;什么是ChatGPT?c&amp;rdquo;，继续输入给函数F，得到新的概率，最终认为答案的第2个字是&amp;quot;h&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719094213880.png" alt="image-20230719094213880">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP3、STEP4、STEP5&lt;/strong>：不断重复STEP1、STEP2的动作，就会得到完整的答案，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719094340618.png" alt="image-20230719094340618">&lt;/p>
&lt;h1 id="3未来的研究方向">3.未来的研究方向？&lt;/h1>
&lt;p>大模型未来的热点研究方向很多很多：提示词工程、神经网络编辑、机器遗忘、自适应计算和自适应模型、跨模态学习、模型压缩和加速、集成多任务和元学习、强化学习和自监督学习等等。&lt;/p>
&lt;p>笔者认为&amp;rdquo;&lt;strong>提示词工程、神经网络编辑、机器遗忘&lt;/strong>&amp;ldquo;最为有趣，因为它们和脑科学有点相近之处，都是在&amp;quot;激发神经网络&amp;rdquo;、&amp;ldquo;修改神经网络&amp;rdquo;、&amp;ldquo;抹除神经网络&amp;rdquo;。&lt;/p>
&lt;h2 id="31提示词工程">3.1.提示词工程&lt;/h2>
&lt;ul>
&lt;li>Prompt Engineering，吴恩达老师的课程让这个热点更热。提示词工程的本质可以认为：大模型好像一个&lt;strong>博学的老人家&lt;/strong>(世界知识几乎都知道)，只是&lt;strong>记性不太好&lt;/strong>。你需要用一些特殊的提示词唤醒他的记忆(俗称会念咒)，他就能很好地回答你的问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719085754568.png" alt="image-20230719085754568">&lt;/p>
&lt;h2 id="32神经网络编辑">3.2.神经网络编辑&lt;/h2>
&lt;p>Neural Editing，随着模型越来越大，训练好一个模型的成本极高，重新训练几乎不可能，因此需要有一种改进神经网络生成结果的技术，可以通过对模型的生成结果进行编辑，来获得更准确、更自然的结果。该技术通常涉及到在生成结果中插入或删除一些元素，或者重新排列生成结果的顺序，以获得更好的输出结果。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719090639814.png" alt="image-20230719090639814">&lt;/p>
&lt;h2 id="33机器遗忘">3.3.机器遗忘&lt;/h2>
&lt;p>Machine Unlearning，机器遗忘是一种让机器学会忘记以前的知识的技术。说白了，就是&amp;quot;你知道了不该知道的事情&amp;rdquo;。这项技术可以应用于安全隐私、AI伦理领域。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719091436950.png" alt="image-20230719091436950">&lt;/p>
&lt;h2 id="34其它方向">3.4.其它方向&lt;/h2>
&lt;p>以下来自于ChatGPT的解释，供参考：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>自适应计算和自适应模型&lt;/strong>：在大模型上进行计算是一项巨大的挑战，因为它们需要大量的计算资源和内存。因此，未来的研究方向将包括自适应计算和自适应模型，旨在优化大模型的计算效率和资源利用率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>跨模态学习&lt;/strong>：指使用多种类型的数据来训练一个模型。例如，将文本、图像和语音数据结合起来训练一个模型，以获得更准确、更全面的结果。未来的研究方向将探索如何在大模型上实现跨模态学习，以进一步提高模型的准确性和泛化能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>模型压缩和加速&lt;/strong>：由于大模型需要大量的计算资源和内存，因此未来的研究方向将包括模型压缩和加速，以减少模型的大小和计算负载，从而使模型更加可扩展和可用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>集成多任务和元学习&lt;/strong>：指在一个模型中同时处理多个任务，例如，在自然语言处理中同时进行命名实体识别和情感分析。元学习是指在训练过程中学习如何学习的过程。未来的研究方向将集中在如何在大模型上实现多任务学习和元学习，以提高模型的泛化能力和效率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>强化学习和自监督学习&lt;/strong>：强化学习是指使用奖励信号来训练一个模型，在自我监督学习中，模型从未标记的数据中学习。未来的研究方向将集中在如何在大型模型上实现强化学习和自我监督学习，以提高模型的效率和准确性。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：本文部分内容由ChatGPT生成。&lt;/p>
&lt;/blockquote></description></item><item><title>【运行时数据区】-并发编程-前置知识(4.并发编程基础)-8</title><link>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-8/</link><pubDate>Wed, 24 May 2023 01:09:00 +0800</pubDate><guid>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-8/</guid><description>&lt;p>本文重点解读JDK中的重要概念——线程组(&lt;code>ThreadGroup&lt;/code>)的源码。&lt;/p>
&lt;h1 id="1线程组的树状结构">1.线程组的树状结构&lt;/h1>
&lt;p>我们先看一下&lt;code>java.lang.ThreadGroup&lt;/code>的成员变量：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230523203618607.png" alt="image-20230523203618607">&lt;/p>
&lt;p>通过JDK源码可以看到，每个&lt;code>ThreadGroup&lt;/code>对象拥有一个父线程组&lt;code>parent&lt;/code>，同时每个&lt;code>ThreadGroup&lt;/code>对象也拥有自己的子线程组集合&lt;code>group[]&lt;/code>和子线程集合&lt;code>threads[]&lt;/code>。&lt;/p>
&lt;p>这样，JDK的&lt;code>ThreadGroup&lt;/code>本质上形成了一个线程组和线程的树状结构。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230523203932254.png" alt="image-20230523203932254">&lt;/p>
&lt;h1 id="2决定线程组父子关系的时机构造函数">2.决定线程组父子关系的时机：构造函数&lt;/h1>
&lt;p>&lt;code>ThreadGroup&lt;/code>提供了4个构造函数(其中2个是&lt;code>public&lt;/code>类型以供程序猿调用)，如下图所示：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524090234413.png" alt="image-20230524090234413">&lt;/p>
&lt;p>这4个构造函数的调用链关系如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524094642711.png" alt="image-20230524094642711">&lt;/p>
&lt;p>我们接下来逐一分析它们的源码。&lt;/p>
&lt;h2 id="21threadgroup">2.1.ThreadGroup()&lt;/h2>
&lt;p>分析第1个无参构造函数，可知：&lt;/p>
&lt;ul>
&lt;li>它仅被JVM调用&lt;/li>
&lt;li>它创建名为&lt;code>system&lt;/code>的线程组&lt;/li>
&lt;li>它的父线程组为&lt;code>null&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524090656391.png" alt="image-20230524090656391">&lt;/p>
&lt;h2 id="22threadgroupstring-name">2.2.ThreadGroup(String name)&lt;/h2>
&lt;p>分析第2个构造函数，可知：&lt;/p>
&lt;ul>
&lt;li>它调用了第3个构造函数&lt;/li>
&lt;li>它获取了当前线程所在的线程组，作为自己的父线程组&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524091312700.png" alt="image-20230524091312700">&lt;/p>
&lt;h2 id="23threadgroupthreadgroup-parent-string-name">2.3.ThreadGroup(ThreadGroup parent, String name)&lt;/h2>
&lt;p>分析第3个构造函数，可知：&lt;/p>
&lt;ul>
&lt;li>它调用了第3个构造函数&lt;/li>
&lt;li>它调用了&lt;code>checkParentAccess&lt;/code>方法，检查了父线程组&lt;code>parent&lt;/code>的权限&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524091449229.png" alt="image-20230524091449229">&lt;/p>
&lt;h2 id="24threadgroupvoid-unused-threadgroup-parent-string-name">2.4.ThreadGroup(Void unused, ThreadGroup parent, String name)&lt;/h2>
&lt;p>分析第4个构造函数，可知：&lt;/p>
&lt;ul>
&lt;li>在这个构造函数中，调用&lt;code>add&lt;/code>方法，将传入的线程组对象设置为父线程组。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524091708771.png" alt="image-20230524091708771">&lt;/p>
&lt;h2 id="25addthreadgroup-g">2.5.add(ThreadGroup g)&lt;/h2>
&lt;p>此方法就是构建线程组父子关系的关键代码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>813行：当前线程组对象的子线程组集合为空，则创建长度为4的子线程组集合&lt;/p>
&lt;/li>
&lt;li>
&lt;p>815行：如果当前线程组对象的子线程组集合长度已经满了，则将子线程组集合扩容为2倍&lt;/p>
&lt;ul>
&lt;li>注：JDK源码中有很多扩容代码，在空间和时间上进行性能平衡，值得学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>817行：将传入的线程组g，加入到当前线程组对象的子线程组集合&lt;/p>
&lt;/li>
&lt;li>
&lt;p>821行：将子线程组集合的游标增1&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524094843439.png" alt="image-20230524094843439">&lt;/p>
&lt;h2 id="26测试">2.6.测试&lt;/h2>
&lt;p>我们可以写一段测试代码：&lt;/p>
&lt;ul>
&lt;li>我们借助&lt;code>getParent()&lt;/code>方法，获得指定线程组的父线程组&lt;/li>
&lt;li>我们借助&lt;code>list()&lt;/code>方法，打印当前进程中的线程组的树状结构&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524093324551.png" alt="image-20230524093324551">&lt;/p>
&lt;p>上述代码的控制台输出如下：&lt;/p>
&lt;ul>
&lt;li>上述代码就形成了1个线程组树：&lt;code>system线程组&lt;/code>&amp;gt;&lt;code>main线程组&lt;/code>&amp;gt;&lt;code>线程组1&lt;/code>&amp;gt;&lt;code>线程组2&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524093905263.png" alt="image-20230524093905263">&lt;/p>
&lt;h1 id="3决定线程父子关系的时机add">3.决定线程父子关系的时机：add&lt;/h1>
&lt;h2 id="31触发">3.1.触发&lt;/h2>
&lt;ul>
&lt;li>&lt;code>Thread.start()&lt;/code>方法会调用&lt;code>ThreadGroup.add(Thread t)&lt;/code>方法，目的是将该线程加入对应线程组的子线程组集合&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524100028755.png" alt="image-20230524100028755">&lt;/p>
&lt;h2 id="32threadgroupaddthread-t">3.2.ThreadGroup.add(Thread t)&lt;/h2>
&lt;p>此方法就是构建线程和线程组父子关系的关键代码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>892行：当前线程组对象的子线程集合为空，则创建长度为4的子线程集合&lt;/p>
&lt;/li>
&lt;li>
&lt;p>894行：如果当前线程组对象的子线程集合长度已经满了，则将子线程集合扩容为2倍&lt;/p>
&lt;/li>
&lt;li>
&lt;p>896行：将传入的线程t，加入到当前线程组对象的子线程集合&lt;/p>
&lt;/li>
&lt;li>
&lt;p>900行：将子线程集合的游标增1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>906行：将未启动的线程数减1&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524100637548.png" alt="image-20230524100637548">&lt;/p>
&lt;h2 id="33测试">3.3.测试&lt;/h2>
&lt;p>我们的测试代码如下：&lt;/p>
&lt;ul>
&lt;li>创建3个线程：线程1、线程2、线程3&lt;/li>
&lt;li>线程1：未指定线程组，&lt;code>Thread&lt;/code>的构造函数会找当前线程对象的父线程对象所属的线程组，指定线程1要加入到该线程组
&lt;ul>
&lt;li>注：此处不展开，读者可通过&lt;code>Thread&lt;/code>构造函数自行分析&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>线程2：指定线程2要加入到线程组1&lt;/li>
&lt;li>线程3：指定线程3要加入到线程组2&lt;/li>
&lt;li>start：线程1、线程2、线程3启动，触发了3个线程真正地加入到对应的线程组&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524101258085.png" alt="image-20230524101258085">&lt;/p>
&lt;p>上述代码的控制台输出如下：&lt;/p>
&lt;ul>
&lt;li>上述代码就形成了1个线程组和线程的树状结构：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524102208868.png" alt="image-20230524102208868">&lt;/p>
&lt;h1 id="4完整测试代码">4.完整测试代码&lt;/h1>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">Main&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">ThreadGroup&lt;/span> &lt;span class="n">threadGroup1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ThreadGroup&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;线程组1&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">ThreadGroup&lt;/span> &lt;span class="n">threadGroup2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ThreadGroup&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;线程组2&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">Thread&lt;/span> &lt;span class="n">thread1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">(()&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;...&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentThread&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getName&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">try&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">TimeUnit&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">SECONDS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">sleep&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span> &lt;span class="k">catch&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">InterruptedException&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">throw&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">RuntimeException&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">},&lt;/span> &lt;span class="s">&amp;#34;线程1&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">Thread&lt;/span> &lt;span class="n">thread2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">()&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;...&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentThread&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getName&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">try&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">TimeUnit&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">SECONDS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">sleep&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span> &lt;span class="k">catch&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">InterruptedException&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">throw&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">RuntimeException&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">},&lt;/span> &lt;span class="s">&amp;#34;线程2&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">Thread&lt;/span> &lt;span class="n">thread3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">()&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;...&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">Thread&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">currentThread&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getName&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">try&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">TimeUnit&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">SECONDS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">sleep&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span> &lt;span class="k">catch&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">InterruptedException&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">throw&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">RuntimeException&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">},&lt;/span> &lt;span class="s">&amp;#34;线程3&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">thread1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">start&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">thread2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">start&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">thread3&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">start&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="c1">// getParent/parentOf
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">ThreadGroup&lt;/span> &lt;span class="n">threadGroupMain&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getParent&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroupMain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getName&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroupMain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">parentOf&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="c1">// activeCount/activeGroupCount
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">activeCount&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">threadGroup1&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">activeGroupCount&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="c1">// list
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">ThreadGroup&lt;/span> &lt;span class="n">threadGroupSystem&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">threadGroupMain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getParent&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">threadGroupSystem&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">list&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="5参考">5.参考&lt;/h1>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">java/lang/ThreadGroup.java
java/lang/Thread.java
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-8/image-20230524104931008.png" alt="image-20230524104931008">&lt;/p></description></item><item><title>【运行时数据区】-并发编程-前置知识(4.并发编程基础)-7</title><link>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-7/</link><pubDate>Sat, 22 Apr 2023 01:10:31 +0800</pubDate><guid>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-7/</guid><description>&lt;h1 id="1实验模拟java线程状态迁移过程">1.实验：模拟Java线程状态迁移过程&lt;/h1>
&lt;p>在《【运行时数据区】-并发编程-前置知识(4.并发编程基础)-6》中，我们分析了Java线程从JDK层面到JVM层面的状态迁移：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230414070017806.png" alt="image-20230414070017806">&lt;/p>
&lt;p>本文我们来尝试自己实现Java线程的状态迁移过程，进一步加深对这个状态迁移过程的理解。&lt;/p>
&lt;h1 id="2简化jvm调用链">2.简化：JVM调用链&lt;/h1>
&lt;ul>
&lt;li>如下是JVM调用链：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230414064052581.png" alt="image-20230414064052581">&lt;/p>
&lt;ul>
&lt;li>我们简化调用链如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422145647662.png" alt="image-20230422145647662">&lt;/p>
&lt;h1 id="3模拟1jvm注册jni接口">3.模拟1：JVM注册JNI接口&lt;/h1>
&lt;ul>
&lt;li>如红框处，我们通过&lt;code>System.load&lt;/code>方法，加载提供了JNI接口的动态库&lt;code>lib.so&lt;/code>文件。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135359625.png" alt="image-20230422135359625">&lt;/p>
&lt;h1 id="4模拟2启动java线程">4.模拟2：启动Java线程&lt;/h1>
&lt;p>如下图：&lt;/p>
&lt;ul>
&lt;li>&lt;code>start()&lt;/code>方法模拟JDK的&lt;code>Thread.start()&lt;/code>方法，该方法调用了&lt;code>start0()&lt;/code>方法。&lt;/li>
&lt;li>&lt;code>start0()&lt;/code>方法模拟JDK的&lt;code>Thread.start0()&lt;/code>方法，该方法为JNI接口。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135522197.png" alt="image-20230422135522197">&lt;/p>
&lt;ul>
&lt;li>通过&lt;code>javah&lt;/code>，我们生成了&lt;code>start0()&lt;/code>方法对应的C++头文件。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135555599.png" alt="image-20230422135555599">&lt;/p>
&lt;h1 id="6模拟3创建native线程">6.模拟3：创建native线程&lt;/h1>
&lt;ul>
&lt;li>在&lt;code>start0()&lt;/code>方法的实现中，我们调用了&lt;code>pthread_create()&lt;/code>方法，创建native线程。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135727912.png" alt="image-20230422135727912">&lt;/p>
&lt;h1 id="7模拟4等待">7.模拟4：等待&lt;/h1>
&lt;ul>
&lt;li>当CPU调度上一步我们创建的native线程，进入该线程的回调函数&lt;code>java_start()&lt;/code>。&lt;/li>
&lt;li>我们创建了标志位&lt;code>flag&lt;/code>，它用于模拟JVM中记录native线程一系列状态的标志位，初始值为0，表示native线程处于&lt;code>INITIALIZED&lt;/code>状态。&lt;/li>
&lt;li>while循环导致native线程一直处于等待状态。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135917734.png" alt="image-20230422135917734">&lt;/p>
&lt;h1 id="8模拟5打破循环">8.模拟5：打破循环&lt;/h1>
&lt;ul>
&lt;li>回到&lt;code>start0&lt;/code>方法的实现，通过sleep了一段时间后修改&lt;code>flag&lt;/code>标志位，模拟JVM中经过一系列准备工作，将native线程设置为&lt;code>RUNNABLE状态&lt;/code>。&lt;/li>
&lt;li>此时，native线程在&lt;code>java_start()&lt;/code>中处于循环等待的状态被打破。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422135949723.png" alt="image-20230422135949723">&lt;/p>
&lt;h1 id="9模拟6回调jdk">9.模拟6：回调JDK&lt;/h1>
&lt;ul>
&lt;li>当native线程在&lt;code>java_start()&lt;/code>中处于循环等待的状态被打破后，通过JNI获得Java侧的Test对象，进一步回调Java侧Test对象的&lt;code>run()&lt;/code>方法。&lt;/li>
&lt;li>这一步，就模拟了JVM在native线程变为&lt;code>RUNNABLE&lt;/code>状态后回调JDK的&lt;code>Thread.run()&lt;/code>方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422140058781.png" alt="image-20230422140058781">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422140113451.png" alt="image-20230422140113451">&lt;/p>
&lt;h1 id="10运行效果">10.运行效果&lt;/h1>
&lt;ul>
&lt;li>运行我们的模拟程序，打印结果如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422140522662.png" alt="image-20230422140522662">&lt;/p>
&lt;ul>
&lt;li>本次实验，最终模拟了JDK中创建Java线程，JVM中对应也创建native线程，native线程的状态迁移后回调Java线程的回调函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-7/image-20230422153048716.png" alt="image-20230422153048716">&lt;/p></description></item><item><title>【运行时数据区】-并发编程-前置知识(4.并发编程基础)-6</title><link>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-6/</link><pubDate>Sat, 15 Apr 2023 01:10:31 +0800</pubDate><guid>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-6/</guid><description>&lt;h1 id="1疑问java线程对应的native线程状态如何迁移">1.疑问：Java线程对应的native线程状态如何迁移？&lt;/h1>
&lt;p>在《【运行时数据区】-并发编程-前置知识(4.并发编程基础)-5》中，我们在JDK层面对Java线程的6种状态迁移进行了实验。&lt;/p>
&lt;p>本文再以Java线程从NEW状态迁移到RUNNABLE状态为引子，观测一下JVM为Java线程创建的native线程的状态如何迁移：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230413160245714.png" alt="">&lt;/p>
&lt;h1 id="2探索jvm调用链全景图">2.探索：JVM调用链全景图&lt;/h1>
&lt;p>为了避免迷失在JVM的源码中，我们先看一下笔者整理的JVM调用链全景：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1.JVM准备&lt;/strong>：JDK通过JNI，加载JVM侧的相关函数。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JDK：Thread.java的registerNatives()
(2)JVM：jdk/src/share/native/java/lang/Thread.c的registerNatives()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP2.启动Java线程&lt;/strong>：JDK侧启动线程，JVM侧进行初始化。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JDK：Thread.java的start()
(2)JVM：src/share/vm/prims/jvm.cpp的JVM_StartThread()
(3)JVM：src/share/vm/runtime/thread.cpp的JavaThread()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP3.记录JDK回调&lt;/strong>：JVM侧记录JDK侧的Java线程回调函数&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JVM：src/share/vm/runtime/thread.hpp的set_entry_point()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP4.创建native线程&lt;/strong>：JVM侧创建native线程，初始化后等待CPU执行这个native线程&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JVM：src/os/linux/vm/os_linux.cpp的create_thread()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP5.等待&lt;/strong>：CPU执行native线程，JVM阻塞，等待native线程进入RUNNABLE状态&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JVM：src/os/linux/vm/os_linux.cpp的java_start()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP6.打破循环&lt;/strong>：JVM在STEP4后，做了其它操作，最终设置native线程、Java线程为RUNNABLE状态，打破STEP5等待&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JVM：src/share/vm/prims/jvm.cpp的JVM_StartThread()
(2)JVM：src/share/vm/runtime/thread.cpp的start()
(3)JVM：src/share/vm/runtime/os.cpp的start_thread()
(4)JVM：src/os/linux/vm/os_linux.cpp的pd_start_thread()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>STEP7.回调JDK&lt;/strong>：JVM通过JNI回调JDK侧的Java线程回调&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">涉及代码：
(1)JVM：src/share/vm/runtime/thread.cpp的run()
(2)JVM：src/share/vm/runtime/thread.cpp的thread_main_inner()
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064052581.png" alt="">&lt;/p>
&lt;h1 id="3细节1jvm准备">3.细节1：JVM准备&lt;/h1>
&lt;p>第一步，看JVM如何将&lt;code>java.lang.Thread&lt;/code>中用到的native方法批量注册的：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414060903544.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在JDK中，&lt;code>java.lang.Thread&lt;/code>在静态代码块中调用了&lt;code>registerNatives&lt;/code>。此方法为native方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061036368.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在JVM中，java/lang/Thread.c定义了JNI方法&lt;code>Java_java_lang_Thread_registerNatives&lt;/code>。此方法使用了&lt;code>methods&lt;/code>变量，此变量定义了&lt;code>java.lang.Thread&lt;/code>类中所有的native方法对应的函数指针。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061109828.png" alt="">&lt;/p>
&lt;h1 id="4细节2启动java线程">4.细节2：启动Java线程&lt;/h1>
&lt;p>第二步，理解&lt;code>java.lang.Thread&lt;/code>调用&lt;code>start&lt;/code>方法时，JVM对应的处理：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061547200.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>java.lang.Thread&lt;/code>的&lt;code>start&lt;/code>方法核心是调用了native方法&lt;code>start0&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061148420.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在JVM中，native的&lt;code>start0&lt;/code>方法对应的实现为&lt;code>JVM_StartThread&lt;/code>函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061324005.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>JVM_StartThread&lt;/code>方法核心是创建了&lt;code>JavaThread&lt;/code>对象。此对象是JVM中表示Java线程对象的抽象。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061356033.png" alt="">&lt;/p>
&lt;h1 id="5细节3记录jdk回调">5.细节3：记录JDK回调&lt;/h1>
&lt;p>第三步，理解JVM如何记录Java线程的回调：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061824024.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在JVM中，thread.cpp中定义了&lt;code>JavaThread&lt;/code>的构造函数，第一个参数&lt;code>entry_point&lt;/code>就表示Java线程的回调函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061653606.png" alt="">&lt;/p>
&lt;ul>
&lt;li>thread.hpp提供了&lt;code>set_entry_point&lt;/code>函数，&lt;code>JavaThread&lt;/code>的构造函数调用此函数，将Java线程的回调记录到内存中，供后续流程调用。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061709047.png" alt="">&lt;/p>
&lt;h1 id="6细节4创建native线程">6.细节4：创建native线程&lt;/h1>
&lt;p>第四步，在JVM做好一切准备工作后，JVM如何在操作系统上创建native线程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064223932.png" alt="">&lt;/p>
&lt;ul>
&lt;li>JVM的&lt;code>thread.cpp&lt;/code>在&lt;code>JavaThread&lt;/code>的构造函数中调用&lt;code>os_linux.cpp&lt;/code>的&lt;code>create_thread&lt;/code>函数。&lt;/li>
&lt;li>&lt;code>create_thread&lt;/code>函数创建了&lt;code>OSThread&lt;/code>对象，此对象记录了&lt;code>JavaThread&lt;/code>和即将创建的&lt;code>native线程&lt;/code>之间的一一对应关系。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061900504.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在建立了&lt;code>JavaThread&lt;/code>对象和&lt;code>native线程对象&lt;/code>后，JVM就调用&lt;code>pthread&lt;/code>库的&lt;code>pthread_create&lt;/code>方法，在操作系统上创建了真正的&lt;code>native线程&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061913975.png" alt="">&lt;/p>
&lt;ul>
&lt;li>到这里，JDK中的Java线程仍然处于&lt;code>NEW&lt;/code>状态，而JVM中的&lt;code>native线程&lt;/code>在一段时间内处于&lt;code>ALLOCATED&lt;/code>状态，JVM一直会等待&lt;code>native线程&lt;/code>突破这个状态。并且，当&lt;code>pthread&lt;/code>库在操作系统层创建&lt;code>native线程&lt;/code>出现问题时，&lt;code>native线程&lt;/code>处于&lt;code>ZOMBIE&lt;/code>状态。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414061932717.png" alt="">&lt;/p>
&lt;h1 id="7细节5等待">7.细节5：等待&lt;/h1>
&lt;p>第五步，在操作系统层面已经存在了和JDK的Java线程一一对应的&lt;code>native线程&lt;/code>，那么我们就要来理解在CPU下一个可能的时间周期中是如何执行的&lt;code>native线程&lt;/code>的回调。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064342579.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_linux.cpp&lt;/code>的&lt;code>java_start&lt;/code>函数是&lt;code>native线程&lt;/code>的回调函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064407101.png" alt="">&lt;/p>
&lt;ul>
&lt;li>这个函数的结束处，&lt;code>native线程&lt;/code>的状态一直处于&lt;code>INITIALIZED&lt;/code>状态，这个状态就对应Java线程的&lt;code>NEW&lt;/code>状态。此时，&lt;code>native线程&lt;/code>阻塞。&lt;/li>
&lt;li>当&lt;code>native线程&lt;/code>被打破了&lt;code>INITIALIZED&lt;/code>状态，&lt;code>native线程&lt;/code>不再原地止步，而是进一步执行&lt;code>thread.cpp&lt;/code>的&lt;code>run&lt;/code>方法。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064418955.png" alt="">&lt;/p>
&lt;h1 id="8细节6打破循环">8.细节6：打破循环&lt;/h1>
&lt;p>第六步，我们再来理解JVM如何打破上一步&lt;code>native线程&lt;/code>止步不前的状态。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414065410715.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在&lt;code>jvm.cpp&lt;/code>的JVM_StartThread方法在创建了&lt;code>JavaThread&lt;/code>对象后，调用了&lt;code>thread.cpp&lt;/code>的&lt;code>start&lt;/code>函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414065209484.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>thread.cpp&lt;/code>的&lt;code>start&lt;/code>方法将&lt;code>JavaThread&lt;/code>对象设置为&lt;code>RUNNABLE&lt;/code>状态，此时JDK对应的Java线程处于&lt;code>RUNNABLE&lt;/code>状态。&lt;/li>
&lt;li>&lt;code>thread.cpp&lt;/code>调用&lt;code>os.cpp&lt;/code>的&lt;code>start_thread&lt;/code>函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414065229125.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>os.cpp&lt;/code>的&lt;code>start_thread&lt;/code>函数将&lt;code>native线程&lt;/code>也设置为&lt;code>RUNNABLE&lt;/code>状态，此时Java线程和&lt;code>native线程&lt;/code>都处于&lt;code>RUNNABLE&lt;/code>状态。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414065247151.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>os.cpp&lt;/code>的&lt;code>start_thread&lt;/code>函数进一步调用&lt;code>os_linux.cpp&lt;/code>的&lt;code>pd_start_thread&lt;/code>函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414065304780.png" alt="">&lt;/p>
&lt;h1 id="9细节7回调jdk">9.细节7：回调JDK&lt;/h1>
&lt;p>第七步，在第六步打破了&lt;code>native线程&lt;/code>止步不前的状态后，&lt;code>native线程&lt;/code>调用&lt;code>thread.cpp&lt;/code>的&lt;code>thread_main_inner&lt;/code>函数。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064752160.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>native线程&lt;/code>调用&lt;code>thread.cpp&lt;/code>的&lt;code>thread_main_inner&lt;/code>函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064845782.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064858699.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;code>thread.cpp&lt;/code>的&lt;code>thread_main_inner&lt;/code>函数中，进一步调用了在第三步中保存的Java线程的回调&lt;code>entry_point&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064911172.png" alt="">&lt;/p>
&lt;ul>
&lt;li>在&lt;code>jvm.cpp&lt;/code>中，&lt;code>thread_entry&lt;/code>函数调用了&lt;code>JavaCalls::call_virtual&lt;/code>函数，这个函数会通过JNI反向调用JDK中的Java代码，此处就是回调了JDK中的Java线程回调函数。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414064925823.png" alt="">&lt;/p>
&lt;h1 id="10结论">10.结论&lt;/h1>
&lt;p>通过前述代码流程的分析，我们可以得到如下结论：&lt;/p>
&lt;ul>
&lt;li>JDK中，&lt;code>new java.lang.Thread()&lt;/code>会创建Java线程，此时Java线程处于&lt;code>NEW&lt;/code>状态。&lt;/li>
&lt;li>JDK中，调用Java线程的&lt;code>start&lt;/code>方法后，在JVM中创建&lt;code>native线程&lt;/code>&lt;/li>
&lt;li>JVM中，&lt;code>native线程&lt;/code>先后经历&lt;code>ALLOCATED&lt;/code>状态，也可能出现&lt;code>ZOMBIE&lt;/code>状态。&lt;/li>
&lt;li>JVM中，&lt;code>native线程&lt;/code>进化为&lt;code>INITIALIZED&lt;/code>状态，可以对标JDK中Java线程的&lt;code>NEW&lt;/code>状态。&lt;/li>
&lt;li>JVM中，&lt;code>native线程&lt;/code>被打破等待循环后，&lt;code>native线程&lt;/code>变迁为&lt;code>RUNNABLE&lt;/code>状态。&lt;/li>
&lt;li>JDK中，&lt;code>Java线程&lt;/code>也变迁为&lt;code>RUNNABLE&lt;/code>状态，&lt;code>Java线程&lt;/code>的回调函数也被JVM调用执行。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230414070017806.png" alt="">&lt;/p>
&lt;h1 id="11随想">11.随想&lt;/h1>
&lt;p>每次分析JVM源码，仿佛在一个生活在三维空间的生物(Java程序猿)，窥探到四维空间(JVM、操作系统、CPU&amp;hellip;&amp;hellip;)，这种感觉令人自在、平静、喜乐。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-6/image-20230415010840596.png" alt="">&lt;/p></description></item><item><title>【运行时数据区】-并发编程-前置知识(4.并发编程基础)-5</title><link>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-5/</link><pubDate>Wed, 05 Apr 2023 09:07:31 +0800</pubDate><guid>https://jherculesqz.github.io/post/java%E6%8B%BE%E9%81%97/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%864.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-5/</guid><description>&lt;h1 id="1疑惑java的线程究竟有几种状态">1.疑惑：Java的线程究竟有几种状态？&lt;/h1>
&lt;p>在Java的并发编程技术中，经常会看到Java线程的状态迁移图。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>有的这样画&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230330130053911.png" alt="image-20230330130053911">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>有的这样画&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230330130208531.png" alt="image-20230330130208531">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>有的这样画：&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230330130240613.png" alt="image-20230330130240613">&lt;/p>
&lt;p>笔者刚接触Java时，看到形形色色的文章和图例，比较大的困扰是&lt;strong>Java的线程究竟有几种状态&lt;/strong>？&lt;/p>
&lt;h1 id="2探索1官方文档">2.探索1：官方文档&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>在Java8，Thread的状态定义在&lt;code>java.lang.Thread.State&lt;/code>中。&lt;strong>Java线程的状态有6种&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NEW&lt;/strong>&lt;/li>
&lt;li>&lt;strong>RUNNABLE&lt;/strong>&lt;/li>
&lt;li>&lt;strong>BLOCKED&lt;/strong>&lt;/li>
&lt;li>&lt;strong>WATING&lt;/strong>&lt;/li>
&lt;li>&lt;strong>TIMED_WAITING&lt;/strong>&lt;/li>
&lt;li>&lt;strong>TERMINATED&lt;/strong>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>详见：https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.State.html&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230330130905543.png" alt="image-20230330130905543">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>在Java20&lt;/strong>(Java最新版本)，Thread的状态定义在&lt;code>java.lang.Thread.State&lt;/code>中**。Java线程的状态依然是6种**。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>详见：https://docs.oracle.com/en/java/javase/20/docs/api/java.base/java/lang/Thread.State.html&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>结论：从官方文档中看，Java线程的状态为6种&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>NEW&lt;/strong>&lt;/li>
&lt;li>&lt;strong>RUNNABLE&lt;/strong>&lt;/li>
&lt;li>&lt;strong>BLOCKED&lt;/strong>&lt;/li>
&lt;li>&lt;strong>WATING&lt;/strong>&lt;/li>
&lt;li>&lt;strong>TIMED_WAITING&lt;/strong>&lt;/li>
&lt;li>&lt;strong>TERMINATED&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3探索2jdk源码">3.探索2：JDK源码&lt;/h1>
&lt;ul>
&lt;li>在Java8中，Java线程状态位于&lt;code>java/lang/Thread.java&lt;/code>的&lt;code>State&lt;/code>枚举类：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="C:%5CUsers%5CJHercules2%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20230331111439960.png" alt="image-20230331111439960">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NEW&lt;/strong>：当Java线程处于此状态时，创建线程那一刻，线程的状态。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405132055300.png" alt="image-20230405132055300">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>RUNNABLE&lt;/strong>：当Java线程处于此状态时，JVM&lt;strong>有可能&lt;/strong>去执行它(此处埋有伏笔)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405132344766.png" alt="image-20230405132344766">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BLOCKED&lt;/strong>：此状态，与&lt;code>synchronized&lt;/code>、&lt;code>Object.wait&lt;/code>强相关(后文代码演示)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405133442702.png" alt="image-20230405133442702">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>WAITING&lt;/strong>：此状态与&lt;code>BLOCKED&lt;/code>一样，与&lt;code>Object.wait&lt;/code>、&lt;code>Thread.join&lt;/code>等相关(后文代码演示)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405133702393.png" alt="image-20230405133702393">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>TIMED_WAITING&lt;/strong>：与&lt;code>WAITING&lt;/code>类似的一种状态，与&lt;code>Thread.sleep&lt;/code>、&lt;code>Object.wait&lt;/code>、&lt;code>Thread.join&lt;/code>等有关(后文代码演示)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405134328837.png" alt="image-20230405134328837">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>TERMINATED&lt;/strong>：终结态，没啥好说的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405134725161.png" alt="image-20230405134725161">&lt;/p>
&lt;h1 id="4思考为什么对java线程的状态描述不同">4.思考：为什么对Java线程的状态描述不同?&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>无论从官方文档，还是JDK源码，都很容易看到Java线程只有6种状态。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>那么，为什么各种资料中对Java线程的状态描述不同呢？&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我认为，可能是视角不同：&lt;/p>
&lt;ul>
&lt;li>笔者在之前的文章中，分析过JVM启动时会产生多少线程，即我们到底在操作系统层面看线程，还是在JVM层面看线程。&lt;/li>
&lt;li>站在JVM层面，Java给Java程序猿看到&lt;code>创建Java线程&lt;/code>、&lt;code>Java线程有6种状态&lt;/code>等等概念。&lt;/li>
&lt;li>站在OS层面，JVM为了实现Java程序猿看到的&lt;code>Java线程&lt;/code>，调用了&lt;code>pthread&lt;/code>这类库，创建了OS层面的线程，OS层面的线程有一系列状态，此状态与&lt;code>Java线程的6种状态&lt;/code>存在映射关系(本文不展开，下一篇笔者对照JVM源码详述)。&lt;/li>
&lt;li>因此，有的资料中试图同时表达JVM层面和OS层面的线程状态，就造成了描述不同。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>结论：我们应该从JVM层面和OS层面分别理解&lt;code>Java线程状态&lt;/code>，以及透彻理解两个层面的&lt;code>线程&lt;/code>的对应关系&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405140832691.png" alt="image-20230405140832691">&lt;/p>
&lt;h1 id="5深入java线程的状态如何迁移">5.深入：Java线程的状态如何迁移?&lt;/h1>
&lt;p>JDK源码中，对Java线程的状态迁移有明确地描述。但为了深入理解，我们有必要写一些示例代码，强化理解。&lt;/p>
&lt;blockquote>
&lt;p>PS：实战中，很多问题就源于我们并不是真的理解这些状态迁移条件。&lt;/p>
&lt;/blockquote>
&lt;h2 id="51newrunnabletime_waiting三态迁移">5.1.NEW、RUNNABLE、TIME_WAITING三态迁移&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>结论&lt;/strong>：
&lt;ul>
&lt;li>&lt;code>new Thread&lt;/code>时，Java线程进入&lt;code>NEW&lt;/code>状态&lt;/li>
&lt;li>调用&lt;code>start&lt;/code>方法时，Java线程进入&lt;code>RUNNABLE&lt;/code>状态&lt;/li>
&lt;li>Java线程运行&lt;code>run&lt;/code>方法体内一旦运行&lt;code>Thread.sleep&lt;/code>方法，Java线程进入&lt;code>TIME_WAITING&lt;/code>状态&lt;/li>
&lt;li>当&lt;code>Thread.sleep&lt;/code>时间到，Java线程回到&lt;code>RUNNABLE&lt;/code>状态&lt;/li>
&lt;li>其它说明：&lt;code>Object.wait&lt;/code>、&lt;code>Thread.join&lt;/code>等方法也可以触发&lt;code>RUNNABLE&lt;/code>态迁移到&lt;code>TIME_WAITING&lt;/code>态，读者可自行实验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405142942728.png" alt="image-20230405142942728">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>示例代码&lt;/strong>：
&lt;ul>
&lt;li>第21行在&lt;code>MyThread&lt;/code>构造函数中打印了线程状态、第9行在&lt;code>new MyThread&lt;/code>后也打印了线程状态，目的是从这两处打印证明&lt;code>new Thread&lt;/code>那一刻，Java线程处于&lt;code>NEW&lt;/code>状态。&lt;/li>
&lt;li>第26行打印线程状态，为了证明第11行调用&lt;code>start&lt;/code>方法后，Java线程进入&lt;code>RUNNABLE&lt;/code>状态。&lt;/li>
&lt;li>第27~第31行调用&lt;code>Thread.sleep&lt;/code>方法后，在第13~15行连续打印3次线程状态，为了证明调用&lt;code>Thread.sleep&lt;/code>方法后，Java线程进入&lt;code>TIME_WAITING&lt;/code>状态。&lt;/li>
&lt;li>第32行打印线程状态，为了证明&lt;code>Thread.sleep&lt;/code>方法时间到后，Java线程回到&lt;code>RUNNABLE&lt;/code>状态。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405144125704.png" alt="image-20230405144125704">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>示例代码运行结果&lt;/strong>：与预期一致。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405144303047.png" alt="image-20230405144303047">&lt;/p>
&lt;h2 id="52newrunnablewaitingblockedterminated五态迁移">5.2.NEW、RUNNABLE、WAITING、BLOCKED、TERMINATED五态迁移&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>结论&lt;/strong>：
&lt;ul>
&lt;li>&lt;code>new Thread&lt;/code>时，Java线程进入&lt;code>NEW&lt;/code>状态&lt;/li>
&lt;li>调用&lt;code>start&lt;/code>方法时，Java线程进入&lt;code>RUNNABLE&lt;/code>状态&lt;/li>
&lt;li>Java线程运行&lt;code>run&lt;/code>方法体内一旦运行&lt;code>Object.wait&lt;/code>方法，Java线程进入&lt;code>WAITING&lt;/code>状态&lt;/li>
&lt;li>当另一个Java线程执行了&lt;code>Object.notify&lt;/code>方法，Java线程进入&lt;code>RUNNABLE&lt;/code>状态&lt;/li>
&lt;li>在Java线程从占有对象锁到失去对象锁到再次获得对象锁，即等待&lt;code>synchronized&lt;/code>到重入&lt;code>synchronized&lt;/code>块，Java线程还会经历&lt;code>BLOCKED&lt;/code>态到&lt;code>RUNNABLE&lt;/code>态的迁移(此处也有一处伏笔，需要解读JVM源码才能清晰，下篇文章详述)&lt;/li>
&lt;li>当Java线程执行完，Java线程进入&lt;code>TERMINATED&lt;/code>状态&lt;/li>
&lt;li>其它说明：&lt;code>Object.wait&lt;/code>、&lt;code>Thread.join&lt;/code>等方法五态迁移，读者可自行实验。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405151245195.png" alt="image-20230405151245195">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>示例代码&lt;/strong>：
&lt;ul>
&lt;li>第15行为了证明&lt;code>Object.wait&lt;/code>后，Java线程进入&lt;code>WAITING&lt;/code>态&lt;/li>
&lt;li>第17行为了证明&lt;code>Object.notify&lt;/code>后，Java线程进入&lt;code>BLOCKED&lt;/code>态，再重入&lt;code>synchronized&lt;/code>后进入&lt;code>RUNNABLE&lt;/code>态&lt;/li>
&lt;li>第20行为了证明Java线程工作完成后，进入&lt;code>TERMINATED&lt;/code>态&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405152223300.png" alt="image-20230405152223300">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>示例代码运行结果&lt;/strong>：与预期一致。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Java%E6%8B%BE%E9%81%97/%E3%80%90%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%91-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86(4.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80)-5/image-20230405152517526.png" alt="image-20230405152517526">&lt;/p>
&lt;h1 id="6总结">6.总结&lt;/h1>
&lt;ul>
&lt;li>本文通过Java官方文档和JDK源码，解读了Java线程有6种状态&lt;/li>
&lt;li>本文又通过两段示例代码，验证了Java线程6种状态的迁移路径&lt;/li>
&lt;li>本文遗留了一个模糊的问题：Java线程在重入&lt;code>synchronized&lt;/code>时如何迁移到&lt;code>BLOCKED&lt;/code>态？此问题需要分析JVM源码得到进一步验证结论&lt;/li>
&lt;/ul></description></item><item><title>【音乐】陈升-牡丹亭外</title><link>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E7%89%A1%E4%B8%B9%E4%BA%AD%E5%A4%96/</link><pubDate>Mon, 27 Feb 2023 00:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E7%89%A1%E4%B8%B9%E4%BA%AD%E5%A4%96/</guid><description>&lt;center>&lt;font size=20>牡丹亭外&lt;/font>&lt;/center>
&lt;p align="right">陈升&lt;/p>
&lt;center>為救李郎離家園，誰料皇榜中狀元&lt;/center>
&lt;center>中狀元，著紅袍，帽插宮花好啊，好新鮮&lt;/center>
&lt;center>李郎一夢已過往，風流人兒如今在何方&lt;/center>
&lt;center>從古到今呀說來慌，不過是情而已&lt;/center>
&lt;center>這人間苦什麼，怕不能遇見你&lt;/center>
&lt;center>是否你走過了我身邊，恍恍惑惑一瞬間&lt;/center>
&lt;center>黃粱一夢二十年，依舊是不懂愛也不懂情&lt;/center>
&lt;center>寫歌的人假正經，聽歌的人最無情&lt;/center>
&lt;center>牡丹亭外雨紛紛，誰是歸人說不準&lt;/center>
&lt;center>是歸人啊，你說分明，你把我心放哪兒&lt;/center>
&lt;center>黃粱一夢二十年啊，依舊是不懂愛也不懂情&lt;/center>
&lt;center>寫歌的人假正經，聽歌的人最無情&lt;/center>
&lt;center>牡丹亭外雨紛紛，誰是歸人說不準&lt;/center>
&lt;center>這人間苦什麼，怕不能遇見你&lt;/center>
&lt;center>是否你走過了我身邊，恍恍惑惑一瞬間&lt;/center>
&lt;center>黃粱一夢二十年，依舊是不懂愛也不懂情&lt;/center>
&lt;center>寫歌的人斷了魂，聽歌的人最無情&lt;/center>
&lt;center>為救李郎離家園，誰料皇榜中狀元&lt;/center>
&lt;center>中狀元，著紅袍，帽插宮花好啊，好新鮮&lt;/center>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E9%9F%B3%E4%B9%90%E3%80%91%E9%99%88%E5%8D%87-%E7%89%A1%E4%B8%B9%E4%BA%AD%E5%A4%96/image-20230227125824349.png" alt="image-20230227125824349">&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/8msqXs6BuBI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>&lt;/iframe></description></item><item><title>【音乐】陈升-六月</title><link>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E5%85%AD%E6%9C%88/</link><pubDate>Sat, 25 Feb 2023 00:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E5%85%AD%E6%9C%88/</guid><description>&lt;center>&lt;font size=20>六月&lt;/font>&lt;/center>
&lt;p align="right">陈升&lt;/p>
&lt;center>六月在夏天之前的心情，總是偶爾晴朗有點雨&lt;/center>
&lt;center>少女憂愁的情懷，自己跌倒就不許哭&lt;/center>
&lt;center>你猜猜六月單純的心中，藏著什麼樣的故事&lt;/center>
&lt;center>忘不了那個男孩，和他滿腮的鬍渣&lt;/center>
&lt;center>Ohoh，有些悲傷卻又不許哭&lt;/center>
&lt;center>Ohoh，有點孤單卻又不認輸&lt;/center>
&lt;center>雙魚座的女生，從來都不怪別人的錯&lt;/center>
&lt;center>六月在春天之後的心情，總是偶而悲傷有點苦&lt;/center>
&lt;center>往日的戀情，自己跌倒就不許哭&lt;/center>
&lt;center>她帶著甜甜的笑容，她說我是快樂的魚&lt;/center>
&lt;center>游在茫茫的人海，男孩你怎會明白&lt;/center>
&lt;center>Ohoh，有些悲傷卻又不許哭&lt;/center>
&lt;center>Ohoh，有點孤單卻又不認輸&lt;/center>
&lt;center>幻想型的女孩，從來都不怪別人的錯&lt;/center>
&lt;center>因為要記得你的模樣，六月在夏天又去了海邊&lt;/center>
&lt;center>只要你知道，愛上你有些難過&lt;/center>
&lt;center>是晴天，是雨天&lt;/center>
&lt;center>走不出愛情的人是呆子，不應該留著一樣的髮型&lt;/center>
&lt;center>只要你知道，離開後別來無恙&lt;/center>
&lt;center>捨不得，忘了他，誒&lt;/center>
&lt;center>Ohoh，想要躲在他的懷裡哭&lt;/center>
&lt;center>Ohoh，沒有他也真的不在乎&lt;/center>
&lt;center>幻想型的女孩，從來都不怪別人的錯&lt;/center>
&lt;center>因為要記得你的模樣，六月在夏天又去了海邊&lt;/center>
&lt;center>只要你知道，愛上你有些難過&lt;/center>
&lt;center>是晴天，是雨天&lt;/center>
&lt;center>走不出愛情的人是呆子，不應該留著一樣的髮型&lt;/center>
&lt;center>只要你知道，離開後別來無恙&lt;/center>
&lt;center>決定要，忘了他&lt;/center>
&lt;center>Ohoh，有些悲傷卻又不許哭&lt;/center>
&lt;center>Ohoh，有點孤單卻又不認輸&lt;/center>
&lt;center>Ohoh，想要躲在他的懷裡哭&lt;/center>
&lt;center>Ohoh，沒有他也真的不在乎&lt;/center>
&lt;center>Ohoh，有些悲傷卻又不許哭&lt;/center>
&lt;center>Ohoh，有點孤單卻又不認輸&lt;/center>
&lt;center>誒&lt;/center>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E9%9F%B3%E4%B9%90%E3%80%91%E9%99%88%E5%8D%87-%E5%85%AD%E6%9C%88/image-20230225143210137.png" alt="image-20230225143210137">&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/X832EGf3i84" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>&lt;/iframe></description></item><item><title>【chatGPT】学习笔记1-机器还需要多久才能像人一样思考</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/</link><pubDate>Fri, 24 Feb 2023 16:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/</guid><description>&lt;p>从深圳返程的路上，开始阅读chatGPT的论文，努力理解其中精妙的理论与公式。&lt;/p>
&lt;p>渴望真正理解chatGPT，所以写下我在学习过程中的思考，希望前辈高手指教。&lt;/p>
&lt;h1 id="1从宏观看">1.从宏观看&lt;/h1>
&lt;h2 id="11软件内驱力制约了目前的ai能力">1.1.软件内驱力，制约了目前的AI能力&lt;/h2>
&lt;h3 id="1非ai程序的特点">(1)非AI程序的特点&lt;/h3>
&lt;blockquote>
&lt;p>当&lt;strong>人类可以归纳&lt;/strong>出某种人类知识的&lt;strong>规则、规律&lt;/strong>时，程序员就可以把它做成软件。&lt;/p>
&lt;/blockquote>
&lt;p>比如：客户需要开发1个路由器产品，路由器遵循的TCP/IP协议就是人类对网络通信的归纳总结。在软件行业的黑话称之为**&amp;ldquo;需求&amp;rdquo;、&amp;ldquo;规格&amp;rdquo;**。&lt;/p>
&lt;p>将非AI的程序抽象成如下公式，&lt;strong>人类的工作重心就在函数f上&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>STEP1.归纳出函数f的&lt;strong>规则、规律&lt;/strong>&lt;/li>
&lt;li>STEP2.交给人类程序员用某种编程语言实现出来&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221083651904.png" alt="image-20230221083651904">&lt;/p>
&lt;p>因此，产品研发过程中，某1个人类传递了错误的**&amp;ldquo;需求&amp;rdquo;、&amp;ldquo;规格&amp;rdquo;**，就会造成软件工程中经典的问题：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230220171408523.png" alt="image-20230220171408523">&lt;/p>
&lt;h3 id="2ai程序的特点">(2)AI程序的特点&lt;/h3>
&lt;blockquote>
&lt;p>当&lt;strong>人类无法归纳&lt;/strong>出某种人类知识的&lt;strong>规则、规律&lt;/strong>时，程序员实现某种数学工具的程序，该数学工具可以从已知的输入输出，自动归纳出规则、规律。&lt;/p>
&lt;/blockquote>
&lt;p>比如：我们需要实现计算机看一张照片，就知道这张照片里有一只狗。&lt;/p>
&lt;p>人类如何归纳出&amp;quot;照片里具备怎样的特征就表示有一只狗&amp;quot;的&lt;strong>规则、规律&lt;/strong>呢？&lt;/p>
&lt;p>暂且把看图识狗的问题放一边，我们换1个更简单的例子：已知历史上每天的白菜价格(假定白菜价格满足某种线性规律)，能否预测出明天的白菜价格？&lt;/p>
&lt;p>我们可以把问题抽象为如下数学问题：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI拾遗/【chatGPT】学习笔记1-机器还需要多久才能像人一样思考/image-20230221092617276.png" alt="image-20230221092617276" style="zoom:50%;" />&lt;/p>
&lt;p>我们可以用最朴实的方法求解：&lt;/p>
&lt;ul>
&lt;li>STEP1.将p1、p2&amp;hellip;.pn在坐标系上描点&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221093207025.png" alt="image-20230221093207025">&lt;/p>
&lt;ul>
&lt;li>STEP2.用蓝色线贯穿p1、p2&amp;hellip;.pn&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221093633696.png" alt="image-20230221093633696">&lt;/p>
&lt;ul>
&lt;li>STEP3.这根蓝色线即可确定a、b，即确定了函数f是什么&lt;/li>
&lt;/ul>
&lt;p>这个函数f就是从历史上众多白菜价格中&lt;strong>归纳出来的规律、规则&lt;/strong>。&lt;/p>
&lt;p>将AI的程序抽象如下，&lt;strong>人类的工作重心在数学工具M上，而函数f由数学工具M自动/半自动产生&lt;/strong>：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221094206072.png" alt="image-20230221094206072">&lt;/p>
&lt;h3 id="3观点">(3)观点&lt;/h3>
&lt;ul>
&lt;li>观点1：软件的内驱力是&lt;strong>具备归纳能力&lt;/strong>的&lt;strong>人脑&lt;/strong>或者&lt;strong>数学工具&lt;/strong>。
&lt;ul>
&lt;li>对比两种程序，非AI程序的归纳来源于人脑，AI程序的归纳来源于数学工具。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点2：如果期望&lt;strong>机器像人一样思考&lt;/strong>，则必须满足&lt;strong>数学工具的能力==人脑的能力&lt;/strong>。
&lt;ul>
&lt;li>非AI程序只能说是机器像人一样工作，而不是思考。&lt;/li>
&lt;li>AI程序中的数学工具的归纳能力和人脑持平，机器就真的像人一样思考了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点3：人脑作为创造了数学工具的&lt;strong>造物主&lt;/strong>，&lt;strong>有可能创造出等于甚至强于自身的数学工具&lt;/strong>吗？
&lt;ul>
&lt;li>这个问题和&amp;quot;万能的上帝能否造出自己举不动的石头？&amp;ldquo;是一样的，但未知的领域太大，未来或许可能。&lt;/li>
&lt;li>无论未来是否可能，目前看起来很难。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221151409891.png" alt="image-20230221151409891">&lt;/p>
&lt;h2 id="12计算理论制约了ai解决问题的范围">1.2.计算理论，制约了AI解决问题的范围&lt;/h2>
&lt;h3 id="1观点">(1)观点&lt;/h3>
&lt;p>面对人类和AI，我的观点是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>观点1：无论非AI程序，还是AI程序，最终&lt;strong>都是用来解决世间万物问题的&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>观点2：世间&lt;strong>有一类问题是不可解决的&lt;/strong>，这种问题&lt;strong>无论人类还是AI都无法解决&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>观点3：世间&lt;strong>还有一类问题是可解决的&lt;/strong>，其中一部分可以&amp;quot;彻底解决&amp;rdquo;，另一部分可以&amp;quot;委婉解决&amp;rdquo;，无论哪种，&lt;strong>AI终将比人类做的好&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>观点4：人类具有通过灵性&lt;strong>突破不可解决的问题&lt;/strong>，和&lt;strong>将&amp;quot;委婉解决&amp;quot;的问题降维成&amp;quot;彻底解决&amp;rdquo;&lt;/strong>，目前&lt;strong>AI很难做到&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="2不可判定问题">(2)不可判定问题&lt;/h3>
&lt;p>在计算理论中，不可解决的问题被称为&lt;strong>不可判定问题&lt;/strong>，人类解决不了的问题，AI也解决不了。&lt;/p>
&lt;ul>
&lt;li>比如：理发师宣称给全村所有不给自己刮胡子的人刮胡子就无法解决。&lt;/li>
&lt;li>类似的问题还有&amp;quot;停机问题&amp;rdquo;、&amp;ldquo;上帝造石问题&amp;quot;等等。&lt;/li>
&lt;li>但人类未知的领域太多了，可能在未知的领域，理发师可以做到他宣称的任务。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：计算理论用数学工具对世间所有问题进行了分类，可简单认为不可解决的问题是不可判定的问题，可解决的问题是可判定问题。&lt;/p>
&lt;/blockquote>
&lt;h3 id="3判定问题">(3)判定问题&lt;/h3>
&lt;p>在计算理论中，对于可解决的问题被称为&lt;strong>可判定问题&lt;/strong>，这些可解决的问题有分两类：&lt;/p>
&lt;ul>
&lt;li>此类问题中的一部分可以&lt;strong>直奔主题、彻底解决&lt;/strong>，在计算理论中叫&lt;strong>P问题&lt;/strong>。
&lt;ul>
&lt;li>比如：计算一个圆的面积，知道了圆心和半径，就能彻底求解。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>此类问题中剩余部分只能&lt;strong>曲线救国，间接解决&lt;/strong>，在计算理论中叫&lt;strong>NP问题&lt;/strong>。
&lt;ul>
&lt;li>比如：要给一张照片中的狗描边需要很高的计算成本，但要回答一张照片有没有狗，计算成本就会变得很低。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>无论是P问题，还是NP问题，AI终将强于人类&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：计算理论针对可解决的问题，用数学工具进行了量化，通过计算时间表现解决这个问题的困难程度。&lt;/p>
&lt;/blockquote>
&lt;h3 id="4灵感灵性灵魂">(4)灵感、灵性、灵魂&lt;/h3>
&lt;p>从解决问题的角度看，人类在如下两方面强于AI：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>将不可判定问题转变为可判定问题，人类强于AI&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>1935年，奥地利物理学家薛定谔提出了&lt;strong>薛定谔的猫&lt;/strong>，直至今日众多科学家还在感叹自己从未懂得量子力学。&lt;/p>
&lt;p>1851年，德国数学家黎曼分享了《论几何学作为基础的假设》，而在相隔近百年后爱因斯坦的《广义相对论的基础》中的空间几何采用了黎曼几何。&lt;/p>
&lt;p>这些例子，就是人脑有机会将&lt;strong>不可判定的问题&lt;/strong>转换为&lt;strong>可判定问题&lt;/strong>，将&lt;strong>未知&lt;/strong>变为&lt;strong>已知&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>将NP问题降维为P问题，人类强于AI&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>和王志兄探讨的时候，他讲到了《模拟游戏》，以当时的算力无论如何都很难破解德军密码，山穷水尽时几个科学家在酒吧中灵感乍现，利用一个常用词，巧妙地简化了破解密码的过程。从计算理论看，破解密码是个&lt;strong>NP问题&lt;/strong>，但因为其中反复出现的关键词，此问题降维成了&lt;strong>P问题&lt;/strong>，也就是可求解的问题了。&lt;/p>
&lt;p>再比如当今世界未解数学难题之一就是证明&lt;strong>NP问题是否等于P问题&lt;/strong>，一旦证明了，世间众多难解问题都可以变为可求解问题了。比如：癌症这个NP问题，就变成可治疗的P问题了。&lt;/p>
&lt;p>&lt;strong>人类文明的跳跃总是这样：在浩渺的历史场合中，微醺中、睡梦中，灵光一现。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>然而，人类的灵感、灵性、灵魂，似乎还没有数学公式可以表达。&lt;/strong>&lt;/p>
&lt;p>这个话题不能再深聊下去了，再聊就哲学(身心二元论)和神学了。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230221152255695.png" alt="image-20230221152255695">&lt;/p>
&lt;h2 id="13小结通用人工智能依然没有到来">1.3.小结：通用人工智能依然没有到来&lt;/h2>
&lt;p>通用人工智能，&lt;strong>Artificial general intelligence&lt;/strong>，简称&lt;strong>AGI&lt;/strong>。与之相反的就是弱人工智能(&lt;strong>Weak AI&lt;/strong>)，也叫做窄人工智能(&lt;strong>Narrow AI&lt;/strong>)。通用人工智能与弱人工智能最大的区别就是是否具备认知能力。弱人工智能有点像绿野仙踪里没有心的铁皮人，他的梦想就是有一颗真正的心。&lt;/p>
&lt;blockquote>
&lt;p>认知能力：详见https://en.wikipedia.org/wiki/Cognition&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230222160053172-16771388478091.png" alt="image-20230222160053172">&lt;/p>
&lt;p>从前述的软件内驱力(驱动力)、计算理论(解决问题能力)看，除非发生了一种&lt;strong>技术突变&lt;/strong>，否则机器要像人一样思考依然很困难。&lt;/p>
&lt;p>但，&lt;strong>AI技术在弱人工智能方向的确越来越强&lt;/strong>。我们接下来再从微观上做进一步分析。&lt;/p>
&lt;h1 id="2从微观看">2.从微观看&lt;/h1>
&lt;h2 id="21监督无监督只要狗粮撒的多人工智能勉强行">2.1.监督&amp;amp;无监督：只要狗粮撒的多，人工智能勉强行&lt;/h2>
&lt;h3 id="1一个例子是猫还是狗">(1)一个例子：是猫还是狗？&lt;/h3>
&lt;p>假设AI的数学工具为w₁x+w₂y(其中w₁初始值1，w₂初始化1)，&lt;/p>
&lt;p>并且，AI的数学工具输出表示：&lt;/p>
&lt;ul>
&lt;li>如果数学工具输出&amp;gt;=0，则表示照片中是1只狗。&lt;/li>
&lt;li>如果数学工具输出&amp;lt;0，则表示照片中是1只猫。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>第一波学习&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>输入第1张照片(是二哈，一种狗)，二哈的数学表示为(x=1, y=1)，则AI的数学工具输出2，进而推理出第1张照片是狗。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>输入第2张照片(是布偶，一种猫)，布偶的数学表示为(x=1, y=-2)，则AI的数学工具输出-1，进而推理出第2张照片是猫。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223162906437.png" alt="image-20230223162906437">&lt;/p>
&lt;p>&lt;strong>第二波学习&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>输入第3张照片(是泰迪，一种狗)，泰迪的数学表达是(x=-1, y=-1)，则AI的数学工具输出-2，进而推理出第3张照片是猫。&lt;/li>
&lt;li>此时就错了，泰迪怎么可能是一种猫。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223164621150.png" alt="image-20230223164621150">&lt;/p>
&lt;p>&lt;strong>第三波学习&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>此时，就要调整w₁和w₂，我掐指一算，调整为w₁=-1，w₂=1。&lt;/li>
&lt;li>输入第1张、第2张、第3张照片，AI的数学工具输出为0、-3、0，进而可知二哈是狗、布偶是猫、泰迪是狗。&lt;/li>
&lt;li>显然，w₁和w₂设计的合适，能够应对更多的照片，而不是每次输入一个新照片就要重新调整，这就是人工智能的黑话——&lt;strong>泛化&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223165156039.png" alt="image-20230223165156039">&lt;/p>
&lt;p>于是，不断地喂进去新的图片，不断地调整w₁和w₂……&lt;/p>
&lt;p>于是，我师傅把上述过程叫做&lt;strong>喂狗&lt;/strong>，这些图片就是&lt;strong>狗粮&lt;/strong>。&lt;/p>
&lt;h3 id="2监督学习狗粮的成本很高">(2)监督学习：狗粮的成本很高&lt;/h3>
&lt;p>根据前面的例子，首先需要人类准备好非常非常多的照片，并且人要标识出来每张照片是猫还是狗，这种学习方式就是&lt;strong>监督学习&lt;/strong>。&lt;/p>
&lt;p>这就好像有个人类小孩儿不太聪明，他爸爸要把世界上所有的习题册都买回来，爸爸还要把每一道题自己先做一遍，再来一道一道教这孩子。&lt;/p>
&lt;p>在这种高昂的教学成本下，我们就可以看到父慈子孝的场面了：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/%E8%BE%85%E5%AF%BC%E5%8A%9F%E8%AF%BE.gif" alt="辅导功课">&lt;/p>
&lt;h3 id="3无监督学习从狗粮中吸收的营养有限">(3)无监督学习：从狗粮中吸收的营养有限&lt;/h3>
&lt;p>与监督学习相对的，是&lt;strong>无监督学习&lt;/strong>。无监督学习还是需要人类准备好非常多的照片，只是不需要人类把每一张照片都标注出是猫还是狗。&lt;/p>
&lt;p>人工智能在学习过程中，也能学会猫和狗两类照片的差别，但是它并没有&lt;strong>猫&lt;/strong>、&lt;strong>狗&lt;/strong>的概念。&lt;/p>
&lt;p>相当于，狗粮吃了，吸收的营养有限。&lt;/p>
&lt;h3 id="4感知机lenet神经网络喂狗工具在进化">(4)感知机、LeNet、神经网络，喂狗工具在进化&lt;/h3>
&lt;p>除了考虑狗粮问题，从Perceptron感知机，到LeNet，再到神经网络，喂狗工具也在进化，这里给出动图，我们感受一下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%BC%94%E7%A4%BA1.gif" alt="神经网络演示1">&lt;/p>
&lt;h3 id="5观点">(5)观点&lt;/h3>
&lt;p>前面介绍了监督学习、无监督学习的方法论，可以看到：&lt;/p>
&lt;ul>
&lt;li>观点1：&lt;strong>对数据要求高，成本高&lt;/strong>。无论是数据清洗，还是数据标注，人工成本都很高。&lt;/li>
&lt;li>观点2：方法论决定了AI的能力&lt;strong>只能解决某些特定领域下的问题&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="22强化学习没有狗粮去喂狗人工智能还能行">2.2.强化学习：没有狗粮去喂狗，人工智能还能行&lt;/h2>
&lt;h3 id="1什么是强化学习">(1)什么是强化学习&lt;/h3>
&lt;p>先看一个实验，对强化学习有一个直观感受：&lt;/p>
&lt;ul>
&lt;li>机器人看到面前的架子，一次次地把盘子放到架子里，盘子碎了减分，盘子放好了加分。&lt;/li>
&lt;li>机器人看到面前随机出现的杯子，一次一次地把水倒进杯子里，倒进去的水越多加分，反之减分。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B.gif" alt="强化学习示例">&lt;/p>
&lt;p>强化学习中，有几个概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Agent&lt;/strong>：智能体，可以简单地认为就是人工智能本身。&lt;/li>
&lt;li>&lt;strong>Environment&lt;/strong>：环境，就是智能体所处的外在环境。&lt;/li>
&lt;li>&lt;strong>Action&lt;/strong>：行为，智能体会不断尝试做出1个行为，触发环境发生变化。&lt;/li>
&lt;li>&lt;strong>State&lt;/strong>：智能体观测环境发生此刻的状态。&lt;/li>
&lt;li>&lt;strong>Reward&lt;/strong>：当智能体做出一个行为以后，环境会对智能体产生奖励，也可能产生惩罚。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223221457273.png" alt="image-20230223221457273">&lt;/p>
&lt;p>再看看强化学习的流程：&lt;/p>
&lt;ul>
&lt;li>STEP1.观察到**Environment(环境)&lt;strong>中的&lt;/strong>State(状态)**有一杯水。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223231206199.png" alt="image-20230223231206199">&lt;/p>
&lt;ul>
&lt;li>STEP2.**Agent(智能体)&lt;strong>做出了一个&lt;/strong>Action(行为)**打翻水。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223231405476.png" alt="image-20230223231405476">&lt;/p>
&lt;ul>
&lt;li>STEP3.&lt;strong>Environment(环境)&lt;strong>产生了一个&lt;/strong>Reward(奖赏)&lt;/strong>——此时是个负向的奖赏——别那么做！&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223231328920.png" alt="image-20230223231328920">&lt;/p>
&lt;ul>
&lt;li>STEP4.**Agent(智能体)&lt;strong>再次看到的&lt;/strong>Environment(环境)&lt;strong>的&lt;/strong>State(状态)**是水杯打翻了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223231651496.png" alt="image-20230223231651496">&lt;/p>
&lt;ul>
&lt;li>STEP5.**Agent(智能体)&lt;strong>做出了新的&lt;/strong>Action(行为)**把水擦干。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223231852875.png" alt="image-20230223231852875">&lt;/p>
&lt;ul>
&lt;li>STEP6.此时&lt;strong>Environment(环境)&lt;strong>发出的&lt;/strong>Reward(奖赏)&lt;/strong>——真棒&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223232148105.png" alt="image-20230223232148105">&lt;/p>
&lt;ul>
&lt;li>最终，人工智能学会了正确的选择。&lt;/li>
&lt;/ul>
&lt;h3 id="2强化学习的核心思想一次次的救赎">(2)强化学习的核心思想：一次次的救赎&lt;/h3>
&lt;p>这有点像电影《Source Code》的剧情，男主不断地经历同一次循环，他在循环里的每一步都在寻找突破，最终突破循环。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223232746654.png" alt="image-20230223232746654">&lt;/p>
&lt;p>作为人类的我，多么嫉妒人工智能具备的这种能力——&lt;strong>后悔药&lt;/strong>——曾经有一份真诚的爱情放在我面前，我没有珍惜，等我失去的时候才后悔莫及，人世间最疼苦的事莫过于此。如果上天能够给我一个再来一次的机会，我会对那个女孩说三个字：我爱你。如果非要在这份爱上加一个期限，我希望是……一万年。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223232959205.png" alt="image-20230223232959205">&lt;/p>
&lt;h3 id="3强化学习的难点延迟奖励探索突破灵性的灵犀一指">(3)强化学习的难点：延迟奖励&amp;amp;探索，突破灵性的灵犀一指&lt;/h3>
&lt;p>强化学习有两个难点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>延迟奖励&lt;/strong>：有一种可能——每一步都看起来得到最高的奖赏，但很可能最终还是输了整个游戏。这就是延迟奖励，&lt;strong>前面几步不仅不要赢，反而要输&lt;/strong>，赢得最终的比赛才是关键——色即是空，空即是色。&lt;/li>
&lt;li>&lt;strong>探索&lt;/strong>：再就是不走寻常路，某一步忽然走了以前没做过的动作，意外地得到奖励，也可能失败，&lt;strong>一把梭哈，撞个大运&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>强化学习的这两个难点，就好像《天龙八部》中的珍珑棋局，那么多青年才俊都无法攻克，虚竹却胡乱下了一步(&lt;strong>探索&lt;/strong>)，死了一大片子却开拓了新的天地(&lt;strong>延迟奖励&lt;/strong>)。如果再来从宏观层面谈，根据计算理论，虚竹这灵犀一指就是将&lt;strong>不可判定问题&lt;/strong>变为了&lt;strong>可判定问题&lt;/strong>，而这一指就是&lt;strong>人类的灵性&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223234346605.png" alt="image-20230223234346605">&lt;/p>
&lt;h3 id="4观点">(4)观点&lt;/h3>
&lt;ul>
&lt;li>观点1：强化学习一定程度地解决了有监督、无监督中的&lt;strong>狗粮问题&lt;/strong>。&lt;/li>
&lt;li>观点2：由于没有直接告知答案，因此需要&lt;strong>等待漫长的时间&lt;/strong>让机器去探索真理。所以强化学习的科学家们就是在不断优化其内部的数学工具，提升训练速度。
&lt;ul>
&lt;li>在强化学习中，有&lt;strong>Policy-Based&lt;/strong>和&lt;strong>Valued-Based&lt;/strong>两种策略，科学家为了解决策略梯度不好的问题，提出了&lt;strong>TROP&lt;/strong>算法，chatGPT进一步改进了这个算法，就是&lt;strong>PPO&lt;/strong>算法。&lt;/li>
&lt;li>我们在本文中不必深入这些术语的公式、原理、细节，只需要大致有个印象，知道chatGPT用这个算法到底要解决什么问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点3：强化学习的底层逻辑，其实就是&lt;strong>主动学习、反复试错&lt;/strong>，&lt;strong>你只需要给我时间，我并不需要你的狗粮&lt;/strong>。
&lt;ul>
&lt;li>这不仅仅是解决训练成本的问题，主动学习是人类的重要特质，&lt;strong>主动学习能力或许是机器具备人类灵性的突破口之一&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点4：强化学习的奖励机制，不一定是一个单纯的奖励模型，也可以是一个&lt;strong>人类偏好模型&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>感受自我、感知他人&lt;/strong>，这也是人类另一种重要特质，这&lt;strong>也可能是机器具备人类灵性的突破口&lt;/strong>。&lt;/li>
&lt;li>chatGPT用了人类偏好模型，这就是为什么它会惊艳到很多人的原因之一，后文会讲到。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="23看chatgpt原理图">2.3.看chatGPT原理图&lt;/h2>
&lt;p>有了前文宏观层面建立的世界观，有了前文微观层面讲述的AI算法，我们可以来理解一下chatGPT的原理图了：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230223234441157.png" alt="image-20230223234441157">&lt;/p>
&lt;h3 id="1拥有一个基础版本的聊天机器人">(1)拥有一个基础版本的聊天机器人&lt;/h3>
&lt;p>chatGPT第一阶段本质是监督+无监督，人类标注了一定数量的问题数据集，这里和大多数AI领域工程师现在的工作差不多。&lt;/p>
&lt;p>在chatGPT，把训练的结果叫做&lt;strong>SFT模型&lt;/strong>，本质就是获得了一个初级版本的聊天机器人。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230224141822069.png" alt="image-20230224141822069">&lt;/p>
&lt;h3 id="2关键步骤我知道人类喜欢什么">(2)关键步骤，我知道人类喜欢什么&lt;/h3>
&lt;p>第二步是关键步骤，用1个问题驱动&lt;strong>SFT&lt;/strong>，机器人返回4个答案，人类只需要对这4个答案进行排序。&lt;/p>
&lt;p>这太有创意了：&lt;strong>一个人类回答问题是困难的&lt;/strong>(需要这个人类是这个问题的领域专家)，但&lt;strong>让人类判断哪个回答更好是简单的&lt;/strong>(这个人类只需要有一定的领域知识)。&lt;/p>
&lt;p>排序也是一种&lt;strong>标注&lt;/strong>，这个标注不再只是&lt;strong>是猫是狗&lt;/strong>的简单标注，而是&lt;strong>&lt;font color=red>人类喜好&lt;/font>&lt;/strong>！&lt;/p>
&lt;p>chatGPT将这种人类喜欢叫做RM模型。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230224143243772.png" alt="image-20230224143243772">&lt;/p>
&lt;h3 id="3学的快我懂你">(3)学的快，我懂你&lt;/h3>
&lt;p>chatGPT到了第三步，似乎就水到渠成了。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230224150150372.png" alt="image-20230224150150372">&lt;/p>
&lt;p>关于&lt;strong>学的快&lt;/strong>，和人类学习一项新技能本质是一样的，抽象为两点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学习有效信息&lt;/strong>：谷歌发明的注意力模型就是解决机器学习信息时，尽量像人一样关注到核心信息。
&lt;ul>
&lt;li>比如：下面这张图，我觉得大部分人类第一反应是注意到照片里有两匹马，而不是背后的风景吧(不过这里的风景真的很美)？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-%E6%9C%BA%E5%99%A8%E8%BF%98%E9%9C%80%E8%A6%81%E5%A4%9A%E4%B9%85%E6%89%8D%E8%83%BD%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/image-20230224151038341.png" alt="image-20230224151038341">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学的过程快&lt;/strong>：PPO算法是强化学习中的一个单点算法，优化梯度问题，本质还是解决学的过程要快。&lt;/li>
&lt;/ul>
&lt;p>关于&lt;strong>我懂你&lt;/strong>，就是第二步的&lt;strong>人类喜好模型RM&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>将人类喜好模型作为&lt;strong>奖赏&lt;/strong>，让机器人的回答不断解决人类喜好的那种回答。
&lt;ul>
&lt;li>这里面还有一个细节，回答类似输入法的自动联想，回答一个词就联想下一个词，下一个词的选择会遵循人类喜好模型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>一个人类的小孩，能从课本中提取到关键知识、理解关键知识很快、并且学习的目标一直围绕着正确的方向，这孩子一定是学霸。&lt;/p>
&lt;p>这就是在&lt;strong>正确的方向上强执行力地做事&lt;/strong>。&lt;/p>
&lt;h3 id="4观点-1">(4)观点&lt;/h3>
&lt;ul>
&lt;li>观点1：chatGPT并不是一个技术突变的产品，AI发展史上所有的技术都是被它使用的，&lt;strong>能灵活运用已有的AI技术是值得我们学习的可贵之处&lt;/strong>。&lt;/li>
&lt;li>观点2：宏观层面人类的灵性是强人工智能最大的挑战，但微观层面&lt;strong>强化学习可能存在突破人类灵性的可能性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="24小结微观层面的术在不断进步">2.4.小结：微观层面的术在不断进步&lt;/h2>
&lt;ul>
&lt;li>监督&amp;amp;无监督学习最大的问题是训练数据的成本，强化学习一定程度上解决了这个成本问题。&lt;/li>
&lt;li>chatGPT对强化学习的贡献在于：&lt;strong>获得有效的知识&lt;/strong>(注意力模型)、&lt;strong>高效的学习速度&lt;/strong>(PPO算法)、&lt;strong>正确的努力方向&lt;/strong>(人类喜好模型RM)&lt;/li>
&lt;/ul>
&lt;h1 id="3理性思考充满好奇虚心求索">3.理性思考、充满好奇、虚心求索&lt;/h1>
&lt;p>如果我的小伙伴们能看到这里，很感谢你们。这篇文章从提笔到写完，占用了我一周所有的业余时间。&lt;/p>
&lt;p>chatGPT的论文涉及到的知识面太广，远超我的认知，同时由于给我的极大震撼，我又有太多想表达的内容。&lt;/p>
&lt;p>在我的学习笔记第一篇，我选择表述我最感兴趣的话题——&lt;strong>机器像人一样思考还有多远？&lt;/strong>&lt;/p>
&lt;p>人类很多工作可能或正在被人工智能替代，这只能叫&lt;strong>岗位转换&lt;/strong>，&lt;strong>只是物理层面的替代&lt;/strong>。&lt;/p>
&lt;p>拖拉机发明了，这个岗位的人类去做了别的岗位，但至今没看到人类被拖拉机统治。&lt;/p>
&lt;p>很感谢学习过程中，身边有那些宏观、微观层面帮助我&lt;strong>理性思考&lt;/strong>的小伙伴。&lt;/p>
&lt;p>写这篇文章时，我被迫看了很多哲学和数学资料，比如：身心二元论、计算理论等等。&lt;/p>
&lt;p>&lt;strong>我们的世界底层逻辑还是数学和哲学&lt;/strong>，可惜功利的我不允许自己去探索这种没有既得利益的知识。&lt;/p>
&lt;p>或许，我偶尔应该&lt;strong>充满好奇&lt;/strong>，而不那么功利。&lt;/p>
&lt;p>最后，面对国内对chatGPT的一片喧嚣，在努力弄懂chatGPT的原理之后，大致能分辨出哪些仅仅是停留在商业数据的分析伪洞察，哪些是没搞清How就乱给Do建议的伪分析。&lt;/p>
&lt;p>&lt;strong>虚心求索&lt;/strong>，欢迎小伙伴们共同研究这些&lt;strong>有趣的技术&lt;/strong>、&lt;strong>有趣的未来&lt;/strong>。&lt;/p></description></item><item><title>【音乐】陈升-恨情歌</title><link>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E6%81%A8%E6%83%85%E6%AD%8C/</link><pubDate>Fri, 24 Feb 2023 00:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/%E6%9D%82%E8%B4%A7%E9%93%BA/%E9%9F%B3%E4%B9%90%E9%99%88%E5%8D%87-%E6%81%A8%E6%83%85%E6%AD%8C/</guid><description>&lt;center>&lt;font size=20>恨情歌&lt;/font>&lt;/center>
&lt;p align="right">陈升&lt;/p>
&lt;center>為了要討好你的歡心，我經常忘記我自己&lt;/center>
&lt;center>感情是件瘋狂的事， 多了並不見得好&lt;/center>
&lt;center>我不能隨便我自己， 快樂輕聲地歌唱&lt;/center>
&lt;center>都說你愛聽情歌， 來分擔你心中的苦&lt;/center>
&lt;center>不要像頑皮的孩子， 老說為我唱情歌&lt;/center>
&lt;center>常常我一個人在夜裡， 擔心迷失我自己&lt;/center>
&lt;center>而原來我是一個， 愛四處遊蕩的人&lt;/center>
&lt;center>如果有那麼一天我停住了，你是否就離開我&lt;/center>
&lt;center>於是我叫我自己恨情歌，假裝我不在乎&lt;/center>
&lt;center>或者我不再去討你歡心，我喜歡這樣的自己&lt;/center>
&lt;center>於是我叫我自己恨情歌，假裝我不在乎&lt;/center>
&lt;center>也許你從來都沒說過，是我想的太多&lt;/center>
&lt;center>而原來我是一個，愛四處遊蕩的人&lt;/center>
&lt;center>都說你愛聽情歌，來分擔你心中的苦&lt;/center>
&lt;center>於是我叫我自己恨情歌，假裝我不在乎&lt;/center>
&lt;center>或者我不再去討你歡心，我喜歡這樣的自己&lt;/center>
&lt;center>於是我叫我自己恨情歌，假裝我不在乎&lt;/center>
&lt;center>也許你從來都沒說過，是我想的太多&lt;/center>
&lt;p>&lt;img src="https://jherculesqz.github.io/%E6%9D%82%E8%B4%A7%E9%93%BA/%E3%80%90%E9%9F%B3%E4%B9%90%E3%80%91%E9%99%88%E5%8D%87-%E6%81%A8%E6%83%85%E6%AD%8C/image-20230224000529962.png" alt="image-20230224000529962">&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/s2xcIjcH_u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>&lt;/iframe></description></item><item><title>【SpringOne 2022】5.Introduction to Testcontainers</title><link>https://jherculesqz.github.io/post/spring%E6%8B%BE%E9%81%97/springone-20225.introduction-to-testcontainers/</link><pubDate>Sat, 18 Feb 2023 10:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/spring%E6%8B%BE%E9%81%97/springone-20225.introduction-to-testcontainers/</guid><description>&lt;p>SpringOne 2022第6个议题《Introduction to Testcontainers》，用3个Demo展示了TestContainers for Java的重要特性，主讲人：Oleg Selajev，辅助人：Cora lberkleid。PS：Oleg Selajev的演讲风格非常有激情。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230217074026357.png" alt="image-20230217074026357">&lt;/p>
&lt;h1 id="1概述">1.概述&lt;/h1>
&lt;h2 id="11集成测试对微服务架构的影响深远">1.1.集成测试对微服务架构的影响深远&lt;/h2>
&lt;p>演讲者核心观点是：&lt;/p>
&lt;ul>
&lt;li>在传统架构下，测试成本的投入情况是**&amp;ldquo;单元测试&amp;quot;&amp;gt;&amp;quot;集成测试&amp;quot;&amp;gt;&amp;quot;E2E测试&amp;rdquo;**(如下图左半部分)。&lt;/li>
&lt;li>在微服务架构下，存在数量巨大的微服务进程，不太可能全量测试和发布。
&lt;ul>
&lt;li>更合理的做法：仅对本轮迭代有关的微服务进行集成测试，局部发布。&lt;/li>
&lt;li>如下图右半部分：如果集成测试投入更多，那么与本次发布相关的微服务影响范围是相对可控的，进而E2E测试可能会更简单。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>总之，&lt;strong>集成测试在微服务架构下越来越重要&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230217091453992.png" alt="image-20230217091453992">&lt;/p>
&lt;h2 id="12集成测试的新方法">1.2.集成测试的新方法&lt;/h2>
&lt;p>主讲人在这部分展示了&lt;strong>没有TestContainers之前&lt;/strong>、&lt;strong>使用TestContainers之后&lt;/strong>的集成测试手段的差异：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>之前&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在服务器上部署被测微服务、依赖的组件(如：数据库、Redis、Kafka等)&lt;/li>
&lt;li>在服务器上以Docker的形式部署被测微服务、依赖的组件&lt;/li>
&lt;li>在本地电脑上部署被测微服务、依赖的组件&lt;/li>
&lt;li>&lt;/li>
&lt;li>对部署成本高的依赖组件，采用&lt;strong>模拟&lt;/strong>的方式
&lt;ul>
&lt;li>如：Wiremock这类Mock Server&lt;/li>
&lt;li>如：采用h2数据库进行测试，规避安装PostgreSQL这种更重型的数据库&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218065338566.png" alt="image-20230218065338566">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>之后&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>STEP1.TestContainers提供了丰富的&lt;code>Module&lt;/code>(这些&lt;code>Module&lt;/code>对应各类流行的中间件、数据库等)
&lt;ul>
&lt;li>比如：&lt;code>Redis Module&lt;/code>对应&lt;code>Redis&lt;/code>&lt;/li>
&lt;li>比如：&lt;code>Kafka Module&lt;/code>对应&lt;code>Kafka&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>STEP2.这些&lt;code>Module&lt;/code>，各自都提供了API(这些API封装了Docker API)
&lt;ul>
&lt;li>这就意味着我们可以通过某个&lt;code>Module&lt;/code>的API，操纵对应的Docker容器&lt;/li>
&lt;li>比如：&lt;code>Redis Module&lt;/code>提供1个&lt;code>启动redis容器&lt;/code>的API，调用以后，就会在服务器上真的运行起来1个&lt;code>Redis&lt;/code>的镜像&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>STEP3.TestContainers再和各类流行的测试框架集成，管理&lt;code>Module&lt;/code>对应的被依赖组件的容器的生命周期
&lt;ul>
&lt;li>比如：TestContainers在JUnit的&lt;code>setUp&lt;/code>方法中创建了&lt;code>Redis&lt;/code>容器，在&lt;code>TearDown&lt;/code>方法中销毁&lt;code>Redis&lt;/code>容器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218102321877.png" alt="image-20230218102321877">&lt;/p>
&lt;h2 id="13testcontainers发展历史">1.3.TestContainers发展历史&lt;/h2>
&lt;p>演讲者在这一部分讲述了TestContainers创建时的小故事：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>2014年，一位名叫Moshe Eshel的大神在GitHub上写了&lt;code>DockerContainerRule.java&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这段代码Override了JUnit的&lt;code>@Before&lt;/code>和&lt;code>@After&lt;/code>&lt;/p>
&lt;ul>
&lt;li>在JUnit执行before方法时，会启动指定的Docker容器&lt;/li>
&lt;li>在JUnit执行after方法时，会关闭指定的Docker容器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>2017年，TestContainers完全替代了&lt;code>DcokerContainerRule.java&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218101112791.png" alt="image-20230218101112791">&lt;/p>
&lt;p>听演讲的时候，没太跟上演讲者的语速，和chatGPT印证了一下：&lt;/p>
&lt;ul>
&lt;li>的确可以认为：&lt;code>DcokerContainerRule.java&lt;/code>是TestContainers的灵感来源&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218101251008.png" alt="image-20230218101251008">&lt;/p>
&lt;h2 id="14testcontainers成熟度">1.4.TestContainers成熟度&lt;/h2>
&lt;p>从主讲人表述的信息，TestContainers的确很成熟：&lt;/p>
&lt;ul>
&lt;li>TestContainers项目在GitHub上是6.6k星&lt;/li>
&lt;li>Docker项目发布的第2年就创建了，是这类测试框架的&lt;strong>老前辈&lt;/strong>&lt;/li>
&lt;li>TestContainers宣称&lt;strong>&lt;code>Works with anything that runs in a Docker container&lt;/code>&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218103001722.png" alt="image-20230218103001722">&lt;/p>
&lt;ul>
&lt;li>在2022年被ThoughtWorks的咨询报告中，TestContainers&lt;strong>被评为&lt;code>Adopt采纳级&lt;/code>&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218103330486.png" alt="image-20230218103330486">&lt;/p>
&lt;h1 id="2demo0">2.Demo0&lt;/h1>
&lt;h2 id="21testcontainers的测试流程">2.1.TestContainers的测试流程&lt;/h2>
&lt;p>这是演讲者的第1个Demo：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：
&lt;ul>
&lt;li>在测试类上，增加了&lt;code>@Testcontainers&lt;/code>注解&lt;/li>
&lt;li>为测试类创建了&lt;code>GenericContainer&lt;/code>对象，此对象表示1个Docker容器&lt;/li>
&lt;li>在&lt;code>@Before&lt;/code>方法中，调用&lt;code>GenericContainer&lt;/code>对象，打印它的Docker容器Id&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>**日志解读：**从日志看，&lt;strong>TestContainers完整地管理了1个容器的生命周期&lt;/strong>。
&lt;ul>
&lt;li>JUnit先执行&lt;code>@BeforeEach&lt;/code>方法&lt;/li>
&lt;li>JUnit会连接Docker服务器&lt;/li>
&lt;li>Ryuk被启动(Ryuk是啥呢？稍后解释)&lt;/li>
&lt;li>进行系统检查&lt;/li>
&lt;li>启动容器镜像&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218103524464.png" alt="image-20230218103524464">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ryuk是什么？&lt;/strong>
&lt;ul>
&lt;li>Ruyk是Moby项目中的1个工具&lt;/li>
&lt;li>这个工具支持对Docker镜像的管理&lt;/li>
&lt;li>chatGPT回答如下：&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218104759166.png" alt="image-20230218104759166">&lt;/p>
&lt;h2 id="22如何重用1个容器">2.2.如何重用1个容器&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：仅需要在测试类创建&lt;code>GenericContainer&lt;/code>对象时，将该对象设置为&lt;code>static&lt;/code>&lt;/li>
&lt;li>&lt;strong>日志解读&lt;/strong>：
&lt;ul>
&lt;li>执行测试用例1时，TestContainers创建的Docker容器ID尾号&lt;code>bc739b5a&lt;/code>&lt;/li>
&lt;li>执行测试用例2时，TestContainers创建的Docker容器ID尾号也是&lt;code>bc739b5a&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218104923764.png" alt="image-20230218104923764">&lt;/p>
&lt;h1 id="3demo1">3.Demo1&lt;/h1>
&lt;h2 id="31端口配置">3.1.端口配置&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：
&lt;ul>
&lt;li>在创建&lt;code>GenericContainer&lt;/code>对象时，通过&lt;code>withExposedPorts&lt;/code>方法&lt;strong>开放了80端口&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218111638548.png" alt="image-20230218111638548">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>在测试用例2中，通过&lt;code>GenericContainer&lt;/code>对象的&lt;code>getHost&lt;/code>和&lt;code>getFirstMappedPort&lt;/code>方法，获得Docker容器的IP和端口&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218111714728.png" alt="image-20230218111714728">&lt;/p>
&lt;h2 id="32日志操作">3.2.日志操作&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：&lt;/li>
&lt;li>获得日志的方式1：调用&lt;code>GenericContainer&lt;/code>对象的&lt;code>getLogs&lt;/code>方法&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218112057805.png" alt="image-20230218112057805">&lt;/p>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>获得日志的方式2：使用&lt;code>Slf4jLogConsumer&lt;/code>对象&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218112301488.png" alt="image-20230218112301488">&lt;/p>
&lt;h2 id="33使用dockfile">3.3.使用Dockfile&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：
&lt;ul>
&lt;li>创建&lt;code>GenericContainer&lt;/code>对象时，指定DockFile文件路径&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218112544145.png" alt="image-20230218112544145">&lt;/p>
&lt;h1 id="4demo2">4.Demo2&lt;/h1>
&lt;h2 id="41rediskafkapostgresql">4.1.redis+kafka+PostgreSQL&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：
&lt;ul>
&lt;li>创建&lt;code>RedisContainer&lt;/code>对象和&lt;code>KafkaContainer&lt;/code>对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218113102821.png" alt="image-20230218113102821">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>创建&lt;code>PostgreSQLContainer&lt;/code>对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218113215484.png" alt="image-20230218113215484">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>设置&lt;code>Redis&lt;/code>容器、&lt;code>PostgreSQL&lt;/code>容器的参数：&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218113306162.png" alt="image-20230218113306162">&lt;/p>
&lt;h2 id="42混沌测试">4.2.混沌测试&lt;/h2>
&lt;p>&lt;strong>Chaos Test&lt;/strong>是主讲人最出彩的一段，通过&lt;strong>Toxiproxy&lt;/strong>，可以很方便地模拟高负载、网络故障等，&lt;strong>实战中非常实用&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>代码解读&lt;/strong>：
&lt;ul>
&lt;li>创建&lt;code>Network&lt;/code>对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218115356716.png" alt="image-20230218115356716">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>创建&lt;code>Toxiproxy&lt;/code>容器对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218115429306.png" alt="image-20230218115429306">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>通过&lt;code>Toxiproxy&lt;/code>容器对象，创建&lt;code>redis&lt;/code>容器的网络连接代理对象&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218115553720.png" alt="image-20230218115553720">&lt;/p>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>设置&lt;code>redis&lt;/code>容器的网络延时为2s，这样，我们就可以模拟生产环境上redis存在2s延时的网络环境了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218115652712.png" alt="image-20230218115652712">&lt;/p>
&lt;ul>
&lt;li>chatGPT对&lt;strong>Toxiproxy&lt;/strong>的介绍：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/Spring%E6%8B%BE%E9%81%97/%E3%80%90SpringOne2022%E3%80%915.IntroductiontoTestcontainers/image-20230218114014618.png" alt="image-20230218114014618">&lt;/p>
&lt;h1 id="5小结">5.小结&lt;/h1>
&lt;p>通过演讲者的3个Demo，可以得到如下的判断：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>TestContainers已经是相当成熟的集成测试组件了&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过TestContainers提供的丰富的&lt;code>Module&lt;/code>，极大地降低了集成测试时，依赖组件的部署和维护成本&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>