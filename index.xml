<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>妙木山</title><link>https://jherculesqz.github.io/</link><description>Recent content on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 29 Oct 2023 18:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>关于</title><link>https://jherculesqz.github.io/about/</link><pubDate>Thu, 05 Aug 2021 13:01:37 +0800</pubDate><guid>https://jherculesqz.github.io/about/</guid><description>&lt;h1 id="关于博客">关于博客&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>独立&lt;/strong>：一直在写技术博客，从微信公众号、头条号、SegmentFault、掘金、简书一路折腾过来，还是希望有一个自己独立的空间。&lt;/li>
&lt;li>&lt;strong>坚持&lt;/strong>：随着年龄增长，逐渐欲说还休，还是文字更有韵味，希望自己能坚持写下去。&lt;/li>
&lt;li>&lt;strong>浪漫&lt;/strong>：按照&lt;a href="https://archiveprogram.github.com">Archive Program&lt;/a>计划的愿景，我的博客会在&amp;rdquo; GitHub北极代码库&amp;quot;中保存千年。想想1000年以后，我的后代们能读到我这个中二祖先的文字，还是一件挺浪漫的事儿。&lt;/li>
&lt;li>&lt;strong>感谢&lt;/strong>：感谢GitHub Pages、Hugo、Jane提供的技术支持。&lt;/li>
&lt;li>&lt;strong>妙木山&lt;/strong>：妙木山是修炼仙术的地方，作为火影的死忠粉，&amp;ldquo;妙木山&amp;quot;无比适合这个博客的定位——修炼、探索。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/MiaoMu.png" alt="MiaoMu">&lt;/p>
&lt;h1 id="关于我">关于我&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>行业&lt;/strong>：软件行业16年，无法用语言表达对编程的喜爱——举个栗子吧：有段时间喜欢在酒吧里写代码，同去的小伙伴无聊地陌陌上约人，自我介绍就是&amp;quot;A+吧台，旁边有个写代码的沙雕&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>技术方向&lt;/strong>：近几年痴迷语言和编译器技术，还有点痴迷计算机图形学。
&lt;ul>
&lt;li>&lt;strong>编程语言&lt;/strong>：目前工作Java和JavaScript用的最多，但我最喜欢C#——PHP是最好的语言，行了吧！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>哲学&lt;/strong>：不知何时，开始期待理解生命的意义。东一本西一本的书拿来乱翻，也没找到答案。不过，也不是全无收获——能模模糊糊地体会诗词的意境、能回味出毛选的奇妙、能敬畏金刚经的高深……继续求索吧……&lt;/li>
&lt;li>&lt;strong>兴趣&lt;/strong>：年轻的时候，喜欢轮滑、滑板、快乐肥仔水。现在，喜欢滑雪、乒乓球、茶(特指正山小种)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/about/Me.png" alt="Me">&lt;/p></description></item><item><title>【chatGPT】学习笔记20-如何搭建ChatGLM3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</link><pubDate>Sun, 29 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</guid><description>&lt;h1 id="1chatglm3更新了什么">1.ChatGLM3更新了什么&lt;/h1>
&lt;h2 id="1模型列表">(1)模型列表&lt;/h2>
&lt;p>智谱AI刚刚发布了ChatGLM3，其中&lt;strong>ChatGLM3-6B&lt;/strong>的能力提升如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>更强大的基础模型&lt;/strong>： 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上表现更好。&lt;/li>
&lt;li>&lt;strong>更完整的功能支持&lt;/strong>：重新设计了Prompt模版格式，支持&lt;code>Function Call&lt;/code>、&lt;code>Code Interpreter&lt;/code>、&lt;code>Agent&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>除了ChatGLM3-6B，还发布了：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ChatGLM3-6B-Base&lt;/strong>：它是ChatGLM3-6B的预训练模型，在10B以下的表现同比更好。&lt;/li>
&lt;li>&lt;strong>ChatGLM3-6B-32K&lt;/strong>：适用于长文本对话场景。&lt;/li>
&lt;/ul>
&lt;p>ChatGLM3发布的&lt;strong>模型列表&lt;/strong>如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seq Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM3-6B&lt;/td>
&lt;td>8k&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-Base&lt;/td>
&lt;td>8k&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-32K&lt;/td>
&lt;td>32k&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="2测评结果">(2)测评结果&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>典型数据集测试&lt;/strong>：在8个中英文典型数据集上，ChatGLM3-6B-Base的性能表现如下：
&lt;ul>
&lt;li>&lt;strong>测试方法&lt;/strong>：BBH 采用3-shot测试，GSM8K(需要推理)采用0-shot CoT测试、MATH(需要推理)采用0-shot CoT测试，MBPP 采用0-shot生成后运行测例计算 Pass@1 ，其它选择题类型数据集均采用0-shot测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>MATH&lt;/th>
&lt;th>BBH&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>C-Eval&lt;/th>
&lt;th>CMMLU&lt;/th>
&lt;th>MBPP&lt;/th>
&lt;th>AGIEval&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM2-6B-Base&lt;/td>
&lt;td>32.4&lt;/td>
&lt;td>6.5&lt;/td>
&lt;td>33.7&lt;/td>
&lt;td>47.9&lt;/td>
&lt;td>51.7&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Best Baseline&lt;/td>
&lt;td>52.1&lt;/td>
&lt;td>13.1&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>60.1&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>62.2&lt;/td>
&lt;td>47.5&lt;/td>
&lt;td>45.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-Base&lt;/td>
&lt;td>72.3&lt;/td>
&lt;td>25.7&lt;/td>
&lt;td>66.1&lt;/td>
&lt;td>61.4&lt;/td>
&lt;td>69.0&lt;/td>
&lt;td>67.5&lt;/td>
&lt;td>52.4&lt;/td>
&lt;td>53.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>长文本测试&lt;/strong>：进行人工评估测试，ChatGLM3-6B-32K的性能表现如下。
&lt;ul>
&lt;li>&lt;strong>测试结论&lt;/strong>：与ChatGLM2相比，效果提升超50%(对论文阅读、文档摘要和财报分析等提升显著)。&lt;/li>
&lt;li>&lt;strong>测试方法&lt;/strong>：在LongBench评测集上进行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>平均&lt;/th>
&lt;th>Summary&lt;/th>
&lt;th>Single-Doc QA&lt;/th>
&lt;th>Multi-Doc QA&lt;/th>
&lt;th>Code&lt;/th>
&lt;th>Few-shot&lt;/th>
&lt;th>Synthetic&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ChatGLM2-6B-32K&lt;/td>
&lt;td>41.5&lt;/td>
&lt;td>24.8&lt;/td>
&lt;td>37.6&lt;/td>
&lt;td>34.7&lt;/td>
&lt;td>52.8&lt;/td>
&lt;td>51.3&lt;/td>
&lt;td>47.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChatGLM3-6B-32K&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>26.6&lt;/td>
&lt;td>45.8&lt;/td>
&lt;td>46.1&lt;/td>
&lt;td>56.2&lt;/td>
&lt;td>61.2&lt;/td>
&lt;td>65&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="2准备硬件资源及基础软件">2.准备硬件资源及基础软件&lt;/h1>
&lt;p>笔者准备的硬件资源及基础软件如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>：V100，32G显存，避免OOM问题。&lt;/li>
&lt;li>&lt;strong>CUDA&lt;/strong>：Cuda11.6&lt;/li>
&lt;li>&lt;strong>OS&lt;/strong>：Ubuntu22.04&lt;/li>
&lt;li>&lt;strong>Conda&lt;/strong>：Miniconda3&lt;/li>
&lt;li>&lt;strong>Python&lt;/strong>：Python3.10&lt;/li>
&lt;li>&lt;strong>Pytorch&lt;/strong>：Pytorch3.8&lt;/li>
&lt;/ul>
&lt;h1 id="3创建虚拟环境">3.创建虚拟环境&lt;/h1>
&lt;ul>
&lt;li>创建虚拟环境：&lt;code>conda create -p ./envs/HCZ_ChatGLM2 python=3.10&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030003623799.png" alt="image-20231030003623799">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>激活虚拟环境&lt;/strong>：&lt;code>conda activate ./envs/HCZ_ChatGLM3&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030003931661.png" alt="image-20231030003931661">&lt;/p>
&lt;h1 id="4上传模型及模型容器">4.上传模型及模型容器&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>chatglm3-6b下载地址&lt;/strong>：https://huggingface.co/THUDM/chatglm3-6b&lt;/li>
&lt;li>&lt;strong>chatglm3-6b容器下载地址&lt;/strong>：https://github.com/THUDM/ChatGLM3/archive/refs/heads/main.zip&lt;/li>
&lt;li>下载完成后，&lt;strong>上传到服务器&lt;/strong>，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030071806371.png" alt="image-20231030071806371">&lt;/p>
&lt;h1 id="5安装依赖包">5.安装依赖包&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>进入容器目录&lt;/strong>：&lt;code>cd /opt/model/THUDM_chatglm3-6b-container&lt;/code>&lt;/li>
&lt;li>&lt;strong>安装依赖包&lt;/strong>：&lt;code>pip install -r requirements.txt&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030071933817.png" alt="image-20231030071933817">&lt;/p>
&lt;h1 id="6构建restful接口">6.构建Restful接口&lt;/h1>
&lt;ul>
&lt;li>借鉴ChatGLM2的&lt;code>api.py&lt;/code>，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030074839307.png" alt="image-20231030074839307">&lt;/p>
&lt;h1 id="7运行chatglm3服务">7.运行ChatGLM3服务&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>进入容器目录&lt;/strong>：&lt;code>cd /opt/model/THUDM_chatglm3-6b-container&lt;/code>&lt;/li>
&lt;li>&lt;strong>运行服务&lt;/strong>：&lt;code>python api.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030075223648.png" alt="image-20231030075223648">&lt;/p>
&lt;h1 id="8测试">8.测试&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>向ChatGLM3提问&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030083402685.png" alt="image-20231030083402685">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>服务器端运行日志如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAChatGLM3/image-20231030083320605.png" alt="image-20231030083320605">&lt;/p>
&lt;h1 id="9小结">9.小结&lt;/h1>
&lt;ul>
&lt;li>本文阐述了ChatGLM3的官宣能力，并演示了如何搭建自己的ChatGLM3。&lt;/li>
&lt;li>ChatGLM3的新能力有待进一步集成到产品中进行验证。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记19-自己实现一个简版ChatGPT(下)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</link><pubDate>Fri, 20 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</guid><description>&lt;p>前两篇实现了简版GPT，并对其进行了SFT，我们接下来看ChatGPT整体训练流程的最后一个环节——&lt;strong>对齐训练(Alignment Training)&lt;/strong>。&lt;/p>
&lt;h1 id="1方法3对齐训练alignment-training">1.方法3：对齐训练(Alignment Training)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;ul>
&lt;li>方法3对应于&lt;strong>ChatGPT整体训练流程的STEP2、STEP3&lt;/strong>。&lt;/li>
&lt;li>方法3的核心思想是利用了强化学习，最终将GPT3演进为了&lt;strong>更通人性的ChatGPT&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020062050959.png" alt="image-20231020062050959">&lt;/p>
&lt;ul>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP2、STEP3&lt;/strong>，就是大名鼎鼎的&lt;strong>RLHF&lt;/strong>——&lt;strong>基于人类反馈的强化学习&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>RL：Reinforcement Learning&lt;/strong>&lt;/li>
&lt;li>&lt;strong>HF：Human Feedback&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP2&lt;/strong>，&lt;strong>对应于&lt;/strong>强化学习模型的&lt;strong>Interpreter模型&lt;/strong>。&lt;/li>
&lt;li>ChatGPT整体训练流程中的&lt;strong>STEP3&lt;/strong>，&lt;strong>对应于&lt;/strong>强化学习模型的&lt;strong>Action模型&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020085551189.png" alt="image-20231020085551189">&lt;/p>
&lt;h2 id="2什么是对齐训练">(2)什么是对齐训练&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>对齐训练&lt;/strong>：Alignment Training，它就是一种机器学习的模型训练方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>核心思想&lt;/strong>：训练出人类主观感受的模型，这个模型具备预测人类的决策的能力。&lt;/p>
&lt;ul>
&lt;li>这样，训练好的模型，就可以在未见过的场景下，按照类似人的行为模式做出选择。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>对齐训练与强化学习的关系&lt;/strong>：OpenAI在对齐训练中，结合了强化学习。&lt;/p>
&lt;ul>
&lt;li>ChatGPT整体训练流程的STEP2就是对齐训练，学习出预测人类回答问题的偏好模型。&lt;/li>
&lt;li>ChatGPT整体训练流程的STEP3就是强化学习，STEP2输出的这个模型，作为强化学习的Interpreter模型。STEP3不断迭代，最终学习到Action模型。
&lt;ul>
&lt;li>通过SFT训练之后GPT3，本质就是一个能机械式地回答问题的机器人。&lt;/li>
&lt;li>通过RLHF学习的Action模型，才是帮助SFT之后的GPT3，类似人类回答问题的关键机关。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>细节&lt;/strong>：ChatGPT整体训练流程图中，出现了PPO算法，PPO算法是近端策略梯度优化，增加一个限制Action模型在训练过程中梯度上升速度，本质就是避免Action模型产生一个离谱的Action。&lt;/p>
&lt;ul>
&lt;li>PPO算法展开说内容太多，本文不赘述，详见论文：https://arxiv.org/abs/1707.06347&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3step2的reward-model模型训练伪码">(3)STEP2的Reward Model模型训练伪码&lt;/h2>
&lt;ul>
&lt;li>我们再来看看STEP2的伪码，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020093407332.png" alt="image-20231020093407332">&lt;/p>
&lt;h2 id="4step3的rlhf训练伪码">(4)STEP3的RLHF训练伪码&lt;/h2>
&lt;ul>
&lt;li>我们再来看看STEP3的伪码，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020095154134.png" alt="image-20231020095154134">&lt;/p>
&lt;h1 id="2deepspeed">2.DeepSpeed&lt;/h1>
&lt;p>RLHF，是ChatGPT最核心的技术机密，除了在《Introducing ChatGPT》(&lt;a href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt&lt;/a>)中提到了，并未公开过源码。&lt;/p>
&lt;p>在前文的伪码实现部分，虽然通过伪码描述了RLHF的核心逻辑，但距离商用还欠缺很多东西(如：分布式训练等)。&lt;/p>
&lt;p>幸好微软开源了类似的框架，DeepSpeed，我们可以通过阅读它的源码、使用它，开展RLHF。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020100752488.png" alt="image-20231020100752488">&lt;/p>
&lt;h1 id="3实例-开展rlhf训练">3.实例-开展RLHF训练&lt;/h1>
&lt;h2 id="step0前置准备">STEP0.前置准备&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>硬件&lt;/strong>：V100一块，32G显存&lt;/li>
&lt;li>&lt;strong>基础软件&lt;/strong>：Ubuntun20.04，Minicoda3，Pytorch3.8，CUDA11.6，Python3.10&lt;/li>
&lt;li>&lt;strong>预训练模型&lt;/strong>：选择Facebook的opt1.3B，即&lt;strong>13亿参数&lt;/strong>的预训练模型。&lt;/li>
&lt;li>&lt;strong>环境初始配置&lt;/strong>：创建虚拟环境，&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020102932305.png" alt="image-20231020102932305">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>安装依赖&lt;/strong>：进入DeepSpeed-Chat目录，安装相关依赖&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020103116451.png" alt="image-20231020103116451">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>环境测试&lt;/strong>：确认相关基础软件版本号。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020103548543.png" alt="image-20231020103548543">&lt;/p>
&lt;h2 id="step1sft">STEP1.SFT&lt;/h2>
&lt;ul>
&lt;li>开展SFT训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020104103388.png" alt="image-20231020104103388">&lt;/p>
&lt;ul>
&lt;li>设置待微调的预训练模型，以及输出路径。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training/step1_supervised_finetuning/evaluation_scripts/run_prompt.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020110616405.png" alt="image-20231020110616405">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_1.3b.sh，触发DeepSpeed开始SFT训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=959965022&amp;bvid=BV1Hp4y1M7Ly&amp;cid=1305868791&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step2rm">STEP2.RM&lt;/h2>
&lt;ul>
&lt;li>开展RM训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：training_scripts/opt/single_gpu/run_350m.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020145149415.png" alt="image-20231020145149415">&lt;/p>
&lt;ul>
&lt;li>指定Reward Model的输出路径。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>路径：evaluation_scripts/run_eval.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020145843742.png" alt="image-20231020145843742">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_350m.sh，触发DeepSpeed开始RW训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=534943541&amp;bvid=BV1gM411R7Z5&amp;cid=1305868897&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step3rlhf">STEP3.RLHF&lt;/h2>
&lt;ul>
&lt;li>开展RLHF训练时，由于服务器资源不足(省钱)，需要避免OOM异常，因此需要修改一下训练脚本。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>training_scripts/opt/single_gpu/run_1.3b.sh&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8B)/image-20231020155704428.png" alt="image-20231020155704428">&lt;/p>
&lt;ul>
&lt;li>执行训练脚本run_1.3b.sh，触发DeepSpeed开始RLHF训练。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=277474259&amp;bvid=BV1Ww411F7xw&amp;cid=1305868938&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="step4模型测试">STEP4.模型测试&lt;/h2>
&lt;ul>
&lt;li>执行测试脚本&lt;code>python chat.py --path training/step3_rlhf_finetuning/output/actor&lt;/code>，不赘述。&lt;/li>
&lt;/ul>
&lt;h1 id="4小结">4.小结&lt;/h1>
&lt;p>本文是实现简版GPT的三篇中的最后一篇，也是最难理解的一部分内容：&lt;/p>
&lt;ul>
&lt;li>对齐训练是什么？&lt;/li>
&lt;li>对齐训练和强化学习的关系是什么？&lt;/li>
&lt;li>ChatGPT整体训练流程的STEP2、STEP3与强化学习的Interpreter和Action模型如何对应？&lt;/li>
&lt;li>DeepSpeed的实际操作？&lt;/li>
&lt;/ul>
&lt;p>本文也有没有展开探讨的内容，待本专栏后续继续展开：&lt;/p>
&lt;ul>
&lt;li>RLHF的策略梯度优化算法&lt;/li>
&lt;li>PPO算法&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>编写本专栏受益匪浅，也非常感恩因为编写本专栏认识的大神们，期待与各位小伙伴持续的讨论和思辨！&lt;/p></description></item><item><title>【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</link><pubDate>Wed, 18 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</guid><description>&lt;p>根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。&lt;/p>
&lt;p>但GPT3距离ChatGPT还有很远的距离，这一段距离涉及OpenAI未公开论文、源码的关键技术。&lt;/p>
&lt;p>我们接下来从OpenAPI已公开的信息来看看&lt;font color=red>&lt;strong>ChatGPT是如何炼成的&lt;/strong>&lt;/font>。&lt;/p>
&lt;h1 id="1chatgpt的历史版本">1.ChatGPT的历史版本&lt;/h1>
&lt;p>下图摘自Yule Wang的技术专栏，阐述了基于Transformer的大语言模型不同版本的脉络：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BERT&lt;/strong>是编码器Only架构，&lt;strong>BART&lt;/strong>是编码器-解码器架构，它们延伸出去大语言模型下载后&lt;font color=red>&lt;strong>不能直接使用，需要垂域微调&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>T5&lt;/strong>是编码器-解码器架构，&lt;strong>GPT-2&lt;/strong>是解码器Only架构，它们下载后&lt;font color=red>&lt;strong>不做垂域微调，也能完成一些AI任务&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>GPT-3、GPT-4&lt;/strong>是解码器Only架构，它们下载后&lt;font color=red>&lt;strong>不做垂域微调，能完成大部分AI任务&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>针对&lt;strong>GPT-3&lt;/strong>进行&lt;font color=red>&lt;strong>SFT(有监督微调)+RLHF(基于人类反馈的强化学习)&lt;strong>&lt;/font>，最终得到了&lt;/strong>ChatGPT&lt;/strong>，&lt;strong>GPT-3.5、InstructGPT&lt;/strong>算是过渡产品。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019083053465.png" alt="image-20231019083053465">&lt;/p>
&lt;h1 id="2chatgpt的整体训练流程">2.ChatGPT的整体训练流程&lt;/h1>
&lt;p>在《Introducing ChatGPT》(详见https://openai.com/blog/chatgpt)中，给出了从GPT3演进到ChatGPT的整体训练流程图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/ChatGPT_Diagram.svg" alt="ChatGPT_Diagram">&lt;/p>
&lt;p>以前看这张图很模糊，通过前面复现Transformer架构、简版GPT的代码，才逐渐变得清晰。&lt;/p>
&lt;p>接下来，我们来逐一拆解。&lt;/p>
&lt;h1 id="3chatgpt的三大训练方法">3.ChatGPT的三大训练方法&lt;/h1>
&lt;p>下图比较形象地归纳了ChatGPT整体训练流程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019094239673.png" alt="image-20231019094239673">&lt;/p>
&lt;h2 id="1方法1预训练pre-traning">(1)方法1：预训练(Pre-Traning)&lt;/h2>
&lt;ul>
&lt;li>构建好大语言模型的神经网络架构后，&lt;strong>通过大数据、大算力进行训练&lt;/strong>，得到预训练模型。如：GPT3就属于这类预训练模型。&lt;/li>
&lt;li>大语言模型的本质是生成内容，构建训练数据基本都是自动化的，所以&lt;strong>预训练的过程属于无监督学习&lt;/strong>。&lt;/li>
&lt;li>预训练模型非常庞大，&lt;strong>算是通才&lt;/strong>，具备&lt;strong>基本的自然语言处理能力、世界知识&lt;/strong>，甚至还有了&lt;strong>顿悟能力(涌现能力)&lt;/strong>。&lt;/li>
&lt;li>就好像下图红框内庞大的AI大脑(&lt;strong>画的有点恶心&lt;/strong>)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150044511.png" alt="image-20231019150044511">&lt;/p>
&lt;h2 id="2方法2指令调优instruction-tuning">(2)方法2：指令调优(Instruction Tuning)&lt;/h2>
&lt;ul>
&lt;li>虽然花了巨大的时间成本和空间成本获得了预训练模型，但是它的水平，依然无法像一个真人，与人类对话。&lt;/li>
&lt;li>接下来还要进行微调——人工&lt;strong>准备少量的数据&lt;/strong>，对&lt;strong>预训练模型进行增量训练&lt;/strong>——因此也称为&lt;strong>有监督微调(SFT，Supervised Fine-tunning)&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>指令微调&lt;/strong>本质是站在巨人的肩膀上&lt;strong>对预训练模型进行增量训练&lt;/strong>。它可以针对世界知识进行增强，也可以针对某个垂直领域进行增强。&lt;/li>
&lt;li>指令微调后得到的大语言训练模型，将会接近于一个真人。&lt;/li>
&lt;li>就好像下图红框内的红色人脸(&lt;strong>画的也挺恶心&lt;/strong>)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019150129023.png" alt="image-20231019150129023">&lt;/p>
&lt;h2 id="3方法3对齐alignment">(3)方法3：对齐(Alignment)&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>在指令调优后，大语言模型虽然接近于真人，但生成的内容依然会很生硬。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为什么呢？这就是人类的主观感受——人类对某些问题的答案，会有情绪、语气、风格等主观的特征。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>那么，又如何&lt;strong>让AI学会人类的主观感受&lt;/strong>呢？这&lt;strong>就是对齐Alignment&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RLHF(基于人类反馈的强化学习)就是对齐的具体实现之一，利用这种强化学习手段，让大语言模型学会人类的主观感受，GPT3就演进成为了ChatGPT。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>就好像下图右侧红框内小黄球(&lt;strong>画的依然挺恶心&lt;/strong>)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151032099.png" alt="image-20231019151032099">&lt;/p>
&lt;p>接下来，我们逐一分析这三大方法。&lt;/p>
&lt;h1 id="4方法1预训练pre-training">4.方法1：预训练(Pre-Training)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;p>我们对比两张图：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>预训练&lt;/strong>得到的大语言模型，就&lt;strong>是ChatGPT整体训练流程STEP1的输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019151849173.png" alt="image-20231019151849173">&lt;/p>
&lt;h2 id="2辩证看待预训练模型涌现能力神经网络">(2)辩证看待预训练模型、涌现能力、神经网络&lt;/h2>
&lt;p>前段时间，师父问了我3个终极问题，让我一时语塞：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>为什么ChatGPT大力能出奇迹，小力就不行？&lt;/strong>&lt;/li>
&lt;li>&lt;strong>为什么这样的神经网络模型就可以大力出奇迹，换个神经网络模型行不行？&lt;/strong>&lt;/li>
&lt;li>&lt;strong>如果无论什么神经网络模型只要能做到大力就能出奇迹，那么这些神经网络的本质是什么？&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>前面半年一直在复现Transformer论文的细节中，的确缺少了对宏观本质的思考，结合预训练这个章节的写作，正好梳理一下我的宏观思考：&lt;/p>
&lt;h3 id="思考1神经网络的本质是什么">思考1：神经网络的本质是什么？&lt;/h3>
&lt;ul>
&lt;li>观点1：客观世界中，&lt;strong>一切问题都能用函数表达&lt;/strong>。
&lt;ul>
&lt;li>只不过有的函数极其复杂，只有上帝才知道这个函数是什么。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>观点2：&lt;strong>神经网络的本质进行函数近似(Function Approximation)的工具&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>神经网络的输入&lt;/strong>：是大量的数据，数据中隐藏了某个问题背后的函数的数学规律。&lt;/li>
&lt;li>&lt;strong>神经网络的输出&lt;/strong>：找到无限逼近于某个问题背后的函数的近似函数。&lt;/li>
&lt;li>数学上，已经证明&lt;strong>神经网络能够近似出任意一个问题背后的函数&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>神经网络的结构&lt;/strong>，&lt;strong>决定了找到这个近似函数的成本&lt;/strong>(时间成本、空间成本……)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="思考2假设思考1正确如何解释神经网络的5种现象">思考2：假设思考1正确，如何解释神经网络的5种现象？&lt;/h3>
&lt;ul>
&lt;li>现象1：&lt;strong>过于简单的神经网络&lt;/strong>，需要很大力才能出奇迹。
&lt;ul>
&lt;li>任务是&amp;quot;让AI对红点和蓝点进行分类&amp;rdquo;，由于神经网络过于简单，所以迭代了2000多次才找出答案。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=364782652&amp;bvid=BV1F94y1b7MW&amp;cid=1304339810&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象2：&lt;strong>过于简单的任务&lt;/strong>，不需要大力也能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=874844018&amp;bvid=BV1EN4y1C7v2&amp;cid=1304340008&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象3：恒定难度的任务，&lt;strong>加宽神经网络&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=234774107&amp;bvid=BV1N8411r78C&amp;cid=1304339757&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象4：恒定难度的任务，&lt;strong>加深神经网络&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=789763364&amp;bvid=BV1Ry4y1N7CU&amp;cid=1304340937&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;ul>
&lt;li>现象5：恒定难度的任务，&lt;strong>增强神经元&lt;/strong>，大力能出奇迹。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=277344904&amp;bvid=BV14w411w7Nx&amp;cid=1304343623&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;p>从上述5种现象，应该可以得到2个结论：&lt;/p>
&lt;ul>
&lt;li>在神经网络结构恒定的前提下，&lt;strong>待执行的任务难度&lt;/strong>，决定了&lt;strong>能否大力出奇迹、是否需要大力&lt;/strong>。&lt;/li>
&lt;li>在待执行的任务难度恒定的前提下，&lt;strong>神经网络结构&lt;/strong>，决定了&lt;strong>能否大力出奇迹、是否需要大力&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h3 id="思考3辩证地看待预训练模型涌现能力transformer">思考3：辩证地看待预训练模型、涌现能力、Transformer&lt;/h3>
&lt;p>当GPT3.5出现&lt;strong>涌现能力(emergent capabilities)&lt;strong>后，它似乎被神话成了&lt;/strong>人类尚无办法解释的神迹&lt;/strong>。&lt;/p>
&lt;p>我们应该如此这般辩证地看待预训练模型、涌现、Transformer&lt;/p>
&lt;ul>
&lt;li>预训练模型不是ChatGPT首创，在深度学习时代就有了，有很多神经网络都是预训练好，节省后来人的训练成本。&lt;/li>
&lt;li>Transformer只是GPT3这种预训练模型遵循的神经网络结构，&lt;strong>Transformer这种神经网络结构本质是换了一种姿势寻找近似函数&lt;/strong>。&lt;/li>
&lt;li>如果问题P1、问题P2、……问题Pn背后的函数都极为相近，当神经网络结构找到了问题P1的近似函数，那么这个近似函数也能用来解决问题P2、……问题Pn——这就是经过训练，神经网络能自我学习到一些额外的能力的原因，即涌现能力。
&lt;ul>
&lt;li>进一步思考：深度学习时代有没有涌现呢？可能有，只是n这个值比较小，不太明显吧。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="5方法2指令调优instruction-tuning">5.方法2：指令调优(Instruction Tuning)&lt;/h1>
&lt;h2 id="1与chatgpt整体训练流程图的对应关系-1">(1)与ChatGPT整体训练流程图的对应关系&lt;/h2>
&lt;ul>
&lt;li>指令调优对应于ChatGPT整体训练流程的STEP1。&lt;/li>
&lt;li>&lt;strong>SFT&lt;/strong>：有监督微调，属于有监督学习。包括In-context Tuning(上下文调优)和Instruction Tuning(指令调优)。
&lt;ul>
&lt;li>&lt;strong>In-context Tuning&lt;/strong>：上下文调优，这种调优方式的本质是将多轮对话的聊天记录一起发送给大语言模型，有了上下文，大语言模型就能更好地回答问题。&lt;/li>
&lt;li>&lt;strong>Instruction Tuning&lt;/strong>：指令调优，这种调优方式的本质就是问题中带有明确的指令、明确的要求。Instruction Tuning是OpenAI在GPT-3.5-turbo模型中引入的一种新方法，是在传统的微调过程上的一种变体。&lt;/li>
&lt;li>对于上述两种调优方式，在提示词工程中都体现了它们的思想——&lt;strong>&amp;ldquo;上下文&amp;rdquo;、&amp;ldquo;指令&amp;rdquo;&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019170641604.png" alt="image-20231019170641604">&lt;/p>
&lt;h2 id="2训练sft模型的伪码">(2)训练SFT模型的伪码&lt;/h2>
&lt;p>前文讲了很多理论，还是需要撸一下代码比较便于理解。&lt;/p>
&lt;blockquote>
&lt;p>由于OpenAI对于SFT、RLHF并未公开源码，所以在这里只编写伪码，在后续实例中展示真实代码。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019181420212.png" alt="image-20231019181420212">&lt;/p>
&lt;h2 id="3sft实例">(3)SFT实例&lt;/h2>
&lt;p>针对简版GPT，只需要增加如下代码：&lt;/p>
&lt;ul>
&lt;li>首先，通过&lt;code>torch.load方法&lt;/code>加载上一篇已经预训练好的简版GPT模型，这个模型的训练数据仅包含了维基百科的基本数据。&lt;/li>
&lt;li>然后，将新增的医学知识数据集，对简版GPT模型进行增量训练。&lt;/li>
&lt;li>最后，通过&lt;code>torch.save方法&lt;/code>保存增量训练的简版GPT模型，此时，这个模型就包含了医学知识了。&lt;/li>
&lt;li>具体代码详见下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%AD)/image-20231019182641050.png" alt="image-20231019182641050">&lt;/p>
&lt;h1 id="6小结">6.小结&lt;/h1>
&lt;p>讲到这里，内容已经比较饱和了，我们将在最后一篇阐述&lt;strong>方法3：对齐训练&lt;/strong>以及&lt;strong>RLHF的实例&lt;/strong>。&lt;/p>
&lt;p>我们接下来对本文进行一下小结：&lt;/p>
&lt;ul>
&lt;li>ChatGPT的历史版本：&lt;strong>GPT3、InstructGPT、ChatGPT&lt;/strong>。&lt;/li>
&lt;li>ChatGPT&lt;strong>整体训练流程&lt;/strong>，支撑了&lt;strong>GPT3到ChatGPT的演进&lt;/strong>。&lt;/li>
&lt;li>预训练模型是什么？如何&lt;strong>辩证地&lt;/strong>看待&lt;strong>预训练模型、涌现能力、神经网络的学习能力&lt;/strong>？&lt;/li>
&lt;li>SFT(有监督微调)的概念、原理，最后展示了&lt;strong>如何针对简版GPT进行SFT&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>我们下一步继续针对简版ChatGPT开展RLHF，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</link><pubDate>Mon, 16 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</guid><description>&lt;p>接下来，我们用三篇文章阐述&lt;font color=red>**如何实现一个简版ChatGPT。**&lt;/font>&lt;/p>
&lt;h1 id="1回顾">1.回顾&lt;/h1>
&lt;p>想实现一个简版ChatGPT，依赖于如下前置知识：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>机器学习基本原理&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>《【chatGPT】学习笔记3-机器学习基本原理(上)》&lt;/li>
&lt;li>《【chatGPT】学习笔记4-机器学习基本原理(下)》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>经典NLP相关技术&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>&lt;strong>N-Gram&lt;/strong>：《【chatGPT】学习笔记6-手撸一个上古GPT》&lt;/li>
&lt;li>&lt;strong>Embedding&lt;/strong>：《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》&lt;/li>
&lt;li>&lt;strong>神经概率语言模型&lt;/strong>：《【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2》&lt;/li>
&lt;li>&lt;strong>Seq2Seq&lt;/strong>：《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>现代NLP相关技术&lt;/strong>，可参考笔者这几篇文章：
&lt;ul>
&lt;li>&lt;strong>注意力机制&lt;/strong>：《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》&lt;/li>
&lt;li>&lt;strong>Transformer&lt;/strong>：《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="2实现简版gpt">2.实现简版GPT&lt;/h1>
&lt;p>这是参考Transformer架构绘制的简版ChatGPT整体架构，我们将对它进行拆解：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017113929233.png" alt="image-20231017113929233">&lt;/p>
&lt;h2 id="1整体">(1)整体&lt;/h2>
&lt;p>将上图抽象化，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>简版GPT遵循Transfomer架构，但是没有实现编码器&lt;/li>
&lt;li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017134713086.png" alt="image-20231017134713086">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1&lt;/strong>：对应上图将&lt;strong>Outputs&lt;/strong>输入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>2&lt;/strong>：对应上图将&lt;strong>Decoder的输出&lt;/strong>，传入给线性层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017135734066.png" alt="image-20231017135734066">&lt;/p>
&lt;h2 id="2局部1正弦位置编码表">(2)局部1：正弦位置编码表&lt;/h2>
&lt;p>首先，我们来细化&lt;strong>Outputs&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间的流程：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.对&lt;strong>Outputs&lt;/strong>实施词嵌入，得到词向量。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.在&lt;strong>词向量&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间增加了&lt;strong>位置编码表&lt;/strong>(也是一个向量)，这个位置编码表体现了&lt;strong>词和词序的关系&lt;/strong>。
&lt;ul>
&lt;li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立&lt;strong>词和词序的关系&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.将STEP2的&lt;strong>位置表码表&lt;/strong>向量和&lt;strong>词向量&lt;/strong>相加，输入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140335543.png" alt="image-20231017140335543">&lt;/p>
&lt;p>正弦位置编码表的计算原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(2)局部1：正弦位置编码表&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017140636308.png" alt="image-20231017140636308">&lt;/p>
&lt;h2 id="3局部2解码器堆栈">(3)局部2：解码器堆栈&lt;/h2>
&lt;p>我们再来细化&lt;strong>解码器&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>编码器本质上由&lt;strong>N个解码器&lt;/strong>串联而成的&lt;strong>解码器堆栈&lt;/strong>。&lt;/li>
&lt;li>我们的实现，也按照论文的设定层数，&lt;strong>N=6&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150909585.png" alt="image-20231017150909585">&lt;/p>
&lt;h2 id="4局部3解码器">(4)局部3：解码器&lt;/h2>
&lt;p>我们再进一步细化&lt;strong>解码器Decoder&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>解码器Decoder&lt;/strong>由&lt;strong>多头注意力&lt;/strong>和&lt;strong>前向传播网络&lt;/strong>组成。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150925061.png" alt="image-20231017150925061">&lt;/p>
&lt;p>&lt;strong>解码器堆栈&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：这就是创建的&lt;strong>位置编码层&lt;/strong>，再将&lt;strong>词向量和位置编码向量相加&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：这就是将多个&lt;strong>解码器&lt;/strong>叠加成&lt;strong>解码器堆栈&lt;/strong>，每个&lt;strong>解码器的输入&lt;/strong>是&lt;strong>上个解码器的输出&lt;/strong>和&lt;strong>上个解码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>③&lt;/strong>：这就是表示&lt;strong>解码器堆栈&lt;/strong>输出的&lt;strong>解码器输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017142049236.png" alt="image-20231017142049236">&lt;/p>
&lt;p>&lt;strong>解码器&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：就是&lt;strong>多头注意力层&lt;/strong>，&lt;strong>第一个解码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>词嵌入+位置编码向量之和&lt;/strong>以及&lt;strong>自注意力掩码&lt;/strong>，&lt;strong>后续解码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>上一个解码器的输出&lt;/strong>和&lt;strong>自注意力掩码&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：就是&lt;strong>前向传播网络&lt;/strong>，它的输入是&lt;strong>多头注意力层的输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145220345.png" alt="image-20231017145220345">&lt;/p>
&lt;p>我们接下来看&lt;strong>多头注意力层、前向传播网络、自注意力位置掩码&lt;/strong>如何实现？&lt;/p>
&lt;h2 id="5局部4多头注意力">(5)局部4：多头注意力&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(5)局部4：多头注意力&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150943298.png" alt="image-20231017150943298">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017145622219.png" alt="image-20231017145622219">&lt;/p>
&lt;h2 id="6局部5前向传播网络">(6)局部5：前向传播网络&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(6)局部5：前向传播网络&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151000709.png" alt="image-20231017151000709">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150114991.png" alt="image-20231017150114991">&lt;/p>
&lt;h2 id="7局部6填充位置掩码">(7)局部6：填充位置掩码&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(7)局部6：填充位置掩码&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151014381.png" alt="image-20231017151014381">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150413819.png" alt="image-20231017150413819">&lt;/p>
&lt;h2 id="8局部7后续位置掩码">(8)局部7：后续位置掩码&lt;/h2>
&lt;p>多头注意力的原理参见《【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5》的&amp;rdquo;(10)局部9：后续位置掩码&amp;quot;章节，本文不再赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151026419.png" alt="image-20231017151026419">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017150515799.png" alt="image-20231017150515799">&lt;/p>
&lt;h2 id="9模型训练">(9)模型训练&lt;/h2>
&lt;p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：&lt;/p>
&lt;ul>
&lt;li>数据集如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151515137.png" alt="image-20231017151515137">&lt;/p>
&lt;ul>
&lt;li>模型训练：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151651529.png" alt="image-20231017151651529">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151717714.png" alt="image-20231017151717714">&lt;/p>
&lt;ul>
&lt;li>训练好后，会生成简版GPT的pth模型文件：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151835127.png" alt="image-20231017151835127">&lt;/p>
&lt;h2 id="10模型测试">(10)模型测试&lt;/h2>
&lt;ul>
&lt;li>测试用例采用贪婪编码和集束编码，比较简单，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88ChatGPT(%E4%B8%8A)/image-20231017151956136.png" alt="image-20231017151956136">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;ul>
&lt;li>本文基于Transformer架构，复现了简版ChatGPT，其关键在于只有解码器。&lt;/li>
&lt;li>理论上，如果有足够算力、足够训练数据，可以将此简版ChatGPT训练到GPT3的水平。&lt;/li>
&lt;li>那么，GPT3到ChatGPT还有一定的距离，我们知道ChatGPT公开的信息中，还对GPT3进行了&lt;strong>监督学习微调SFT&lt;/strong>、&lt;strong>基于人类反馈的强化学习RLHF&lt;/strong>等，得到了InstructGPT，进而得到了ChatGPT。&lt;/li>
&lt;/ul>
&lt;p>我们下一步继续针对简版ChatGPT开展SFT和RLHF，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</link><pubDate>Tue, 26 Sep 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</guid><description>&lt;p>在《AI拾遗》这个专栏中，我们建立了从&lt;strong>N-Gram&lt;/strong>到&lt;strong>词嵌入&lt;/strong>再到&lt;strong>神经概率语言模型&lt;/strong>，从&lt;strong>Seq2Seq&lt;/strong>到&lt;strong>注意力机制&lt;/strong>的知识脉络。&lt;/p>
&lt;p>这条脉络本质就是NLP发展的路线图，有了这些知识储备，我们终于可以来理解论文**《Attention Is All You Need》**中大名鼎鼎的**Transformer架构**了！&lt;/p>
&lt;h1 id="1问题">1.问题&lt;/h1>
&lt;p>在《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》中，笔者为&lt;strong>编码器-解码器架构&lt;/strong>增加了&lt;strong>注意力机制&lt;/strong>，进而实现了&lt;strong>增强版的Seq2Seq模型&lt;/strong>，模型能力的确有所增强，但并不是彻底解决了&lt;strong>长距离依赖&lt;/strong>问题和&lt;strong>信息压缩&lt;/strong>问题。&lt;/p>
&lt;p>在《Attention Is All You Need》的第一章阐述了这个观点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>howerver, remains&lt;/strong>(第二段最后一句)：历史上很多论文和技术都在增强&lt;strong>编码器-解码器架构&lt;/strong>，注意力机制也成为序列建模必备的技术，但&lt;strong>长距离依赖&lt;/strong>问题依然存在。&lt;/li>
&lt;li>&lt;strong>parallelization&lt;/strong>(第三段)：这里提到了并行化，这是前面没有提到的问题——RNN网络决定了Seq2Seq只能一个词一个词的处理。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926072417665.png" alt="image-20230926072417665">&lt;/p>
&lt;h1 id="2transformer逐步拆解">2.Transformer逐步拆解&lt;/h1>
&lt;p>这是论文中第三段绘制的Transformer整体架构，我们将对它进行拆解：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926074022967.png" alt="image-20230926074022967">&lt;/p>
&lt;h2 id="1整体transformer">(1)整体：Transformer&lt;/h2>
&lt;p>将上图抽象化，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>Transformer依然可以遵循Encoder-Decoder架构&lt;/li>
&lt;li>Decoder的输出交给一个线性层，将解码器的输出转换为目标词汇表大小的概率分布——这属于常规操作，与Transformer核心思想关系不大。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926082223230.png" alt="image-20230926082223230">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1&lt;/strong>：对应上图将&lt;strong>Inputs&lt;/strong>输入给&lt;strong>Encoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>2&lt;/strong>：对应上图将&lt;strong>Outputs+Encoder的输出&lt;/strong>，传入给&lt;strong>Decoder&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>3&lt;/strong>：对应上图将&lt;strong>Decoder的输出&lt;/strong>，传入给线性层。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083838772.png" alt="image-20230926083838772">&lt;/p>
&lt;p>PS：这里的&lt;code>corpus&lt;/code>是一个封装了&lt;code>Inputs&lt;/code>和&lt;code>Outputs&lt;/code>的工具类，代码如下(比较简单，不赘述)：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926083336984.png" alt="image-20230926083336984">&lt;/p>
&lt;h2 id="2局部1正弦位置编码表">(2)局部1：正弦位置编码表&lt;/h2>
&lt;p>首先，我们来细化&lt;strong>Inputs&lt;/strong>和&lt;strong>Encoder&lt;/strong>之间的流程：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.对&lt;strong>Inputs&lt;/strong>实施词嵌入，得到词向量。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.在&lt;strong>词向量&lt;/strong>和&lt;strong>Encoder&lt;/strong>之间增加了&lt;strong>位置编码表&lt;/strong>(也是一个向量)，这个位置编码表体现了&lt;strong>词和词序的关系&lt;/strong>。
&lt;ul>
&lt;li>由于Transformer取消了RNN，也就不再逐个词串行处理，所以必须建立&lt;strong>词和词序的关系&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.将STEP2的&lt;strong>位置表码表&lt;/strong>向量和&lt;strong>词向量&lt;/strong>相加，输入给&lt;strong>Encoder&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Outputs&lt;/strong>和&lt;strong>Decoder&lt;/strong>之间的流程和上述流程一样。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926092023205.png" alt="image-20230926092023205">&lt;/p>
&lt;p>那么位置编码表如何计算呢？论文3.5章节详细阐述如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>d&lt;/strong>：词向量的维度。&lt;/li>
&lt;li>&lt;strong>pos&lt;/strong>：单词在句子中的位置。&lt;/li>
&lt;li>&lt;strong>i&lt;/strong>：词向量的维度的奇数维。&lt;/li>
&lt;li>&lt;strong>PE&lt;/strong>：指定位置的单词，在词向量的某一个维度上的数值。&lt;/li>
&lt;li>通俗地理解，&lt;strong>d个PE值构成了指定单词在整个句子中的位置向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926093855305.png" alt="image-20230926093855305">&lt;/p>
&lt;p>我们不必纠结于论文中这两个公式的证明，笔者绘制一个例子，可视化地理解正弦位置编码表的作用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：输入序列为&amp;rdquo;&lt;strong>想去新疆&lt;/strong>&amp;ldquo;四个字，词向量的维度为4维，即&lt;strong>d=4&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：通过正弦位置编码公式，&lt;strong>想&lt;/strong>字的&lt;strong>位置0&lt;/strong>，求得位置0的&lt;strong>位置向量[0, 1, 0, 1]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：通过正弦位置编码公式，&lt;strong>去&lt;/strong>字的&lt;strong>位置1&lt;/strong>，求得位置1的&lt;strong>位置向量[0.84, 0.54, 0.01, 1.0]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：通过正弦位置编码公式，&lt;strong>新&lt;/strong>字的&lt;strong>位置2&lt;/strong>，求得位置2的&lt;strong>位置向量[0.91, -0.42, 0.02, 1.0]&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP4&lt;/strong>：通过正弦位置编码公式，&lt;strong>疆&lt;/strong>字的&lt;strong>位置3&lt;/strong>，求得位置3的&lt;strong>位置向量[0.14, -0.99, 0.03, 1.0]&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926111712233.png" alt="image-20230926111712233">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926110113612.png" alt="image-20230926110113612">&lt;/p>
&lt;h2 id="3局部2编码器堆栈">(3)局部2：编码器堆栈&lt;/h2>
&lt;p>我们再来细化&lt;strong>编码器&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>编码器本质上由&lt;strong>N个编码器&lt;/strong>串联而成的&lt;strong>编码器堆栈&lt;/strong>。&lt;/li>
&lt;li>论文中，&lt;strong>N=6&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926112341735.png" alt="image-20230926112341735">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>论文原文&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926113124754.png" alt="image-20230926113124754">&lt;/p>
&lt;h2 id="4局部3编码器">(4)局部3：编码器&lt;/h2>
&lt;p>我们再进一步细化&lt;strong>编码器Encoder&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编码器Encoder&lt;/strong>由&lt;strong>多头注意力&lt;/strong>和&lt;strong>前向传播网络&lt;/strong>组成。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926114259743.png" alt="image-20230926114259743">&lt;/p>
&lt;p>&lt;strong>编码器堆栈&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：这就是调用&lt;strong>正弦位置编码表&lt;/strong>，创建的&lt;strong>位置编码层&lt;/strong>，再将&lt;strong>词向量和位置编码向量相加&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：这就是将多个&lt;strong>编码器&lt;/strong>叠加成&lt;strong>编码器堆栈&lt;/strong>，每个&lt;strong>编码器的输入&lt;/strong>是&lt;strong>上个编码器的输出&lt;/strong>和&lt;strong>上个编码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>③&lt;/strong>：这就是表示&lt;strong>编码器堆栈&lt;/strong>输出的&lt;strong>编码器输出&lt;/strong>、&lt;strong>编码器输出的注意力权重&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926115605435.png" alt="image-20230926115605435">&lt;/p>
&lt;p>&lt;strong>编码器&lt;/strong>的代码实现如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>①&lt;/strong>：就是&lt;strong>多头注意力层&lt;/strong>，&lt;strong>第一个编码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>词嵌入+位置编码向量之和&lt;/strong>以及&lt;strong>自注意力掩码&lt;/strong>，&lt;strong>后续编码器&lt;/strong>的&lt;strong>多头注意力层&lt;/strong>的输入是&lt;strong>上一个编码器的输出&lt;/strong>和&lt;strong>自注意力掩码&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>②&lt;/strong>：就是&lt;strong>前向传播网络&lt;/strong>，它的输入是&lt;strong>多头注意力层的输出&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926122832935.png" alt="image-20230926122832935">&lt;/p>
&lt;p>这里又埋下了几个问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多头注意力层&lt;/strong>如何实现？&lt;/li>
&lt;li>&lt;strong>前向传播网络&lt;/strong>如何实现？&lt;/li>
&lt;li>&lt;strong>自注意力位置掩码&lt;/strong>如何实现？&lt;/li>
&lt;/ul>
&lt;h2 id="5局部4多头注意力">(5)局部4：多头注意力&lt;/h2>
&lt;p>我们再来细化多头注意力：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>不赘述&lt;/strong>：关于&lt;strong>点积注意力&lt;/strong>、&lt;strong>缩放点积注意力&lt;/strong>、&lt;strong>编码器-解码器注意力&lt;/strong>、&lt;strong>QKV&lt;/strong>、&lt;strong>自注意力&lt;/strong>、&lt;strong>多头注意力&lt;/strong>，本文就不再赘述了。如果理解不太清晰，可以回看《【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4》。&lt;/li>
&lt;li>&lt;strong>多头注意力&lt;/strong>的结构：&lt;strong>多头注意力的输入&lt;/strong>是&lt;strong>词向量与位置编码向量之和&lt;/strong>，每一个注意力头都是对&lt;strong>多头注意力的输入&lt;/strong>进行矩阵乘法得到&lt;strong>QKV&lt;/strong>，再输入给&lt;strong>缩放点积注意力组件&lt;/strong>，这个组件输出的是&lt;strong>注意力权重&lt;/strong>。最后，将每个注意力头输出的注意力权重求和，输入给一个线性层。&lt;/li>
&lt;li>&lt;strong>缩放点积注意力&lt;/strong>的结构：就是典型的缩放点积注意力的计算公式，即：Q、K求点积=&amp;gt;缩放=&amp;gt;注意力掩码=&amp;gt;Softmax=&amp;gt;和V点积。&lt;/li>
&lt;li>&lt;strong>细节&lt;/strong>：这里增加了Add &amp;amp; Norm，就是深度学习里面的残差连接、层归一化，为了解决梯度爆炸问题，这不是Transformer特有的新知识。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926162228532.png" alt="image-20230926162228532">&lt;/p>
&lt;p>论文原文：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>缩放点积注意力&lt;/strong>计算公式：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161049010.png" alt="image-20230926161049010">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>多头注意力&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161136259.png" alt="image-20230926161136259">&lt;/p>
&lt;p>&lt;strong>缩放点积注意力&lt;/strong>的代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161300468.png" alt="image-20230926161300468">&lt;/p>
&lt;p>&lt;strong>多头注意力&lt;/strong>的代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926161409926.png" alt="image-20230926161409926">&lt;/p>
&lt;h2 id="6局部5前向传播网络">(6)局部5：前向传播网络&lt;/h2>
&lt;p>我们再来细化前向神经网络：&lt;/p>
&lt;ul>
&lt;li>前向神经网络的全称是&lt;strong>Position-wise Feed-Forward Network&lt;/strong>，即&lt;strong>基于位置的前馈神经网络&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>two linear transformations with a ReLU activation&lt;/strong>：首先使用第一个线性层做升维，接着使用ReLU激活函数，再使用第二个线性层做降维。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163125569.png" alt="image-20230926163125569">&lt;/p>
&lt;p>这个基于位置的前馈神经网络到底有啥用呢？&lt;/p>
&lt;ul>
&lt;li>就是论文中，多头注意力结构中的最后一步&lt;strong>Linear&lt;/strong>！&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163933555.png" alt="image-20230926163933555">&lt;/p>
&lt;p>论文原文：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926163424383.png" alt="image-20230926163424383">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926164434302.png" alt="image-20230926164434302">&lt;/p>
&lt;h2 id="7局部6填充位置掩码">(7)局部6：填充位置掩码&lt;/h2>
&lt;p>我们再来细化填充位置掩码：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>填充位置掩码&lt;/strong>用在&lt;strong>词嵌入&lt;/strong>之后，&lt;strong>编码器&lt;/strong>输入之前。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png" alt="image-20230926170827083">&lt;/p>
&lt;ul>
&lt;li>填充位置掩码有什么作用呢？
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：给编码器输入了一句话——&amp;ldquo;我想去新疆滑雪&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：给编码器输入第二句话——&amp;ldquo;想去就去啊&amp;rdquo;。为了和上一句话保持长度统一，我们就会在在第二句话末尾增加两个占位符。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：生成填充位置掩码[1, 1, 1, 1, 1, 0, 0]。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：编码器会将第二句话和填充位置掩码求与，这样编码器实施多头注意力的时候，就不会注意毫无意义的两个占位符。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171245346.png" alt="image-20230926171245346">&lt;/p>
&lt;p>代码实现如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926171345069.png" alt="image-20230926171345069">&lt;/p>
&lt;p>至此，我们就把Transformer架构中编码器部分细化完成了，我们继续细化解码器部分：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926170827083.png" alt="image-20230926170827083">&lt;/p>
&lt;h2 id="8局部7解码器堆栈">(8)局部7：解码器堆栈&lt;/h2>
&lt;p>解码器堆栈的思想到实现，和编码器堆栈完全一样，这里不再赘述，直接上图和代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174658980.png" alt="image-20230926174658980">&lt;/p>
&lt;h2 id="9局部8解码器">(9)局部8：解码器&lt;/h2>
&lt;p>解码器的思想到实现，和编码器堆栈大致一样，这里不再赘述，直接上图和代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174249742.png" alt="image-20230926174249742">&lt;/p>
&lt;p>&lt;strong>解码器堆栈&lt;/strong>代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174857116.png" alt="image-20230926174857116">&lt;/p>
&lt;p>&lt;strong>解码器&lt;/strong>代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926174916494.png" alt="image-20230926174916494">&lt;/p>
&lt;h2 id="10局部9后续位置掩码">(10)局部9：后续位置掩码&lt;/h2>
&lt;p>在解码器中，还有最后一个遗留问题——后续位置掩码。&lt;/p>
&lt;p>后续位置掩码只是因为解码器实施多头注意力的时候，是不能注意到&lt;strong>未来&lt;/strong>的，也就是它还没有预测的后续词，所以要屏蔽掉。&lt;/p>
&lt;p>后续位置掩码和填充位置掩码的思想是一致的，不赘述。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175344740.png" alt="image-20230926175344740">&lt;/p>
&lt;p>&lt;strong>后续位置掩码&lt;/strong>的代码实现：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926175131684.png" alt="image-20230926175131684">&lt;/p>
&lt;h2 id="11模型训练">(11)模型训练&lt;/h2>
&lt;p>至此，我们已经完整地实现了Transformer架构，我们开始对其进行训练：&lt;/p>
&lt;ul>
&lt;li>数据集如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180145967.png" alt="image-20230926180145967">&lt;/p>
&lt;ul>
&lt;li>模型训练：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180246638.png" alt="image-20230926180246638">&lt;/p>
&lt;h2 id="12模型测试">(12)模型测试&lt;/h2>
&lt;ul>
&lt;li>测试用例采用贪婪编码，比较简单，具体代码如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-Transformer%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/image-20230926180356687.png" alt="image-20230926180356687">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>笔者在今年2月份第一次阅读论文《Attention Is All You Need》，读了好几遍，不得要领，只觉得非常抽象。&lt;/p>
&lt;p>随着在网上阅读各类资料、逐步摸索复现论文中Transformer架构的源码，逐渐理解这篇论文中所说的——&lt;strong>Attention Is All You Need&lt;/strong>的含义。&lt;/p>
&lt;p>本以为理解了论文含义，提笔准备写出这篇文章时，又卡了壳——因为理解了，又很难表达出来Transformer的精妙原理！&lt;/p>
&lt;p>此时，笔者才真正领悟之前听过一位大神所说的：&lt;font color=red>”LLM涉及的每一篇经典论文，不仅值得&lt;strong>反复阅读&lt;/strong>，甚至应该&lt;strong>背诵下来&lt;/strong>。“&lt;/font>的含义。&lt;/p>
&lt;p>这篇文章写完，我们下一步就可以实现并训练我们平民版的ChatGPT了，且听下回分解了。&lt;/p></description></item><item><title>【chatGPT】学习笔记15-LangChain之Chain，对LLM的抽象3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</link><pubDate>Wed, 20 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</guid><description>&lt;p>我们继续写点儿偏工程实践的内容——LangChain的核心模块3——Chain。&lt;/p>
&lt;h1 id="1核心模块3chain">1.核心模块3：Chain&lt;/h1>
&lt;p>在《【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)》中，我们知道LangChain-ChatChat有如下工作流程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921092837843.png" alt="image-20230921092837843">&lt;/p>
&lt;p>如何实现呢？其实，LangChain抽象了&lt;strong>Chain&lt;/strong>的概念。&lt;/p>
&lt;h2 id="11chain-class">1.1.Chain Class&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：Chain &amp;ndash;&amp;gt; &lt;name>Chain # Examples: LLMChain, MapReduceChain, RouterChain&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/base.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Chain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Serializable&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Runnable&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for creating structured sequences of calls to components.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Chains should be used to encode a sequence of calls to components like
&lt;/span>&lt;span class="s2"> models, document retrievers, other chains, etc., and provide a simple interface
&lt;/span>&lt;span class="s2"> to this sequence.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The Chain interface makes it easy to create apps that are:
&lt;/span>&lt;span class="s2"> - Stateful: add Memory to any Chain to give it state,
&lt;/span>&lt;span class="s2"> 有状态的：给Chain添加Memory，使其具有状态
&lt;/span>&lt;span class="s2"> - Observable: pass Callbacks to a Chain to execute additional functionality,
&lt;/span>&lt;span class="s2"> like logging, outside the main sequence of component calls,
&lt;/span>&lt;span class="s2"> 可观察的：向Chain传递Callback来执行额外的功能。
&lt;/span>&lt;span class="s2"> - Composable: the Chain API is flexible enough that it is easy to combine
&lt;/span>&lt;span class="s2"> Chains with other components, including other Chains.
&lt;/span>&lt;span class="s2"> 可组合的：Chain API足够灵活，可以轻松地将Chains与其他组件组合在一起，包括组合其他的Chain。
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> The main methods exposed by chains are:
&lt;/span>&lt;span class="s2"> - `__call__`: Chains are callable. The `__call__` method is the primary way to
&lt;/span>&lt;span class="s2"> execute a Chain. This takes inputs as a dictionary and returns a
&lt;/span>&lt;span class="s2"> dictionary output.
&lt;/span>&lt;span class="s2"> 执行Chain的主要方式，输入是一个字典，输出也是一个字典。
&lt;/span>&lt;span class="s2"> - `run`: A convenience method that takes inputs as args/kwargs and returns the
&lt;/span>&lt;span class="s2"> output as a string or object. This method can only be used for a subset of
&lt;/span>&lt;span class="s2"> chains and cannot return as rich of an output as `__call__`.
&lt;/span>&lt;span class="s2"> 输入是args/kwargs，输出是字符串or对象。仅用于部分链，没有__call__方法通用。
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="o">............&lt;/span>
&lt;span class="n">memory&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMemory&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Optional memory object. Defaults to None.
&lt;/span>&lt;span class="s2"> Memory is a class that gets called at the start
&lt;/span>&lt;span class="s2"> and at the end of every chain. At the start, memory loads variables and passes
&lt;/span>&lt;span class="s2"> them along in the chain. At the end, it saves any returned variables.
&lt;/span>&lt;span class="s2"> There are many different types of memory - please see memory docs
&lt;/span>&lt;span class="s2"> for the full catalog.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">每个链开始和结束时调用。存储&lt;/span>&lt;span class="n">Memory&lt;/span>&lt;span class="err">。&lt;/span>
&lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exclude&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Optional list of callback handlers (or callback manager). Defaults to None.
&lt;/span>&lt;span class="s2"> Callback handlers are called throughout the lifecycle of a call to a chain,
&lt;/span>&lt;span class="s2"> starting with on_chain_start, ending with on_chain_end or on_chain_error.
&lt;/span>&lt;span class="s2"> Each custom chain can optionally call additional callback methods, see Callback docs
&lt;/span>&lt;span class="s2"> for full details.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">callback_manager&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseCallbackManager&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exclude&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Deprecated, use `callbacks` instead.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">verbose&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default_factory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">_get_verbosity&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Whether or not run in verbose mode. In verbose mode, some intermediate logs
&lt;/span>&lt;span class="s2"> will be printed to the console. Defaults to `langchain.verbose` value.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="err">开启后，输出更纤细的日志。&lt;/span>
&lt;span class="o">............&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LLMChain&lt;/strong>：创建了一个&lt;strong>步骤&lt;/strong>，该步骤可执行&lt;code>prompt&lt;/code>提示词。&lt;/li>
&lt;li>&lt;strong>run方法&lt;/strong>：调用LLMChain的&lt;code>run&lt;/code>方法，可以和LLM进行问答。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921094132633.png" alt="image-20230921094132633">&lt;/p>
&lt;h2 id="12顺序链">1.2.顺序链&lt;/h2>
&lt;h3 id="1源码解读">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>SequentialChain&lt;/strong>：继承于Chain，顺序链，允许将多个Chain链接起来，形成&lt;strong>Pipeline(流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/sequential.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">SequentialChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Chain where the outputs of one chain feed directly into next.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chains&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">input_variables&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">output_variables&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">return_all&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="err">……………………&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;strong>SimpleSequentialChain&lt;/strong>：继承于SequentialChain，一种简化版的顺序链，允许将多个Chain链接起来，形成&lt;strong>Pipeline(流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/sequential.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">SimpleSequentialChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Simple chain where the outputs of one step feed directly into next.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chains&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">strip_outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="n">input_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;input&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">output_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;output&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2simplesequentialchain的代码示例">(2)SimpleSequentialChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了LLM模仿医生对病情进行介绍，并给出治疗方案。&lt;/p>
&lt;ul>
&lt;li>通过&lt;strong>SimpleSequentialChain&lt;/strong>，连接了病情摘要Chain和病情评论Chain。&lt;/li>
&lt;li>病情摘要Chain的&lt;strong>输出&lt;/strong>，作为了病情评论Chain的&lt;strong>输入&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921095733390.png" alt="image-20230921095733390">&lt;/p>
&lt;h3 id="3sequentialchain的代码示例">(3)SequentialChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了LLM模仿医生对病情进行介绍，并给出治疗方案。&lt;/p>
&lt;p>不同的是，要支持多输入、多输出。&lt;/p>
&lt;ul>
&lt;li>通过&lt;strong>SequentialChain&lt;/strong>，连接了病情摘要Chain和病情评论Chain。&lt;/li>
&lt;li>病情摘要Chain的&lt;strong>输入有多个&lt;/strong>，病情摘要Chain的&lt;strong>输出&lt;/strong>作为了病情评论Chain的&lt;strong>输入&lt;/strong>，病情评论Chain的&lt;strong>输出有多个&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921100917222.png" alt="image-20230921100917222">&lt;/p>
&lt;h2 id="13决策链">1.3.决策链&lt;/h2>
&lt;h3 id="1源码解读-1">(1)源码解读&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>RouterChain&lt;/strong>：继承于Chain，决策链，允许将多个Chain链接起来，可以实现条件判断的&lt;strong>分支Pipeline(分支流水线)&lt;/strong>。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/chains/router/base.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">RouterChain&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Chain&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Chain that outputs the name of a destination chain and the inputs to it.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">output_keys&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2"> Route inputs to a destination chain.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Args:
&lt;/span>&lt;span class="s2"> inputs: inputs to the chain
&lt;/span>&lt;span class="s2"> callbacks: callbacks to use for the chain
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Returns:
&lt;/span>&lt;span class="s2"> a Route object
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">async&lt;/span> &lt;span class="k">def&lt;/span> &lt;span class="nf">aroute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callbacks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">await&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">acall&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">Route&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;destination&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;next_inputs&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2routerchain的代码示例">(2)RouterChain的代码示例&lt;/h3>
&lt;p>我们来看如下的代码，实现了根据问题内容，选择程序员角色的AI或测试工程师AI出来回答问题。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>创建两个决策分支&lt;/strong>：能回答软件开发问题的程序员、能回答软件测试问题的程序员。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921102913545.png" alt="image-20230921102913545">&lt;/p>
&lt;ul>
&lt;li>创建根据问题内容进行决策的&lt;strong>RouterChain&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921102959963.png" alt="image-20230921102959963">&lt;/p>
&lt;p>这个决策链是如何实现的决策功能呢？进一步看一下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>destinations_str&lt;/strong>：描述了根据问题内容，期望选择哪个AI来回答问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103109653.png" alt="image-20230921103109653">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>MULTI_PROMPT_ROUTER_TEMPLATE&lt;/strong>：LangChain提供了决策链的提示词模板。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103154608.png" alt="image-20230921103154608">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>router_template&lt;/strong>：最终的决策链提示词为：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103225927.png" alt="image-20230921103225927">&lt;/p>
&lt;ul>
&lt;li>将决策链和两个回答问题的链进行连接。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103257969.png" alt="image-20230921103257969">&lt;/p>
&lt;ul>
&lt;li>测试一下关于软件开发的问题：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103327682.png" alt="image-20230921103327682">&lt;/p>
&lt;ul>
&lt;li>测试一下关于软件测试的问题：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-LangChain%E4%B9%8BChain%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A13/image-20230921103359806.png" alt="image-20230921103359806">&lt;/p>
&lt;h1 id="2小结">2.小结&lt;/h1>
&lt;p>本文阐述了Chain模块的内部实现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Chain类&lt;/strong>：抽象了&lt;strong>PipeLine流水线&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>顺序链&lt;/strong>：SequentialChain，实现了多输入多输出的串行执行的工作流。&lt;/li>
&lt;li>&lt;strong>决策链&lt;/strong>：RouterChain，实现了根据问题内容，选择工作流走向的能力。&lt;/li>
&lt;li>Chain类拥有很多子类，实现不同的业务流程，可以根据实战需要继续阅读源码和实践。&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记14-LangChain之Memory，对LLM的抽象2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</link><pubDate>Tue, 19 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</guid><description>&lt;p>我们继续写点儿偏工程实践的内容——LangChain的核心模块2——Chain。&lt;/p>
&lt;h1 id="1核心模块2memory">1.核心模块2：Memory&lt;/h1>
&lt;p>实现一个问答系统，通常需要将历史上的问题和答案，作为本次问题的上下文。&lt;/p>
&lt;p>因此，LangChain提供了Memory模块，这个模块对记忆进行了抽象：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>.当用户提出问题时，LangChain会去读&lt;strong>Memory&lt;/strong>，获得过去的消息&lt;strong>past_messages&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>.LangChain构造提示词，格式为&amp;rdquo;&lt;strong>{past_messages}{question}&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>.LLM进行回答后，得到答案**{answer:&amp;hellip;}**。&lt;/li>
&lt;li>STEP4.LangChain将本次的答案**{answer:&amp;hellip;}**，**写入Memory**。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920062809577.png" alt="image-20230920062809577">&lt;/p>
&lt;p>LangChain提供了多种ChatMessageHistory、ChatMemory，我们接下来详细解读。&lt;/p>
&lt;h2 id="11chatmessagehistory">1.1.ChatMessageHistory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：BaseChatMessageHistory &amp;ndash;&amp;gt; &lt;name>ChatMessageHistory # Example: ZepChatMessageHistory&lt;/li>
&lt;li>&lt;strong>BaseChatMessageHistory&lt;/strong>：聊天消息历史记录的基类，定义了一系列方法，由子类实现&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/schema/chat_history.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920064148308.png" alt="image-20230920064148308">&lt;/p>
&lt;h2 id="12basememory">1.2.BaseMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>类的继承关系&lt;/strong>：BaseMemory &amp;ndash;&amp;gt; BaseChatMemory &amp;ndash;&amp;gt; &lt;name>Memory # Examples: ZepMemory, MotorheadMemory&lt;/li>
&lt;li>&lt;strong>BaseMemory&lt;/strong>：Memory基类，定义了一系列方法，由子类实现&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/schema/memory.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">BaseMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Serializable&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for memory in Chains.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> Memory refers to state in Chains. Memory can be used to store information about
&lt;/span>&lt;span class="s2"> past executions of a Chain and inject that information into the inputs of
&lt;/span>&lt;span class="s2"> future executions of the Chain. For example, for conversational Chains Memory
&lt;/span>&lt;span class="s2"> can be used to store conversations and automatically add them to future model
&lt;/span>&lt;span class="s2"> prompts so that the model has the necessary context to respond coherently to
&lt;/span>&lt;span class="s2"> the latest input.
&lt;/span>&lt;span class="s2">这里的内存指的是Chains中的状态。内存可以用来存储Chain过去执行的信息，并将信息注入到Chain的未来执行的输入中。
&lt;/span>&lt;span class="s2">例如：对于会话型Chains，内存可以用来存储会话，并自动将它们添加到未来的模型提示词中，以便模型具有必要的上下文来连贯地响应最新的输入。
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> class Config:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Configuration&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">pydantic&lt;/span> &lt;span class="nb">object&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2"> 使用pydantic库，并在本类中定义抽象方法，待子类实现
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> arbitrary_types_allowed = True
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @property
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def memory_variables(self) -&amp;gt; List[str]:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">The&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">keys&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">memory&lt;/span> &lt;span class="k">class&lt;/span> &lt;span class="nc">will&lt;/span> &lt;span class="n">add&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">chain&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def load_memory_variables(self, inputs: Dict[str, Any]) -&amp;gt; Dict[str, Any]:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Return&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="n">pairs&lt;/span> &lt;span class="n">given&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">text&lt;/span> &lt;span class="nb">input&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">chain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -&amp;gt; None:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Save&lt;/span> &lt;span class="n">the&lt;/span> &lt;span class="n">context&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">this&lt;/span> &lt;span class="n">chain&lt;/span> &lt;span class="n">run&lt;/span> &lt;span class="n">to&lt;/span> &lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> @abstractmethod
&lt;/span>&lt;span class="s2"> def clear(self) -&amp;gt; None:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>&lt;span class="n">Clear&lt;/span> &lt;span class="n">memory&lt;/span> &lt;span class="n">contents&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="13basechatmemory">1.3.BaseChatMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>BaseChatMemory&lt;/strong>：BaseMemoryd的子类，实现了一部分通用方法，剩余由子类扩展&lt;/li>
&lt;li>BaseChatMemory维护了ChatMessageHistory&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/chat_memory.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">BaseChatMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseMemory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ABC&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Abstract base class for chat memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">chat_memory&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">BaseChatMessageHistory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">default_factory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">ChatMessageHistory&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">output_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="n">input_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">None&lt;/span>
&lt;span class="n">return_messages&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">False&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_get_input_output&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prompt_input_key&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">output_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Save context from this conversation to buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">input_str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_get_input_output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_user_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_str&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add_ai_message&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output_str&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Clear memory contents.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来就可以看一下常用的几种Memory了。&lt;/p>
&lt;h2 id="14conversationbuffermemory">1.4.ConversationBufferMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationBufferMemory&lt;/strong>：一种Memory的具体实现。提供了记录历史聊天记录的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/buffer.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationBufferMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer for storing conversation memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Human&amp;#34;&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;AI&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;String buffer of memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_messages&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_str&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a string in case return_messages is True.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a list of messages in case return_messages is False.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们再来看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920072953322.png" alt="image-20230920072953322">&lt;/p>
&lt;ul>
&lt;li>针对第一个问题，LangChain发送给LLM真实的问题如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920073630423.png" alt="image-20230920073630423">&lt;/p>
&lt;ul>
&lt;li>针对第二个问题，LangChain会把&lt;strong>第一次发给LLM的问题和答案&lt;/strong>+&lt;strong>第二次的问题&lt;/strong>发送给LLM：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920073804453.png" alt="image-20230920073804453">&lt;/p>
&lt;h2 id="15conversationbufferwindowmemory">1.5.ConversationBufferWindowMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationBufferWindowMemory&lt;/strong>：一种Memory的具体实现。提供了带有滑动窗口的记录历史聊天记录的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/buffer_window.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationBufferWindowMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer for storing conversation memory inside a limited size window.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Human&amp;#34;&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;AI&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span> &lt;span class="c1">#: :meta private:&lt;/span>
&lt;span class="n">k&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Number of messages to store in buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Union&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;String buffer of memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_messages&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer_as_str&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a string in case return_messages is True.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">messages&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">messages&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer_as_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Exposes the buffer as a list of messages in case return_messages is False.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074018406.png" alt="image-20230920074018406">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第一次问答&lt;/strong>：LangChain真实发送的问题只有&amp;quot;你好&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074115794.png" alt="image-20230920074115794">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第二次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第一次问题答案&lt;/strong>+&lt;strong>第二次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074225759.png" alt="image-20230920074225759">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第三次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第一次问题答案&lt;/strong>+&lt;strong>第二次问题答案&lt;/strong>+&lt;strong>第三次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074328411.png" alt="image-20230920074328411">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>第四次问答&lt;/strong>：LangChain真实发送的问题是&lt;strong>第二次问题答案&lt;/strong>+&lt;strong>第三次问题答案&lt;/strong>+&lt;strong>第四次问题&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074412212.png" alt="image-20230920074412212">&lt;/p>
&lt;p>为什么会有近3次问答内容的限制呢？因为初始化ConversationBufferWindowMemory时，设置了&lt;code>k=2&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074524136.png" alt="image-20230920074524136">&lt;/p>
&lt;h2 id="16conversationsummarybuffermemory">1.6.ConversationSummaryBufferMemory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ConversationSummaryBufferMemory&lt;/strong>：一种Memory的具体实现。提供了记录历史聊天记录，并对历史聊天记录进行归纳总结的能力。&lt;/li>
&lt;li>代码路径/libs/langchain/langchain/memory/summary_buffer.py，&lt;strong>详细源码如下&lt;/strong>：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">ConversationSummaryBufferMemory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BaseChatMemory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">SummarizerMixin&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Buffer with summarizer for storing conversation memory.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2000&lt;/span>
&lt;span class="n">moving_summary_buffer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;history&amp;#34;&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="nd">@property&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Will always return list of memory variables.
&lt;/span>&lt;span class="s2">
&lt;/span>&lt;span class="s2"> :meta private:
&lt;/span>&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">load_memory_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Return history buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">buffer&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">first_messages&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">BaseMessage&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">summary_message_cls&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">]&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">first_messages&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nb">buffer&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">return_messages&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">final_buffer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Any&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">buffer&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">final_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_buffer_string&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="nb">buffer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">human_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">human_prefix&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ai_prefix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ai_prefix&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">memory_key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">final_buffer&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="nd">@root_validator&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">validate_prompt_input_variables&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">cls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Validate that prompt input variables are consistent.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="n">prompt_variables&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;prompt&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_variables&lt;/span>
&lt;span class="n">expected_keys&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;summary&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;new_lines&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">expected_keys&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prompt_variables&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">raise&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s2">&amp;#34;Got unexpected prompt input variables. The prompt expects &amp;#34;&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="s2">&amp;#34;{prompt_variables}, but it should have {expected_keys}.&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">values&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Any&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Dict&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Save context from this conversation to buffer.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">save_context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prune&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">prune&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Prune buffer if it exceeds max token limit&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">messages&lt;/span>
&lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">llm&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_num_tokens_from_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">pruned_memory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_token_limit&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">pruned_memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">curr_buffer_length&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">llm&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_num_tokens_from_messages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">buffer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict_new_summary&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">pruned_memory&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="bp">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Clear memory contents.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moving_summary_buffer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们看一个例子：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-LangChain%E4%B9%8BMemory%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A12/image-20230920074831714.png" alt="image-20230920074831714">&lt;/p>
&lt;ul>
&lt;li>从这段代码的输出，可以看到LangChain对历史问题和答案进行了概括总结：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>&lt;span class="err">&amp;#39;history&amp;#39;:&lt;/span> &lt;span class="err">[&lt;/span>
&lt;span class="err">SystemMessage(content=&amp;#39;\nThe&lt;/span> &lt;span class="err">human&lt;/span> &lt;span class="err">asks&lt;/span> &lt;span class="err">what&lt;/span> &lt;span class="err">the&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">thinks&lt;/span> &lt;span class="err">of&lt;/span> &lt;span class="err">artificial&lt;/span> &lt;span class="err">intelligence.&lt;/span> &lt;span class="err">The&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">thinks&lt;/span> &lt;span class="err">artificial&lt;/span> &lt;span class="err">intelligence&lt;/span> &lt;span class="err">is&lt;/span> &lt;span class="err">a&lt;/span> &lt;span class="err">force&lt;/span> &lt;span class="err">for&lt;/span> &lt;span class="err">good&lt;/span> &lt;span class="err">because&lt;/span> &lt;span class="err">it&lt;/span> &lt;span class="err">will&lt;/span> &lt;span class="err">help&lt;/span> &lt;span class="err">humans&lt;/span> &lt;span class="err">reach&lt;/span> &lt;span class="err">their&lt;/span> &lt;span class="err">full&lt;/span> &lt;span class="err">potential.&lt;/span> &lt;span class="err">The&lt;/span> &lt;span class="err">human&lt;/span> &lt;span class="err">then&lt;/span> &lt;span class="err">asks&lt;/span> &lt;span class="err">what&lt;/span> &lt;span class="err">LLM&lt;/span> &lt;span class="err">is,&lt;/span> &lt;span class="err">to&lt;/span> &lt;span class="err">which&lt;/span> &lt;span class="err">the&lt;/span> &lt;span class="err">AI&lt;/span> &lt;span class="err">responds&lt;/span> &lt;span class="err">that&lt;/span> &lt;span class="err">it&lt;/span> &lt;span class="err">stands&lt;/span> &lt;span class="err">for&lt;/span> &lt;span class="err">Large&lt;/span> &lt;span class="err">Language&lt;/span> &lt;span class="err">Model,&lt;/span> &lt;span class="err">and&lt;/span> &lt;span class="err">provides&lt;/span> &lt;span class="err">a&lt;/span> &lt;span class="err">list&lt;/span> &lt;span class="err">of&lt;/span> &lt;span class="err">LLM&lt;/span> &lt;span class="err">models,&lt;/span> &lt;span class="err">including&lt;/span> &lt;span class="err">GPT-3,&lt;/span> &lt;span class="err">GPT-J-6B,&lt;/span> &lt;span class="err">CLIP,&lt;/span> &lt;span class="err">BERT,&lt;/span> &lt;span class="err">and&lt;/span> &lt;span class="err">T5.&amp;#39;,&lt;/span>
&lt;span class="err">additional_kwargs={&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="err">)&lt;/span>
&lt;span class="err">]}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="2小结">2.小结&lt;/h1>
&lt;p>本文阐述了Memory模块的内部实现：&lt;/p>
&lt;ul>
&lt;li>ChatMessageHistory：提供记录历史聊天记录的对象&lt;/li>
&lt;li>BaseChatMemory：维护1个ChatMessageHistory对象，并对外提供CRUD历史聊天记录的接口&lt;/li>
&lt;li>ConversationBufferMemory：BaseChatMemory的1种子类，对外提供最终的CRUD历史聊天记录的接口&lt;/li>
&lt;li>ConversationBufferWindowMemory：在ConversationBufferMemory的基础上，提供了滑动窗口能力&lt;/li>
&lt;li>ConversationSummaryBufferMemory：在ConversationBufferMemory的基础上，提供了历史聊天记录的摘要能力&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</link><pubDate>Mon, 18 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</guid><description>&lt;h1 id="1问题">1.问题&lt;/h1>
&lt;p>在《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》中，我们实现了Seq2Seq，看到了编码器-解码器架构的诸多优势。&lt;/p>
&lt;p>但，Seq2Seq也有不完美的地方：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>长距离依赖问题&lt;/strong>：读了后面，忘了前面。&lt;/li>
&lt;li>&lt;strong>信息压缩问题&lt;/strong>：Seq2Seq的上下文向量是固定长度的，很难将无限的信息压缩到有限长度的向量中。&lt;/li>
&lt;/ul>
&lt;p>这个视频，体现了长距离依赖问题和信息压缩问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>长距离依赖问题&lt;/strong>：沈腾的几个问题，对于老大爷，都是听了后面忘了前面。&lt;/li>
&lt;li>&lt;strong>信息压缩问题&lt;/strong>：沈腾的每个问题，对于老大爷，字数都太多。&lt;/li>
&lt;/ul>
&lt;iframe src="//player.bilibili.com/player.html?aid=961160816&amp;bvid=BV1tH4y1S7Uu&amp;cid=1271780225&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px autoplay=0> &lt;/iframe>
&lt;p>&lt;font color=red>&lt;strong>长距离问题、信息压缩问题的本质，都是信息损失的问题&lt;/strong>。&lt;/font>&lt;/p>
&lt;h1 id="2注意力机制的核心思想">2.注意力机制的核心思想&lt;/h1>
&lt;p>理解了Seq2Seq的问题，我们会很自然地产生一种思路：如果将原始信息化繁为简，喂给LLM的是有效信息，而不是全量信息，是否可以解决长距离问题、信息压缩问题？&lt;/p>
&lt;p>我们再来看一段视频：&lt;/p>
&lt;ul>
&lt;li>从全量信息看，&amp;ldquo;关键问题&amp;quot;四个字高频出现。&lt;/li>
&lt;li>从有效信息看，这么长一段话也就是一句有效信息——&amp;ldquo;关键问题很重要&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>在人类世界，这些没用的废话叫艺术。在AI的世界，这些没用的废话叫干扰。&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919082648294.png" alt="image-20230919082648294">&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=661221372&amp;bvid=BV1Vh4y1a7uD&amp;cid=1271806425&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px autoplay=0> &lt;/iframe>
&lt;p>从直觉上理解，人类接收信息时，会做两件事：&lt;/p>
&lt;ul>
&lt;li>关注&lt;strong>关键信息&lt;/strong>。&lt;/li>
&lt;li>关注&lt;strong>不同维度的关键信息&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>比如：《八佰》这张海报，你看到了什么？&lt;/p>
&lt;ul>
&lt;li>&lt;strong>四行仓库&lt;/strong>：它是海报的背景，为什么会吸引观众的注意？&lt;/li>
&lt;li>&lt;strong>残垣断壁&lt;/strong>：正常的第一眼感觉是这里应该经历过多轮惨烈战斗，为什么我们不会关注断壁、废楼、电线杆的破坏程度？为什么我们不会关注零落的士兵？&lt;/li>
&lt;li>&lt;strong>对比鲜明&lt;/strong>：四行仓库和残垣断壁对比鲜明，我们应该可以感受到导演想表达的一种情绪。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919083211415.png" alt="image-20230919083211415">&lt;/p>
&lt;p>上述这些，就是&lt;font color=red>&lt;strong>注意力&lt;/strong>&lt;/font>，而且是&lt;font color=red>&lt;strong>不同维度的注意力&lt;/strong>&lt;/font>：&lt;/p>
&lt;ul>
&lt;li>当有人问&amp;rdquo;《八佰》这部电影发生的地点在哪里？&amp;quot;，我们的注意力在&amp;rdquo;&lt;strong>四行仓库&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>当游人问&amp;rdquo;《八佰》这部电影想表达什么主题？&amp;quot;，我们的注意力在&amp;quot;四行仓库与残垣断壁&amp;quot;的&lt;strong>对比鲜明&lt;/strong>——表现&amp;quot;八佰&amp;quot;勇士保卫上海最后一寸土地的英勇决绝。&lt;/li>
&lt;/ul>
&lt;p>论文《Attention Mechanisms in Computer Vision: A Survey》，更加形象化地展示了注意力在计算机视觉领域的实验效果：&lt;/p>
&lt;ul>
&lt;li>我们可以发现，加入了注意力机制的AI，会更加关注原始图像中的关键要素。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919084959023.png" alt="image-20230919084959023">&lt;/p>
&lt;h1 id="3注意力原理解读">3.注意力原理解读&lt;/h1>
&lt;p>接下来，我们尝试一下用代码实现一下直觉上理解的注意力机制。&lt;/p>
&lt;h2 id="31点积注意力">3.1.点积注意力&lt;/h2>
&lt;p>首先，回顾一下点积的定义，特别关注一下点积的代数表达：&lt;/p>
&lt;blockquote>
&lt;p>在数学中，&lt;strong>点积&lt;/strong>又称&lt;strong>数量积&lt;/strong>或&lt;strong>标量积&lt;/strong>，是一种接受两串等长的数字序列（通常是坐标向量）、返回单一数字的代数运算)。在欧几里得几何，两条笛卡尔坐标向量的点积常称为&lt;strong>内积&lt;/strong>。&lt;/p>
&lt;p>从代数角度看，先求两数字序列中每组对应元素的积，再求所有积之和，结果即为点积。&lt;/p>
&lt;p>从几何角度看，点积则是两向量的长度与它们夹角余弦的积。这两种定义在笛卡尔坐标系中等价。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919085940309.png" alt="image-20230919085940309">&lt;/p>
&lt;p>然后，我们再来看看&lt;strong>点积注意力&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：向量X1表示帅哥，假设向量X2表示美女。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：计算X1和X2的点积&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919090930858.png" alt="image-20230919090930858">，这个叫&lt;strong>原始权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对原始权重进行softmax&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919091338872.png" alt="image-20230919091338872">，得到&lt;strong>归一化注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：计算归一化注意力与X2的加权和&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092013095.png" alt="image-20230919092013095">，得到X1对X2的&lt;strong>最终注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>结果&lt;/strong>：经过上述代数计算，最终注意力的信息主体肯定是X1，最终注意力还包含了X2的信息。因此可以这么理解：
&lt;ul>
&lt;li>向量X1表示：没有坠入爱河的帅哥。&lt;/li>
&lt;li>最终向量表示：心中有美女的，坠入爱河的帅哥。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092053498.png" alt="image-20230919092053498">&lt;/p>
&lt;p>运行结果：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919092401876.png" alt="image-20230919092401876">&lt;/p>
&lt;h2 id="32缩放点积注意力">3.2.缩放点积注意力&lt;/h2>
&lt;p>在&amp;quot;3.1.点积注意力&amp;quot;的STEP2中，对原始权重进行softmax，可能由于原始权重过大导致梯度过小甚至梯度消失。&lt;/p>
&lt;p>因此，缩放点积注意力的本质是对原始权重除以一个系数后，再进行softmax，具体如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>假设&lt;/strong>：向量X1表示帅哥，假设向量X2表示美女。&lt;/li>
&lt;li>&lt;strong>STEP1&lt;/strong>：计算X1和X2的点积&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919090930858.png" alt="image-20230919090930858">，这个叫&lt;strong>原始权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对原始权重缩放&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105751443.png" alt="image-20230919105751443">，得到&lt;strong>缩放权重&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP2&lt;/strong>：对&lt;strong>缩放权重&lt;/strong>进行softmax&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105527069.png" alt="image-20230919105527069">，得到&lt;strong>归一化注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>STEP3&lt;/strong>：计算归一化注意力与X2的加权和&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105715585.png" alt="image-20230919105715585">，得到X1对X2的&lt;strong>最终注意力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>结果&lt;/strong>：经过上述代数计算，最终注意力的信息主体肯定是X1，最终注意力还包含了X2的信息。因此可以这么理解：
&lt;ul>
&lt;li>向量X1表示：没有坠入爱河的帅哥。&lt;/li>
&lt;li>最终向量表示：心中有美女的，坠入爱河的帅哥。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105832012.png" alt="image-20230919105832012">&lt;/p>
&lt;p>运行结果：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919105847714.png" alt="image-20230919105847714">&lt;/p>
&lt;h2 id="33解码器-编码器注意力">3.3.解码器-编码器注意力&lt;/h2>
&lt;p>有了&lt;strong>缩放点积注意力&lt;/strong>，我们就可以尝试一下：&lt;strong>对编码器-解码器架构&lt;/strong>增加&lt;strong>注意力机制&lt;/strong>了。&lt;/p>
&lt;p>回顾一下《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》，我们详细分析了各个时间点上编码器、解码器的处理流程。截取t5和t6时刻：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t5时刻&lt;/strong>：第一个解码器的输入是&lt;strong>编码器输出的上下文向量&lt;/strong>+&lt;strong>TeachForcing的第一个单词&lt;/strong>Who，第一个解码器的输出是最终答案的首词Who+&lt;strong>隐藏状态&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：第二个解码器的输入是&lt;strong>第一个解码器的隐藏状态&lt;/strong>+&lt;strong>TeachForcing的第二个单词&lt;/strong>are+第一个解码器预测的最终答案的首词Who。&lt;/li>
&lt;/ul>
&lt;p>我们可以看出，每一个解码器的输入本质包含了三要素：&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>&lt;strong>问题是什么&lt;/strong>&lt;/font>：每个编码器输出的隐藏状态，但隐藏状态无法记忆太长的问题序列，所以&lt;strong>每个解码器只知道问题的只言片语&lt;/strong>。&lt;/li>
&lt;li>&lt;font color=red>&lt;strong>参考答案是什么&lt;/strong>&lt;/font>：TeachForcing会告诉每个解码器，参考答案是什么。&lt;/li>
&lt;li>&lt;font color=red>&lt;strong>上一个解码器的答案是什么&lt;/strong>&lt;/font>：每个解码器都能知道上一个解码器预测的答案的一部分是什么。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919110424183.png" alt="image-20230919110424183">&lt;/p>
&lt;p>思考一下：注意力机制，可以&lt;strong>如何优化编码器-解码器架构&lt;/strong>？&lt;/p>
&lt;p>显然，只有优化&amp;quot;问题是什么&amp;quot;这个要素——如果我们将第一个解码器增加注意力层，就可以让输出的隐藏状态注意编码器输出的上下文向量——这样每个解码器就&lt;font color=red>&lt;strong>能用有限长度的向量&lt;/strong>&lt;/font>了解&lt;font color=red>&lt;strong>原始问题中最关键的部分&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161621858.png" alt="image-20230919161621858">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161638084.png" alt="image-20230919161638084">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161652216.png" alt="image-20230919161652216">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161724555.png" alt="image-20230919161724555">&lt;/p>
&lt;p>运行结果如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919161901056.png" alt="image-20230919161901056">&lt;/p>
&lt;p>写到这里，我们要稍微小结一下(避免陷入术语、概念、数学公式的细节中)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>神经网络架构&lt;/strong>：实现Seq2Seq的&lt;strong>编码器-解码器&lt;/strong>架构，是神经网络架构中的一种。&lt;/li>
&lt;li>&lt;strong>注意力机制&lt;/strong>：注意力机制是一种专项技术，也有很多种实现，无论是哪一种，都是在&lt;strong>解决神经网络架构的长距离问题、信息压缩问题&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>如果您对&lt;strong>编码器-解码器&lt;/strong>这种&lt;strong>神经网络架构&lt;/strong>的细节记不太清，可以回看这篇《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》。&lt;/p>
&lt;p>如果您对&lt;strong>注意力机制&lt;/strong>这种专项技术的细节记不太清，可以回看本文的&lt;strong>点积注意力、缩放点积注意力、编码器-解码器注意力&lt;/strong>章节。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174642861.png" alt="image-20230919174642861">&lt;/p>
&lt;p>当我们对上述概念有了基本认识后，我们也就可以理解，自1990年注意力机制提出后，有很多科学家在思考：&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>有没有一种&lt;strong>更好的神经网络架构&lt;/strong>，充分发挥这种&lt;strong>仿人类的注意力机制&lt;/strong>的独特优势&lt;/font>？&lt;/li>
&lt;/ul>
&lt;p>于是如雷贯耳的Transformer架构诞生了，于是如雷贯耳的论文《Attention Is All Your Need》诞生了。&lt;/p>
&lt;p>笔者不在本文展开描述Transformer的实现原理，但我们可以在本文接下来的部分解读一下QKV、自注意力、多头注意力等Transformer架构中与注意力机制强相关的技术点，为后续解读Transformer做知识铺垫。&lt;/p>
&lt;h2 id="34qkv">3.4.QKV&lt;/h2>
&lt;p>QKV注意力(query-key-value attention)是注意力机制中的一种变体。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919165526267.png" alt="image-20230919165526267">&lt;/p>
&lt;p>上图来自于维基百科，但维基百科对于QKV的介绍非常详细，但笔者试图给出这些过于理论化的QKV理论的通俗解释：&lt;/p>
&lt;ul>
&lt;li>注意力机制的本质是&lt;strong>让向量X1关注向量X2&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>Q&lt;/strong>uery，就是&lt;strong>向量X1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>K&lt;/strong>ey、&lt;strong>V&lt;/strong>alue，就是&lt;strong>向量X2&lt;/strong>。Key和Value的区别，仅仅是数学公式的差异(注意力权重、注意力权重加权和)。&lt;/li>
&lt;li>&lt;strong>向量X1&lt;/strong>，也就是&lt;strong>Q&lt;/strong>uery，在编码器-解码器架构中，就是&lt;strong>解码器输出的隐藏状态&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>向量X2&lt;/strong>，也就是&lt;strong>K&lt;/strong>ey或者&lt;strong>V&lt;/strong>alue，在编码器-解码器架构中，就是&lt;strong>编码器输出的上下文向量&lt;/strong>。&lt;/li>
&lt;li>说白了，就是让&lt;strong>每个解码器的输出&lt;/strong>都关注&lt;strong>编码器输出的上下文向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>我们来看一下代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919171621721.png" alt="image-20230919171621721">&lt;/p>
&lt;h2 id="35自注意力">3.5.自注意力&lt;/h2>
&lt;p>自注意力的理论也非常抽象，笔者试图给出这些理论的通俗解释：&lt;/p>
&lt;ul>
&lt;li>注意力机制的本质是&lt;strong>让向量X1关注向量X2&lt;/strong>。自注意力的本质是&lt;strong>自己对自己&lt;/strong>的注意。&lt;/li>
&lt;li>&lt;strong>Q&lt;/strong>uery，就是&lt;strong>向量X1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>K&lt;/strong>ey、&lt;strong>V&lt;/strong>alue，也是&lt;strong>向量X2&lt;/strong>。而向量X2的信息本质还是向量X1，只是线性变换的公式不同而已。&lt;/li>
&lt;/ul>
&lt;p>我们来看看实现自注意力的代码：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919172737253.png" alt="image-20230919172737253">&lt;/p>
&lt;h2 id="36多头注意力">3.6.多头注意力&lt;/h2>
&lt;ul>
&lt;li>理解了QKV注意力，那么多头注意力仅仅是从更多维度对信息进行点积、缩放、Softmax、加权和而已。&lt;/li>
&lt;/ul>
&lt;p>具体代码如下：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919173225726.png" alt="image-20230919173225726">&lt;/p>
&lt;h1 id="4总结">4.总结&lt;/h1>
&lt;ul>
&lt;li>本文通过介绍点积注意力、缩放点积注意力，进而解读了如何在编码器-解码器神经网络架构中，增加注意力机制。
&lt;ul>
&lt;li>本文的依据源于论文《An Attentive Survey of Attention Models》(&lt;a href="https://arxiv.org/abs/1904.02874">https://arxiv.org/abs/1904.02874&lt;/a>)&lt;/li>
&lt;li>本文试图通过通俗的比喻、实际的代码，解读论文中复杂、抽象的算法流程。如下图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174833717.png" alt="image-20230919174833717">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>本文进一步解读了QKV注意力、自注意力、多头注意力，为后续解读Transformer神经网络架构(如下图)奠定基础知识。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-Transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/image-20230919174725878.png" alt="image-20230919174725878">&lt;/p>
&lt;p>解读GPT背后的论文及实现技术实属不易，&lt;/p>
&lt;p>从N-Gram到词嵌入，再到神经概率语言模型，再到Seq2Seq，直到本文的注意力机制，&lt;/p>
&lt;p>我们距离自行实现一个简化版大语言模型越来越近了，同时笔者也受益匪浅。&lt;/p>
&lt;p>欢迎各位小伙伴探讨交流，我们继续探索这个有趣的技术领域！&lt;/p></description></item><item><title>【chatGPT】学习笔记12-昇腾计算产业发展白皮书解读</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</link><pubDate>Tue, 12 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</guid><description>&lt;p>本文来解读华为的《昇腾计算产业发展白皮书》，跟踪一下国内AI行业的宏观动态。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912224057905.png" alt="image-20230912224057905">&lt;/p>
&lt;h2 id="1ai发展趋势和挑战">1.AI发展趋势和挑战&lt;/h2>
&lt;h3 id="11ai发展趋势">1.1.AI发展趋势&lt;/h3>
&lt;p>白皮书首先阐述了AI发展趋势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI已成为推动社会发展的关键引擎&lt;/strong>：
&lt;ul>
&lt;li>AI在诸多特定领域超过人类能力。如：计算机视觉、语音识别、自然语言处理领域。&lt;/li>
&lt;li>AI将助力各产业实现智能化转型升级。根据弗若斯特沙利文数据，2019年中国AI市场规模为598.6亿元，2020~2024年复合增长率达34.8%。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>AI处于爆发式创新的前夜&lt;/strong>：
&lt;ul>
&lt;li>联接、AI、云、计算、行业应用等多种先进技术和机会将会互相催化、有机融合。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="12ai产业挑战">1.2.AI产业挑战&lt;/h3>
&lt;p>白皮书再阐述了AI产业的挑战：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>计算系统&lt;/strong>要满足AI场景的&lt;strong>复杂巨大、多样性的计算需求&lt;/strong>。
&lt;ul>
&lt;li>&lt;strong>算力增速快&lt;/strong>：2012~2018年，算力需求增加30万倍，远超摩尔定律。大语言模型时代，算力需求从TFLOPS级别，增至PFLOPS级别，甚至EFLOPS级别。&lt;/li>
&lt;li>&lt;strong>计算架构设计面临挑战&lt;/strong>：大规模算力需求，对计算系统的&lt;strong>计算性能、通信性能、可扩展性&lt;/strong>是巨大的挑战。&lt;/li>
&lt;li>&lt;strong>全方位面临挑战&lt;/strong>：基础软件、编程模型、编程语言、编译器、工具链、大规模运行时、调度系统、平台软件、通信组件、加速组件、加速引擎、AI框架、行业软件，都需要适配大规模算力需求。&lt;/li>
&lt;li>参考：https://openai.com/research/ai-and-compute&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/ai-and-compute-all.png" alt="ai-and-compute-all">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从算法到产品化落地&lt;/strong>面临8大鸿沟：
&lt;ul>
&lt;li>&lt;strong>模型获取&lt;/strong>鸿沟：针对行业数据，选择、测试合适地模型，需要巨大的时间成本和算力成本。&lt;/li>
&lt;li>&lt;strong>数据准备&lt;/strong>鸿沟：构建能够真实反映实际业务数据分布的数据集，面临较大挑战。&lt;/li>
&lt;li>&lt;strong>模型训练&lt;/strong>鸿沟：超参数调优等模型训练环节，对大量传统行业开发者挡在AI门外。&lt;/li>
&lt;li>&lt;strong>准确度验证&lt;/strong>鸿沟：模型泛化能力的验证，是阻碍算法快速落地的因素之一。&lt;/li>
&lt;li>&lt;strong>应用开发&lt;/strong>鸿沟：AI需要接受包含行业知识的各种输入数据，对AI应用开发系统产生整合要求，开发效率是重要影响因素之一。&lt;/li>
&lt;li>&lt;strong>NPU性能&lt;/strong>鸿沟：AI算力真正转化为行业生产率，实际的运行性能将决定系统最终的性价比和业务执行能力。&lt;/li>
&lt;li>&lt;strong>业务流程监控&lt;/strong>鸿沟：确保AI系统在业务环境的持续准确运行，是行业应用的重中之重。&lt;strong>AI算法需要具备持续更新、增量学习的能力&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>适配开发&lt;/strong>鸿沟：面对不同业务场景，需要以服务化和API形式封装AI算法，封装AI服务、部署AI服务于复杂的计算系统上，都将面临挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912232100695.png" alt="image-20230912232100695">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>可信AI&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI具有自我演化潜力&lt;/strong>：人类对于AI核心运行机制、AI隐式编程、学习能力，依然没有彻底研究清楚。&lt;/li>
&lt;li>&lt;strong>可信AI是大规模商用的基础&lt;/strong>：保护隐私、避免偏见、防止滥用、可靠边界、可解释、鲁棒性、防攻击等方面，都是AI能够真正大规模应用的挑战。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>人才需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>AI人才缺口巨大&lt;/strong>：中国AI人才缺口500万，供求比1:10。&lt;/li>
&lt;li>&lt;strong>与美国差距巨大&lt;/strong>：中国的AI人才数量仅为美国四分之一。&lt;/li>
&lt;li>&lt;strong>新一代AI发展规划&lt;/strong>：国务院《新一代AI发展规划》发布，明确指出建立AI高端人才队伍是AI发展重中之重。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2昇腾计算产业">2.昇腾计算产业&lt;/h2>
&lt;h3 id="21昇腾计算产业体系和定位">2.1.昇腾计算产业体系和定位&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>昇腾计算产业覆盖全产业链，全生态链&lt;/strong>，包括：昇腾处理器、硬件、CANN(Compute Architecture for Neural Networks，异构计算架构)、AI计算框架、应用使能、开发工具链、运维工具、行业应用及服务。&lt;/li>
&lt;li>&lt;strong>昇腾计算硬件体系&lt;/strong>包括：
&lt;ul>
&lt;li>基于达芬奇内核的昇腾系列芯片，提供多样化AI算力。&lt;/li>
&lt;li>基于昇腾系列芯片的硬件产品，如嵌入式模组、板卡、小站、服务器、集群等。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>昇腾计算软件体系&lt;/strong>包括：
&lt;ul>
&lt;li>异构计算架构CANN，对标CUDA。&lt;/li>
&lt;li>AI计算框架MindSpore，对标Pytorch。&lt;/li>
&lt;li>工具链：运行时、加速库、编译器、调试调优工具、开发工具链MindStudio及运维工具。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>昇腾应用使能&lt;/strong>：基于MindX支撑的ModelArts、HiAI等。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912234104109.png" alt="image-20230912234104109">&lt;/p>
&lt;h3 id="22昇腾计算产业价值">2.2.昇腾计算产业价值&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>硬件开放、软件开元、使能合作伙伴&lt;/strong>是华为构建昇腾生态的方式。&lt;/li>
&lt;li>华为&lt;strong>聚焦AI芯片、基础软件&lt;/strong>的创新与研发。&lt;/li>
&lt;li>&lt;strong>自有硬件+伙伴硬件&lt;/strong>，为客户提供多样化算力选择。&lt;/li>
&lt;li>昇腾通过模组、板卡、小站、服务器、集群等产品形态，&lt;strong>打造&amp;quot;端、边、云&amp;quot;的全场景AI基础设施解决方案&lt;/strong>。&lt;/li>
&lt;li>昇腾计算产业的三个愿景：
&lt;ul>
&lt;li>&lt;strong>用得起&lt;/strong>&lt;/li>
&lt;li>&lt;strong>用得好&lt;/strong>&lt;/li>
&lt;li>&lt;strong>用得放心&lt;/strong>：特指某国的芯片封锁吧。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="3昇腾计算技术体系">3.昇腾计算技术体系&lt;/h2>
&lt;h3 id="31昇腾计算架构">3.1.昇腾计算架构&lt;/h3>
&lt;p>下图展开了昇腾计算架构细节，从架构中可以看出：&lt;/p>
&lt;ul>
&lt;li>昇腾计算架构支持端边云全场景。&lt;/li>
&lt;li>超强算力：Atlas训练卡提供320 TFLOPS FP16高算力，Atlas集群提供1024P FLOPS算力。&lt;/li>
&lt;li>全站开放，模块间具备相互协同能力、各层之间支持独立演进。&lt;/li>
&lt;li>MindX将设备资源、算力资源抽象并管理，应用软件无需了解底层硬件的复杂配置和调度。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230912235737889.png" alt="image-20230912235737889">&lt;/p>
&lt;h3 id="32硬件体系">3.2.硬件体系&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>昇腾系列处理器&lt;/strong>：NPU针对矩阵运算进行专门优化设计，华为达芬奇架构面向AI计算设计的架构，独创16&lt;em>16&lt;/em>16的3D Cube设计。&lt;/li>
&lt;li>&lt;strong>模组和板卡&lt;/strong>：Atlas 200加速模块、Atlas 300推理卡、Atlas 900。&lt;/li>
&lt;li>&lt;strong>小站&lt;/strong>：基于昇腾系列处理器的边缘计算盒子。&lt;/li>
&lt;li>&lt;strong>服务器&lt;/strong>：Atlas 800、Atlas500。&lt;/li>
&lt;li>&lt;strong>集群&lt;/strong>：Atlas 900。&lt;/li>
&lt;/ul>
&lt;h3 id="33基础软件">3.3.基础软件&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>异构计算架构&lt;/strong>：
&lt;ul>
&lt;li>CANN支持10种设备形态、EMUI、Android、openEuler、UOS、Ubuntu、Debian、Suse等14种操作系统和AI计算框架。&lt;/li>
&lt;li>CANN是一个开发体系，包含了编程语言、编译器等编程模型。&lt;/li>
&lt;li>CANN包含4层：
&lt;ul>
&lt;li>Driver实现硬件和操作系统的适配。&lt;/li>
&lt;li>Runtime、DVPP、HCCL提供内存管理、算力分配、资源调度。其中，HCCL，Huawei Collective Communication Library，华为集合通信库，提供板间和框间通信能力。&lt;/li>
&lt;li>图引擎实现了大计算图拆分、图融合，最大化芯片算力利用率。&lt;/li>
&lt;li>AscendCL提供插件适配、开放图融合接口、支持自定义算子融合、提供Ascend IR中间表达接口、支持自定义模型、开放预置算子库。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CANN提供两种算子开发方式，
&lt;ul>
&lt;li>一种是TBE-DSL(Tensor Boost Engine-Domain Specific Language)，实现数据切分和调度。&lt;/li>
&lt;li>一种是TBE-TIK(Tensor Iterator Kernel)，通过指令级编程，实现数据编排、计算表达。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913000935774.png" alt="image-20230913000935774">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>开发工具链MindStudio，提供工程管理、编译、调试、运行、性能分析等全流程开发。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AI计算框架MindSpore，提供动静态图转换、自动并行、端边云协同能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>应用使能的核心是MindX DL和MindX Edge，封装了底层硬件、算子的协同调度能力。&lt;/p>
&lt;ul>
&lt;li>MindX DL架构图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913001119293.png" alt="image-20230913001119293">&lt;/li>
&lt;li>MindX Edge架构图：&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913001132605.png" alt="image-20230913001132605">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="4行业实践">4.行业实践&lt;/h2>
&lt;p>白皮书在本章节阐述了基于昇腾计算，构建的各行各业的解决方案：&lt;/p>
&lt;ul>
&lt;li>AI计算中心&lt;/li>
&lt;li>互联网&lt;/li>
&lt;li>制造&lt;/li>
&lt;li>机器人&lt;/li>
&lt;li>能源&lt;/li>
&lt;li>金融&lt;/li>
&lt;li>平安城市&lt;/li>
&lt;li>电信&lt;/li>
&lt;li>交通&lt;/li>
&lt;li>医疗&lt;/li>
&lt;/ul>
&lt;p>这里展开3个行业解决方案：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>计算中心&lt;/strong>解决方案：从解决方案看，昇腾计算覆盖了AI芯片、AI驱动，是AI最核心的基础设施，完全国产化，很牛。
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004001231.png" alt="image-20230913004001231">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>工业&lt;/strong>解决方案：针对半导体晶圆质检，基于昇腾强大算力，实现工业领域的目标定位、测量、质检等能力。这一点真正突破了AI在工业领域落地的，很牛。
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004208756.png" alt="image-20230913004208756">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>交通&lt;/strong>解决方案：
&lt;ul>
&lt;li>高速自由流收费稽核&lt;/li>
&lt;li>交通视频云&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="5产业生态">5.产业生态&lt;/h2>
&lt;p>在产业政策方面：&lt;/p>
&lt;ul>
&lt;li>昇腾与各地政府共建昇腾生态创新中心&lt;/li>
&lt;li>通过行业联盟聚拢厂家、ISV、用户，建立各行业标杆。&lt;/li>
&lt;/ul>
&lt;p>在开发者方面：&lt;/p>
&lt;ul>
&lt;li>90%的开发者聚焦于基于AI算法的应用软件开发。&lt;/li>
&lt;li>10%的开发者聚焦AI算法、AI驱动的开发。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913004825461.png" alt="image-20230913004825461">&lt;/p>
&lt;p>在高校培养方面：&lt;/p>
&lt;ul>
&lt;li>昇腾与高校共建AI人才培养基地，与各高校推出昇腾计算体系课程与教材。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913005027655.png" alt="image-20230913005027655">&lt;/p>
&lt;p>在合作伙伴方面：&lt;/p>
&lt;ul>
&lt;li>昇腾推出覆盖软件开发商、硬件开发商、云服务供应商、设备制造商等各类合作伙伴计划，华为为合作伙伴提供了资金+市场。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/image-20230913005123533.png" alt="image-20230913005123533">&lt;/p>
&lt;h2 id="6未来展望昇腾推动ai成为通用目的技术">6.未来展望：昇腾推动AI成为通用目的技术&lt;/h2>
&lt;p>引用白皮书中的一段话：&lt;strong>把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界&lt;/strong>。&lt;/p>
&lt;p>昇腾计算产业的愿景是作为中国的AI基石，AI的基础设施，成为通用目的技术。&lt;/p>
&lt;h2 id="7总结">7.总结&lt;/h2>
&lt;p>当我第一次看到昇腾的AI芯片层，CANN对标CUDA，无比惊讶、激动、自豪，这可是对标英伟达的大事，技术难度、商业难度都不可想象。&lt;/p>
&lt;p>昇腾计算已雏形初见，这可不是短期突击出来的。不得不说，华为在AI领域的持续投资令人钦佩！&lt;/p></description></item><item><title>【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</link><pubDate>Tue, 05 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</guid><description>&lt;p>今天我们再来写一篇关于大语言模型的实战应用——如何开发一个垂直领域的知识问答系统？&lt;/p>
&lt;h1 id="1原理">1.原理&lt;/h1>
&lt;h2 id="1基于传统技术的实现方案">(1)基于传统技术的实现方案&lt;/h2>
&lt;p>在没有大语言模型之前，有如下传统技术实现知识问答系统：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>关键词匹配&lt;/strong>：在系统内预设一些关键词，系统根据用户提出的问题进行关键词匹配，从中提取出匹配的答案。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：仅适用于简单、明确的问题，但复杂问题、多义词等，效果就不好。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>规则匹配&lt;/strong>：在系统内预设一些规则模板，系统根据用户提出的问题结构进行匹配，这些规则模板包括：语法规则、语义规则、业务领域规则。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家编写规则模板，对领域知识的抽象和表达能力有一定要求。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>统计方法&lt;/strong>：系统基于统计学模型构建，如：条件随机场(CRF，Conditional Random Field)，进行问题分类、命名实体识别等。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：强依赖数据，数据采集、数据清洗、数据标注，都要消耗人类巨大的工作量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>知识图谱&lt;/strong>：系统基于领域相关的知识图谱或实体构建，可以通过图谱中的实体、关系和属性实现问题解析和答案生成。
&lt;ul>
&lt;li>&lt;strong>局限性&lt;/strong>：需要人类专家对业务领域的知识进行建模、抽象，好不容易构建好，知识刷新了。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2基于大语言模型的实现方案">(2)基于大语言模型的实现方案&lt;/h2>
&lt;p>和老李师傅聊大语言模型，他说：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>大语言模型，是从&lt;strong>word&lt;/strong>到&lt;strong>world&lt;/strong>的过程——AI学习一堆word的关系，然后就具备了概括、抽象、推理的能力去描述世界。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>人类程序员，是从&lt;strong>world&lt;/strong>到&lt;strong>word&lt;/strong>的过程——人类学习这个世界，然后用代码描述世界(代码就是一堆word的组合)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>最后，他老人家感叹道：&amp;ldquo;N年后，别人看我们，就像我们看伏尔加河上的纤夫。纤夫光着腚，我们光着头。&amp;rdquo;&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230905163751495.png" alt="image-20230905163751495">&lt;/p>
&lt;p>的确如此，相较于前述传统技术，基于大语言模型实现知识问答系统，有很多天然优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>学习效率高&lt;/strong>：大语言模型学习速度极快。后文实践章节中，有这么一个例子：我找公路军团的同学要了1000多份交通专业的文档和书籍，大语言模型&lt;strong>1分钟学习10个文档，2小时学完1000份&lt;/strong>，而&lt;strong>每小时只消耗了1.39元&lt;/strong>。这位交通领域的专家无比惊讶地告诉我——这1000多个文档，他也只详细看完了其中200个。&lt;/li>
&lt;li>&lt;strong>无需人工干预&lt;/strong>：传统技术需要大量的人工干预，特别强依赖人类专家那些&lt;strong>只可意会、不可言传&lt;/strong>的经验。而大语言模型可以自动学习和更新知识，无需人工干预。这意味着问答应用可以及时获取最新的知识，随着时间的推移变得更加智能和准确。&lt;/li>
&lt;li>&lt;strong>多轮对话&lt;/strong>：大语言模型还可以处理复杂的问题和多轮对话。它能够理解问题的语义和上下文，并根据用户的追问进行适当的回答。这使得问答应用更加交互式和人性化，提供更好的用户体验。&lt;/li>
&lt;li>……好处太多，省略千言万语……&lt;/li>
&lt;/ul>
&lt;h2 id="3大语言模型选型">(3)大语言模型选型&lt;/h2>
&lt;p>大语言模型的选型需要根据LLM App的应用场景，并且也不是只选1个，而是选择N个，形成大语言模型矩阵。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906090930061.png" alt="image-20230906090930061">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>世界知识&lt;/strong>：从世界知识看，目前主流的LLM如下：
&lt;ul>
&lt;li>&lt;strong>GPT系&lt;/strong>：当红炸子鸡的它，提供的接口包括&lt;code>Model&lt;/code>、&lt;code>Completions&lt;/code>、&lt;code>Chat&lt;/code>、&lt;code>Edits&lt;/code>、&lt;code>Images&lt;/code>、&lt;code>Embeddings&lt;/code>、&lt;code>Audio&lt;/code>、&lt;code>Files&lt;/code>、&lt;code>Fine-tunes&lt;/code>、&lt;code>Others&lt;/code>等10大类，完整地覆盖了&lt;strong>训练&lt;/strong>、&lt;strong>推理&lt;/strong>场景。针对&lt;strong>微调场景&lt;/strong>，覆盖了词嵌入、微调，在&lt;strong>推理&lt;/strong>场景，覆盖了预测、聊天、修正、多模态。&lt;strong>知识文档不涉密且不差钱&lt;/strong>的公司，可直接使用。笔者为了测试GPT4的API，&lt;font color=green>&lt;strong>半小时就花光了40大洋&lt;/strong>&lt;/font>，求赞助！&lt;/li>
&lt;li>&lt;strong>LLama系&lt;/strong>：卷王Meta的开源大戏，LLama一出，瞬间抢走了chatGPT的焦点。随后各顶流大学就开始了基于LLama的训练微调，推出了以Vicuna、Alpaca为代表的一系列模型。LLama2发布后更是好评无数，没几天又推出了最会写代码的CodeLLama。。。大语言模型界的卷王实至名归！&lt;/li>
&lt;li>&lt;strong>Claude系&lt;/strong>：谷歌前员工的大神们拉旗单干的产品，也是GPT系的强劲对手。&lt;/li>
&lt;li>&lt;strong>GLM系&lt;/strong>：清华大学出品，公司化商业运作。chatGLM的中文能力非常不错，GLM-130B成为2022年亚洲唯一入选全球30个主流大模型全方位测评报告的候选对象。国货之光，本文的知识问答系统就是演示的它。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPT系&lt;/td>
&lt;td>GPT3.5Turbo、GPT4&lt;/td>
&lt;td>OpenAI&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>LLama、LLama2、CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Claude系&lt;/td>
&lt;td>Claude-instant、Claude-2-100k&lt;/td>
&lt;td>Anthropic&lt;/td>
&lt;td>闭源&lt;/td>
&lt;td>远程调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>chatGLM2、GLM-130B、VisualGLM-6B&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>领域知识&lt;/strong>：以编程辅助领域为例，目前表现不错的LLM如下：&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型系列&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>厂商/机构&lt;/th>
&lt;th>开源or闭源&lt;/th>
&lt;th>调用形式&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BigCode系&lt;/td>
&lt;td>StarCoder、OctoPack、SantaCoder&lt;/td>
&lt;td>Hugging Face&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLama系&lt;/td>
&lt;td>CodeLLama&lt;/td>
&lt;td>Meta&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM系&lt;/td>
&lt;td>CodeGeeX2&lt;/td>
&lt;td>清华大学&lt;/td>
&lt;td>开源&lt;/td>
&lt;td>本地调用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>综上分析：&lt;/p>
&lt;ul>
&lt;li>对世界知识的表现体现的是大语言模型的&lt;strong>通才&lt;/strong>，对领域知识的表现体现的是大语言模型的&lt;strong>专才&lt;/strong>。&lt;/li>
&lt;li>在没有一个大语言模型即是通才又是专才的限制下，垂直领域知识问答系统需要的是&lt;strong>大语言模型矩阵&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="4大语言模型之外的关键技术">(4)大语言模型之外的关键技术&lt;/h2>
&lt;p>是不是选择好了大语言模型，LLM APP就信手拈来呢？很不幸，不是！&lt;/p>
&lt;p>在最近的一次行业交流中，有两个议题值得思考：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>量变引起质变&lt;/strong>：百度智能云/技术委员会主席王耀，针对大语言模型的训练推理场景，阐述了通用云计算、分布式云、智能计算的计算范式的变化。其中，最心酸的一个故事是：以前硬件上某个万分之一的故障是可以接受的，现在不可以了，因为大语言模型训练的计算量级无比巨大！&lt;/li>
&lt;li>&lt;strong>淘金时代的卖铲人&lt;/strong>：LangChain CEO Harrision Chase分享的议题是《Why LangChain，What I Saw When Building AI Application with LLM》，在各大厂商逐鹿中原的时候，LangChain忽然火了，按照这个趋势，它应该会成为LLM APP的开发框架了。&lt;/li>
&lt;/ul>
&lt;p>因为，&lt;strong>训练&lt;/strong>大语言模型的计算量剧增，对原有的云计算有新的诉求。&lt;/p>
&lt;p>因为，&lt;strong>粘合&lt;/strong>大语言模型需要有很多工作，需要有LLM APP的开发框架。&lt;/p>
&lt;p>通过LangChain的特性介绍和架构，我们可以知道大语言模型之外，我们还需要做如下事情：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>具体可以看笔者这篇文章：《【chatGPT】学习笔记10-LangChain之ModelIO，对LLM的抽象1》&lt;/p>
&lt;h2 id="5架构示例langchain-chatchat">(5)架构示例：LangChain-ChatChat&lt;/h2>
&lt;p>垂直领域问答系统的开源项目已经有很多了，以LangChain-ChatChat为例，可以看到这类LLM APP的软件架构：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095324183.png" alt="image-20230906095324183">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>训练环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unstructured Loader&lt;/strong>：负责将知识文档加载并解析。&lt;/li>
&lt;li>&lt;strong>Text Splitter&lt;/strong>：将文档按照标点符号，拆分断句。&lt;/li>
&lt;li>&lt;strong>Text Chunks&lt;/strong>：将断句分词分组，分词分组间存在一定的上下文关联。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906095608282.png" alt="image-20230906095608282">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Embedding&lt;/strong>：词嵌入，并将词向量存储于向量数据库中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>应用环节&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Query Embedding&lt;/strong>：将用户提问转换了词向量。&lt;/li>
&lt;li>&lt;strong>Vector Similarity&lt;/strong>：在向量数据库中，用问题词向量，去搜索相关的知识文本向量。&lt;/li>
&lt;li>&lt;strong>Related Text Chunks&lt;/strong>：根据相关的知识文本向量，反查出对应的知识文本分词分组。这里有个细节，还会将文本分词分组的前后分词分组一并返回(因为这些文字可能有上下文关联关系)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100110319.png" alt="image-20230906100110319">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Prompt Template&lt;/strong>：提示词模板，这个模块是知识问答系统的关键！最简单的提示词模板就是：
&lt;ul>
&lt;li>&lt;strong>已知&lt;/strong>：[Related Text Chunks获得的知识文本分词分组]，&lt;strong>请问&lt;/strong>[用户的问题]如何答复？&lt;/li>
&lt;li>这，不就是开卷考试吗？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906100510639.png" alt="image-20230906100510639">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>从上述架构可以看出来，曾经很复杂的知识问答系统，大部分核心难点，都被大语言模型搞定了。&lt;/p>
&lt;p>接下来，我们来动手实践一下，构建一个我们自己的知识问答系统。&lt;/p>
&lt;h1 id="2实践">2.实践&lt;/h1>
&lt;h2 id="21推理环境搭建">2.1.推理环境搭建&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.基于MiniCoda，创建虚拟环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101650577.png" alt="image-20230906101650577">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.激活虚拟环境，安装运行时环境&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906101852342.png" alt="image-20230906101852342">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP3.下载依赖。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102057137.png" alt="image-20230906102057137">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP5.初始化向量数据库。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102553686.png" alt="image-20230906102553686">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP6.配置model_config.py、server_config.py、llm_api.py。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102733935.png" alt="image-20230906102733935">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102716203.png" alt="image-20230906102716203">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102703074.png" alt="image-20230906102703074">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102652268.png" alt="image-20230906102652268">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP7.启动应用&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103336869.png" alt="image-20230906103336869">&lt;/p>
&lt;h2 id="22模型部署">2.2.模型部署&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.下载大语言模型和词嵌入模型&lt;/strong>——chatGLM2-6B、m3e-base&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102213647.png" alt="image-20230906102213647">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906102246753.png" alt="image-20230906102246753">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2.部署大语言模型和词嵌入模型。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103747404.png" alt="image-20230906103747404">&lt;/p>
&lt;h2 id="23模型微调">2.3.模型微调&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>STEP1.通过WebUI，上传知识文档。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906103254463.png" alt="image-20230906103254463">&lt;/p>
&lt;ul>
&lt;li>&lt;font color=red>&lt;strong>STEP2.编写提示词模板&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104642146.png" alt="image-20230906104642146">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906104709680.png" alt="image-20230906104709680">&lt;/p>
&lt;ul>
&lt;li>经过2小时的学习，LLM修炼完了1000多份智慧交通的资料，这学习效率杠杠滴！
&lt;ul>
&lt;li>PS：感谢公路军团的专家，提供珍藏多年的资料，改天请您吃饭！&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-LLM%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F(%E5%9F%BA%E4%BA%8EChatGLM2)/image-20230906105032126.png" alt="image-20230906105032126">&lt;/p>
&lt;h2 id="24模型测试">2.4.模型测试&lt;/h2>
&lt;p>直接上视频，咨询了一下交通行业的这位专家，回答挺靠谱：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=830744059&amp;bvid=BV1C34y1N7QG&amp;cid=1258811184&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;p>本文阐述了基于大语言模型的知识问答系统的实现原理及原型实践过程，原型距离可商用还有很多细节需打磨，但我们可以得到如下观点：&lt;/p>
&lt;ul>
&lt;li>从实现原理看：
&lt;ul>
&lt;li>&lt;strong>技术关键点1&lt;/strong>：要&lt;font color=red>&lt;strong>选择合适的多种大语言模型，形成大语言模型的矩阵&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>技术关键点2&lt;/strong>：除大语言模型选型，还需&lt;strong>借助LangChain处理好诸多环节&lt;/strong>(文档结构化、分段分词、词嵌入、上下文、向量搜索、提示词等)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>从实践看：
&lt;ul>
&lt;li>&lt;strong>数据&lt;/strong>：数据收集、数据清洗、词嵌入等，极为重要。&lt;/li>
&lt;li>&lt;strong>大语言模型&lt;/strong>：需跟踪各大厂商大语言模型的最新能力，快速替换到应用软件中，&lt;font color=red>&lt;strong>保持大语言模型的先进性&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>提示词&lt;/strong>：提示词模板的设计，决定了能否&lt;font color=red>&lt;strong>充分发挥大语言模型的潜力&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>工具链&lt;/strong>：熟练运用LangChain、向量数据库等工具，有助于设计并实现适应业务场景的问答业务流。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>今天就写到这里，后续专栏会继续展示基于大语言模型的LLM应用，欢迎共同探索(上述环境已做成镜像，需要的同学可后台联系作者)。&lt;/p></description></item><item><title>【chatGPT】学习笔记10-LangChain之Model IO，对LLM的抽象1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</link><pubDate>Mon, 28 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</guid><description>&lt;p>最近的专栏都是在拆解大语言模型的内部实现及论文，我们再来写点儿偏工程实践的内容——LangChain。&lt;/p>
&lt;h1 id="1langchain简介">1.LangChain简介&lt;/h1>
&lt;h2 id="1如何实现基于ai的app">(1)如何实现基于AI的App？&lt;/h2>
&lt;p>如果我们想开发一个基于大语言模型的AI知识库，怎么做呢？这个App的架构如下图：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>准备环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP1.数据管理&lt;/strong>：需要将垂直领域的知识进行词嵌入，放到向量数据库中。&lt;/li>
&lt;li>&lt;strong>STEP2.提示词管理&lt;/strong>：需要构造好提示词模板。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>微调&amp;amp;应用环节&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>STEP3.知识查询&lt;/strong>：将用户输入的自然语言问题向量化，寻找与输入问题相关的知识向量。&lt;/li>
&lt;li>&lt;strong>STEP4.提示词查询&lt;/strong>：找到和输入问题有关的提示词模板。&lt;/li>
&lt;li>&lt;strong>STEP5.调用大语言模型&lt;/strong>：将&lt;code>知识向量&lt;/code>+&lt;code>提示词模板&lt;/code>传递给大语言模型(大语言模型可能在本地，也可能在云端，大语言模型可能是1个也可能是多个)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901152726410.png" alt="image-20230901152726410">&lt;/p>
&lt;p>从这个架构可以看出，存在以下问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文档解析多样&lt;/strong>：垂直领域的知识文档，存在不同的文件格式，如何解析？&lt;/li>
&lt;li>&lt;strong>提示词需要复用&lt;/strong>：无论面向多少种不同厂商的大语言模型，提示词如何最大化复用？&lt;/li>
&lt;li>&lt;strong>模型多样&lt;/strong>：需要面向不同厂商大语言模型的不同调用接口，如何适配？&lt;/li>
&lt;/ul>
&lt;h2 id="2rag">(2)RAG&lt;/h2>
&lt;p>LangChain，为私有知识库App这类应用，定义了一个新的领域——&lt;strong>RAG&lt;/strong>(Retrieval Augmented Generation，生成式检索增强)。&lt;/p>
&lt;p>在RAG领域，通常有5类需要考虑的因素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本处理&lt;/strong>：对各类型文档的加载，对文本的切分等。&lt;/li>
&lt;li>&lt;strong>向量存储&lt;/strong>：适配不同向量数据库，将文档内容向量化并存储。&lt;/li>
&lt;li>&lt;strong>提示词管理&lt;/strong>：创建提示词模板，最大化重用提示词。&lt;/li>
&lt;li>&lt;strong>模型适配&lt;/strong>：针对不同厂商，适配各类大语言模型的接口。&lt;/li>
&lt;li>&lt;strong>输出解析&lt;/strong>：对大语言模型输出的文本进行结构化解析。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154308580.png" alt="image-20230901154308580">&lt;/p>
&lt;h2 id="3langchain的体系结构">(3)LangChain的体系结构&lt;/h2>
&lt;p>LangChain为了解决RAG领域的五类问题，提供了6大模块，如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901154845846.png" alt="image-20230901154845846">&lt;/p>
&lt;p>我们在接下来的内容中，逐一解读各个模块的特性以及关键源码。&lt;/p>
&lt;h1 id="2核心模块1model-io">2.核心模块1：Model I/O&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>LangChain的第一个核心模块，就是&lt;code>Model I/O&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O最重要的能力就是封装了各大厂商大语言模型的不同接口。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Model I/O包括三个子特性：&lt;/p>
&lt;ul>
&lt;li>Prompts&lt;/li>
&lt;li>Models&lt;/li>
&lt;li>Output Parsers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/model_io-1f23a36233d7731e93576d6885da2750.jpg" alt="model_io_diagram">&lt;/p>
&lt;h2 id="21models模型">2.1.Models(模型)&lt;/h2>
&lt;ul>
&lt;li>在Models特性中，LangChain抽象了两类模型：
&lt;ul>
&lt;li>语言模型：LLMs，LangChain在这里封装了各厂商大语言模型的接口。&lt;/li>
&lt;li>聊天模型：ChatModels，对语言模型的高层封装，提供输入一组聊天消息对象、输出聊天结果对象的模式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="211llm语言模型">2.1.1.LLM(语言模型)&lt;/h3>
&lt;h4 id="1langchain源码解读">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过3个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLM&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>与LLM相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>LLMReusult&lt;/code>，&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManagerForLLMRun&lt;/code>，&lt;code>AsyncCallbackManagerForLLMRun&lt;/code>&lt;/li>
&lt;li>&lt;code>CallbackManager&lt;/code>, &lt;code>AsyncCallbackManager&lt;/code>&lt;/li>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901160851034.png" alt="image-20230901160851034">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>class &lt;strong>BaseLanguageModel&lt;/strong>：语言模型的基类，所有语言模型封装子类，都继承自本类。&lt;/p>
&lt;ul>
&lt;li>def &lt;strong>generate_prompt&lt;/strong>：这是一个抽象函数，该函数由各个语言模型封装子类自行实现，输入提示词序列，输出是语言模型返回的结果。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901161907636.png" alt="image-20230901161907636">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>BaseLLM&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>，实现了各语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162431013.png" alt="image-20230901162431013">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>class &lt;strong>LLM&lt;/strong>：继承class &lt;strong>BaseLLM&lt;/strong>，进一步封装个语言模型封装子类的共性函数。&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901162650458.png" alt="image-20230901162650458">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>LangChain实现的各类语言模型封装的子类：&lt;/p>
&lt;ul>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163511284.png" alt="image-20230901163511284">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="2例子">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT的达芬奇003模型&lt;/strong>。其中，class &lt;strong>OpenAI&lt;/strong>就是对GPT的模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：给我一个Java的ArrayList的代码示例。&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：是一段ArrayList代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901163837808.png" alt="image-20230901163837808">&lt;/p>
&lt;h3 id="212chatmodel聊天模型">2.1.2.ChatModel(聊天模型)&lt;/h3>
&lt;h4 id="1langchain源码解读-1">(1)LangChain源码解读&lt;/h4>
&lt;ul>
&lt;li>LangChain通过2个关键类实现聊天模型：
&lt;ul>
&lt;li>class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;li>class &lt;strong>BaseChatModel&lt;/strong>：继承class &lt;strong>BaseLanguageModel&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>AIMessage&lt;/code>, &lt;code>BaseMessage&lt;/code>, &lt;code>HumanMessage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901164637931.png" alt="image-20230901164637931">&lt;/p>
&lt;h4 id="2例子-1">(2)例子&lt;/h4>
&lt;ul>
&lt;li>示例代码中，&lt;strong>调用了GPT3.5 Turbo模型&lt;/strong>。其中，class &lt;strong>ChatOpenAI&lt;/strong>就是对GPT的聊天模型进行的封装。&lt;/li>
&lt;li>&lt;strong>输入&lt;/strong>：一组聊天消息对象&lt;/li>
&lt;li>&lt;strong>输出&lt;/strong>：聊天消息的回答对象。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>屏蔽了GPT的API未来可能的变化&lt;/strong>&lt;/font>，适配是由LangChain完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165247536.png" alt="image-20230901165247536">&lt;/p>
&lt;h2 id="22prompts提示词">2.2.Prompts(提示词)&lt;/h2>
&lt;h3 id="1langchain源码解读-2">(1)LangChain源码解读&lt;/h3>
&lt;ul>
&lt;li>LangChain通过2个关键类实现LLM：
&lt;ul>
&lt;li>class &lt;strong>BasePromptTemplate&lt;/strong>：基础提示词的基类。&lt;/li>
&lt;li>class &lt;strong>BaseMessagePromptTemplate&lt;/strong>：聊天模型提示词的基类。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相关的关键类如下：
&lt;ul>
&lt;li>&lt;code>PromptValue&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901165527920.png" alt="image-20230901165527920">&lt;/p>
&lt;h3 id="2例子-基础提示词">(2)例子-基础提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_template&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>lang&lt;/code>变量，通过1个提示词模板，实现了生成&lt;code>java&lt;/code>、&lt;code>python&lt;/code>、&lt;code>C++&lt;/code>的冒泡排序代码。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170042600.png" alt="image-20230901170042600">&lt;/p>
&lt;h3 id="2例子-chatmodel提示词">(2)例子-ChatModel提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>from_messages&lt;/code>方法构造了提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>user_input&lt;/code>变量，通过1个提示词模板，实现了生成对Java的ArrayList的知识点摘要。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了聊天模型的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170414376.png" alt="image-20230901170414376">&lt;/p>
&lt;h3 id="3例子-fewshot提示词">(3)例子-FewShot提示词&lt;/h3>
&lt;ul>
&lt;li>示例代码中，通过&lt;code>FewShotPromptTemplate&lt;/code>构造了FewShot提示词模版&lt;/li>
&lt;li>调用时，传入了&lt;code>input&lt;/code>变量，通过1个提示词模板，实现了一系列FewShot语料。&lt;/li>
&lt;li>这样，就&lt;font color=red>&lt;strong>实现了一个模板引擎，最大化复用了FewShot的提示词&lt;/strong>&lt;/font>。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901170805004.png" alt="image-20230901170805004">&lt;/p>
&lt;h2 id="23output-parsers输出解析器">2.3.Output Parsers(输出解析器)&lt;/h2>
&lt;ul>
&lt;li>输出解析器是一个很有趣的特性，大语言模型返回的答案是千奇百怪的，如何解析呢？&lt;/li>
&lt;li>这里以&lt;code>List Parser&lt;/code>为例：
&lt;ul>
&lt;li>LangChain提供了class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>，这个类会在提示词中限定大语言模型的返回。&lt;/li>
&lt;li>比如：输入的问题是&lt;code>请问《圣斗士星矢》中，有哪几个主角？&lt;/code>。&lt;/li>
&lt;li>class &lt;strong>CommaSeparatedListOutputParser&lt;/strong>会构造提示词：Your response should be a list of comma separated values, eg: &lt;code>foo, bar, baz&lt;/code>。&lt;/li>
&lt;li>大语言模型回答问题的文本就有了固定格式：&lt;code>星矢、紫龙、冰河、瞬、一辉&lt;/code>，这样就可以解析成list了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-LangChain%E4%B9%8BModelIO%EF%BC%8C%E5%AF%B9LLM%E7%9A%84%E6%8A%BD%E8%B1%A11/image-20230901171701655.png" alt="image-20230901171701655">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;p>本文介绍了：&lt;/p>
&lt;ul>
&lt;li>LangChain的功能，为大家构建了对LangChain的宏观认识。&lt;/li>
&lt;li>LangChain的Model I/O核心模块：
&lt;ul>
&lt;li>该模块通过Models，抽象了各厂商的大语言模型，我们可以使用统一接口去调用不同的大语言模型。&lt;/li>
&lt;li>该模块通过Prompts，提供了多种提示词模板的构建。&lt;/li>
&lt;li>该模块通过Output Parsers，提供了对大语言模型输出结果的结构化解析。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>后续文章，我们继续解读LangChain的核心模块，感谢阅读。&lt;/p></description></item><item><title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</link><pubDate>Mon, 21 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</guid><description>&lt;h1 id="1seq2seqtransformer的雏形">1.Seq2Seq，Transformer的雏形&lt;/h1>
&lt;h2 id="11为什么会出现seq2seq">1.1.为什么会出现Seq2Seq？&lt;/h2>
&lt;p>在神经概率语言模型NPLM出现后的很长一段时间，都是在这种网络架构下进行优化。但，依然面临很多难题(主要是循环神经网络RNNs的局限)：&lt;/p>
&lt;ul>
&lt;li>如：输入序列长度增加时性能下降&lt;/li>
&lt;li>如：顺讯处理导致计算效率低&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>在2014年，Seq2Seq的提出，给人类一个不错的启发。&lt;/p>
&lt;p>随后在自注意力机制的加持下，Transformer就诞生了。&lt;/p>
&lt;h2 id="12seq2seq架构概览">1.2.Seq2Seq架构概览&lt;/h2>
&lt;p>大神&lt;code>Ilya Sutskever&lt;/code>在2014年，以第一作者的身份，发表了论文《Sequence to Sequence Learning with Neural Networks》。&lt;/p>
&lt;blockquote>
&lt;p>论文地址：https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162206474.png" alt="image-20230831162206474">&lt;/p>
&lt;p>顺便说一嘴，大神&lt;code>Ilya Sutskever&lt;/code>就是这位大哥，OpenAI联合创始人和首席科学家，各大自媒体都在播放他老人家的演讲视频：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162529973.png" alt="image-20230831162529973">&lt;/p>
&lt;p>这篇论文的核心就是这张图，阐述了Seq2Seq的&lt;strong>编码器-解码器架构&lt;/strong>：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/p>
&lt;p>初次理解这张图，&lt;strong>需要费点儿脑细胞&lt;/strong>，我们接下来详细拆解。&lt;/p>
&lt;h2 id="13seq2seq架构详解">1.3.Seq2Seq架构详解&lt;/h2>
&lt;h3 id="1整体模型架构">(1)&amp;ldquo;整体模型&amp;quot;架构&lt;/h3>
&lt;p>Seq2Seq会将变长的&lt;strong>输入序列&lt;/strong>，转换为变长的&lt;strong>输出序列&lt;/strong>。如下图：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170025140.png" alt="image-20230831170025140">&lt;/p>
&lt;p>这里举一个例子——将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;rdquo;：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给Seq2Seq。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：Seq2Seq输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：Seq2Seq输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：Seq2Seq输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170404160.png" alt="image-20230831170404160">&lt;/p>
&lt;h3 id="2编码器-解码器架构">(2)&amp;ldquo;编码器-解码器&amp;quot;架构&lt;/h3>
&lt;p>进一步拆解整体模型架构，Seq2Seq由编码器+解码器组成：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171003578.png" alt="image-20230831171003578">&lt;/p>
&lt;p>这里还是以将&amp;quot;你是谁&amp;quot;翻译为英文&amp;quot;Who are u&amp;quot;例子来解析：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字输入给编码器。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：编码器将学习到上下文向量，传递给解码器&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：解码器输出&amp;quot;Who&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：解码器输出&amp;quot;are&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>t8时刻&lt;/strong>：解码器输出&amp;quot;u&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171625206.png" alt="image-20230831171625206">&lt;/p>
&lt;h3 id="3编码器解码器微观逻辑">(3)&amp;ldquo;编码器、解码器&amp;quot;微观逻辑&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>编码器、解码器的具体实现&lt;/strong>：在论文中，它们都是采用RNN实现。如下图所示：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>编码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的输出&lt;/strong>：1号隐藏层状态向量 + 1号词输出向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输入&lt;/strong>：1号隐藏层状态向量 + 2号输入词向量&lt;/li>
&lt;li>&lt;strong>编码器的下一次输出&lt;/strong>：2号隐藏层状态向量 + 2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831222457684.png" alt="image-20230831222457684">&lt;/li>
&lt;li>&lt;strong>解码器的输入&lt;/strong>：0号隐藏层状态向量 + 1号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的输出&lt;/strong>：1号词输出向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输入&lt;/strong>：0号隐藏层状态向量 + 1号词输出向量 + 2号Teach Forcing输入词向量&lt;/li>
&lt;li>&lt;strong>解码器的下一次输出&lt;/strong>：2号输出词向量&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225007834.png" alt="image-20230831225007834">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>这还是不太直观，我们再把编码器、解码器&lt;strong>按时序进一步展开&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t1时刻&lt;/strong>：用户输入序列&amp;quot;你是谁&amp;quot;三个字。&lt;/li>
&lt;li>&lt;strong>t2时刻&lt;/strong>：&amp;ldquo;你&amp;quot;字输入给编码器，编码器输出&lt;code>1号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t3时刻&lt;/strong>：&amp;ldquo;是&amp;quot;字 + &lt;code>1号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>2号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t4时刻&lt;/strong>：&amp;ldquo;谁&amp;quot;字 + &lt;code>2号隐藏层状态&lt;/code>，输入给编码器，编码器输出&lt;code>3号隐藏层状态&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t5时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;Who&amp;quot;字，输入给解码器，解码器输出&lt;code>Who&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t6时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;are&amp;quot;字 + 解码器输出&lt;code>who&lt;/code>，输入给解码器，解码器输出&lt;code>are&lt;/code>。&lt;/li>
&lt;li>&lt;strong>t7时刻&lt;/strong>：&lt;code>3号隐藏层状态&lt;/code> + TeachForcing的&amp;quot;u&amp;quot;字 + 解码器输出&lt;code>are&lt;/code>，输入给解码器，解码器输出&lt;code>u&lt;/code>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225749199.png" alt="image-20230831225749199">&lt;/p>
&lt;ul>
&lt;li>这里有一个细节，什么是&lt;strong>Teach Forcing&lt;/strong>？
&lt;ul>
&lt;li>我们可以想象，如果1号解码器的预测错了，那么2号、3号解码器都会错，进而导致学习效率非常低。&lt;/li>
&lt;li>Teach Forcing好像&lt;strong>开卷考试&lt;/strong>，如果1号解码器预测错了，Teach Forcing会纠正预测结果，进而加速学习效率。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3优点">(3)优点&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>变长序列&lt;/strong>：由于编码器、解码器采用RNN实现，所以输入序列可以是变长、输出序列也可以是变长。而CNN、DNN都不支持变长序列。&lt;/li>
&lt;li>&lt;strong>信息压缩&lt;/strong>：隐藏层状态，或者叫上下文向量，本质上将输入序列进行了信息压缩，转变为含有上下文语义的向量。&lt;/li>
&lt;/ul>
&lt;h3 id="4劣势">(4)劣势&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>长序列信息损失&lt;/strong>：由于上下文向量为定长，所以当输入序列过长时编码器会出现信息损失。&lt;font color=red>&lt;strong>这就是注意力机制的发力点，先埋个伏笔&lt;/strong>&lt;/font>。&lt;/li>
&lt;li>&lt;strong>效率&lt;/strong>：采用RNN(如：LSTM、GRU)实现编码器、解码器，RNN会面临梯度消失、梯度爆炸等问题。&lt;/li>
&lt;/ul>
&lt;h1 id="2代码">2.代码&lt;/h1>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231557775.png" alt="image-20230831231557775">&lt;/p>
&lt;h2 id="step12结构化训练数据">STEP1.2.结构化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231753952.png" alt="image-20230831231753952">&lt;/p>
&lt;h2 id="step21定义编码器和解码器类">STEP2.1.定义编码器和解码器类&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231855888.png" alt="image-20230831231855888">&lt;/p>
&lt;h2 id="step22定义seq2seq模型">STEP2.2.定义Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231925444.png" alt="image-20230831231925444">&lt;/p>
&lt;h2 id="step3训练seq2seq模型">STEP3.训练Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232035429.png" alt="image-20230831232035429">&lt;/p>
&lt;h2 id="step4测试seq2seq模型">STEP4.测试Seq2Seq模型&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232116723.png" alt="image-20230831232116723">&lt;/p>
&lt;h1 id="3小结">3.小结&lt;/h1>
&lt;ul>
&lt;li>本文解析了Seq2Seq的内部实现，理解下图的关键是——&lt;strong>按照时序&lt;/strong>，分析清楚每个编码器or解码器&lt;strong>有几个输入、几个输出&lt;/strong>。&lt;/li>
&lt;li>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png" alt="image-20230831162726199">&lt;/li>
&lt;li>理解了Seq2Seq，我们接下来就可以逐步实现完整的Transformer了，且听下回分解。&lt;/li>
&lt;/ul></description></item><item><title>【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</link><pubDate>Sat, 19 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</guid><description>&lt;h1 id="1问题词嵌入的局限性">1.问题：词嵌入的局限性&lt;/h1>
&lt;p>在《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》中，我们了解了词嵌入。&lt;/p>
&lt;p>获取词向量后，在向量空间中可以获得每个词的向量表示，也可以通过向量了解词和词之间的语义关联性。&lt;/p>
&lt;p>似乎，仅通过词嵌入就能解决很多NLP问题了，但词嵌入存在如下局限性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>：Word2Vec，以及后续的GloVe，在训练完成后，学习到的词向量是不会再被更新的。&lt;/li>
&lt;li>&lt;strong>词向量的静态性&lt;/strong>决定了：
&lt;ul>
&lt;li>&lt;strong>词嵌入无法应对多义词&lt;/strong>：1个词只有1个向量，无法表示多义词。&lt;/li>
&lt;li>&lt;strong>词嵌入无法应对未知词&lt;/strong>：没见过的词，显然Word2Vec不可能在向量空间中无中生有它对应的向量。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>于是，神经概率语言模型NPLM横空出世。&lt;/p>
&lt;h1 id="2对策神经概率语言模型nplm">2.对策：神经概率语言模型NPLM&lt;/h1>
&lt;h2 id="1什么是神经概率语言模型">(1)什么是神经概率语言模型？&lt;/h2>
&lt;p>科学家和大神们，很早就有了引入具有强大表示力和学习力的神经网络的想法。大致思想是：&lt;/p>
&lt;ul>
&lt;li>输入海量语料，神经网络学习词语在不同上下文中的概率分布。&lt;/li>
&lt;li>词最终还是会被向量化，只是这些词向量的学习过程成为神经网络的一部分，词向量表达的人类语言规律(词语、语义等)最终被记录到神经网络的参数中。&lt;/li>
&lt;/ul>
&lt;p>接下来，深度学习三巨头之一Bengio，在2003年推出了神作《A Neural Probabilistic Language Model》：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NPLM&lt;/strong>：Neural Probabilistic Language Model，神经概率语言模型。NPLM包含输入层、隐藏层、输出层。&lt;/li>
&lt;li>&lt;strong>输入层&lt;/strong>：将单词映射到连续的词向量空间。&lt;/li>
&lt;li>&lt;strong>隐藏层&lt;/strong>：通过非线性激活函数学习单词间的复杂关系。&lt;/li>
&lt;li>&lt;strong>输出层&lt;/strong>：通过Softmax层产生下一个单词的概率分布。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>论文链接：https://dl.acm.org/doi/pdf/10.5555/944919.944966&lt;/p>
&lt;/blockquote>
&lt;p>下图是Bengio论文中的阐述的神经概率语言模型的架构，后续NLP方向上的各种技术都是围绕这个架构进行各层的优化！&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230818065825957.png" alt="image-20230818065825957">&lt;/p>
&lt;p>对上图进行细化，笔者标出了输入层、隐藏层、输出层：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825094252544.png" alt="image-20230825094252544">&lt;/p>
&lt;h2 id="2输入层">(2)输入层&lt;/h2>
&lt;ul>
&lt;li>输入层的&lt;strong>职责&lt;/strong>：将输入的词，开展词嵌入，学习到词向量后存储在输入层。因此，输入层也可以叫嵌入层。&lt;/li>
&lt;li>输入层的&lt;strong>输入&lt;/strong>：词本身&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095132476.png" alt="image-20230825095132476">&lt;/li>
&lt;li>输入层的&lt;strong>输出&lt;/strong>：词向量&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095205472.png" alt="image-20230825095205472">&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095238666.png" alt="image-20230825095238666">&lt;/p>
&lt;h2 id="3隐藏层">(3)隐藏层&lt;/h2>
&lt;ul>
&lt;li>隐藏层的&lt;strong>职责&lt;/strong>：学习词与词之间的关系&lt;/li>
&lt;li>隐藏层采用了&lt;strong>非线性激活函数&lt;/strong>：采用非线性激活函数的本质是让神经网络的**&amp;ldquo;脑回路&amp;rdquo;**复杂起来(如：使用激活函数可以将线性层提升为非线性层)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825095635192.png" alt="image-20230825095635192">&lt;/p>
&lt;h2 id="4输出层">(4)输出层&lt;/h2>
&lt;ul>
&lt;li>输出层的&lt;strong>职责&lt;/strong>：通过Softmax层，输出下个单词的概率分布&lt;/li>
&lt;li>&lt;strong>Softmax&lt;/strong>：归一化，Softmax层的输入是各个词得到的分数，输出是将这些分数归一化到0~1的值域内。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230825102215771.png" alt="image-20230825102215771">&lt;/p>
&lt;h2 id="4隐藏层的优化">(4)隐藏层的优化&lt;/h2>
&lt;ul>
&lt;li>神经网络似乎天生不擅长长序列问题，所以后续很多NLP发展的技术，都是在&lt;strong>优化NPLM在长序列上的表现&lt;/strong>。
&lt;ul>
&lt;li>浅层网络无法捕捉文本中复杂的信息规律。&lt;/li>
&lt;li>普通的深层神经网络也不能很好处理长距离的依赖关系。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>NPLM的巧妙之处在于：&lt;strong>隐藏层可以使用任意的神经网络去替换&lt;/strong>。&lt;/li>
&lt;li>RNN、LSTM横空出世，在NLPM中长期霸榜：
&lt;ul>
&lt;li>RNN，循环神经网络，这种特殊的神经网络结构，可以将网络的输出作为网络的输入，使得神经网络能够处理数据的同时保留了&lt;strong>前世的记忆&lt;/strong>。&lt;/li>
&lt;li>LSTM，是RNN的经典代表作，很长一段时间作为NLP问题的SOTA(state of the art)模型
&lt;ul>
&lt;li>注意：被评为SOTA，而不是benchmark，或者baseline，是一种极高的荣誉。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="3代码nplm">3.代码：NPLM&lt;/h1>
&lt;p>接下来，进入代码环节。&lt;/p>
&lt;h2 id="step11构建数据集">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络">STEP2.构建NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093351356.png" alt="image-20230826093351356">&lt;/p>
&lt;h2 id="step3训练nplm网络">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093407105.png" alt="image-20230826093407105">&lt;/p>
&lt;h2 id="step4测试nplm">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093420906.png" alt="image-20230826093420906">&lt;/p>
&lt;h1 id="4代码nplm优化">4.代码：NPLM优化&lt;/h1>
&lt;p>再使用RNN、LSTM优化NPLM架构。&lt;/p>
&lt;h2 id="step11构建数据集-1">STEP1.1.构建数据集&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093203843.png" alt="image-20230826093203843">&lt;/p>
&lt;h2 id="step12格式化训练数据-1">STEP1.2.格式化训练数据&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093227289.png" alt="image-20230826093227289">&lt;/p>
&lt;h2 id="step2构建nplm网络-1">STEP2.构建NPLM网络&lt;/h2>
&lt;ul>
&lt;li>核心就是在此替换隐藏层和输出层&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093621577.png" alt="image-20230826093621577">&lt;/p>
&lt;h2 id="step3训练nplm网络-1">STEP3.训练NPLM网络&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093643273.png" alt="image-20230826093643273">&lt;/p>
&lt;h2 id="step4测试nplm-1">STEP4.测试NPLM&lt;/h2>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/image-20230826093701798.png" alt="image-20230826093701798">&lt;/p>
&lt;h1 id="5小结">5.小结&lt;/h1>
&lt;p>最后，我们来做一些对比和小结：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>从目标看&lt;/strong>：NPLM是解决词汇出现概率的问题，Word2Vec是解决如何将词转换为向量的问题。&lt;/li>
&lt;li>&lt;strong>从实现看&lt;/strong>：NPLM和Word2Vec都是基于神经网络的模型，但Word2Vec没有激活函数，属于利用了浅层神经网络。&lt;/li>
&lt;li>&lt;strong>从词向量看&lt;/strong>：NPLM中的词向量是神经网络的一部分，基于NPLM的目标，人类对它的训练是不会停止的，训练一次，词向量就会变化一次。而基于Word2Vec的目标，人类只会对它训练一次，训练好，词向量就固化不变了。&lt;/li>
&lt;/ul>
&lt;p>NPLM可以算作如今大语言模型的祖师爷了，在学界存在极高的地位和价值，后续很多年的RNN、LSTM都只能算作对NPLM的架构优化。&lt;/p>
&lt;p>但，NPLM也存在历史局限性，于是才有了后来的大模型的关键部件Transformer，且听下回分解。&lt;/p></description></item><item><title>【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</link><pubDate>Thu, 17 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/</guid><description>&lt;p>人类的科学发展就是这样：每个十年百年就有一个天才振臂一呼，用一个理论解决那个时代的一个问题，同时成为下一个十年百年的天才理论的基石。&lt;/p>
&lt;p>目前炙手可热的Transformer既是如此，LSTM、Word2Vec等是它的基石，共同构筑了现在的大语言模型的关键部件和理论基础。&lt;/p>
&lt;p>之前发了一个朋友圈，将对大语言模型影响深远的论文，梳理了三条脉络：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>LSTM&lt;/strong>：在1997年那会儿解决了AI的记忆问题。&lt;/li>
&lt;li>&lt;strong>Word2Vec&lt;/strong>：在2013~2014年解决了将词转换为向量，将&amp;quot;文字游戏&amp;quot;转换为了高维向量空间中的&amp;quot;数学游戏&amp;rdquo;。&lt;/li>
&lt;li>&lt;strong>Transformer&lt;/strong>：2014年出现了注意力机制雏形，2017年那篇著名的《Attention is All you Need》引出的Transformer，随后是OpenAI2018年的GPT-1、Google2019的BERT。&lt;/li>
&lt;/ul>
&lt;p>本文重点阐述第2条技术线：&lt;strong>Word2Vec&lt;/strong>涉及的技术——词的向量化，这也是大语言模型的关键部件之一。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230817105511.jpg" alt="微信图片_20230817105511">&lt;/p>
&lt;h1 id="1什么是表示学习">1.什么是表示学习？&lt;/h1>
&lt;p>我们先看一下表示学习的定义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>表示学习&lt;/strong>：Representation Learning，通过学习算法&lt;strong>自动&lt;/strong>地从原始数据中&lt;strong>学习到一种数据表达形式&lt;/strong>。
&lt;ul>
&lt;li>表示学习的目标是将输入的数据转换到具有良好表现能力的特征空间中。&lt;/li>
&lt;li>特征空间中的数据有&lt;strong>可分性&lt;/strong>、&lt;strong>可解释性&lt;/strong>、&lt;strong>可推理性&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>我们来解读一下上述这段话：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>：
&lt;ul>
&lt;li>我们知道，人类彼此的沟通介质是：文字、图像、视频、语音等。这些介质的本质是数据。&lt;/li>
&lt;li>我们还知道，从数据中寻找规律，是连接主义的哲学基础。&lt;/li>
&lt;li>我们还知道，可以被人类大脑理解的文字、图像、视频等数据，人工智能能理解吗？不能。&lt;/li>
&lt;li>人工智能无法理解这些数据，人工智能就无法从数据中寻找规律——因此，&lt;strong>数据的可表示性，是连接主义的哲学基础&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>向量化是数据可表示的一种实现&lt;/strong>：
&lt;ul>
&lt;li>向量化，可以让人工智能理解人类才能理解的数据。&lt;/li>
&lt;li>向量化，只是数据可表示的&lt;strong>一种&lt;/strong>实现，当然可以有其它的实现方式。
&lt;ul>
&lt;li>如：笔者前一篇《【chatGPT】学习笔记6-手撸一个上古GPT》中实现的N-Gram算法，并未对人类语言向量化，也让人工智能具备理解人类语言以及语言概率的能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>向量化，依然是目前数据可表示的若干种实现中最好的一种。
&lt;ul>
&lt;li>基于N-Gram的人工智能，能力很弱。它的智能停留在人类语言的&lt;strong>表面&lt;/strong>，它并不理解&lt;strong>语义&lt;/strong>。&lt;/li>
&lt;li>比如：&amp;ldquo;我去！&amp;ldquo;这个句子，基于N-Gram的人工智能无法识别，这句话到底想表达&amp;quot;我艹&amp;rdquo;，还是想表达&amp;quot;我要去某个地方&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>上述对表示学习的解读还是挺抽象。没关系，后文有形象的例子，各位小伙伴可以先看完后文，再回头看这一段，加深对表示学习的理论理解。&lt;/p>
&lt;h1 id="2什么是向量什么是嵌入">2.什么是向量？什么是嵌入？&lt;/h1>
&lt;h2 id="1向量">(1)向量&lt;/h2>
&lt;p>在NLP领域，无论语言模型的大小，都必须将词先表达为向量，词向量就是语言模型的输入。&lt;/p>
&lt;p>在CV领域，无论视觉模型的大小，也必须将图像先表达为向量。&lt;/p>
&lt;p>将文字、图像、视频、音频等数据向量化，本质是&lt;font color=red>**将&amp;quot;人类可理解的数据问题&amp;quot;转换为&amp;quot;机器可理解的机器学习问题&amp;rdquo;**&lt;/font>。&lt;/p>
&lt;p>我们来看一个向量化的例子：&lt;/p>
&lt;ul>
&lt;li>人类小孩儿第一次看到苹果，人类小孩儿是怎么记住这种东西就是苹果呢？&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143249956.png" alt="image-20230817143249956">&lt;/p>
&lt;ul>
&lt;li>人类小孩儿会从不同维度描述苹果的特征(如：纹理、颜色、形状、大小等)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143358437.png" alt="image-20230817143358437">&lt;/p>
&lt;ul>
&lt;li>假设人类小孩大脑的工作机制，是将不同维度的特征用数字表达并存储，这些维度的特征值就是&lt;strong>向量&lt;/strong>了。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143456600.png" alt="image-20230817143456600">&lt;/p>
&lt;ul>
&lt;li>当人类小孩看到新的苹果，大脑也会将新苹果的特征提取为向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817143619055.png" alt="image-20230817143619055">&lt;/p>
&lt;ul>
&lt;li>大苹果、小苹果，从颜色、大小、形状、纹理等等维度是很相似的，显然，把这些苹果抽象成多个向量，这些向量在向量空间中的距离肯定是很近的。&lt;/li>
&lt;li>这就是信息向量化，词的向量化就是信息向量化的一种。&lt;/li>
&lt;/ul>
&lt;p>上述例子，来自于一段腾讯云介绍向量数据库的视频，各位小伙伴可以通过视频，看到更多形象化理解向量的例子：&lt;/p>
&lt;iframe src="//player.bilibili.com/player.html?aid=489914023&amp;bvid=BV13N41167Q9&amp;cid=1237875667&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=600px> &lt;/iframe>
&lt;h2 id="2嵌入">(2)嵌入&lt;/h2>
&lt;p>先看一下嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>嵌入：Embedding，表示学习的一种形式，用于&lt;strong>将高维数据映射到低维空间&lt;/strong>。嵌入包括：
&lt;ul>
&lt;li>词嵌入&lt;/li>
&lt;li>图像嵌入&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这里以图像嵌入的一种经典算法t-SNE(t-Distributed Stochastic Neighbor Embedding)为例：&lt;/p>
&lt;ul>
&lt;li>图像嵌入的第一阶段是将图像转换为高维向量。&lt;/li>
&lt;li>图像嵌入的第二阶段是将高维向量映射到低维向量，但是要保证高维向量中邻近的点，在低维空间中也有相同的距离关系。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/t-sne_optimise.gif" alt="t-sne_optimise">&lt;/p>
&lt;p>&lt;strong>嵌入&lt;/strong>本身是一个很抽象的术语，但我们可以简单地认为&lt;strong>嵌入就是向量化&lt;/strong>的过程。&lt;/p>
&lt;p>对于嵌入，最关键的信息只有一个：&lt;strong>高维&lt;/strong>映射到&lt;strong>低维&lt;/strong>。&lt;/p>
&lt;p>什么是高维向量？什么是低维向量？为什么要从高维映射到低维？&lt;/p>
&lt;p>这些问题的有趣之处，和人类的学习过程非常相似：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>有一日未死之身，则有一日未闻之道&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类开始学习一门新知识，越学越觉得这个知识领域博大精深。这是人类学习的第一阶段——&amp;quot;&lt;strong>把书读厚&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>在嵌入过程中，也就是把信息向量化过程中，显然是把一个信息用更高维度的向量去描述，信息越准确。&lt;/li>
&lt;li>比如：小时候练习看图说话，老师会鼓励小朋友把图上看到的东西从更多角度去描绘出来，这些角度就是向量的维度。&lt;/li>
&lt;li>再比如：一个哲学思想，不同流派的哲学家会从不同角度、不同立场去阐述、论证和思辨，这些角度、立场也等效于向量的维度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>读书之道，愈进愈简，百卷如一页&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>人类学习新知识到了一个阶段，会出现&lt;strong>顿悟&lt;/strong>——发现、归纳各个知识点的内在联系。这是人类学习的第二阶段——&amp;quot;&lt;strong>把书读薄&lt;/strong>&amp;quot;。&lt;/li>
&lt;li>比如：杨过的重剑无锋，大巧不工。比如：老子的大道至简，有言无言。&lt;/li>
&lt;li>在嵌入过程中，也有类似的过程，虽然单个高维向量有丰富的表达，但是多个高维向量存在某些内在联系，把它们降低为低维向量，不仅抓住了信息的本质，而且更加精炼、大道至简。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这就是&lt;strong>嵌入&lt;/strong>的&lt;strong>核心思想&lt;/strong>——将&lt;strong>高维向量降低成低维向量&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817221709020.png" alt="image-20230817221709020">&lt;/p>
&lt;h1 id="3什么是词向量什么是词嵌入">3.什么是词向量？什么是词嵌入？&lt;/h1>
&lt;p>先来看词向量、词嵌入的学术概念：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词向量的定义&lt;/strong>：将词语转换成对应数值向量的表达形式，便于计算机读取和运算。&lt;/li>
&lt;li>&lt;strong>词向量的数学表达&lt;/strong>：将字典D中的任意词w，设定固定长度的实值向量V(w)。其中：V(w)就是词向量，m表示词向量长度。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817142112513.png" alt="image-20230817142112513">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>词嵌入&lt;/strong>：Word Embedding，将词映射到向量空间，形成词向量的过程和方法，就是词嵌入。&lt;/li>
&lt;/ul>
&lt;p>再来看一个经典的例子：&lt;/p>
&lt;ul>
&lt;li>对&lt;strong>男人、女人、国王、皇后&lt;/strong>开展词嵌入，它们变成了四个向量，存在与三维的向量空间中。&lt;/li>
&lt;li>假设词嵌入的具体代码实现没有问题，那么男人和国王的向量距离应该很近，女人和皇后的向量距离应该很近。&lt;/li>
&lt;li>这样，对人类的文字开展词嵌入后，有两个关键的输出：
&lt;ul>
&lt;li>&lt;strong>词以向量的形式存在&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>有关联的词对应的向量的距离也会很小&lt;/strong>(如：对两个词向量求余弦相似度)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817222631960.png" alt="image-20230817222631960">&lt;/p>
&lt;h1 id="4如何实现词嵌入">4.如何实现词嵌入？&lt;/h1>
&lt;p>词嵌入，或者说词的向量化过程分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>文本变为One-Hot编码。&lt;/li>
&lt;li>One-Hot编码变为低维词向量。&lt;/li>
&lt;/ul>
&lt;h2 id="1one-hot编码">(1)One-Hot编码&lt;/h2>
&lt;p>&lt;strong>One-Hot编码&lt;/strong>：One-Hot Representation，也叫独热编码。&lt;/p>
&lt;p>比如：对于男、女两个词，One-Hot编码可以表示为：&lt;/p>
&lt;ul>
&lt;li>男：[1, 0]，女：[0, 1]&lt;/li>
&lt;/ul>
&lt;p>再比如：对于初一、初二、初三三个词，可以表示为：&lt;/p>
&lt;ul>
&lt;li>初一：[1, 0, 0]，初二：[0, 1, 0]，初三：[0, 0, 1]&lt;/li>
&lt;/ul>
&lt;p>如果说One-Hot编码算得上广义的向量，那么它明显的缺点是：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>过于稀疏&lt;/strong>：在各个维度上，只有1个维度是1，其它维度都是0。&lt;/li>
&lt;/ul>
&lt;h2 id="2distributed表示">(2)Distributed表示&lt;/h2>
&lt;p>&lt;strong>Distributed表示&lt;/strong>：Distributed Representation，分布式表示，它是表示学习中的一种。&lt;/p>
&lt;ul>
&lt;li>Word2Vec、GloVe、fastText等都是其中一种具体实现。&lt;/li>
&lt;/ul>
&lt;p>分布式表示的核心思想是：&lt;/p>
&lt;ul>
&lt;li>通过训练，将词典里每个单词转换为固定长度的低维向量。&lt;/li>
&lt;li>这些向量之间可以通过余弦相似度之类的数学工具，表示向量之间的距离。语义越接近的向量之间，距离越近。&lt;/li>
&lt;/ul>
&lt;p>Distributed表示才算得上真正的向量，它明显的优点是：&lt;/p>
&lt;ul>
&lt;li>从&lt;strong>过于稀疏&lt;/strong>的One-Hot编码，变为了&lt;strong>密集向量&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="3word2vecdistributed表示的一种实现">(3)Word2Vec，Distributed表示的一种实现&lt;/h2>
&lt;p>Word2Vec是Distributed表示的一种具体实现：&lt;/p>
&lt;ul>
&lt;li>Word2Vec将词汇表中的每个词表示成固定长度的向量。&lt;/li>
&lt;li>Word2Vec的核心思想本质是——&lt;strong>近朱者赤，近墨者黑&lt;/strong>：
&lt;ul>
&lt;li>一个词可以根据它的的周边词，推测出自己的语义。&lt;/li>
&lt;li>一个词也可以通过自己的语义，推测出它的周边词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Word2Vec的实现算法有两种：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CBOW&lt;/strong>：Continuous Bag of Words，连续词袋，输入是周围词，输出是中心词。&lt;/li>
&lt;li>&lt;strong>Skip-Gram&lt;/strong>：跳字模型，输入是中心词，输出是周围词。&lt;/li>
&lt;/ul>
&lt;p>Word2Vec基于神经网络实现：&lt;/p>
&lt;ul>
&lt;li>但，无论是CBOW，还是Skip-Gram，都属于浅层神经网络。&lt;/li>
&lt;li>一说浅，显然就不那么高级。浅层神经网络的效果可以参考Word2Vec的原始论文：https://arxiv.org/pdf/1301.3781.pdf&lt;/li>
&lt;/ul>
&lt;h1 id="5代码实例手撸word2vec">5.代码实例：手撸Word2Vec&lt;/h1>
&lt;h2 id="step1构建语料库">STEP1.构建语料库&lt;/h2>
&lt;ul>
&lt;li>首先，以四个句子作为训练语料&lt;/li>
&lt;li>然后，形成&lt;strong>词+索引的Map&lt;/strong>和&lt;strong>索引+词的Map&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233124145.png" alt="image-20230817233124145">&lt;/p>
&lt;h2 id="step2生成skip-gram数据">STEP2.生成Skip-Gram数据&lt;/h2>
&lt;ul>
&lt;li>根据句子中的各个词，将相邻的词形成词对。&lt;/li>
&lt;li>如：(小美，是)、(是，小美)，(美女，小美)，(小美，美女)等&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233354716.png" alt="image-20230817233354716">&lt;/p>
&lt;h2 id="step3对skip-gram数据进行one-hot编码">STEP3.对Skip-Gram数据进行One-Hot编码&lt;/h2>
&lt;ul>
&lt;li>将STEP2的各个词对进行One-Hot编码&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233557328.png" alt="image-20230817233557328">&lt;/p>
&lt;h2 id="step4定义skip-gram模型">STEP4.定义Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>我们复现出论文描述的神经网络结构
&lt;ul>
&lt;li>一个线性层学习出词向量&lt;/li>
&lt;li>一个输出层验证学习到词向量作为中心词，是否能够预测出周围词。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817233757848.png" alt="image-20230817233757848">&lt;/p>
&lt;p>看一下具体的代码实现：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234116518.png" alt="image-20230817234116518">&lt;/p>
&lt;h2 id="step5训练skip-gram模型">STEP5.训练Skip-Gram模型&lt;/h2>
&lt;ul>
&lt;li>训练3000次，每一次都会从输出层触发反向传播，更新线性层的参数，最终确定本次学习到的词向量。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234302231.png" alt="image-20230817234302231">&lt;/p>
&lt;ul>
&lt;li>这是损失函数曲线，经过3000次的训练，损失逐渐降低。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234532709.png" alt="image-20230817234532709">&lt;/p>
&lt;h2 id="step6显示词向量">STEP6.显示词向量&lt;/h2>
&lt;ul>
&lt;li>将词向量打印出来，并绘制在向量空间中&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234606438.png" alt="image-20230817234606438">&lt;/p>
&lt;ul>
&lt;li>绘制出来的向量空间，结果如下：
&lt;ul>
&lt;li>小美和大漂亮的词向量分别为： [-0.45549557 0.4471412 ]，[-0.47936678 0.40784693]&lt;/li>
&lt;li>小帅和大壮的词向量分别为：[0.0154954 1.6643344]，[0.01839364 1.6814741 ]&lt;/li>
&lt;li>小美和大漂亮两个词关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>小帅和大壮两个词也关系密切，甚至重合，说明这两个向量有关联性。&lt;/li>
&lt;li>但，美女、帅哥两个词向量学习的不好。小美、大漂亮更接近于帅哥，出现了&lt;strong>不一定斩男但是一定斩女&lt;/strong>的现象。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B6/image-20230817234631498.png" alt="image-20230817234631498">&lt;/p>
&lt;h1 id="6再看表示学习嵌入">6.再看表示学习、嵌入&lt;/h1>
&lt;p>笔者在第5章以Word2Vec的Skip-Gram的代码实现，给大家阐述了词嵌入的实现过程和训练过程。&lt;/p>
&lt;p>回头再来看1~4章出现的各种概念和理论，可以归纳词嵌入的几个关键知识点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>自然语言问题转换为数学问题&lt;/strong>：将文字向量化的本质是将信息转换为向量，所以文字的语义关系巧妙地转换为了向量距离的数学问题。&lt;/li>
&lt;li>&lt;strong>自动化的学习过程&lt;/strong>：获得文字的向量是一个自动化的过程，通过神经网络这种工具自动学习得到词向量。&lt;/li>
&lt;li>&lt;strong>词向量的降维技术&lt;/strong>：对于词嵌入，学界有很多种降维技术，Word2Vec的CBOW和Skip-Gram都是实现了降维的具体算法。&lt;/li>
&lt;/ul>
&lt;p>再看第1章的表示学习，我们可以重新理解一下表示学习的三个特性：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>可分性&lt;/strong>：不相关的向量距离会很远，不相关的向量之间应该会有明显的分界线。这就体现了可分性——不同类别之间的向量样本应该有明显的边界或区别。&lt;/li>
&lt;li>&lt;strong>可解释性&lt;/strong>：为什么提到唐伯虎，人类会立刻联想到秋香？向量空间中相关的向量距离就会很近，这就体现了可解释性。&lt;/li>
&lt;li>&lt;strong>可推理性&lt;/strong>：给出小美，我们可以联想到大漂亮、帅哥，进而可能就会联想到小帅、大壮，说不定小美和小帅之间就一段故事，这就体现了可推理性。&lt;/li>
&lt;/ul>
&lt;p>写到这里，我们可以看到：&lt;/p>
&lt;ul>
&lt;li>以Word2Vec为代表的词向量化技术极大地推进了语言模型的进步&lt;/li>
&lt;li>词嵌入也成为了如今大语言模型GPT、LLama、Bert的核心组件之一。&lt;/li>
&lt;/ul>
&lt;p>在了解了词嵌入相关技术之后，我们下一步就来看大语言模型的另一个核心组件的起源：神经概率语言模型。&lt;/p></description></item><item><title>【chatGPT】学习笔记6-手撸一个上古GPT</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/</link><pubDate>Wed, 26 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4gpt/</guid><description>&lt;p>大家都在说chatGPT的本质是成语接龙——基于下一个词出现的概率，生成完整的句子，那么如何实现呢？&lt;/p>
&lt;p>今天我们来手撸一个上古GPT，理解一下其中的基本原理。&lt;/p>
&lt;h1 id="1语言模型简介">1.语言模型简介&lt;/h1>
&lt;h2 id="11什么是语言模型">1.1.什么是语言模型&lt;/h2>
&lt;p>&lt;strong>语言模型是一个用来&lt;font color=red>估计文本概率分布的数学模型&lt;/font>&lt;/strong>。通俗的说，你给它一个词，它能告诉你这个词之后通常大概率会接什么词。&lt;/p>
&lt;p>写这篇文章的时候，笔者拉了个人类做测试——我说一个词，他凭直觉补全他这句话：&lt;/p>
&lt;ul>
&lt;li>我说：孙悟。他答：孙悟空&lt;/li>
&lt;li>我说：哪吒。他答：哪吒三太子&lt;/li>
&lt;li>我说：白龙马。他答：白龙马蹄朝西&lt;/li>
&lt;li>我说：猪八戒。他答：猪八戒背媳妇&lt;/li>
&lt;/ul>
&lt;p>你看，这就是人脑中隐含了一个语言模型，直觉并不是直觉，而是一种文本概率分布。&lt;/p>
&lt;p>&lt;strong>GPT家族&lt;/strong>(如：chatGPT、GPT-4)、&lt;strong>BERT家族&lt;/strong>(如：MT-DNN、ERNIE)，都是语言模型中的一种。&lt;/p>
&lt;p>除了GPT、BERT这些新一代的语言模型，古早的语言模型还有&lt;strong>N-Gram&lt;/strong>、&lt;strong>RNN&lt;/strong>、&lt;strong>LSTM&lt;/strong>、&lt;strong>GRU&lt;/strong>等。&lt;/p>
&lt;p>但是，无论是新同志，还是老同志，它们作为语言模型的初心不会变——&lt;strong>估计文本概率分布&lt;/strong>。&lt;/p>
&lt;h2 id="12语言模型分类">1.2.语言模型分类&lt;/h2>
&lt;h3 id="121基于规则的语言模型">1.2.1.基于规则的语言模型&lt;/h3>
&lt;p>1970年，出现基于规则的语言模型。它通过语法树，将人类语言语法的规则描述出来。比如下面描述的最简单的语法树：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803090559437.png" alt="image-20230803090559437">&lt;/p>
&lt;p>很显然，人类语言的规则复杂且会变化，这种&lt;font color=red>&lt;strong>基于规则的语言模型过于死板&lt;/strong>&lt;/font>，不太可能有很好的效果。&lt;/p>
&lt;h3 id="122基于概率的语言模型">1.2.2.基于概率的语言模型&lt;/h3>
&lt;p>1990年，出现基于数据驱动的统计概率语言模型。&lt;/p>
&lt;p>这里不得不敬仰一下语音识别和自然语言处理专家——贾里尼克(Frederek Jelinek)。这位大神学术上极其严谨和务实，在IBM期间极度厌恶夸夸其谈的语言学家，曾经抛出了那句：&amp;ldquo;我每开除一名语言学家,我的语音识别系统错误率就降低一个百分点&amp;quot;的名言。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803091349097.png" alt="image-20230803091349097">&lt;/p>
&lt;p>著名的贾里尼克假设(&lt;font color=red>&lt;strong>一个句子是否合理，就看看它的可能性大小如何，至于可能性就用概率来衡量&lt;/strong>&lt;/font>)奠定了基于概率的语言模型。&lt;/p>
&lt;p>举个例子：小美同学听到一句英文&lt;code>The apple and 「pear」 salad is delicious.&lt;/code>，由于&lt;code>pear&lt;/code>和&lt;code>pair&lt;/code>发音很类似，那么到底是在这个句子中是&lt;code>pear&lt;/code>还是&lt;code>pair&lt;/code>呢？按照贾里尼克假设，&lt;code>pear&lt;/code>显然可能性更多，或者说概率更大。&lt;/p>
&lt;p>我们进一步将这里再展开一点：假设Y是一个有意义的句子，由一连串特定顺序排列的词X1、X2、X3&amp;hellip;Xn组成。如何求Y在自然语言中出现的可能性呢？&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803093047437.png" alt="image-20230803093047437">&lt;/p>
&lt;p>首先，根据条件概率进行转换：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803095827392.png" alt="image-20230803095827392">&lt;/p>
&lt;p>然后，根据马尔科夫假设进一步简化：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png" alt="image-20230803103655078">&lt;/p>
&lt;p>最终，就可以得到一个简化的语言模型&lt;code>Bi-Gram&lt;/code>：&lt;font color=red>&lt;strong>每个词称为1个&lt;code>Gram&lt;/code>，当前词由前一个词决定&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>这就是笔者在《【ChatGPT】ChatGPT学习笔记2-不是什么?是什么?有何方向?》中提到的GPT基础原理：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230719094340618.png" alt="image-20230719094340618">&lt;/p>
&lt;h3 id="123基于深度学习的语言模型">1.2.3.基于深度学习的语言模型&lt;/h3>
&lt;p>基于深度学习的语言模型的侧重点是学习工具，即强调使用神经网络进行学习。而基于概率的语言模型的侧重点是数据，即强调从数据中寻找规律。&lt;/p>
&lt;p>因此，基于深度学习的语言模型没有突破&lt;strong>基于概率&lt;/strong>的核心思想。&lt;/p>
&lt;h2 id="13语言模型的关键里程碑">1.3.语言模型的关键里程碑&lt;/h2>
&lt;p>从模型本身的发展看：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>N-Gram&lt;/strong>：基于前N−1个词，预测序列中的下一个词&lt;/li>
&lt;li>&lt;strong>Neural Probabilistic Language Model&lt;/strong>：神经概率语言模型。&lt;/li>
&lt;li>&lt;strong>Pre-trained Language Model&lt;/strong>：通过更大的语料库、更深的神经网络进行预训练和微调。&lt;/li>
&lt;/ul>
&lt;p>从词向量表示的发展看：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Bag-of-words&lt;/strong>：BOW，词袋模型。首次提出把词用向量表示，把文本表示为单词的集合，仅看词频(不考虑单词的顺序)。&lt;/li>
&lt;li>&lt;strong>Distributed Representation&lt;/strong>：以分布式激活的形式表示词，也是词用向量表示。&lt;/li>
&lt;li>&lt;strong>Word2vec&lt;/strong>：先把单词的含义学习好、用向量表示好，我们再继续用它进行后续训练。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803113828737.png" alt="image-20230803113828737">&lt;/p>
&lt;h1 id="2上古语言模型n-gram">2.上古语言模型N-Gram&lt;/h1>
&lt;h2 id="21五步实现n-gram">2.1.五步实现N-Gram&lt;/h2>
&lt;ul>
&lt;li>制作数据集&lt;/li>
&lt;li>分词&lt;/li>
&lt;li>计算每个N-Gram的词频&lt;/li>
&lt;li>计算每个N-Gram的概率&lt;/li>
&lt;li>根据输入词，生成连续文本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180415107.png" alt="image-20230803180415107">&lt;/p>
&lt;h2 id="22制作数据集">2.2.制作数据集&lt;/h2>
&lt;ul>
&lt;li>我们录入了三首唐诗作为数据集。实战中会录入更多的数据。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803172748930.png" alt="image-20230803172748930">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174612589.png" alt="image-20230803174612589">&lt;/p>
&lt;h2 id="23分词">2.3.分词&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>N-Gram模型会将数据集中每一条语句，拆分成N个词。每个词就是一个Gram。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分词本身不是N-Gram的重点，因此本文没有使用JieBa等分词三方件，直接按照单字来拆分。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分词时有个细节是子词，为了解决错别字等问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>看一下代码：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803173712704.png" alt="image-20230803173712704">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174634047.png" alt="image-20230803174634047">&lt;/p>
&lt;h2 id="24划分n-gram计算词频">2.4.划分N-Gram，计算词频&lt;/h2>
&lt;ul>
&lt;li>我们会根据N的具体数值，去计算N个字之后紧跟的字出现的词频。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174346725.png" alt="image-20230803174346725">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174709174.png" alt="image-20230803174709174">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803174751374.png" alt="image-20230803174751374">&lt;/p>
&lt;h2 id="25进一步计算概率">2.5.进一步计算概率&lt;/h2>
&lt;ul>
&lt;li>我们会进一步计算N个字之后紧跟的字出现的概率。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175136237.png" alt="image-20230803175136237">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=2时，就是看每个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175520160.png" alt="image-20230803175520160">&lt;/p>
&lt;ul>
&lt;li>看一下结果(当N=3时，就是看每两个字后紧跟的字出现的词频)：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803175541199.png" alt="image-20230803175541199">&lt;/p>
&lt;ul>
&lt;li>走到这里，我们会有一个重要发现：&lt;font color=red>&lt;strong>当N越大，后面紧跟的字的概率越趋于确定&lt;/strong>！&lt;/font>&lt;/li>
&lt;/ul>
&lt;h2 id="26使用n-gram模型生成文本">2.6.使用N-Gram模型，生成文本&lt;/h2>
&lt;ul>
&lt;li>当我们有了N-Gram模型，就可以根据输入词，预测生成完整的文本了。&lt;/li>
&lt;li>看一下代码：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180013173.png" alt="image-20230803180013173">&lt;/p>
&lt;ul>
&lt;li>看一下结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803180226046.png" alt="image-20230803180226046">&lt;/p>
&lt;h1 id="3总结">3.总结&lt;/h1>
&lt;ul>
&lt;li>语言模型本质是一个文本概率统计模型&lt;/li>
&lt;li>语言模型分为：基于规则的语言模型、基于概率的语言模型、基于深度学习的语言模型&lt;/li>
&lt;li>N-Gram是最古老的语言模型，通过分词，统计出每N-1个词后紧跟的词的概率，进而形成了文本概率统计模型。&lt;/li>
&lt;li>N-Gram的理论基础是：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E4%B8%8A%E5%8F%A4GPT/image-20230803103655078.png" alt="image-20230803103655078">&lt;/p></description></item><item><title>【chatGPT】学习笔记5-四次发展&amp;三个世界</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/</link><pubDate>Tue, 25 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/</guid><description>&lt;p>红杉资本在2022年7月，发布了大语言模型洞察报告。一年后的今天回看这篇洞察报告，洞察地很准。&lt;/p>
&lt;p>我们详细解读一下这份报告，从中提炼一些观点和知识，以提升我们对生成式AI的宏观洞见。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230726094029267.png" alt="image-20230726094029267">&lt;/p>
&lt;h1 id="1ai的四次大发展">1.AI的四次大发展&lt;/h1>
&lt;p>AI历史上经历了四次大发展：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>早期人工智能时代&lt;/strong>：AI的第一次大发展，1950年早期人工智能出现。&lt;/li>
&lt;li>&lt;strong>机器学习时代&lt;/strong>：AI的第二次大发展，1980年，机器学习逐步兴起，并蓬勃发展。&lt;/li>
&lt;li>&lt;strong>深度学习时代&lt;/strong>：AI的第三次大发展，2010年，深度学习的突破，驱动了AI的进一步发展。&lt;/li>
&lt;li>&lt;strong>大语言模型时代&lt;/strong>：AI的第四次大发展，2020年，大语言模型在&lt;strong>生成式学习&lt;/strong>路线上大放异彩。&lt;/li>
&lt;/ul>
&lt;p>报告洞察的时间段就是从后深度学习时代，到大语言模型时代，阐述了大语言模型时代的四次浪潮。&lt;/p>
&lt;p>另外，我们还可以发现一个有趣的现象：&lt;font color="red">&lt;strong>AI历次大发展的时间间隔逐步变小&lt;/strong>&lt;/font>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230726102141799.png" alt="image-20230726102141799">&lt;/p>
&lt;h1 id="2生成式ai的四次浪潮">2.生成式AI的四次浪潮&lt;/h1>
&lt;blockquote>
&lt;p>Sure enough, as the models get bigger and bigger, they begin to deliver human-level, and then superhuman results.&lt;/p>
&lt;p>果然，随着模型变得越来越大，它们开始提供人类的水平，然后是超人的结果。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第一个核心观点是：生成式AI越来越强大，在某些领域开始超越人类的水平。&lt;/p>
&lt;h2 id="21生成式ai与分析式ai的边界">2.1.生成式AI与分析式AI的边界&lt;/h2>
&lt;p>报告首先划定了&lt;strong>生成式AI&lt;/strong>与&lt;strong>分析式AI&lt;/strong>的边界：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>分析式AI&lt;/strong>：&lt;strong>Analytical AI&lt;/strong>，从早期人工智能时代到深度学习时代，人工智能更多地应用领域覆盖的是&lt;strong>人类的分析能力&lt;/strong>。如：识别车牌、识别垃圾邮件、用户画像与智能推荐等等。人工智能在这个时代，主要是在&lt;strong>分析已有的数据&lt;/strong>，进行&lt;strong>回归&lt;/strong>与&lt;strong>分类&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>生成式AI&lt;/strong>：&lt;strong>Generative AI&lt;/strong>，人工智能的应用领域开始覆盖&lt;strong>人类的创造能力&lt;/strong>。如：GPT写诗、Midjourney绘画等。以前人工智能在这些方面无法与人类抗衡，但现在人工智能已经开始可以创造有意义并具备美感的作品了。在这个时代，人工智能&lt;strong>不仅仅是分析已有的数据&lt;/strong>，而是在&lt;strong>生成新的内容&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="22agi第一波浪潮">2.2.AGI第一波浪潮&lt;/h2>
&lt;p>在前深度学习时代(2015年之前)，&lt;strong>小模型&lt;/strong>是主流模型，是某个领域的&lt;strong>专才&lt;/strong>，这些一个个的小模型都擅长做特定领域的&lt;strong>人类分析工作&lt;/strong>。&lt;/p>
&lt;p>但，对于生成内容的任务，小模型普遍都不擅长。&lt;/p>
&lt;h2 id="23agi第二波浪潮">2.3.AGI第二波浪潮&lt;/h2>
&lt;p>2019年，谷歌发表的《Attention is All You Need》，向我们描述了一种用于NLP的新型神经网络架构Transformer，随后各大厂在这条赛道上展开了激烈竞争。&lt;/p>
&lt;p>这个时期的大语言模型由于极其庞大、强依赖GPU等原因，大部分都没有公开可商用版本，或者都是内测版本，更没有可以落地的应用。&lt;/p>
&lt;p>但，随着大语言模型越来越大，也出现了很多神奇的现象(如：涌现能力)，在某些方面开始超越人类水平。&lt;/p>
&lt;p>在《Compute Trends Across Three Eras of Machine Learning》论文中，详细展示了人工智能在手写识别、语音识别、图像识别、阅读、语言理解的领域中，超越人类基准水平的测试结果。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727090714788.png" alt="image-20230727090714788">&lt;/p>
&lt;h2 id="24agi第三波浪潮">2.4.AGI第三波浪潮&lt;/h2>
&lt;p>在2022年之后，大语言模型开始进入&lt;strong>更好、更快、更便宜&lt;/strong>的阶段。计算成本开始下降、新的技术出现(如：diffusion models、CoT、ToT等)，都降低了训练和推理的成本。最大的进展不是大语言模型本身的进步，而是开源，很多学术界机构能够基于基础大语言模型去进一步训练和研究。&lt;/p>
&lt;p>笔者认为特别是训练技术方面，成本下降很快，成本的下降带来了门槛的下降。几个月前我去搭建自己的本地GPT环境花了很长时间和费用，但上周LLama2发布后，搭建自己的本地大模型就非常便宜和容易。&lt;/p>
&lt;p>此时，随之而来的应用开始出现(比如：chatGPT)。很多国人是这个时候才听说了大语言模型的概念，甚至很多人的知识结构还停留在机器学习时代。&lt;/p>
&lt;h2 id="24agi第四波浪潮">2.4.AGI第四波浪潮&lt;/h2>
&lt;p>这是红杉资本对眼前和未来的预期：杀手级应用涌现。随着大语言模型继续变得更好、更快、更便宜，会出现越来越多的免费、开源的模型。应用层也会出现大爆发。红杉资本在本文中多次强调：&amp;quot;&lt;strong>我们预计杀手级应用也将出现在生成式AI领域中，大家争相发力，前景让人期待&lt;/strong>&amp;quot;。&lt;/p>
&lt;h1 id="3生成式ai的市场格局">3.生成式AI的市场格局&lt;/h1>
&lt;blockquote>
&lt;p>The best Generative AI companies can generate a sustainable competitive advantage by executing relentlessly on the flywheel between user engagement/data and model performance.&lt;/p>
&lt;p>最好的生成式人工智能公司可以通过在用户参与度、数据、模型能力之间不断地迭代，进而产生可持续的竞争优势。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第二个核心观点是：根据生成式AI的市场格局，在文本、代码、图像、语音、视频、3D等领域，需要应用、模型、数据的相互配合、共同成熟。&lt;/p>
&lt;h2 id="31市场格局">3.1.市场格局&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>文本方向&lt;/strong>：模型以GPT-3为代表，应用领域的机会点会出现在行销、销售、服务、写作、笔记等垂直领域。&lt;/li>
&lt;li>&lt;strong>代码方向&lt;/strong>：模型以GPT-3为代表，应用领域的机会点会出现在代码生成、文档生成、自然语言转代码、Web/App快速构建。&lt;/li>
&lt;li>&lt;strong>图像方向&lt;/strong>：模型以DallE、Stable Diffusion为代表，应用领域机会点在设计领域。&lt;/li>
&lt;li>&lt;strong>语音/视频/3D方向&lt;/strong>：目前这些方向不明。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/genai-landscape-8.webp" alt="genai-landscape-8">&lt;/p>
&lt;h2 id="32市场机会窗">3.2.市场机会窗&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>文本方向&lt;/strong>：关窗时间大约在2025年。&lt;/li>
&lt;li>&lt;strong>代码方向&lt;/strong>：关窗时间大约在2025年。&lt;/li>
&lt;li>&lt;strong>图像方向&lt;/strong>：关窗时间大约在2030年。&lt;/li>
&lt;/ul>
&lt;p>&lt;font color="red" size=bold>**看到这个时间窗，是不是有一种时不我待的危机感？**&lt;/font>&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/genai-timeline-7.webp" alt="genai-timeline-7">&lt;/p>
&lt;h1 id="4对杀手级应用的预测">4.对杀手级应用的预测&lt;/h1>
&lt;blockquote>
&lt;p>The best Generative AI companies can generate a sustainable competitive advantage by executing relentlessly on the flywheel between user engagement/data and model performance.&lt;/p>
&lt;p>最好的生成式人工智能公司可以通过在用户参与度/数据和模型性能之间不断地执行飞轮来产生可持续的竞争优势。&lt;/p>
&lt;/blockquote>
&lt;p>上述是报告的第三个核心观点是：强调数据和模型的良性循环是杀手级应用的成功关键。&lt;/p>
&lt;h2 id="41杀手级应用的形态">4.1.杀手级应用的形态&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>强大底座&lt;/strong>：杀手级应用一定依托一个强大的大模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>插件化&lt;/strong>：这些杀手级应用会以插件的形式寄生于现有的软件中，比如：VSCode中集成Code Copilot、PhotoShop中集成某种自动生成图片的插件。&lt;/p>
&lt;ul>
&lt;li>PS：笔者刚开始不太认同这一点，难道就不能是一个新的应用软件吗？&lt;strong>看到4.2才理解红杉资本的思路&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>交互式体验&lt;/strong>：目前的GPT都是&lt;strong>一次交互&lt;/strong>——比如：用户问一次，AI就生成一张图片。未来应该会出现&lt;strong>多次交互&lt;/strong>——用户问一次，AI生成一个图片。用户再要求AI根据这个图片继续修正，直到做出用户满意的图片。&lt;/p>
&lt;ul>
&lt;li>PS：这一点很有道理。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="42如何成为杀手级应用">4.2.如何成为杀手级应用？&lt;/h2>
&lt;p>红杉资本强调**用户参与度/数据(user engagement/data)&lt;strong>与&lt;/strong>模型能力(model performance)**要形成良性循环：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>良性循环&lt;/strong>：①获得极高的用户参与度→②获得更多用户数据以训练出更好的模型(如：提示词工程、Fine-tuning，将用户行为作为标记的训练数据等)→③优秀的模型吸引更多的用户并提升参与度。&lt;/li>
&lt;li>&lt;strong>聚焦垂直领域&lt;/strong>：杀手级应用不必做的大而全，而是聚焦某个垂直领域。&lt;/li>
&lt;li>&lt;strong>植入客户工作流程&lt;/strong>：第一步先将AI以插件的形式整合到用户的生产流程中，进而实现用户增长和产品分发。第二步再做出新的软件系统替代用户的老软件系统。
&lt;ul>
&lt;li>PS：红杉资本给大伙儿指明了赚钱方法了，还不快去。。。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="5你我只知道图灵测试它却知道三个世界">5.你我只知道图灵测试，它却知道三个世界&lt;/h1>
&lt;blockquote>
&lt;p>Generative AI is still very early. The platform layer is just getting good, and the application space has barely gotten going.&lt;/p>
&lt;p>生成式人工智能仍处于早期阶段。平台层刚刚变得越来越好，而应用程序空间几乎没有发展起来。&lt;/p>
&lt;/blockquote>
&lt;p>这是红杉资本的最后一个观点，报告中的这一部分不太有更多干货了。笔者阅读这一部分的时候，思路情不自禁地跳跃到那个一直没有想通的&lt;strong>哲学问题&lt;/strong>：&lt;font color=red>**人类智能到底是什么？通用人工智能到底是什么？是什么驱动了人类智能和通用人工智能的发展？**&lt;/font>&lt;/p>
&lt;p>其实很多人只听说过图灵测试，并且误解了图灵测试。图灵测试是一种思想测试，这个测试的理论基础是什么？如果理论基础是错的，通过了图灵测试又能证明什么？笔者在《【chatGPT】学习笔记1-机器还需要多久才能像人一样思考》文中，试图用&lt;strong>计算理论&lt;/strong>来解释人类智能、通用人工智能的关系与差异，但&lt;strong>计算理论&lt;/strong>作为图灵测试的理论基础也不太充分。&lt;/p>
&lt;p>最近阅读&lt;strong>中文房间(Chinese Room)思想实验&lt;/strong>时，找到了&lt;strong>Karl Raimund Popper&lt;/strong>的&lt;strong>三个世界理论&lt;/strong>，这个理论似乎可以支撑图灵测试：&lt;/p>
&lt;ul>
&lt;li>世界1：物质的世界(客观世界)，如：物质、能量、有机物质or无机物质&lt;/li>
&lt;li>世界2：心理的世界(主观世界)，如：感觉、意识、心里状态的过程&lt;/li>
&lt;li>世界3：思想的世界，或者叫知识的世界。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727113454114.png" alt="image-20230727113454114">&lt;/p>
&lt;p>那么，三个世界理论的内在逻辑是什么呢？&lt;/p>
&lt;ul>
&lt;li>宇宙的发展是从世界1，到世界2，到世界3的连续过程。&lt;/li>
&lt;li>世界1和世界2存在直接相互作用，世界2和世界3存在直接相互作用，世界1和世界3的相互作用需要通过世界2为中介。&lt;/li>
&lt;li>再通俗一点，人的意识(世界2)可以通过知识(世界3)创造新的物质(世界1)。&lt;/li>
&lt;li>再通俗一点，现有知识(世界3)不足以解决客观问题(世界1)，会抛出&lt;strong>问题&lt;/strong>，驱动人的意识(世界2)发现、总结出新的知识(世界3)，同时解决问题(世界1)。&lt;/li>
&lt;li>再通俗一点，人工智能可以看作一个知识大合集，人工智能存在于(世界3)。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727114108607.png" alt="image-20230727114108607">&lt;/p>
&lt;p>最后，提出三个世界的哲学价值是什么？&lt;/p>
&lt;ul>
&lt;li>正因为知识(世界3)能够&lt;strong>抛出问题&lt;/strong>，直接影响世界2，间接影响世界3，所以世界3的知识是可以自我发展的。&lt;/li>
&lt;li>比如：是人类发现了&lt;strong>数列&lt;/strong>这种知识，于是世界3就增加了&lt;strong>数列&lt;/strong>这个新知识，因为世界3中有了数列，世界3还会增加新知识&lt;strong>奇偶数&lt;/strong>。奇偶数并不是人类意识(世界2)的新发现，而是由于&lt;strong>人类在世界3创造了数列这种新知识&lt;/strong>而产生的&lt;strong>结果&lt;/strong>。&lt;/li>
&lt;li>通俗一点说，世界3知识的起点可能是人类，但世界3知识的发展、终点不是人类可以决定的。&lt;/li>
&lt;li>这就是三个世界的哲学价值！这就是哲学意义上的&lt;font color=red>&lt;strong>人工智能科学发展观&lt;/strong>&lt;/font>！是不是可以自洽地解释通用人工智能是什么、通用人工智能会如何发展？&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05-%E5%9B%9B%E6%AC%A1%E5%8F%91%E5%B1%95&amp;amp;%E4%B8%89%E4%B8%AA%E4%B8%96%E7%95%8C/image-20230727115506988.png" alt="image-20230727115506988">&lt;/p>
&lt;p>写到这里，有两点感慨：&lt;/p>
&lt;ul>
&lt;li>很多时候，&lt;strong>人类没有&lt;/strong>自己的&lt;strong>独立思想和观点&lt;/strong>：可能没有(比如：他什么都不说)、可能只是重复(比如：他常常将图灵测试挂在嘴边，却不理解其中深意)、可能混乱(比如：他把图灵测试这种结果当做本质的起因)。&lt;/li>
&lt;li>&lt;strong>AI是独立思想和观点本身&lt;/strong>：AI存在于世界3，AI是知识的合集，那么可不可以说AI就是知识、思想、观点本身呢？&lt;/li>
&lt;/ul>
&lt;p>所以，&lt;strong>你我只知道图灵测试，它却知道三个世界&lt;/strong>。&lt;/p>
&lt;p>所以，&lt;strong>我们应该拥抱它，还是恐惧它，还是服从它。。。&lt;/strong>？&lt;/p>
&lt;h1 id="6参考">6.参考&lt;/h1>
&lt;blockquote>
&lt;p>红杉资本报告原文：https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记4-机器学习基本原理(下)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8B/</link><pubDate>Sat, 22 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8B/</guid><description>&lt;p>上篇为大家建立了宏观的机器学习概念，下篇我们通过一个真实的机器学习任务来理解一下机器学习的微观逻辑。&lt;/p>
&lt;h1 id="1机器学习任务介绍">1.机器学习任务介绍&lt;/h1>
&lt;p>前段时间，同事上班路上骑车摔泥坑里骨折了，我就萌生了让人工智能自动识别路面泥坑的想法。&lt;/p>
&lt;p>毕竟，同事要互相关怀、相亲相爱。另外，我们也可以通过这个机器学习的任务理解一下机器学习的基本原理。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722092656554.png" alt="image-20230722092656554">&lt;/p>
&lt;h1 id="2机器学习中的常用术语一览">2.机器学习中的常用术语一览&lt;/h1>
&lt;p>开始机器学习的训练任务之前，我们快速浏览一下机器学习领域中的常用术语。假设AI识别路面泥坑是一个函数：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722093341203.png" alt="image-20230722093341203">&lt;/p>
&lt;h2 id="21模型是什么">2.1.模型是什么?&lt;/h2>
&lt;p>函数f的输入是路面的视频，输出是路面上是否有泥坑。输入称为&lt;strong>自变量&lt;/strong>，在机器学习中也称为&lt;strong>特征&lt;/strong>。输出称为&lt;strong>因变量&lt;/strong>，在机器学习中也称为&lt;strong>标签&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722093803701.png" alt="image-20230722093803701">&lt;/p>
&lt;p>这个函数f称为&lt;strong>模型&lt;/strong>，复杂的模型可能有数量庞大的自变量。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722094251783.png" alt="image-20230722094251783">&lt;/p>
&lt;h2 id="22怎样才算找到了函数f">2.2.怎样才算找到了函数f？&lt;/h2>
&lt;p>怎么才算找到了函数f呢？假设这个函数是个简单的函数：&lt;code>y=ax+b&lt;/code>，那么找到a和b，就算找到了这个函数f。a、b也被称为&lt;strong>模型的参数&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722095702788.png" alt="image-20230722095702788">&lt;/p>
&lt;p>除了模型的参数，还有一种参数叫做&lt;strong>超参数(hyperparameter)&lt;/strong>，这些模型的外部参数，属于训练和调试的过程参数。&lt;/p>
&lt;h2 id="23如何找到函数f">2.3.如何找到函数f？&lt;/h2>
&lt;p>那么，我们又如何找到函数f呢？机器学习的&lt;strong>核心思想&lt;/strong>就是&amp;rdquo;&lt;strong>给机器一堆数据，让机器自己从数据中提取特征、寻找规律&lt;/strong>&amp;quot;。这个过程，也称为&lt;strong>炼丹&lt;/strong>。&lt;/p>
&lt;p>这些数据称为&lt;strong>数据集&lt;/strong>，这些数据通常被分为三波，一波用于训练、一波用于验证、一波用于最终的测试。&lt;/p>
&lt;p>用于训练的称为&lt;strong>训练集&lt;/strong>，用于验证的称为&lt;strong>验证集&lt;/strong>，用于测试的称为&lt;strong>测试集&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722100415709.png" alt="image-20230722100415709">&lt;/p>
&lt;h2 id="24找函数f的流派">2.4.找函数f的流派&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>从数据的角度看，人类对全部训练数据做了标注称为&lt;strong>自监督学习&lt;/strong>，对部分训练数据做了标注称为&lt;strong>半监督学习&lt;/strong>、对训练数据没有任何标注称为&lt;strong>无监督学习&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从数学工具的角度看，使用神经网络开展学习称为&lt;strong>机器学习&lt;/strong>，将神经网络做深称为&lt;strong>深度学习&lt;/strong>(不严谨，通俗点儿就这么理解吧)，因此深度学习是机器学习中的一种。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>除了机器学习流派，&lt;strong>强化学习&lt;/strong>(目前也很热门的流派)、&lt;strong>生成式学习&lt;/strong>、&lt;strong>迁移学习&lt;/strong>等等。这些流派的边界没有太清晰，彼此会有交集和互相引用。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这里想到两个有趣的小点：&lt;/p>
&lt;ul>
&lt;li>神经网络很早就发明了，但后续却要换个机器学习、深度学习的名字，可能是联结主义之前被符号主义学派打的太惨了，需要换个响亮的名号吧。&lt;/li>
&lt;li>GPT火了以后，有很多名词术语，听到有人把&amp;quot;用提示词的引导行为&amp;quot;称为&amp;quot;zero shot、few shot&amp;rdquo;，乱抛名词、概念搞混，根因就是没理解前述这些机器学习的基本概念。&lt;/li>
&lt;/ul>
&lt;h1 id="3从1个例子看机器学习过程">3.从1个例子看机器学习过程&lt;/h1>
&lt;h2 id="31数据的收集和预处理">3.1.数据的收集和预处理&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>我们要识别路面泥坑，首先要获得路面视频数据。这个过程称为&lt;strong>原始数据收集&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>我们需要通过一些数据标注的工具，在视频上标注出路面泥坑，这个过程称为&lt;strong>数据标注&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据标注完后，需要将&lt;strong>数据向量化&lt;/strong>：data vectorization，就是把原始数据格式化，使机器可以读取(如：将原始图片转换为机器可以读取的矩阵)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>处理坏数据和缺失值：数据不是全都用的，要利用数据处理工具把&amp;rdquo;&lt;strong>捣乱数据&lt;/strong>&amp;ldquo;处理掉(冗余数据、离群数据、错误数据)，把&amp;rdquo;&lt;strong>缺失值&lt;/strong>&amp;ldquo;补上。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>数据预处理有很多技术和技巧：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>特征缩放&lt;/strong>：feature scaling，包括&lt;strong>数据标准化&lt;/strong>(standardization)和&lt;strong>数据规范化&lt;/strong>(normalization)等：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据标准化&lt;/strong>：对数据特征分布的转换，目标是使数据特征符合正态分布(均值为0，标准差为1)。如果数据特征不符合正态分布，会影响机器学习效率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据规范化&lt;/strong>：它是数据标准化的一种变体，将数据特征压缩到给定的最小值和最大值之间(通常为0~1)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>特征提取&lt;/strong>：feature extraction，通过子特征的选择，减少冗余特征，使初始测量数据更简洁，同时保留最有用的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>特征工程&lt;/strong>：使用数据的领域知识，来创建使得机器学习算法起作用的特征过程。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>通俗地理解：&lt;/p>
&lt;ul>
&lt;li>数据预处理的本质：就是&lt;strong>准备好机器看得懂、看得清的教材&lt;/strong>，方便机器进行学习。&lt;/li>
&lt;/ul>
&lt;p>我们再来看看实战中我们具体怎么做的：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>原始数据收集&lt;/strong>：通过行车记录仪，我收集了上班路上的路面情况。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723000930967.png" alt="image-20230723000930967">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据标注&lt;/strong>：通过标注工具，对视频中的泥坑进行标注。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001158467.png" alt="image-20230723001158467">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>数据集划分&lt;/strong>：我们通常会将标注好的数据划分为训练集、验证集、测试集。划分有一些方法论，如：K-fold validation。我将数据集划分编写成了一段脚本，执行后就自动划分好了训练集、验证集、测试集。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001237851.png" alt="image-20230723001237851">&lt;/p>
&lt;blockquote>
&lt;p>实战中，特征工程、数据向量化等，大多都被封装到了相应的AI训练框架中，本文不赘述。&lt;/p>
&lt;/blockquote>
&lt;h2 id="32设定范围">3.2.设定范围&lt;/h2>
&lt;p>设定范围指的是设定选择模型的范围，这要根据训练任务选择合适的数学模型。常见的模型如下：&lt;/p>
&lt;ul>
&lt;li>线性模型：线性回归、逻辑回归&lt;/li>
&lt;li>非线性模型：支持向量机、k最邻近分类&lt;/li>
&lt;li>基于树和集成的模型：决策树、随机森林、梯度提升树等&lt;/li>
&lt;li>神经网络：人工神经网络、卷积神经网络、长短期记忆网络等&lt;/li>
&lt;/ul>
&lt;p>模型选择是最为关键的环节，实战中，训练资料越少，模型就要圈定在很小很严格的范围内。&lt;/p>
&lt;p>否则，会产生一种现象：选择的这种模型在训练中表现很好，但换一波数据就不行了，说白了就是这个模型把数据死记硬背下来了。&lt;/p>
&lt;h2 id="33设定标准">3.3.设定标准&lt;/h2>
&lt;p>在确定了模型之后，就需要设定损失函数，损失函数用来表示机器&lt;strong>学到位&lt;/strong>了。&lt;/p>
&lt;p>以监督学习为例：&lt;/p>
&lt;ul>
&lt;li>人类老师标定了猪用103表示(如右图)&lt;/li>
&lt;li>人类老师标定了蓝精灵用17表示(如右图)&lt;/li>
&lt;li>当机器学习到了一个函数f1时，
&lt;ul>
&lt;li>输入一个猪头，函数f1输出100，那么距离正确答案103的差值为3。&lt;/li>
&lt;li>输入一个蓝精灵，函数f1输出18，那么距离正确答案17的差值为1。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>因此，函数f1整体距离正确答案为4，这就表示函数f1的损失函数Loss(f1)=4。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722221402457.png" alt="image-20230722221402457">&lt;/p>
&lt;h2 id="34达成目标">3.4.达成目标&lt;/h2>
&lt;p>当机器开始学习后，可能学到了N个不同的函数，那么就要从里面选择一个最优的函数，这个过程叫&lt;strong>最佳化(Optimization)&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722222441299.png" alt="image-20230722222441299">&lt;/p>
&lt;p>整个过程，可以归纳为如下公式：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230722223058532.png" alt="image-20230722223058532">&lt;/p>
&lt;h2 id="35炼丹师超参数调优">3.5.炼丹师：超参数调优&lt;/h2>
&lt;p>从&amp;quot;设定范围&amp;quot;到&amp;quot;达成目标&amp;rdquo;，整个过程不是一帆风顺的，在训练过程中、在超参数调优过程中，都会出现&lt;strong>过拟合&lt;/strong>(过拟合就是当前这批数据表现不错，换一批数据就不灵了)。因此需要反复迭代。人工智能工程师此时就很像一个古代的炼丹师，不断地通过调参，对模型进行优化和泛化。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>优化&lt;/strong>：optimazation，就是让模型更好地拟合现有的数据。&lt;/li>
&lt;li>&lt;strong>泛化&lt;/strong>：generalization，就是将当前的模型泛化到其他数据集上。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723000619543-16900419818941.png" alt="image-20230723000619543">&lt;/p>
&lt;p>实战中需要考虑算法工程化，比如本训练任务，我们只需要简单的命令行，就能把训练过程跑起来。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001453414.png" alt="image-20230723001453414">&lt;/p>
&lt;h2 id="36模型测试">3.6.模型测试&lt;/h2>
&lt;p>当模型训练好后，就要用测试数据集进行测试了，我们可以通过：&lt;/p>
&lt;ul>
&lt;li>各类训练指标进行判断。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723001817880.png" alt="image-20230723001817880">&lt;/p>
&lt;ul>
&lt;li>当然，也可以用直观的方法观看(PS：本次训练任务有点出乎意料，除了能识别泥坑，路面的裂缝也被识别出来了。)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002107388.png" alt="image-20230723002107388">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002143084.png" alt="image-20230723002143084">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002208208.png" alt="image-20230723002208208">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002237309.png" alt="image-20230723002237309">&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8B)/image-20230723002257695.png" alt="image-20230723002257695">&lt;/p>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记3-机器学习基本原理(上)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8A/</link><pubDate>Tue, 18 Jul 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8A/</guid><description>&lt;p>现在的热点是大语言模型，为什么我们还要了解机器学习？因为从机器学习到深度学习，再到如今的大语言模型，环环相扣。天底下没有新鲜事，并不是突然出现了一个划时代的技术。从历史、从论文都能按图索骥地找到大语言模型的源头——机器学习。&lt;/p>
&lt;h1 id="1机器学习的目标是什么">1.机器学习的目标是什么?&lt;/h1>
&lt;p>机器学习的目标就是从数据中发现规律。&lt;/p>
&lt;p>这个过程本质是一种统计建模的过程，是统计学方法的一种具体应用。&lt;/p>
&lt;p>下图引用自李宏毅老师的课件：&lt;/p>
&lt;ul>
&lt;li>ChatGPT找到了回答问题的函数f，输入&amp;quot;什么是机器学习？&amp;quot;，函数f就能生成答案。&lt;/li>
&lt;li>Midjourney找到了生成图片的函数f，输入&amp;quot;一只可爱的猫&amp;rdquo;，函数f就能生成小猫的图片。&lt;/li>
&lt;li>AlphaGo找到了下棋的函数f，输入&amp;quot;对手的一步棋&amp;rdquo;，函数f就能生成下一步应该怎么走。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720150809229.png" alt="image-20230720150809229">&lt;/p>
&lt;h1 id="2机器学习和人类学习的相似之处">2.机器学习和人类学习的相似之处&lt;/h1>
&lt;p>人类学习和机器学习有很多相似之处，本质上：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>人类学习&lt;/strong>：从经验中总结规律——俗称经受社会的毒打。&lt;/li>
&lt;li>&lt;strong>机器学习&lt;/strong>：从数据中总结模型——俗称经受数据的洗礼。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720152149924.png" alt="image-20230720152149924">&lt;/p>
&lt;h1 id="3机器学习打破了早期ai范式">3.机器学习打破了早期AI范式&lt;/h1>
&lt;p>人工智能的发展大致经历了四代，第一代AI以符号主义为主流，第二代至今以联结主义为主流(机器学习、深度学习就属于这个流派)。&lt;/p>
&lt;p>后来还有三个法国神人，绘制了人工智能发展历史上的两个流派的竞争过程：&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720153137135.png" alt="image-20230720153137135">&lt;/p>
&lt;p>早期的人工智能到底是怎样的呢？大致如下：&lt;/p>
&lt;ul>
&lt;li>STEP1.人类某个领域的专家总结该领域的业务规则。&lt;/li>
&lt;li>STEP2.人类程序员将业务规则以代码的形式实现，形成专家系统。&lt;/li>
&lt;li>STEP3.最终用户输入问题后，专家系统匹配规则，进行答复。&lt;/li>
&lt;/ul>
&lt;p>早期的人工智能的本质就是&lt;strong>人类总结规则、设定规则&lt;/strong>。这种范式的最大问题就是人类总结的规则不足，甚至人类本身总结不出来过于复杂的规则，导致早期的人工智能无法推广应用。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86(%E4%B8%8A)/image-20230720154059662.png" alt="image-20230720154059662">&lt;/p>
&lt;p>而出自于联结主义的机器学习推崇的就是&lt;strong>不用人类总结规则，给我数据我来总结规律&lt;/strong>。&lt;/p>
&lt;p>有一种言论说：神经网络的产生、海量数据的出现、算力的提升造就了机器学习的风靡。&lt;/p>
&lt;p>这种言论应该不是机器学习风靡的根因，真正的根因应该是&lt;strong>机器学习改变了早期AI的范式&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>本文部分内容由ChatGPT生成&lt;/p>
&lt;/blockquote></description></item><item><title>【chatGPT】学习笔记2-不是什么?是什么?有何方向?</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/</link><pubDate>Tue, 18 Jul 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/</guid><description>&lt;p>最近有小伙伴提意见，希望我的专栏能用小短篇的形式，将大模型相关知识通俗阐述一下。&lt;/p>
&lt;p>特此请ChatGPT协助我一起将近几个月阅读的资料以笔记体记录下来，与大家分享。&lt;/p>
&lt;h1 id="1chatgpt不是什么">1.ChatGPT不是什么?&lt;/h1>
&lt;h2 id="11古早的聊天机器人">1.1.古早的聊天机器人&lt;/h2>
&lt;p>一些古早的聊天机器人、客服机器人，有采用如下方式进行实现：&lt;/p>
&lt;ul>
&lt;li>利用分词组件对用户输入的问题进行分词，获得问题中的关键词。&lt;/li>
&lt;li>将关键词，在数据库、在互联网的上中进行搜索。
&lt;ul>
&lt;li>其中，数据库中预置了很多问题和答案。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将得到的答案过滤清洗、随机返回其中1个答案。&lt;/li>
&lt;/ul>
&lt;h2 id="12现代的聊天机器人">1.2.现代的聊天机器人&lt;/h2>
&lt;p>&lt;strong>ChatGPT及其更早的NLP领域技术&lt;/strong>，早已&lt;strong>不是从数据库、互联网&lt;/strong>生成答案的。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230718234119962.png" alt="image-20230718234119962">&lt;/p>
&lt;p>ChatGPT官方也明确表达了&amp;quot;ChatGPT is not connected to the internet&amp;rdquo;。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719065005501.png" alt="image-20230719065005501">&lt;/p>
&lt;h1 id="2chatgpt是什么">2.ChatGPT是什么？&lt;/h1>
&lt;h2 id="21大模型的生成原理chatgpt版">2.1.大模型的生成原理(ChatGPT版)&lt;/h2>
&lt;ul>
&lt;li>请ChatGPT解释一下大模型的生成原理，如下：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719092626552.png" alt="image-20230719092626552">&lt;/p>
&lt;h2 id="22大模型的生成原理通俗版">2.2.大模型的生成原理(通俗版)&lt;/h2>
&lt;p>我们再来通俗理解一下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP1&lt;/strong>：问一个问题&amp;quot;什么是ChatGPT？&amp;quot;，可以将ChatGPT看成1个函数F，这个函数会输出即将回答的答案中第一个字(词)的概率。比如：根据问题，答案第一个字是&amp;quot;c&amp;quot;的概率是最大，最终就认为答案的第1个字是&amp;quot;c&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719093253167.png" alt="image-20230719093253167">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP2&lt;/strong>：将STEP1的输出追加到问题上，问题就变成了&amp;quot;什么是ChatGPT?c&amp;rdquo;，继续输入给函数F，得到新的概率，最终认为答案的第2个字是&amp;quot;h&amp;rdquo;。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719094213880.png" alt="image-20230719094213880">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>STEP3、STEP4、STEP5&lt;/strong>：不断重复STEP1、STEP2的动作，就会得到完整的答案，如下图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719094340618.png" alt="image-20230719094340618">&lt;/p>
&lt;h1 id="3未来的研究方向">3.未来的研究方向？&lt;/h1>
&lt;p>大模型未来的热点研究方向很多很多：提示词工程、神经网络编辑、机器遗忘、自适应计算和自适应模型、跨模态学习、模型压缩和加速、集成多任务和元学习、强化学习和自监督学习等等。&lt;/p>
&lt;p>笔者认为&amp;rdquo;&lt;strong>提示词工程、神经网络编辑、机器遗忘&lt;/strong>&amp;ldquo;最为有趣，因为它们和脑科学有点相近之处，都是在&amp;quot;激发神经网络&amp;rdquo;、&amp;ldquo;修改神经网络&amp;rdquo;、&amp;ldquo;抹除神经网络&amp;rdquo;。&lt;/p>
&lt;h2 id="31提示词工程">3.1.提示词工程&lt;/h2>
&lt;ul>
&lt;li>Prompt Engineering，吴恩达老师的课程让这个热点更热。提示词工程的本质可以认为：大模型好像一个&lt;strong>博学的老人家&lt;/strong>(世界知识几乎都知道)，只是&lt;strong>记性不太好&lt;/strong>。你需要用一些特殊的提示词唤醒他的记忆(俗称会念咒)，他就能很好地回答你的问题。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719085754568.png" alt="image-20230719085754568">&lt;/p>
&lt;h2 id="32神经网络编辑">3.2.神经网络编辑&lt;/h2>
&lt;p>Neural Editing，随着模型越来越大，训练好一个模型的成本极高，重新训练几乎不可能，因此需要有一种改进神经网络生成结果的技术，可以通过对模型的生成结果进行编辑，来获得更准确、更自然的结果。该技术通常涉及到在生成结果中插入或删除一些元素，或者重新排列生成结果的顺序，以获得更好的输出结果。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719090639814.png" alt="image-20230719090639814">&lt;/p>
&lt;h2 id="33机器遗忘">3.3.机器遗忘&lt;/h2>
&lt;p>Machine Unlearning，机器遗忘是一种让机器学会忘记以前的知识的技术。说白了，就是&amp;quot;你知道了不该知道的事情&amp;rdquo;。这项技术可以应用于安全隐私、AI伦理领域。&lt;/p>
&lt;p>&lt;img src="https://jherculesqz.github.io/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02-%E4%B8%8D%E6%98%AF%E4%BB%80%E4%B9%88_%E6%98%AF%E4%BB%80%E4%B9%88_%E6%9C%89%E4%BD%95%E6%96%B9%E5%90%91/image-20230719091436950.png" alt="image-20230719091436950">&lt;/p>
&lt;h2 id="34其它方向">3.4.其它方向&lt;/h2>
&lt;p>以下来自于ChatGPT的解释，供参考：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>自适应计算和自适应模型&lt;/strong>：在大模型上进行计算是一项巨大的挑战，因为它们需要大量的计算资源和内存。因此，未来的研究方向将包括自适应计算和自适应模型，旨在优化大模型的计算效率和资源利用率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>跨模态学习&lt;/strong>：指使用多种类型的数据来训练一个模型。例如，将文本、图像和语音数据结合起来训练一个模型，以获得更准确、更全面的结果。未来的研究方向将探索如何在大模型上实现跨模态学习，以进一步提高模型的准确性和泛化能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>模型压缩和加速&lt;/strong>：由于大模型需要大量的计算资源和内存，因此未来的研究方向将包括模型压缩和加速，以减少模型的大小和计算负载，从而使模型更加可扩展和可用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>集成多任务和元学习&lt;/strong>：指在一个模型中同时处理多个任务，例如，在自然语言处理中同时进行命名实体识别和情感分析。元学习是指在训练过程中学习如何学习的过程。未来的研究方向将集中在如何在大模型上实现多任务学习和元学习，以提高模型的泛化能力和效率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>强化学习和自监督学习&lt;/strong>：强化学习是指使用奖励信号来训练一个模型，在自我监督学习中，模型从未标记的数据中学习。未来的研究方向将集中在如何在大型模型上实现强化学习和自我监督学习，以提高模型的效率和准确性。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：本文部分内容由ChatGPT生成。&lt;/p>
&lt;/blockquote></description></item></channel></rss>