<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3 - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="1.Seq2Seq，Transformer的雏形 1.1.为什么会出现Seq2Seq？ 在神经概率语言模型NPLM出现后的很长一段时间，都是在这"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3"><meta property="og:description" content="1.Seq2Seq，Transformer的雏形 1.1.为什么会出现Seq2Seq？ 在神经概率语言模型NPLM出现后的很长一段时间，都是在这"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/"><meta property="article:published_time" content="2023-08-21T19:00:59+08:00"><meta property="article:modified_time" content="2023-08-21T19:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3"><meta itemprop=description content="1.Seq2Seq，Transformer的雏形 1.1.为什么会出现Seq2Seq？ 在神经概率语言模型NPLM出现后的很长一段时间，都是在这"><meta itemprop=datePublished content="2023-08-21T19:00:59+08:00"><meta itemprop=dateModified content="2023-08-21T19:00:59+08:00"><meta itemprop=wordCount content="2369"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3"><meta name=twitter:description content="1.Seq2Seq，Transformer的雏形 1.1.为什么会出现Seq2Seq？ 在神经概率语言模型NPLM出现后的很长一段时间，都是在这"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3</h1><div class=post-meta><time datetime=2023-08-21 class=post-time>2023-08-21</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 2369 字</span>
<span class=more-meta>预计阅读 5 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1seq2seqtransformer的雏形>1.Seq2Seq，Transformer的雏形</a><ul><li><a href=#11为什么会出现seq2seq>1.1.为什么会出现Seq2Seq？</a></li><li><a href=#12seq2seq架构概览>1.2.Seq2Seq架构概览</a></li><li><a href=#13seq2seq架构详解>1.3.Seq2Seq架构详解</a><ul><li><a href=#1整体模型架构>(1)&ldquo;整体模型"架构</a></li><li><a href=#2编码器-解码器架构>(2)&ldquo;编码器-解码器"架构</a></li><li><a href=#3编码器解码器微观逻辑>(3)&ldquo;编码器、解码器"微观逻辑</a></li><li><a href=#3优点>(3)优点</a></li><li><a href=#4劣势>(4)劣势</a></li></ul></li></ul></li><li><a href=#2代码>2.代码</a><ul><li><a href=#step11构建数据集>STEP1.1.构建数据集</a></li><li><a href=#step12结构化训练数据>STEP1.2.结构化训练数据</a></li><li><a href=#step21定义编码器和解码器类>STEP2.1.定义编码器和解码器类</a></li><li><a href=#step22定义seq2seq模型>STEP2.2.定义Seq2Seq模型</a></li><li><a href=#step3训练seq2seq模型>STEP3.训练Seq2Seq模型</a></li><li><a href=#step4测试seq2seq模型>STEP4.测试Seq2Seq模型</a></li></ul></li><li><a href=#3小结>3.小结</a></li></ul></nav></div></div><div class=post-content><h1 id=1seq2seqtransformer的雏形>1.Seq2Seq，Transformer的雏形</h1><h2 id=11为什么会出现seq2seq>1.1.为什么会出现Seq2Seq？</h2><p>在神经概率语言模型NPLM出现后的很长一段时间，都是在这种网络架构下进行优化。但，依然面临很多难题(主要是循环神经网络RNNs的局限)：</p><ul><li>如：输入序列长度增加时性能下降</li><li>如：顺讯处理导致计算效率低</li><li>……</li></ul><p>在2014年，Seq2Seq的提出，给人类一个不错的启发。</p><p>随后在自注意力机制的加持下，Transformer就诞生了。</p><h2 id=12seq2seq架构概览>1.2.Seq2Seq架构概览</h2><p>大神<code>Ilya Sutskever</code>在2014年，以第一作者的身份，发表了论文《Sequence to Sequence Learning with Neural Networks》。</p><blockquote><p>论文地址：https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf</p></blockquote><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162206474.png alt=image-20230831162206474></p><p>顺便说一嘴，大神<code>Ilya Sutskever</code>就是这位大哥，OpenAI联合创始人和首席科学家，各大自媒体都在播放他老人家的演讲视频：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162529973.png alt=image-20230831162529973></p><p>这篇论文的核心就是这张图，阐述了Seq2Seq的<strong>编码器-解码器架构</strong>：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png alt=image-20230831162726199></p><p>初次理解这张图，<strong>需要费点儿脑细胞</strong>，我们接下来详细拆解。</p><h2 id=13seq2seq架构详解>1.3.Seq2Seq架构详解</h2><h3 id=1整体模型架构>(1)&ldquo;整体模型"架构</h3><p>Seq2Seq会将变长的<strong>输入序列</strong>，转换为变长的<strong>输出序列</strong>。如下图：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170025140.png alt=image-20230831170025140></p><p>这里举一个例子——将"你是谁"翻译为英文"Who are u&rdquo;：</p><ul><li><strong>t1时刻</strong>：用户输入序列"你是谁"三个字。</li><li><strong>t2时刻</strong>：&ldquo;你"字输入给Seq2Seq。</li><li><strong>t3时刻</strong>：&ldquo;是"字输入给Seq2Seq。</li><li><strong>t4时刻</strong>：&ldquo;谁"字输入给Seq2Seq。</li><li><strong>t5时刻</strong>：Seq2Seq输出"Who&rdquo;。</li><li><strong>t6时刻</strong>：Seq2Seq输出"are&rdquo;。</li><li><strong>t7时刻</strong>：Seq2Seq输出"u&rdquo;。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831170404160.png alt=image-20230831170404160></p><h3 id=2编码器-解码器架构>(2)&ldquo;编码器-解码器"架构</h3><p>进一步拆解整体模型架构，Seq2Seq由编码器+解码器组成：</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171003578.png alt=image-20230831171003578></p><p>这里还是以将"你是谁"翻译为英文"Who are u"例子来解析：</p><ul><li><strong>t1时刻</strong>：用户输入序列"你是谁"三个字。</li><li><strong>t2时刻</strong>：&ldquo;你"字输入给编码器。</li><li><strong>t3时刻</strong>：&ldquo;是"字输入给编码器。</li><li><strong>t4时刻</strong>：&ldquo;谁"字输入给编码器。</li><li><strong>t5时刻</strong>：编码器将学习到上下文向量，传递给解码器</li><li><strong>t6时刻</strong>：解码器输出"Who&rdquo;。</li><li><strong>t7时刻</strong>：解码器输出"are&rdquo;。</li><li><strong>t8时刻</strong>：解码器输出"u&rdquo;。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831171625206.png alt=image-20230831171625206></p><h3 id=3编码器解码器微观逻辑>(3)&ldquo;编码器、解码器"微观逻辑</h3><ul><li><p><strong>编码器、解码器的具体实现</strong>：在论文中，它们都是采用RNN实现。如下图所示：</p><ul><li><strong>编码器的输入</strong>：0号隐藏层状态向量 + 1号输入词向量</li><li><strong>编码器的输出</strong>：1号隐藏层状态向量 + 1号词输出向量</li><li><strong>编码器的下一次输入</strong>：1号隐藏层状态向量 + 2号输入词向量</li><li><strong>编码器的下一次输出</strong>：2号隐藏层状态向量 + 2号输出词向量</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831222457684.png alt=image-20230831222457684></li><li><strong>解码器的输入</strong>：0号隐藏层状态向量 + 1号Teach Forcing输入词向量</li><li><strong>解码器的输出</strong>：1号词输出向量</li><li><strong>解码器的下一次输入</strong>：0号隐藏层状态向量 + 1号词输出向量 + 2号Teach Forcing输入词向量</li><li><strong>解码器的下一次输出</strong>：2号输出词向量</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225007834.png alt=image-20230831225007834></li></ul></li><li><p>这还是不太直观，我们再把编码器、解码器<strong>按时序进一步展开</strong>：</p><ul><li><strong>t1时刻</strong>：用户输入序列"你是谁"三个字。</li><li><strong>t2时刻</strong>：&ldquo;你"字输入给编码器，编码器输出<code>1号隐藏层状态</code>。</li><li><strong>t3时刻</strong>：&ldquo;是"字 + <code>1号隐藏层状态</code>，输入给编码器，编码器输出<code>2号隐藏层状态</code>。</li><li><strong>t4时刻</strong>：&ldquo;谁"字 + <code>2号隐藏层状态</code>，输入给编码器，编码器输出<code>3号隐藏层状态</code>。</li><li><strong>t5时刻</strong>：<code>3号隐藏层状态</code> + TeachForcing的"Who"字，输入给解码器，解码器输出<code>Who</code>。</li><li><strong>t6时刻</strong>：<code>3号隐藏层状态</code> + TeachForcing的"are"字 + 解码器输出<code>who</code>，输入给解码器，解码器输出<code>are</code>。</li><li><strong>t7时刻</strong>：<code>3号隐藏层状态</code> + TeachForcing的"u"字 + 解码器输出<code>are</code>，输入给解码器，解码器输出<code>u</code>。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831225749199.png alt=image-20230831225749199></p><ul><li>这里有一个细节，什么是<strong>Teach Forcing</strong>？<ul><li>我们可以想象，如果1号解码器的预测错了，那么2号、3号解码器都会错，进而导致学习效率非常低。</li><li>Teach Forcing好像<strong>开卷考试</strong>，如果1号解码器预测错了，Teach Forcing会纠正预测结果，进而加速学习效率。</li></ul></li></ul><h3 id=3优点>(3)优点</h3><ul><li><strong>变长序列</strong>：由于编码器、解码器采用RNN实现，所以输入序列可以是变长、输出序列也可以是变长。而CNN、DNN都不支持变长序列。</li><li><strong>信息压缩</strong>：隐藏层状态，或者叫上下文向量，本质上将输入序列进行了信息压缩，转变为含有上下文语义的向量。</li></ul><h3 id=4劣势>(4)劣势</h3><ul><li><strong>长序列信息损失</strong>：由于上下文向量为定长，所以当输入序列过长时编码器会出现信息损失。<font color=red><strong>这就是注意力机制的发力点，先埋个伏笔</strong></font>。</li><li><strong>效率</strong>：采用RNN(如：LSTM、GRU)实现编码器、解码器，RNN会面临梯度消失、梯度爆炸等问题。</li></ul><h1 id=2代码>2.代码</h1><h2 id=step11构建数据集>STEP1.1.构建数据集</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231557775.png alt=image-20230831231557775></p><h2 id=step12结构化训练数据>STEP1.2.结构化训练数据</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231753952.png alt=image-20230831231753952></p><h2 id=step21定义编码器和解码器类>STEP2.1.定义编码器和解码器类</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231855888.png alt=image-20230831231855888></p><h2 id=step22定义seq2seq模型>STEP2.2.定义Seq2Seq模型</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831231925444.png alt=image-20230831231925444></p><h2 id=step3训练seq2seq模型>STEP3.训练Seq2Seq模型</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232035429.png alt=image-20230831232035429></p><h2 id=step4测试seq2seq模型>STEP4.测试Seq2Seq模型</h2><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831232116723.png alt=image-20230831232116723></p><h1 id=3小结>3.小结</h1><ul><li>本文解析了Seq2Seq的内部实现，理解下图的关键是——<strong>按照时序</strong>，分析清楚每个编码器or解码器<strong>有几个输入、几个输出</strong>。</li><li><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-Transformer%E4%B9%8BSeq2Seq%EF%BC%8C%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/image-20230831162726199.png alt=image-20230831162726199></li><li>理解了Seq2Seq，我们接下来就可以逐步实现完整的Transformer了，且听下回分解。</li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2023-08-21</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记10-LangChain之Model IO，对LLM的抽象1</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/><span class="next-text nav-default">【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2023
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>