<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记57-LLM微调技术之QLoRA(3) - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)"><meta property="og:description" content="在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora3/"><meta property="article:published_time" content="2024-11-14T11:00:59+08:00"><meta property="article:modified_time" content="2024-11-14T11:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)"><meta itemprop=description content="在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精"><meta itemprop=datePublished content="2024-11-14T11:00:59+08:00"><meta itemprop=dateModified content="2024-11-14T11:00:59+08:00"><meta itemprop=wordCount content="4204"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)"><meta name=twitter:description content="在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记57-LLM微调技术之QLoRA(3)</h1><div class=post-meta><time datetime=2024-11-14 class=post-time>2024-11-14</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 4204 字</span>
<span class=more-meta>预计阅读 9 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1gptq相关技术>1.GPTQ相关技术</a><ul><li><a href=#11量化方法分类>1.1.量化方法分类</a><ul><li><a href=#1quantization-during-training>(1)Quantization During Training</a></li><li><a href=#2post-traning-quantization>(2)Post-Traning Quantization</a></li><li><a href=#3qat-vs-ptq>(3)QAT vs PTQ</a></li></ul></li><li><a href=#12模型剪枝技术回顾>1.2.模型剪枝技术回顾</a><ul><li><a href=#1optimal-brain-damage>(1)Optimal Brain Damage</a></li><li><a href=#2optimal-brain-surgeon>(2)Optimal Brain Surgeon</a></li><li><a href=#3optimal-brain-quantization>(3)Optimal Brain Quantization</a></li></ul></li><li><a href=#13大模型量化技术>1.3.大模型量化技术</a></li><li><a href=#14关键技术1层级量化>1.4.关键技术1：层级量化</a></li><li><a href=#15关键技术2obq>1.5.关键技术2：OBQ</a></li></ul></li><li><a href=#2gptq原理>2.GPTQ原理</a><ul><li><a href=#21gptq对obq的改进>2.1.GPTQ对OBQ的改进</a><ul><li><a href=#1arbitrary-order-insight>(1)Arbitrary Order Insight</a></li><li><a href=#2lazy-batch-updates>(2)Lazy Batch-Updates</a></li><li><a href=#3cholesky-reformulation>(3)Cholesky Reformulation</a></li></ul></li><li><a href=#22gptq算法的全流程>2.2.GPTQ算法的全流程</a></li><li><a href=#23gptq核心代码解读>2.3.GPTQ核心代码解读</a><ul><li><a href=#1关闭cuda的tf32精度>(1)关闭CUDA的TF32精度</a></li><li><a href=#2gptq类的构造函数>(2)GPTQ类的构造函数</a></li><li><a href=#3add_batch>(3)add_batch</a></li><li><a href=#4fasterquant>(4)fasterquant</a></li></ul></li></ul></li><li><a href=#4小结>4.小结</a></li></ul></nav></div></div><div class=post-content><p>在论文《GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers》中，提出了GPTQ这种模型量化的方法。它针对生成式预训练大模型，实现精准的模型事后模型量化(Post-Training Quantization)。</p><blockquote><p>PS：理解GPTQ，就需要理解前两篇专栏介绍的<strong>模型量化基本概念</strong>、<strong>模型剪枝及OBD/OBS</strong>，不熟悉的小伙伴可以翻一下前面的专栏。</p></blockquote><h1 id=1gptq相关技术>1.GPTQ相关技术</h1><h2 id=11量化方法分类>1.1.量化方法分类</h2><p>从量化时机看，有两类量化方法：</p><ul><li><strong>QAT</strong>：Quantization During Training，训练期量化。</li><li><strong>PTQ</strong>：Post-Training Quantization，训练后量化。</li></ul><p>这两类量化方法由于量化时机不同，所以训练方法也有所不同。</p><h3 id=1quantization-during-training>(1)Quantization During Training</h3><p>训练期量化(Quantization During Training)，也称为<strong>量化感知训练</strong>，是在模型训练的过程中进行量化，使得模型在训练时就适应量化带来的误差。</p><p>QAT算法的主要步骤如下：</p><ul><li><p><strong>STEP1.模拟量化</strong>：在训练过程中，权重和激活值会被临时量化到较低精度(如：8bit)。</p></li><li><p><strong>STEP2.反量化</strong>：量化后的值会在后续的计算中被反量化回浮点数，用于计算梯度和更新权重。</p></li><li><p><strong>STEP3.梯度传递</strong>：量化和反量化的过程会引入额外的误差，模型进一步学习如何最小化这些额外误差对最终性能的影响。</p></li><li><p><strong>STEP4.微调</strong>：在训练的最后阶段会移除量化操作，对模型进行微调以恢复由于量化而可能损失的精度。</p></li></ul><p>QAT的优点是可以更好地保持模型的精度，使得模型在训练过程中适应量化带来的变化。</p><p>QAT的缺点是增加训练复杂度和训练成本。</p><h3 id=2post-traning-quantization>(2)Post-Traning Quantization</h3><p>训练后量化(Post-Training Quantization)，是在模型训练完成后，再对模型权重、激活值进行量化。</p><p>PTQ算法的主要步骤如下：</p><ul><li><p><strong>STEP1.离线校准</strong>：分析模型在特定数据集上的激活值分布来确定量化参数(如：比例因子)。</p></li><li><p><strong>STEP2.量化</strong>：使用STEP1离线校准得到的量化参数，将模型的权重和激活值量化到较低的精度。</p></li><li><p><strong>STEP3.转换</strong>：将量化后的模型转换为可以在目标硬件上运行的形式。</p></li></ul><p>PTQ的的优点是简单且计算成本较低，它不需要在训练过程中模拟量化操作</p><p>PTQ的缺点是可能不如训练期间量化那样能够保持模型的精度，因为它没有给模型机会去适应量化带来的误差。</p><h3 id=3qat-vs-ptq>(3)QAT vs PTQ</h3><ul><li><strong>从精度看</strong>：QAT通常能够更好地保持模型的精度，因为它允许模型在训练过程中适应量化操作。</li><li><strong>从计算成本看</strong>：PTQ的计算成本较低，因为它避免了在训练过程中模拟量化的额外计算。</li><li><strong>从灵活性看</strong>：PTQ提供了更大的灵活性，因为它允许在不同的硬件和部署环境中使用相同的量化模型。</li></ul><p><strong>GPTQ属于PTQ量化方法，GPTQ就是G-PTQ，G表示GPT系列大语言模型。</strong></p><h2 id=12模型剪枝技术回顾>1.2.模型剪枝技术回顾</h2><p>在PTQ(训练后量化)的思想下，模型剪枝技术得以发展与应用。这些技术更集中应用于视觉模型，但在大模型上应用面临诸多挑战。</p><ul><li><strong>OBD</strong>：Opimal Brain Damage</li><li><strong>OBS</strong>：Optimal Brain Surgeon</li><li><strong>OBQ</strong>：Optimal Brain Quantization</li></ul><h3 id=1optimal-brain-damage>(1)Optimal Brain Damage</h3><p>OBD是一种早期的剪枝算法(由LeCun提出)，其核心思想是通过Loss二阶导制定参数的影响，从而在剪枝过程中自动化地找出并裁剪"无影响"的参数。</p><p><strong>优点</strong>：</p><ul><li><strong>剪枝自动化</strong>：寻找到参数与Loss关系的数学方法，使剪枝过程自动化</li><li><strong>计算简单</strong>：从数学意义上，证明了可以忽略参数间的相互作用等因素，极大降低计算复杂度。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>局部最优</strong>：OBD可能只找到局部最优解(由于忽略了参数间相互作用，可能会在剪枝过程中在错误的方向上越走越远)，而不是全局最优解。</li></ul><blockquote><p>关于OBD详细内容，可阅读本专栏《【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)》相关章节</p></blockquote><h3 id=2optimal-brain-surgeon>(2)Optimal Brain Surgeon</h3><p>OBS是OBD的改进版，它建立了模型参数间的关联关系。</p><p>OBS的核心思想是在剪枝过程中<strong>找到对模型误差影响最小的权重</strong>，<strong>并更新其他权重以补偿剪枝带来的影响</strong>。</p><p><strong>优点</strong>：</p><ul><li><strong>更精确</strong>：考虑了权重之间的交叉项，可以更准确地评估剪枝的影响。</li><li><strong>全局最优</strong>：相比于OBD，OBS更可能找到全局最优解。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>计算复杂</strong>：由于需要考虑交叉项，计算复杂度较高，尤其是在大规模网络中。</li><li><strong>资源消耗</strong>：对于大规模网络，计算和存储资源消耗较大。</li></ul><blockquote><p>关于OBD详细内容，可阅读本专栏《【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)》相关章节</p></blockquote><h3 id=3optimal-brain-quantization>(3)Optimal Brain Quantization</h3><p>OBQ是将OBS的思想应用到量化问题中，它处理的是将浮点数权重转换为整数表示，而不是直接将权重设置为零。</p><p><strong>优点</strong>：</p><ul><li><strong>精度保持</strong>：相比于直接剪枝，量化可以保持模型的数值精度，减少信息损失。</li><li><strong>硬件友好</strong>：量化后的模型更适合在低精度硬件上运行，如移动设备和嵌入式系统。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>量化策略选择</strong>：需要精心设计量化策略，以平衡模型性能和计算效率。</li><li><strong>后处理需求</strong>：量化后可能需要额外的步骤来微调模型，以恢复性能。</li></ul><p>关于OBD、OBS、OBQ的简述以及GPTQ与之的关联，详见论文Related Work章节。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114083900824.png alt=image-20241114083900824></p><h2 id=13大模型量化技术>1.3.大模型量化技术</h2><p><strong>大模型量化技术</strong>：Large-model Quantization，专门针对大模型实施模型量化的技术。</p><ul><li>大型模型量化技术的实验对象通常是BLOOM和OPT-175B这样的大型语言模型，</li><li>论文中列举的量化方法有：ZeroQuant、LLM.int8()和nuQmm。<ul><li>ZeroQuant提出了逐层知识蒸馏，但只能应用于最多13亿参数的模型，且计算时间较长。</li><li>LLM.int8()关注到激活值中的异常值会破坏大型模型的量化，并提出通过保持这些维度的高精度来解决。</li><li>nuQmm则开发了针对特定二进制编码量化方案的高效GPU内核。</li></ul></li></ul><p>而GPTQ的实验证明，它能在更短的时间内量化更大的模型。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114091053556.png alt=image-20241114091053556></p><h2 id=14关键技术1层级量化>1.4.关键技术1：层级量化</h2><p>在宏观层面，GPTQ遵循层级量化思想(Layer-Wise Quantization)，层级量化属于PTQ方法，通过逐层量化、逐层重构。层级量化的目标是找到量化权重矩阵，这个矩阵用来保障最小化各层的平方误差。</p><ul><li><strong>数学表达</strong>：W表示线性层的权重，X表示线性层的一小批数据点。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114102155494.png alt=image-20241114102155494></p><ul><li><strong>量化目标</strong>：找到一组量化权重 W^，使得与全精度层输出的平方误差最小化。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114102506034.png alt=image-20241114102506034></p><h2 id=15关键技术2obq>1.5.关键技术2：OBQ</h2><p>OBQ是GPTQ的数学方法基础：</p><ul><li><strong>量化目标</strong>：OBQ独立处理每一行权重，一次量化一个权重，同时更新其它未量化的权重，以最小化该层的输出误差。</li><li><strong>数学表达</strong>：w<sub>q</sub>表示需要量化的权重，quant(w<sub>q</sub>)是w<sub>q</sub>的量化值，H<sub>F</sub><sup>-1</sup>表示海森矩阵的逆。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114103135744.png alt=image-20241114103135744></p><ul><li><strong>高效更新海森矩阵</strong>：在量化过程中，Hessian矩阵用来表征剩余全精度权重的集合。如下公式表示了OBQ更新海森矩阵的逻辑，OBQ在量化一个权重后，移除已量化权重的行和列，避免后续量化中的重复计算，从而避免了更新海森矩阵对算力的巨大消耗。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114140940163.png alt=image-20241114140940163></p><ul><li><strong>量化并行</strong>：OBQ方法并行处理W的多行，实验中单GPU上消耗4小时量化OPT-175B模型(1750亿参数)，相较于其它量化方法提升了3个数量级的速度。</li></ul><p>GPTQ就是在OBQ的基础上进行了重大修改，使其能够扩展到大型语言模型，使得在保持模型性能的同时，显著减少模型的存储和计算需求，使得大型模型的部署和推理变得更加可行。</p><h1 id=2gptq原理>2.GPTQ原理</h1><h2 id=21gptq对obq的改进>2.1.GPTQ对OBQ的改进</h2><h3 id=1arbitrary-order-insight>(1)Arbitrary Order Insight</h3><p>OBQ量化权重的顺序是按照贪婪法进行量化的，即总是选择当前量化误差最小的权重进行量化。然而GPTQ实验发现，这种量化顺序的策略在大模型中效果不佳。</p><p>GPTQ按照相同顺序量化所有行的权重相反能获得不错的效果，其数学原理是海森矩阵H<sub>F</sub>仅依赖于层输入X<sub>F</sub>，而不依赖于权重。因此可以对所有行的H<sub>F</sub><sup>-1</sup>进行相同的更新，而替代了对每个权重单独更新。</p><blockquote><p>PS：Arbitrary Order Insight，我刚开始一脸懵逼地直译为"任意顺序洞察&rdquo;，按照上述数学逻辑，应该翻译为“块量化”更为合适，正因为海森矩阵的特性才实现了块量化。</p></blockquote><h3 id=2lazy-batch-updates>(2)Lazy Batch-Updates</h3><p>Lazy Batch-Updates，批量更新，是一种工程化方法。量化在GPU内非常快，但是更新剩余未量化的权重的内存访问将成为QPTQ的性能瓶颈。</p><ul><li>GPTQ的解决思路是每次处理128列，只更新这些列对应的子块(即，<strong>B * B block of H<sup>-1</sup></strong>)。只有一个块完全处理后，才会驱动如下两个公式，对整个H<sup>-1</sup>和W矩阵进行全局更新。其中，Q表示一组索引，H<sup>-1</sup><sub>-Q</sub>表示移除了相应行和列的逆矩阵。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114161325271.png alt=image-20241114161325271></p><ul><li>上述更新方法，本质上解决的是内存吞吐量瓶颈。</li></ul><h3 id=3cholesky-reformulation>(3)Cholesky Reformulation</h3><p>Cholesky Reformulation，切尔斯基分解。</p><ul><li><p>在量化过程中，可能会出现数值不准确性，进而导致海森矩阵的逆变为奇异(即不可逆) 。</p></li><li><p><strong>数值稳定性</strong>：GPTQ利用了Cholesky分解的数值稳定性，通过预先计算和更新Cholesky分解，避免直接计算和更新Hessian矩阵的逆。</p></li></ul><h2 id=22gptq算法的全流程>2.2.GPTQ算法的全流程</h2><ul><li><strong>STEP1.块量化(Arbitrary Order Insight)</strong>：选择一个连续块，如下图中粗线包含的区域。</li><li><strong>STEP2.Cholesky分解(Cholesky Reformulation)</strong>：利用Cholesky分解得到海森矩阵的逆，进而量化STEP1选择的块。橙色为已量化的权重，白色为正在量化的权重，蓝色为未量化的权重。</li><li><strong>STEP3.权重批量更新(Lazy Batch-Updates)</strong>：在当前块量化的最后，就更新所有剩余未量化的权重。</li><li>完成STEP1~STEP3后，<strong>递归量化</strong>下一个块。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114164012033.png alt=image-20241114164012033></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114163908213.png alt=image-20241114163908213></p><h2 id=23gptq核心代码解读>2.3.GPTQ核心代码解读</h2><p>通过对GPTQ的论文解读，我们了解了GPTQ的相关技术，以及算法流程。</p><p>最后我们再来看一下GPTQ的核心代码，以加深对论文的理解：</p><blockquote><p><a href=https://github.com/AutoGPTQ/AutoGPTQ/blob/main/auto_gptq/quantization/gptq.py>https://github.com/AutoGPTQ/AutoGPTQ/blob/main/auto_gptq/quantization/gptq.py</a></p></blockquote><h3 id=1关闭cuda的tf32精度>(1)关闭CUDA的TF32精度</h3><ul><li>TF32是一种混合精度计算，关闭它可以保证计算精确性。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165123240.png alt=image-20241114165123240></p><h3 id=2gptq类的构造函数>(2)GPTQ类的构造函数</h3><ul><li>构造函数的输入是神经网络中的一个层。</li><li>复制层的权重，得到W。</li><li>根据层类型(Conv1D、Conv2d)，对权重矩阵的形状进行调整。</li><li>H就是海森矩阵，初始为零矩阵。</li><li>quantizer是量化器对象。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165410715.png alt=image-20241114165410715></p><h3 id=3add_batch>(3)add_batch</h3><ul><li>处理输入和输出的batch</li><li>根据层类型，调整输入的形状</li><li>根据调整好的输入形状，更新海森矩阵H</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114165844565.png alt=image-20241114165844565></p><h3 id=4fasterquant>(4)fasterquant</h3><ul><li>克隆并调整当前层的权重矩阵的形状。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114170835985.png alt=image-20241114170835985></p><ul><li>根据块分组进行量化，逐步地计算损失并更新权重。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114170950349.png alt=image-20241114170950349></p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114171103060.png alt=image-20241114171103060></p><ul><li>最终返回量化参数</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B057-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BQLoRA(3)/image-20241114171215236.png alt=image-20241114171215236></p><h1 id=4小结>4.小结</h1><p>本文介绍了：</p><ul><li><strong>GPTQ相关技术</strong>：QAT和PTQ、OBD/OBS/OBQ、大模型量化技术<ul><li>其中，层级量化和OBQ是GPTQ的关键技术基础</li></ul></li><li><strong>GPTQ的技术原理</strong>：<ul><li>块量化</li><li>批量更新</li><li>Cholesky分解</li></ul></li><li><strong>GPTQ的算法流程及代码解读</strong></li></ul><p>论文地址：https://arxiv.org/abs/2210.17323</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-11-14</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B058-agentscope%E8%A7%A3%E8%AF%BB-%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记58-AgentScope解读-流式输出</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B055-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bqlora2/><span class="next-text nav-default">【chatGPT】学习笔记55-LLM微调技术之QLoRA(2)</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2025
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>