<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI拾遗 on 妙木山</title><link>https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/</link><description>Recent content in AI拾遗 on 妙木山</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 14 Nov 2023 13:00:59 +0800</lastBuildDate><atom:link href="https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/index.xml" rel="self" type="application/rss+xml"/><item><title>【chatGPT】学习笔记27-提示词解读3-实战案例之摘要总结</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B027-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB3-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%91%98%E8%A6%81%E6%80%BB%E7%BB%93/</link><pubDate>Tue, 14 Nov 2023 13:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B027-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB3-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E4%B9%8B%E6%91%98%E8%A6%81%E6%80%BB%E7%BB%93/</guid><description>在《【chatGPT】学习笔记25-提示词解读2-通用技巧》中，我们看到了提示词的各种通用技巧，但无论哪种技巧，都是为了激发大语言模型的某种</description></item><item><title>【chatGPT】学习笔记26-CNCC 2023参会纪要</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B026-cncc-2023%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</link><pubDate>Fri, 10 Nov 2023 13:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B026-cncc-2023%E5%8F%82%E4%BC%9A%E7%BA%AA%E8%A6%81/</guid><description>本文记录笔者参加CNCC 2023 AI相关的议题，方便各位小伙伴快速了解学界在AI的理论研究和行业应用情况。 1.CNCC 2023概览 (1)会议简介 本届大会以“</description></item><item><title>【chatGPT】学习笔记25-提示词解读2-通用技巧</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/</link><pubDate>Fri, 10 Nov 2023 12:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B025-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB2-%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7/</guid><description>上一篇，我们了解了提示词基本概念，本篇我们继续解读吴恩达老师的《ChatGPT Prompt Engineering for Developers》课程，看一下提示词常用技巧。 1.技</description></item><item><title>【chatGPT】学习笔记24-提示词解读1-提示词基本概念</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link><pubDate>Thu, 09 Nov 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B024-%E6%8F%90%E7%A4%BA%E8%AF%8D%E8%A7%A3%E8%AF%BB1-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid><description>吴恩达老师的《ChatGPT Prompt Engineering for Developers》是一门学习提示词工程不错的课程，我们接下来用几篇文章来解读这门课程。 1.什么是提示词</description></item><item><title>【chatGPT】学习笔记23-让我们一起来翻译吴恩达LLM课程</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BEllm%E8%AF%BE%E7%A8%8B/</link><pubDate>Thu, 02 Nov 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B023-%E8%AE%A9%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%BF%BB%E8%AF%91%E5%90%B4%E6%81%A9%E8%BE%BEllm%E8%AF%BE%E7%A8%8B/</guid><description>哈喽小伙伴，我是小牛，初次见面，请多关照。 最近跟着猴哥的学习笔记各种手撸GPT，理解了LLM原理。 但我跟GPT的交情好像只停留在say hel</description></item><item><title>【chatGPT】学习笔记22-LangChain之Agent，对LLM的抽象5</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-langchain%E4%B9%8Bagent%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A15/</link><pubDate>Tue, 31 Oct 2023 15:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B022-langchain%E4%B9%8Bagent%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A15/</guid><description>业界目前的AI助手应用集中于两大功能： 基于垂直领域知识的智能问答 基于垂直领域知识的智能行动 在《【chatGPT】学习笔记21-LangCha</description></item><item><title>【chatGPT】学习笔记21-LangChain之Retrieval，对LLM的抽象4</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-langchain%E4%B9%8Bretrieval%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A14/</link><pubDate>Mon, 30 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B021-langchain%E4%B9%8Bretrieval%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A14/</guid><description>本专栏通过解读了Transformer模型，实现简版GPT后，帮大家建立了对NLP原理、关键技术的理解。 接下来，我们再关注一下应用层面的技术</description></item><item><title>【chatGPT】学习笔记20-如何搭建ChatGLM3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</link><pubDate>Sun, 29 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020-%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAchatglm3/</guid><description>1.ChatGLM3更新了什么 (1)模型列表 智谱AI刚刚发布了ChatGLM3，其中ChatGLM3-6B的能力提升如下： 更强大的基础模型：</description></item><item><title>【chatGPT】学习笔记19-自己实现一个简版ChatGPT(下)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</link><pubDate>Fri, 20 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B019-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8B/</guid><description>前两篇实现了简版GPT，并对其进行了SFT，我们接下来看ChatGPT整体训练流程的最后一个环节——对齐训练(Alignment Traini</description></item><item><title>【chatGPT】学习笔记18-自己实现一个简版ChatGPT(中)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</link><pubDate>Wed, 18 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B018-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%AD/</guid><description>根据上文我们实现的简版GPT，在足够数据、足够算力的前提下，理论上是可以训练出类GPT3的大语言模型的。 但GPT3距离ChatGPT还有很远</description></item><item><title>【chatGPT】学习笔记17-自己实现一个简版ChatGPT(上)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</link><pubDate>Mon, 16 Oct 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B017-%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E7%89%88chatgpt%E4%B8%8A/</guid><description>接下来，我们用三篇文章阐述**如何实现一个简版ChatGPT。** 1.回顾 想实现一个简版ChatGPT，依赖于如下前置知识： 机器学习基本原理</description></item><item><title>【chatGPT】学习笔记16-Transformer架构，大语言模型的关键部件5</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</link><pubDate>Tue, 26 Sep 2023 18:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016-transformer%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B65/</guid><description>在《AI拾遗》这个专栏中，我们建立了从N-Gram到词嵌入再到神经概率语言模型，从Seq2Seq到注意力机制的知识脉络。 这条脉络本质就是NL</description></item><item><title>【chatGPT】学习笔记15-LangChain之Chain，对LLM的抽象3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</link><pubDate>Wed, 20 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B015-langchain%E4%B9%8Bchain%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A13/</guid><description>我们继续写点儿偏工程实践的内容——LangChain的核心模块3——Chain。 1.核心模块3：Chain 在《【chatGPT】学习笔记11</description></item><item><title>【chatGPT】学习笔记14-LangChain之Memory，对LLM的抽象2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</link><pubDate>Tue, 19 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014-langchain%E4%B9%8Bmemory%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A12/</guid><description>我们继续写点儿偏工程实践的内容——LangChain的核心模块2——Chain。 1.核心模块2：Memory 实现一个问答系统，通常需要将历史</description></item><item><title>【chatGPT】学习笔记13-Transformer之注意力机制，大语言模型的关键部件4</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</link><pubDate>Mon, 18 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013-transformer%E4%B9%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B64/</guid><description>1.问题 在《【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3》中，我们实现了Seq2Seq，看到</description></item><item><title>【chatGPT】学习笔记12-昇腾计算产业发展白皮书解读</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</link><pubDate>Tue, 12 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012-%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E4%BA%A7%E4%B8%9A%E5%8F%91%E5%B1%95%E7%99%BD%E7%9A%AE%E4%B9%A6%E8%A7%A3%E8%AF%BB/</guid><description>本文来解读华为的《昇腾计算产业发展白皮书》，跟踪一下国内AI行业的宏观动态。 1.AI发展趋势和挑战 1.1.AI发展趋势 白皮书首先阐述了AI发</description></item><item><title>【chatGPT】学习笔记11-LLM应用-垂直领域知识问答系统(基于ChatGLM2)</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</link><pubDate>Tue, 05 Sep 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011-llm%E5%BA%94%E7%94%A8-%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%9F%BA%E4%BA%8Echatglm2/</guid><description>今天我们再来写一篇关于大语言模型的实战应用——如何开发一个垂直领域的知识问答系统？ 1.原理 (1)基于传统技术的实现方案 在没有大语言模型之前，</description></item><item><title>【chatGPT】学习笔记10-LangChain之Model IO，对LLM的抽象1</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</link><pubDate>Mon, 28 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-langchain%E4%B9%8Bmodelio%E5%AF%B9llm%E7%9A%84%E6%8A%BD%E8%B1%A11/</guid><description>最近的专栏都是在拆解大语言模型的内部实现及论文，我们再来写点儿偏工程实践的内容——LangChain。 1.LangChain简介 (1)如何实</description></item><item><title>【chatGPT】学习笔记9-Transformer之Seq2Seq，大语言模型的关键部件3</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</link><pubDate>Mon, 21 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09-transformer%E4%B9%8Bseq2seq%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B63/</guid><description>1.Seq2Seq，Transformer的雏形 1.1.为什么会出现Seq2Seq？ 在神经概率语言模型NPLM出现后的很长一段时间，都是在这</description></item><item><title>【chatGPT】学习笔记8-神经概率语言模型，大语言模型的关键部件2</title><link>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</link><pubDate>Sat, 19 Aug 2023 19:00:59 +0800</pubDate><guid>https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E9%83%A8%E4%BB%B62/</guid><description>1.问题：词嵌入的局限性 在《【chatGPT】学习笔记7-词的向量化，大语言模型的关键部件》中，我们了解了词嵌入。 获取词向量后，在向量空间中</description></item></channel></rss>