<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning - 妙木山</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="猴王无敌"><meta name=description content="Prefix Tuning是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。 本文通过解读论文**《Prefix-Tuning:"><meta name=keywords content="Hugo,theme,jane"><meta name=generator content="Hugo 0.74.2"><link rel=canonical href=https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.b3a8813c06e6d785beba22bf8264e174fa2cb3a396b22f9ba24e2c00c18aaf7f.css integrity="sha256-s6iBPAbm14W+uiK/gmThdPoss6OWsi+bok4sAMGKr38=" media=screen crossorigin=anonymous><meta property="og:title" content="【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning"><meta property="og:description" content="Prefix Tuning是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。 本文通过解读论文**《Prefix-Tuning:"><meta property="og:type" content="article"><meta property="og:url" content="https://jherculesqz.github.io/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprefix-tuning/"><meta property="article:published_time" content="2024-03-29T10:00:59+08:00"><meta property="article:modified_time" content="2024-03-29T10:00:59+08:00"><meta itemprop=name content="【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning"><meta itemprop=description content="Prefix Tuning是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。 本文通过解读论文**《Prefix-Tuning:"><meta itemprop=datePublished content="2024-03-29T10:00:59+08:00"><meta itemprop=dateModified content="2024-03-29T10:00:59+08:00"><meta itemprop=wordCount content="4224"><meta itemprop=keywords content="AI拾遗-chat GPT,"><meta name=twitter:card content="summary"><meta name=twitter:title content="【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning"><meta name=twitter:description content="Prefix Tuning是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。 本文通过解读论文**《Prefix-Tuning:"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>妙木山</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>妙木山</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/>首页</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/categories/>技术专栏</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/post/>全部文章</a></li><li class=menu-item><a class=menu-item-link href=https://jherculesqz.github.io/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>【chatGPT】学习笔记44-LLM微调技术之Prefix Tuning</h1><div class=post-meta><time datetime=2024-03-29 class=post-time>2024-03-29</time><div class=post-category><a href=https://jherculesqz.github.io/categories/ai%E6%8B%BE%E9%81%97/>AI拾遗</a></div><span class=more-meta>约 4224 字</span>
<span class=more-meta>预计阅读 9 分钟</span>
<span id=busuanzi_container_page_pv>| 阅读 <span id=busuanzi_value_page_pv></span></span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1abstract摘要>1.Abstract(摘要)</a></li><li><a href=#2introduction介绍>2.Introduction(介绍)</a></li><li><a href=#3prefix-tuning>3.Prefix Tuning</a><ul><li><a href=#31problem-statement问题陈述>3.1.Problem Statement(问题陈述)</a></li><li><a href=#32intuition直觉>3.2.Intuition(直觉)</a></li><li><a href=#33methodparametrization-of-psubθsub原理>3.3.Method/Parametrization of P<sub>θ</sub>(原理)</a></li></ul></li><li><a href=#4experiments实验结果>4.Experiments(实验结果)</a><ul><li><a href=#41实验结果>4.1.实验结果</a></li><li><a href=#42intrinsic-evaluation重要发现>4.2.Intrinsic Evaluation(重要发现)</a></li></ul></li><li><a href=#5总结>5.总结</a></li></ul></nav></div></div><div class=post-content><p><code>Prefix Tuning</code>是LLM微调技术中另一个重要的技术分支，于2021年由Stanford提出。</p><p>本文通过解读论文**《Prefix-Tuning: Optimizing Continuous Prompts for Generation》**，与小伙伴们一起学习理解<code>Prefix Tuning</code>思想和方法。</p><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329153821295.png alt=image-20240329153821295></p><h1 id=1abstract摘要>1.Abstract(摘要)</h1><p>首先我们看一下论文摘要，快速理解论文的<strong>核心内容</strong>：</p><ul><li><p><strong>问题</strong>：**FFT(全参数微调)**针对不同的下游任务都需要产生一个新的微调后大模型，存在成本效率等诸多工程问题。</p></li><li><p><strong>解决方案</strong>：论文提出的<code>Prefix Tuning</code>，是一种使用<strong><code>Soft Prompt(软提示)</code></strong>进行迁移学习的方法。针对不同下游任务创建不同的<strong><code>Prefix(前缀向量模块)</code></strong>，这样不同下游任务只需要在一套预训练大模型上加载不同<strong>Prefix小模型</strong>即可。</p></li><li><p><strong>实验效果</strong>：</p><ul><li>在GPT-2的<strong><code>Table-To-Text(表格生成文本)</code></strong>下游任务中，<code>Prefix</code>模型参数仅占GPT-2参数的0.1%，即可达到GPT-2同等水平。</li><li>在BART的<strong><code>Sumarization(摘要)</code></strong>下游任务中，<code>Prefix</code>模型参数仅占BART参数的0.1%，即可达到BART同等水平。</li><li>在上述两种实验中，额外还观察到一定的泛化涌现能力，<code>Prefix Tuning</code>可外推到训练期间未见过的任务主题。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329111747247.png alt=image-20240329111747247></p><h1 id=2introduction介绍>2.Introduction(介绍)</h1><ul><li><p><strong>背景技术1</strong>：<strong>Adapter Tuning</strong>是向预训练模型中增加新的小模型，仅微调小模型参数以达到高效微调的目的。实验效果证明仅需微调2%~4%的参数即可达到全参数微调的效果。</p><ul><li>Adapter Tuning的详细解读可参见本技术专栏这篇文章《【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning》</li></ul></li><li><p><strong>背景技术2</strong>：GPT-3的一个重要贡献就是<strong>Context Learning</strong>和<strong>Prompt Engineering</strong>，GPT-3是一套统一的大语言模型，用户不需要针对下游任务单独微调，直接通过提示词和上下文，影响GPT-3输出的答案。</p></li><li><p><strong>Prefix Tuning的核心思想</strong>：<strong>Prefix Tuning</strong>借鉴了<strong>Adapter Tuning</strong>和<strong>Prompt Engineering</strong>的思想：</p><ul><li>Prefix Tuning也额外<strong>增加了N个小模型</strong>，这些小模型外挂于同一套预训练模型上，<strong>不同小模型解决不同的下游任务</strong>。</li><li>Prefix Tuning增加的这些小模型的作用类似<strong>Prompt(提示词)</strong>，它们会在用户输入的文本前额外增加针对不同下游任务的<strong>提示词前缀Prefix</strong>。这些前缀不是自然语言，而是Transformer架构中向量形式的Token，这种Token叫做<strong>虚拟Token</strong>。</li></ul></li><li><p><strong>实验效果</strong>：在<strong>GPT-2的Table-To-Text</strong>和<strong>BART的Sumarization</strong>的测试效果：</p><ul><li><strong>GPT-2的Table-To-Text</strong>：在完整数据集上训练时，<strong>Prefix Tuning</strong>和<strong>FFT</strong>在表格到文本的下游任务的性能相当。</li><li><strong>BART的Sumarization</strong>：在摘要方面，<strong>Prefix Tuning</strong>和<strong>FFT</strong>性能略有下降。</li><li><strong>Low Data Settting</strong>：在数据量少的数据集上，<strong>Prefix Tuning</strong>能够克服数据集样本不足、提炼数据特征困难的问题，表现出了泛化涌现能力。在上述两项任务中性能表现优于FFT。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329113337615.png alt=image-20240329113337615></p><h1 id=3prefix-tuning>3.Prefix Tuning</h1><h2 id=31problem-statement问题陈述>3.1.Problem Statement(问题陈述)</h2><p>Prefix Tuning采用了严谨的数学描述来阐述待解决的问题，这也是算法工程中的算法建模环节，我们尽量通俗地理解这个问题模型，也感受一下算法工程师的思维模式。</p><ul><li><p><strong>x和y</strong>：x是大语言模型的输入，y是大语言模型的输出。</p><ul><li>如下图右侧上方所示，在摘要型下游任务中，人类输入的原始文本就是x，大语言模型输出的总结结果就是y。</li><li>如下图右侧下方所示，在表格转文本的下游任务中，人类输入的结构化的表格字符串是x，大语言模型输出的表格描述就是y。</li></ul></li><li><p><strong>自回归语言模型的问题子域</strong>(如下图左上所示)：</p><ul><li><strong>pφ(y | x)</strong>：根据Transformer这种自回归模型的网络结构，其本质可抽象为<strong>pφ(y | x)</strong>，φ为大语言模型的参数，p就是在**&ldquo;人类输入字符串x的条件下，大语言模型输出y的概率分布&rdquo;**。</li><li><strong>z = [x; y]</strong>：z被定义为x和y的序列，其中X<sub>idx</sub>表示了x的索引，Y<sub>idx</sub>表示y的索引。</li><li><strong>h<sub>i</sub></strong>：h<sub>i</sub>表示<strong>时间步i的激活(activation)</strong>，h<sub>i</sub>又是由第i个时间步中的n层激活组成的序列(即h<sub>i</sub>= [h<sub>i</sub><sup>(1)</sup>;···;h<sub>i</sub><sup>(n)</sup>])，其中h<sub>i</sub><sup>(n)</sup>表示在Transformer架构中的第i个时间步的第n层的激活。</li><li><strong>h<sub>i</sub>=LM<sub>φ</sub>(z<sub>i</sub>, h<sub>&lt;i</sub>)</strong>：此数学公式表示——根据z<sub>i</sub>，以及h<sub>1</sub>~h<sub>i-1</sub>，计算当前的h<sub>i</sub>。展开解释一下就是，Transformer模型会根据第i个时间步的x、y，以及第1个时间步~第i-1个时间步计算的各时间步计算的各层激活向量，计算当前时间步下Transformer各层的激活。</li><li><strong>p<sub>φ</sub>(z<sub>i+1</sub> | h<sub>≤i</sub>) = softmax(W<sub>φ</sub>h<sub>i</sub><sup>(n)</sup>)</strong>：最后一层的h<sub>i</sub>是Transformer训练后获得的概率分布，用来根据当前Token预测下一个Token。其中，W<sub>φ</sub>是一个用于将h<sub>i</sub><sup>(n)</sup>映射到词汇表上logits的预训练矩阵。</li><li>上述数学描述中，包含很多AI相关术语(如：自回归模型、时间步i的激活(activation)、各层激活向量、词汇表上logits等)，可参见本技术专栏**《NLP底层原理》篇**的系列文章。</li></ul></li><li><p><strong>Encoder-Decoder模型的问题子域</strong>(如下图左下所示)：</p><ul><li>与自回归语言模型的问题子域大部分数学描述相同。</li><li>不同点在于基于Encoder-Decoder架构的结构特点，x和y被拆分开了。</li></ul></li><li><p><strong>Prefix Tuning的微调求解目标</strong>：</p><ul><li><strong>max<sub>φ</sub> log p<sub>φ</sub>(y | x) = Σ<sub>i∈Y<sub>idx</sub></sub> log p<sub>φ</sub>(z<sub>i</sub> | h<sub>&lt;i</sub>)</strong>：这个看着复杂的公式，就是在表达微调的最终目标就是在各个时间步的各时间步激活的条件下x和y序列的概率分布求和。说人话就是，<strong>微调后的模型能够根据x预测最大概率应该输出y</strong>。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329125952458.png alt=image-20240329125952458></p><h2 id=32intuition直觉>3.2.Intuition(直觉)</h2><p>论文在正式阐述<strong>Prefix Tuning</strong>的原理之前，描述了研究员的灵感来源：</p><ul><li><strong>Based on intuition from prompting</strong>：提示词被证明是可以影响大语言模型输出，提示词的思想本质是——大语言模型像一个什么都知道的老人，人类的输入需要使用一定的提示技巧唤醒老人的记忆，从而帮助他输出正确的答案。</li><li><strong>z = [x; y]<strong>和</strong>h<sub>i</sub>=LM<sub>φ</sub>(z<sub>i</sub>, h<sub>&lt;i</sub>)</strong>：再看3.1问题陈述章节的这两个数学公式，提示词有两个作用：<ul><li><strong>作用1</strong>：提示词作为人类输入x的一部分，它起到了<strong>影响大语言模型关注x的哪些Token</strong>的作用。</li><li><strong>作用2</strong>：在作用1的驱动下，影响了大语言模型各层的激活计算结果h<sub>i</sub>，进而<strong>影响了在x条件下应该接什么y的概率</strong>。</li></ul></li><li><strong>but fail for most pretrained LMs</strong>：虽然提示词从理论上可以影响大语言模型的输出，但论文进一步阐述了提示词的局限性——做过提示词工程的小伙伴应该知道，仅仅通过自然语言形式的提示词，对大语言模型输出的效果提升很有限。</li><li><strong>continuous word embeddings</strong>：论文根据上述头脑实验，准备新增1个Prefix模型，在人类输入x词嵌入为向量后，在这个词嵌入向量前增加1个<strong>Prefix Token(前缀向量)</strong>(这种向量不同于离散向量，不会引发计算的困难)。为了保证Prefix Token有足够的提示性，论文在Transformer的所有层都增加了这种前缀向量。</li></ul><h2 id=33methodparametrization-of-psubθsub原理>3.3.Method/Parametrization of P<sub>θ</sub>(原理)</h2><p>论文至此，正式阐述了<strong>Prefix Tuning</strong>的实现方法：</p><ul><li><strong>z=[x;y]到z=[prefix;x;y]</strong>：给自回归模型和Encoder-Decoder模型的z向量增加了前缀<strong>PREFIX</strong>向量。其中P<sub>idx</sub>表示了前缀的索引。(如：)</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329142053466.png alt=image-20240329142053466></p><ul><li>基于前缀向量，h<sub>i</sub>的数学公式表示为各层的激活包含前缀向量的计算和大语言模型预测的概率分布。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329143101780.png alt=image-20240329143101780></p><ul><li><p><strong>|P<sub>idx</sub>| × dim(h<sub>i</sub>)</strong>：P<sub>θ</sub>是Prefix Tuning新增的小模型的参数，参数的维数必然是有限的，因为它的维数等于前缀向量个数乘以h<sub>i</sub>的维度。这也解释了为什么Prefix Tuning新增的小模型参数规模仅占预训练模型的0.1%。</p></li><li><p><strong>P<sub>θ</sub>[i, :] = MLP<sub>θ</sub>(P ′<sub>θ</sub>[i, :])</strong>：论文还发现直接微调P<sub>θ</sub>会导致训练效果不好，于是采用了重参数化方法，引入了前馈神经网络MLP<sub>θ</sub>。</p></li></ul><h1 id=4experiments实验结果>4.Experiments(实验结果)</h1><h2 id=41实验结果>4.1.实验结果</h2><p>论文的实验结论：<strong>Prefix Tuning</strong>在Table-To-Text下游任务表现良好、在外推涌现方面表现良好，具体如下：</p><ul><li><strong>Table-To-Text实验</strong>：用GPT2-Medium和GPT2-Large对比FFT和Prefix Tuning，Prefix Tuning都达到了SOTA水平。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150136166.png alt=image-20240329150136166></p><ul><li><strong>Low-Data-Setting</strong>实验：在少数据测试中，Prefix Tuning的性能表现和训练稳定度优于FFT。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150354433.png alt=image-20240329150354433></p><ul><li><strong>Summarization实验</strong>：使用XSUM数据集，Prefix Tuning性能略低于FFT。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150524132.png alt=image-20240329150524132></p><ul><li><strong>Extrapolation实验</strong>：外推涌现实验中，使用XSUM数据集，Prefix Tuning效果优于FFT。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329150541755.png alt=image-20240329150541755></p><h2 id=42intrinsic-evaluation重要发现>4.2.Intrinsic Evaluation(重要发现)</h2><p>论文在前述实验结果下，有如下重要发现：</p><ul><li><strong>Prefix长度的影响</strong>：<ul><li>不同下游任务需要增加不同长度的前缀向量。</li><li>Prefix长度越长，提示效果越好，但过犹不及——超过了一定的阈值，就会出现推理性能下降。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151553754.png alt=image-20240329151553754></p><ul><li><strong>在哪些层做Prefix的影响</strong>：仅在词嵌入层做Prefix的效果远差于在Transformer各层做Prefx的效果。</li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329151830368.png alt=image-20240329151830368></p><ul><li><strong>Prefix和Infix的影响</strong>：用前缀法的性能效果优于中缀法。<ul><li>研究员猜测，前缀法可能影响x和y，中缀法可能只影响y。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152257513.png alt=image-20240329152257513></p><ul><li><strong>Prefix的初始值选择的影响</strong>：随机初始化Prefix向量的效果远差于与下游任务相关联的Prefix向量初始化的效果。<ul><li>这种现象可能由于用与下游任务不相关的提示词向量，会导致更长时间的前缀神经网络的收敛(甚至不收敛)。</li></ul></li></ul><p><img src=/AI%E6%8B%BE%E9%81%97/%E3%80%90chatGPT%E3%80%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B044-LLM%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8BPrefixTuning/image-20240329152826581.png alt=image-20240329152826581></p><h1 id=5总结>5.总结</h1><p>从上述论文解读中，我们收获了如下技术观点：</p><ul><li><strong>Prefix Tuning的价值</strong>：追求一套预训练模型，搞定多个下游任务。</li><li><strong>Prefix Tuning的核心思想</strong>：增加一个新的具备提示能力的前缀向量小模型，微调小模型的少量参数，冻结预训练模型的海量参数。</li><li><strong>Prefix Tuning的工程实践经验</strong>：<ul><li>Prefix长度不宜过长或过短，需根据下游任务实验获得。</li><li>对Transformer做全层的Prefix效果更好。</li><li>Prefix会影响x和y，效果优于Infix。</li><li>Prefix的初始值需选择与下游任务相关的提示向量。</li></ul></li></ul><p>论文链接：https://arxiv.org/pdf/2101.00190.pdf</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>猴王无敌</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2024-03-29</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/weixin.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/alipay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags></div><nav class=post-nav><a class=prev href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B045-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Bprompt-tuning/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">【chatGPT】学习笔记45-LLM微调技术之Prompt Tuning</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai%E6%8B%BE%E9%81%97/chatgpt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B043-llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B9%8Badapter-tuning/><span class="next-text nav-default">【chatGPT】学习笔记43-LLM微调技术之Adapter Tuning</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:JHercules_qz@qq.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408 1361.641813S1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523L83.726336 1024H682.532949 753.579947 1348.948139L1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955C777.248 802.205449 742.347691 811.03081 718.063616 811.603883z"/></svg></a><a href=https://github.com/JHerculesqz rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://jherculesqz.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2024
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>猴王无敌</span></span>
<span id=busuanzi_container>访客数/访问量：<span id=busuanzi_value_site_uv></span>/<span id=busuanzi_value_site_pv></span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/load-photoswipe.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script><script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></body></html>